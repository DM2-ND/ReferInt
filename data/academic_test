b	587	[1]	information networks are becoming ubiquitous across a large spectrum of real world applications in forms of social networks , citation networks , telecommunication networks and biological networks , etc . // the scale of these networks ranges from hundreds to millions or even billions of vertices . // analyzing information networks plays a crucial role in a variety of emerging applications across many disciplines .
h-	587	[5]	the idea is to find a low dimensional manifold structure hidden in the high dimensional data geometry reflected by the constructed graph , so that connected nodes are kept closer to each other in the new embedding space . // isomap , locally linear embedding ( lle ) and laplacian eigenmap are examples of algorithms based on this rationale . // however , graph embedding algorithms are designed on i .i .d . data mainly for dimensionality reduction purpose .
h+	587	[9]	since 2008 , significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks . // many nrl algorithms , e .g . , , , , , have been proposed to embed existing networks , showing promising performance for various applications . // these algorithms embed a network into a latent , lowdimensional space that preserves structure proximity and attribute affinity , such that the original vertices of the network can be represented as low dimensional vectors .
b	587	[13]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[16]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[19]	to learn informative vertex representations , network representation learning should preserve network structure , such that vertices similar/close to each other in the original structure space should also be represented similarly in the learned vector space . // however , as stated in , , the structure level similarity between vertices is reflected not only at the local neighborhood structure but also at the more global community structure . // therefore , the local and global structure should be simultaneously preserved in network representation learning .
b	587	[23]	the first is , which reviews a few representative methods for network representation learning and visits some key concepts aroundthe idea of representation learning and its connections to other related field such as dimensionality reduction , deep learning , and network science . // the other two surveys , focus on reviewing graph embedding techniques aiming to preserve only network structure . // recently , , extend to cover work leveraging other side information , such as vertex attributes and/or vertex labels , to harness representation learning .
b	587	[1]	definition 3 ( second order proximity and high order proximity ) . // the second order proximity captures the 2 step relations between each pair of vertices . // for each vertex pair ( vi , vj ) , the second order proximity is determined by the number of common neighbors shared by the two vertices , which can also be measured by the 2 step transition probability from vi to vj equivalently
ho	587	[7]	in addition to network structure , vertex attributes can provide direct evidence to measure contentlevel similarity between vertices . // as shown in , , , vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations . // vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations .
b	587	[7]	in general , there are three main types of information sources : network structure , vertex attributes , and vertex labels . // most of the unsupervised nrl algorithms focus on preserving network structure for learning vertex representations , and only a few algorithms ( e .g . , tadw , hsca ) attempt to leverage vertex attributes . // by contrast , under the semi supervised learning setting , half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations .
b	587	[32]	most of the unsupervised nrl algorithms focus on preserving network structure for learning vertex representations , and only a few algorithms ( e .g . , tadw , hsca ) attempt to leverage vertex attributes . // by contrast , under the semi supervised learning setting , half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations . on both settings , most of the algorithms focus on preserving microscopic structure , while very few algorithms ( e .g . , mnmf , dp , harp ) attempt to take advantage of the mesoscopic and macroscopic structure . // approaches to network representation learning in the above two different settings can be summarized into five categories from algorithmic perspectives .
h-	587	[34]	factorization strategies vary across different algorithms according to their objectives . // for example , in the modularity maximization method , eigen decomposition is performed on the modularity matrix to learn community indicative vertex representations ; in the tadw algorithm , inductive matrix factorization is carried out on the vertexcontext matrix to simultaneously preserve vertex textual features and network structure in the learning of vertex representations . // although matrix factorization based methods have been proved effective in learning informative vertex representations , the scalability is a major bottleneck because carrying out factorization on a matrix with millions of rows and columns is memory intensive and computationally expensive or , sometime , even infeasible .
b	587	[37]	deepwalk is the pioneer work in using random walks to learn vertex representations . // node2vec further exploits a biased random walk strategy to capture more global structure . // as the extensions of the structure preserving only version , algorithms like ddrw , gene and semne incorporate vertex labels with network structure to harness representation learning , ppne imports vertex attributes , and tri dnr enforces the model with both vertex labels and attributes .
b	587	[30]	node2vec further exploits a biased random walk strategy to capture more global structure . // as the extensions of the structure preserving only version , algorithms like ddrw , gene and semne incorporate vertex labels with network structure to harness representation learning , ppne imports vertex attributes , and tri dnr enforces the model with both vertex labels and attributes . // as these models can be trained in an online manner , they have great potential to scale up .
b	587	[28]	to learn the representations of linked documents , lde models the document document relationships by maximizing the conditional probability between connected documents . // prbm adapts the rbm model to linked data by making the hidden rbm representations of connected vertices similar to each other . // graphgan adopts generative adversarial nets ( gan ) to accurately model the vertex connectivity probability .
b	587	[46]	deep learning based methods . // to extract complex structure features and learn deep , highly nonlinear vertex representations , the deep learning techniques , are also applied to network representation learning . // for example , dngr applies the stacked denoising autoencoders ( sdae ) on the high dimensional matrix representations to learn deep low dimensional vertex representations .
h-	587	[19]	for example , dngr applies the stacked denoising autoencoders ( sdae ) on the high dimensional matrix representations to learn deep low dimensional vertex representations . // sdne uses a semi supervised deep autoencoder model to model non linearity in network structure . // deep learning based methods have theability to capture non linearity in networks , but their computational time cost is usually high .
b	587	[6]	hybrid methods . // some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network .
b	587	[1]	some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network . // we summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in table 3 .
b	587	[1]	large scale information network embedding ( line ) . // instead of exploiting random walks to capture network structure , line learns vertex representations by explicitly modeling the first order and second order proximity . // to preserve the first order proximity , line minimizes the following objective : o1 = d ( ˆp1 ( · , · ) , p1 ( · , · ) ) . ( 3 ) for each vertex pair vi and vj with ( vi , vj ) ∈ e , p1 ( · , · ) is the joint distribution modeled by their latent embeddings rvi and rvj . pˆ1 ( vi , vj ) is the empirical distribution between them . d ( · , · ) is the distance between two distributions .
b	587	[47]	to overcome the weakness of truncated random walks in exploiting vertex contextual information , i .e . , the difficulty in capturing correct contextual information for vertices at the boundary of sequences and the difficulty in determining the walk length and the number of walks , dngr utilizes the random surfing model to capture contextual relatedness between each pair of vertices and preserves them into |v | dimensional vertex representations x . // to extract complex features and model non linearities , dngr applies the stacked denoising autoencoders ( sdae ) to the high dimensional vertex representations x to learn deep low dimensional vertex representations . // structural deep network embedding ( sdne ) .
b	587	[49]	to preserve the asymmetric transitivity , hope learns two vertex embedding vectors u s , ut ∈ r |v |×d , which is called source and target embedding vectors , respectively . // after constructing the high order proximity matrix s from four proximity measures , i .e . , katz index , rooted pagerank , common neighbors and adamicadar . hope learns vertex embeddings by solving the following matrix factorization problem : min us , ut ks − u s · u tt k 2 f . // asymmetric proximity preserving graph embedding ( app ) .
b	587	[44]	graphgan . // graphgan learns vertex representations by modeling the connectivity behavior through an adversarial learning framework . // inspired by gan ( generative adversarial nets ) , graphgan works through two components : ( i ) generator g ( v|vc ) , which fits the distribution of the vertices connected to vc across v and generates the likely connected vertices , and ( ii ) discriminator d ( v , vc ) , which outputs a connecting probability for the vertex pair ( v , vc ) , to differentiate the vertex pairs generated by g ( v|vc ) from the ground truth .
b	587	[44]	the proximity preserved by microscopic structure preserving nrl algorithms is summarized in table 4 . // most algorithms in this category preserve the secondorder and high order proximity , whereas only line , sdne and graphgan consider the first order proximity . // from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure .
h-	587	[26]	from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure . // grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up . // line and graphgan directly model the connectivity behavior , while deep learning based methods ( dngr and sdne ) learn non linear vertex representations .
b	587	[9]	grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up . // line and graphgan directly model the connectivity behavior , while deep learning based methods ( dngr and sdne ) learn non linear vertex representations . // structural role proximity preserving nrl .
b	587	[53]	graphwave . // by making use of the spectral graph wavelet diffusion patterns , graphwave embeds vertex neighborhood structure into a low dimensional space and preserves the structural role proximity . // the assumption is that , if two vertices residing distantly in the network share similar structural roles , the graph wavelets starting at them will diffuse similarly across their neighbors .
b	587	[56]	thus , the problem boils down to one classical network analytic task—community detection—that aims to discover a set of communities with denser within group connections than between group connections . // three clustering techniques , including modularity maximization , spectral clustering and edge clustering are employed to discover latent social dimensions . each social dimension describes the likelihood of a vertex belonging to a plausible affiliation . //
h-	587	[56]	summary . // the algorithms of learning latent social dimensions , , only consider the community structure to learn vertex representation , while m nmf integrates microscopic structure preserving ( the secondorder and high order proximity ) with the intra community proximity . // these methods primarily rely on matrix factorization to detect community structure , which makes them hard to scale to large scale networks .
b	587	[6]	many real world networks present the macroscopic scale free property , which depicts the phenomenon that vertex degree follows a longtailed distribution , i .e . , most vertices are sparsely connected and only few vertices have dense edges . // to capture the scale free property , proposes the degree penalty principle ( dp ) : penalizing the proximity between high degree vertices . this principle is then coupled with two nrl algorithms ( i .e . , spectral embedding and deepwalk ) to learn scale free property preserving vertex representations . // hierarchical representation learning for networks ( harp ) .
b	587	[31]	in harp , deepwalk and line are used to learn vertex representations . // summary : dp and harp are both realized by adapting the existing nrl algorithms to capture the macroscopic structure . // the former tries to preserve the scale free property , while the latter makes the learned vertex representations respect the global network structure .
h-	587	[7]	homophily , structure , and content augmented network representation learning ( hsca ) . // despite its ability to incorporate textural features , tadw only considers structural context of network vertices , i .e . , the second order and high order proximity , but ignores the important homophily property ( the first order proximity ) in its learning framework . // hsca is proposed to simultaneously integrates homophily , structural context , and vertex content to learn effective network representations . for tadw , the learned representation for the i th vertex vi is wt i , ( hti : ) t t , where wi : and ti : is the i th row of w and t , respectively .
b	587	[57]	user profile preserving social network embedding ( upp sne ) . // upp sne leverages user profile features to enhance the embedding learning of users in social networks . compared with textural content features , user profiles have two unique properties : ( 1 ) user profiles are noise , sparse and incomplete and ( 2 ) different dimensions of user profile features are topic inconsistent . // to filter out noise and extract useful information from user profiles , upp sne constructs user representations by performing a non linear mapping on user profile features , which is guided by network structure .
b	587	[7]	the above unsupervised content augmented nrl algorithms incorporate vertex content features in three ways . // the first , used by tadw and hsca , is to couple the network structure with vertex content features via inductive matrix factorization . // this process can be considered as a linear transformation on vertex attributes constrained by network structure .
b	587	[58]	the second is to perform a non linear mapping to construct new vertex embeddings that respect network structure . // for example , rbm and the approximated kernel mapping is used by prbm and upp sne , respectively , to achieve this goal . // the third used by ppne is to add an attribute preserving constraint to the structure preserving optimization objective .
b	587	[59]	discriminative deep random walk ( ddrw ) . // inspired by the discriminative representation learning , , ddrw proposes to learn discriminative network representations through jointly optimizing the objective of deepwalk together with the following l2 loss support vector classification classification objective : lc = c x |v | i=1 ( σ ( 1 − yikβ t rvi ) ) 2 + 1 2 β t β , ( 30 ) where σ ( x ) = x , if x > 0 and otherwise σ ( x ) = 0 . //
b	587	[61]	discriminative deep random walk ( ddrw ) . // ddrw is generalized to handle multi class classification by using the one against rest strategy . // deepwalk together with the following l2 loss support vector classification classification objective .
b	587	[1]	transductive line ( tline ) . // along similar lines , tline is proposed as a semisupervised extension of line that simultaneously learns line ’ s vertex representations and an svm classifier . given a set of labeled vertices { v1 , v2 , · · · , vl } and { vl+1 , · · · , v|v | } , tline trains a multi class svm classifier on { v1 , v2 , · · · , vl } by optimizing the objective : osvm = xl i=1 xk k=1 max ( 0 , 1 − yikwk t rvi ) + λkwkk 2 2 . // based on line ’ s formulations that preserve the firstorder and second order proximity , tline optimizes two objective functions .
b	587	[65]	gene assumes that vertices should be embedded closely in lowdimensional space , if they share similar neighbors or join similar groups . // inspired by deepwalk and document modeling , , the mechanism of gene for learning group label informed vertex representations is achieved by maximizing the following log probability : l = x gi∈y  α x w∈wgi x vj∈w log pr ( vj |vj−t , · · · , vj+t , gi ) + β x vˆj∈wˆ gj log pr ( ˆvj |gi )    , ( 37 ) where y is the set of different groups , wgi is the set of random walk sequences labeled with gi , wˆ gi is the set of vertices randomly sampled from group gi . //
b	587	[42]	linked document embedding ( lde ) . // lde is proposed to learn representations for linked documents , which are actually the vertices of citation or webpage networks . // similar to tridnr , lde learns vertex representations by modeling three kinds of relations , i .e . , word word document relations , document document relations , and document label relations .
b	587	[38]	we now summarize and compare the discriminative learning strategies used by semi supervised nrl algorithms in table 5 in terms of their advantages and disadvantages . // three strategies are used to achieve discriminative learning . the first strategy ( i .e . , ddrw , mmdw , tline , dmf , semine ) is to enforce classification loss minimization on vertex representations , i .e . , fitting the vertex representations to a classifier . // this provides a direct way to separate vertices of different categories from each other in the new embedding space .
b	587	[40]	we now summarize and compare the discriminative learning strategies used by semi supervised nrl algorithms in table 5 in terms of their advantages and disadvantages . // three strategies are used to achieve discriminative learning . the first strategy ( i .e . , ddrw , mmdw , tline , dmf , semine ) is to enforce classification loss minimization on vertex representations , i .e . , fitting the vertex representations to a classifier . // this provides a direct way to separate vertices of different categories from each other in the new embedding space .
b	587	[66]	this provides a direct way to separate vertices of different categories from each other in the new embedding space . // the second strategy ( used by gene , tridnr , lde and planetoid ) is achieved by modeling vertex label relation , such that vertices with same labels have similar vector representations . // the third strategy used by lane is to jointly embed vertices and labels into a common space .
b	587	[11]	often , because network vertices are partially or sparsely labeled due to high labeling costs , a large portion of vertices in networks have unknown labels . // the problem of vertex classification aims to predict the labels of unlabeled vertices given a partially labeled network , . // since vertices are not independent but connected to each other in the form of a network via links , vertex classification should exploit these dependencies for jointly classifying the labels of vertices .
b	587	[6]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[8]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[63]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[48]	good network representations should be able to capture explicit and implicit connections between network vertices thus enabling application to link prediction . // and predict missing links based on the learned vertex representations on social networks . // also applies network representation learning to collaboration networks and protein protein interaction networks . they demonstrate that on these networks links predicted using the learned representations achieve better performance than traditional similarity based link prediction approaches .
b	587	[73]	researchers have proposed a large body of network clustering algorithms based on various metrics of similarity or strength of connection between vertices . // min max cut and normalized cut methods , seek to recursively partition a graph into two clusters that maximize the number of intra cluster connections and minimize the number of intercluster connections . // modularity based methods ( e .g . , , ) aim to maximize the modularity of a clustering , which is the fraction of intra cluster edges minus the expected fraction assuming the edges were randomly distributed . a network partitioning with high modularity would have dense intra cluster connections but sparse inter cluster connections .
b	587	[26]	some other methods ( e .g . , ) try to identify nodes with similar structural roles like bridges and outliers . // recent nrl methods ( e .g . , grarep , dngr , mnmf , and prbm ) used the clustering performance to evaluate the quality of the learned network representations on different networks . intuitively , better representations would lead to better clustering performance . // intuitively , better representations would lead to better clustering performance .
h+	587	[28]	these works followed the common approach that first applies an unsupervised nrl algorithm to learn vertex representations , and then performs k means clustering on the learned representations to cluster the vertices . // in particular , prbm showed that nrl methods outperforms the baseline that uses original features for clustering without learning representations . // this suggests that effective representation learning can improve the clustering performance .
b	587	[20]	by taking the learned vertex representations as input , line used the t sne package to visualize the dblp co author network after the authors are mapped into a 2 d space , and showed that line is able to cluster authors in the same field to the same community . // hsca illustrated the advantages of the content augmented nrl algorithm by visualizing the citations networks . // semi supervised algorithms ( e .g . , tline , tridnr , and dmf ) demonstrated that the visualization results have better clustering structures with vertex labels properly imported .
h-	587	[79]	for these types of social networks , poi recommendation intends to recommend user interested objects , depending on their own context , such as the geographic location of the users and their interests . // traditionally , this is solved by using approaches , such as collaborative filtering , to leverage spatial and temporal correlation between user activities and geographical distance . // however , because each user ’ s check in records are very sparse , finding similar users or calculating transition probability between users and locations is a significant challenge .
b	587	[81]	traditionally , knowledge graph search is carried out through database driven approaches to explore schema mapping between entities , including entity relationships . // recent advancement in network representation learning has inspired structured embeddings of knowledge bases . // such embedding methods intend to learn a low dimensional vector representation for knowledge graph entities , such that generic database queries , such as top k search , can be carried out by comparing vector representation of the query object and objects in the database .
b	587	[1]	the citation networks are directed information networks formed by author author citation relationships or paper paper citation relationships . they are collected from different databases of academic papers , such as dblp and citeseer . // among the commonly used citation networks , dblp ( authorcitation ) , cora , citeseer , pubmed and citeseer m10 are the binary paper citation networks , which are also attached with vertex text attributes as the content of papers . // compared with user profile features in social network , the vertex text features here are more topic centric , informative and can better complement network structure to learn effective vertex representations .
b	587	[86]	biological network . // as a typical biological network , the protein protein interaction network is a subgraph of the ppi network for homo sapiens . the vertex here represents a protein and the edge indicates that there is an interaction between proteins . the labels of vertices are obtained from the hallmark gene sets and represent biological states . // communication network .
b	587	[88]	to validate the effectiveness of nrl algorithms , vertex clustering is also carried out by applying k means clustering algorithm to the learned vertex representations . // communities in networks are served as the ground truth to assess the quality of clustering results , which is measured by accuracy and nmi ( normalized mutual information ) . // the hypothesis is that , if the learned vertex representations are indeed informative , vertex clustering on learned vertex representations should be able to discover community structures . that is , good vertex representations are expected to generate good clustering results .
b	587	[37]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
b	587	[57]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
ho	587	[64]	for each vertex context pair ( vi , vj ) , deepwalk and node2vec use two different strategies to approximate the probability pr ( vj |vi ) : hierarchical softmax , and negative sampling . // the better clustering performance of node2vec over deepwalk proves the advantage of negative sampling over hierarchical softmax , which is consistent with the word embedding results as reported in . // complexity analysis .
b	587	[48]	efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition . // future research directions .
b	587	[48]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
b	587	[97]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
ho	587	[6]	there is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results . // to better understand deepwalk , line , and node2vec , discovers the theoretical connections between them and graph laplacians . // however , in depth theoretical analysis about network representation learning is necessary , as it provides a deep understanding of algorithms and helps interpret empirical results .
b	587	[102]	the vertices/edges may also be described by some time varying information . // dynamic networks have unique characteristics that make static network embedding fail to work : ( i ) vertex content features may drift over time ; ( ii ) the addition of new vertices/edges requires learning or updating vertex representations to be efficient ; and ( iii ) network size is not fixed . the work on dynamic network embedding is rather limited ; the majority of existing approaches ( e .g . , , , ) assume that the node set is fixed and deal with the dynamics caused by the deletion/addition of edges only . // however , a more challenging problem is to predict the representations of new added vertices , which is referred to as “ out of sample ” problem .
b	587	[105]	however , a more challenging problem is to predict the representations of new added vertices , which is referred to as “ out of sample ” problem . // a few attempts such as , , are made to exploit inductive learning to address this issue . // they learn an explicit mapping function from a network at a snapshot , and use this function to infer the representations of out ofsample vertices , based on their available information such as attributes or neighborhood structure .
b	587	[16]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[112]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[116]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[117]	the existence of negative links makes the traditional homophily based network representation learning algorithms unable to be directly applied . // some studies , , tackles signed network representation learning through directly modeling the polar of links . // how to fully encode network structure and vertex attributes for signed network embedding remains an open question .
b	587	[103]	ane ( adversarial network embedding ) and arga ( adversarially regularized graph autoencoder ) learn robust vertex representations via enforcing an adversarial learning regularizer . // to deal with the uncertainty in the existence of edges , urge ( uncertain graph embedding ) encodes the edge existence probability into the vertex representation learning process . // it is of great importance to have more research efforts on enhancing the robustness of network representation learning .
ho	6	[70]	sparsity enables the design of efficient discrete algorithms , but can make it harder to generalize in statistical learning . // machine learning applications in networks ( such as network classification , content recommendation , anomaly detection , and missing link prediction ) must be able to deal with this sparsity in order to survive . // in this paper we introduce deep learning ( unsupervised feature learning ) techniques , which have proven successful in natural language processing , into network analysis for the first time . we develop an algorithm ( deepwalk ) that learns social representations of a graph ’ s vertices , by modeling a stream of short random walks .
h-	6	[127, 128]	in the relational classification problem , the links between feature vectors violate the traditional i .i .d . assumption . // techniques to address this problem typically use approximate inference techniques to leverage the dependency information to improve classification results . // we distance ourselves from these approaches by learning label independent representations of the graph .
h-	6	[129]	in the literature , this is known as the relational classification ( or the collective classification problem ) . // traditional approaches to relational classification pose the problem as an inference in an undirected markov network , and then use iterative approximate inference algorithms ( such as the iterative classification algorithm , gibbs sampling , or label relaxation ) to compute the posterior distribution of labels given the network structure . // we propose a different approach to capture the network topology information .
b	6	[132]	it is a stochastic process with random variables w1 vi , w2 vi , . . . , wk vi such that wk+1 vi is a vertex chosen at random from the neighbors of vertex vk . // random walks have been used as a similarity measure for a variety of problems in content recommendation and community detection . // they are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph .
b	6	[35]	skipgram . // skipgram is a language model that maximizes the cooccurrence probability among the words that appear within a window , w , in a sentence . // algorithm 2 iterates over all possible collocations in random walk that appear within the window w ( lines 1 2 ) .
b	6	[33]	an overview of the graphs we consider in our experiments is given in figure 1 . // blogcatalog is a network of social relationships provided by blogger authors . // the labels represent the topic categories provided by the authors .
b	6	[33]	utilizing the eigenvectors of le implicitly assumes that graph cuts will be useful for classification . // modularity : this method generates a representation in r d from the top d eigenvectors of b , the modularity matrix of g . the eigenvectors of b encode information about modular graph partitions of g . // using them as features assumes that modular graph partitions will be useful for classification .
h+	6	[138]	given the neighborhood ni of vertex vi , wvrn estimates pr ( yi|ni ) with the ( appropriately normalized ) weighted mean of its neighbors ( i .e pr ( yi|ni ) = 1 zpvj∈ni wij pr ( yj | nj ) ) . // it has shown surprisingly good performance in real networks , and has been advocated as a sensible relational classification baseline . // majority : this na¨ıve method simply chooses the most frequent labels in the training set .
h-	6	[139]	the main differences between our proposed method and previous work can be summarized as follows . // we learn our latent social representations , instead of computing statistics related to centrality or partitioning . // we do not attempt to extend the classification procedure itself ( through collective inference or graph kernels )
b	6	[129, 137, 127, 128]	relational learning . // relational classification ( or collective classification ) methods use links between data items as part of the classification process . // exact inference in the collective classification problem is np hard , and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge .
h-	6	[143]	exact inference in the collective classification problem is np hard , and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge . // the most relevant relational classification algorithms to our work incorporate community information by learning clusters , by adding edges between nearby nodes , by using pagerank , or by extending relational classification to take additional features into account . // our work takes a substantially different approach .
b	6	[146]	unsupervised feature learning . // distributed representations have been proposed to model structural relationship between concepts . // these representations are trained by the back propagation and gradient descent .
ho	6	[150]	recently , distributed computing allowed for larger models to be trained , and the growth of data for unsupervised learning algorithms to emerge . // distributed representations usually are trained through neural networks , these networks have made advancements in diverse fields such as computer vision , speech recognition , and natural language processing . // conclusions
h+	1	[70]	this paper studies the problem of embedding information networks into lowdimensional spaces , in which every vertex is represented as a low dimensional vector . // such a low dimensional embedding is very useful in a variety of applications such as visualization , node classification , link prediction , and recommendation . // various methods of graph embedding have been proposed in the machine learning literature ( e .g . , ) .
h-	1	[152]	most existing graph embedding algorithms do not scale for networks of this size . // for example , the time complexity of classical graph embedding algorithms such as mds , isomap , laplacian eigenmap are at least quadratic to the number of vertices , which is too expensive for networks with millions of nodes . // although a few very recent studies approach the embedding of large scale networks , these methods either use an indirect approach that is not designed for networks ( e .g . , ) or lack a clear objective function tailored for network embedding ( e .g . , ) .
h-	1	[6]	for example , the time complexity of classical graph embedding algorithms such as mds , isomap , laplacian eigenmap are at least quadratic to the number of vertices , which is too expensive for networks with millions of nodes . // although a few very recent studies approach the embedding of large scale networks , these methods either use an indirect approach that is not designed for networks ( e .g . , ) or lack a clear objective function tailored for network embedding ( e .g . , ) . // we anticipate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the embedding of millions of nodes .
b	1	[156]	the general notion of the second order proximity can be interpreted as nodes with shared neighbors being likely to be similar . such an intuition can be found in the theories of sociology and linguistics . // for example , “ the degree of overlap of two people ’ s friendship networks correlates with the strength of ties between them , ” in a social network ; and “ you shall know a word by the company it keeps ” ( firth , j . r . 1957:11 ) in text corpora . // indeed , people who share many common friends are likely to share the same interest and become friends , and words that are used together with many similar words are likely to have similar meanings .
b	1	[5]	related work . // our work is related to classical methods of graph embedding or dimension reduction in general , such as multidimensional scaling ( mds ) , isomap , lle and laplacian eigenmap . // these approaches typically first construct the affinity graph using the feature vectors of the data points , e .g . , the k nearest neighbor graph of data , and then embed the affinity graph into a low dimensional space .
ho	1	[70]	because of this importance , many existing graph embedding algorithms such as isomap , lle , laplacian eigenmap , and graph factorization have the objective to preserve the first order proximity // however , in a real world information network , the links observed are only a small proportion , with many others missing . // a pair of nodes on a missing link has a zero first order proximity , even though they are intrinsically very similar to each other .
b	1	[6]	flickr and youtube2 . // the flickr network is denser than the youtube network ( the same network as used in deepwalk ) . // ( 3 ) citation networks .
ho	1	[36]	the mini batch size of the stochastic gradient descent is set as 1 for all the methods . // similar to , the learning rate is set with the starting value ρ0 = 0 .025 and ρt = ρ0 ( 1−t/t ) , where t is the total number of mini batches or edge samples . // for fair comparisons , the dimensionality of the embeddings of the language network is set to 200 , as used in word embedding .
b	1	[35]	for deepwalk , di erent cuto thresholds are tried to convert the language network into a binary network , and the best performance is achieved when all the edges are kept in the network . // we also compare with the state of the art word embedding model skipgram , which learns the word embeddings directly from the original wikipedia pages and is also implicitly a matrix factorization approach . // the window size is set as 5 , the same as used for constructing the language network .
b	37	[162]	many important tasks in network analysis involve predictions over nodes and edges . // in a typical node classification task , we are interested in predicting the most probable labels of nodes in a network . // for example , in a social network , we might be interested in predicting interests of users , or in a protein protein interaction network we might be interested in predicting functional labels of proteins .
b	37	[124]	even if one discounts the tedious effort required for feature engineering , such features are usually designed for specific tasks and do not generalize across different prediction tasks . // an alternative approach is to learn feature representations by solving an optimization problem . // the challenge in feature learning is defining an objective function , which involves a trade off in balancing computational efficiency and predictive accuracy .
b	37	[71, 169, 170]	recent attempts in this direction propose efficient algorithms but rely on a rigid notion of a network neighborhood , which results in these approaches being largely insensitive to connectivity patterns unique to networks . // specifically , nodes in networks could be organized based on communities they belong to ( i .e . , homophily ) ; in other cases , the organization could be based on the structural roles of nodes in the network ( i .e . , structural equivalence ) . // for instance , in figure 1 , we observe nodes u and s1 belonging to the same tightly knit community of nodes , while the nodes u and s6 in the two distinct communities share the same structural role of a hub node .
h-	37	[139, 171]	feature engineering has been extensively studied by the machine learning community under various headings . // in networks , the conventional paradigm for generating features for nodes is based on feature extraction techniques which typically involve some seed hand crafted features based on network properties . // in contrast , our goal is to automate the whole process by casting feature extraction as a representation learning problem in which case we do not require any hand engineered features .
b	37	[36]	it scans over the words of a document , and for every word it aims to embed it such that the word ’ s features can predict nearby words ( i .e . , words inside some context window ) . // the word feature representations are learned by optmizing the likelihood objective using sgd with negative sampling . // the skip gram objective is based on the distributional hypothesis which states that words in similar contexts tend to have similar meanings .
h-	37	[173, 174, 175, 176, 177]	our algorithm node2vec overcomes this limitation by designing a flexible objective that is not tied to a particular sampling strategy and provides parameters to tune the explored search space ( see section 3 ) . // finally , for both node and edge based prediction tasks , there is a body of recent work for supervised feature learning based on existing and novel graph specific deep network architectures . // these architectures directly minimize the loss function for a downstream prediction task using several layers of non linear transformations which results in high accuracy , but at the cost of scalability due to high training time requirements .
b	37	[71, 170]	in particular , prediction tasks on nodes in networks often shuttle between two kinds of similarities : homophily and structural equivalence . // under the homophily hypothesis nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together ( e .g . , nodes s1 and u in figure 1 belong to the same network community ) . // in contrast , under the structural equivalence hypothesis nodes that have similar structural roles in networks should be embedded closely together ( e .g . , nodes u and s6 in figure 1 act as hubs of their corresponding communities ) .
b	37	[6]	spectral clustering : this is a matrix factorization approach in which we take the top d eigenvectors of the normalized laplacian matrix of graph g as the feature vector representations for nodes . // deepwalk : this approach learns d dimensional feature representations by simulating uniform random walks . the sampling strategy in deepwalk can be seen as a special case of node2vec with p = 1 and q = 1 . // line : this approach learns d dimensional feature representations in two separate phases .
ho	37	[36]	deepwalk uses hierarchical sampling to approximate the softmax probabilities with an objective similar to the one use by node2vec . // however , hierarchical softmax is inefficient when compared with negative sampling . // hence , keeping everything else the same , we switch to negative sampling in deepwalk which is also the de facto approximation in node2vec and line .
b	37	[182]	the network has 3 ,890 nodes , 76 ,584 edges , and 50 different labels . // wikipedia : this is a cooccurrence network of words appearing in the first million bytes of the wikipedia dump . // the labels represent the part of speech ( pos ) tags inferred using the stanford pos tagger .
b	37	[184]	the network has 19 ,706 nodes and 390 ,633 edges . // arxiv astro ph : this is a collaboration network generated from papers submitted to the e print arxiv where nodes represent scientists , and an edge is present between two scientists if they have collaborated in a paper . // the network has 18 ,722 nodes and 198 ,110 edges .
ho	417	[186]	// behaviors are the products of the interaction between multiple types of contexts . // the multi type contexts often include operators , goals , resources , spatiotemporal and social dimensions . consider paper publishing behavior as an example : it has authors , target conference/journal , datasets , problems , methods , references , and so on .
b	417	[191, 192, 193]	contextual behavior modeling . // there has been a wide line of research on learning latent representations of context items in behavior data towards various applications . // agarwal et al . proposed localized factor models combining multi context information to improve predictive accuracy in recommender systems .
b	417	[187]	besides factor models , tensor decompositions have been widely used for modeling multi contextual data . // jiang et al . proposed a tensor sequence decomposition approach for discovering multifaceted behavioral patterns . // ermiş et al . studied various alternative tensor models for link prediction in heterogeneous data .
h-	417	[198]	yang et al . developed a predictive task guided tensor decomposition model for representation learning from electronic health records . // perros et al . designed a scalable parafac2 tensor model for large and sparse datasets . // however , the computational cost of factorizing a large scale matrix or tensor is usually very expensive , and none of the existing behavior modeling methods can e ciently learn item representations to optimize success rate on massive behavior data .
b	417	[1]	node2vec presented biased random walkers to diversify the neighborhood . // line provided clear objectives for homogeneous network embedding that articulates what network properties are preserved . // we have spotted a series of heterogeneous network embedding work that capture heterogeneous structural properties in network data .
b	417	[203]	chen et al . proposed a task guided and path augmented heterogeneous network embedding model to identify author names given anonymized paper ’ s title . // the most recent work pne decomposes a partially labeled network into two bipartite networks and encodes the node label information by learning label and context vectors from the label context network . // in our case , we only have the success rate for behaviors , which can be binary or real values ; and , we aims at learning the item representations preserving behavior success .
b	417	[205]	le et al . extended the embedded objects from words or phrases to paragraphs . // recently , nichel et al . proposed poincaré embedding based on a non euclidean space to preserve hierarchical semantic structures . // our work focuses on representation learning from behavior data .
h-	417	[1]	in this way , the contributions of each context item towards behavior ’ s success are preserved . // unlike network embedding models such as deepwalk , line and node2vec that preserve proximities and were evaluated on clustering tasks , our behavior data embedding model , also a multi type itemset embedding model , preserves success property . // we will evaluate it on the two tasks of behavior modeling we have introduced in section 1 and compete with existing works in experiments .
b	417	[6]	line : this homogeneous network embedding method preserves the 1st and 2nd order of node proximity . // deepwalk : it uses local information obtained from truncated random walks to learn latent representations of vertices in a network . // node2vec : it learns continuous feature representations for nodes in networks by maximizing the likelihood of preserving network neighborhoods of nodes .
ho	417	[208]	the best variant of our itemset embedding method learnsuc holds ( 1 ) type distribution constrained negative behavior sampling strategy and ( 2 ) type weights as { 3 ,1 ,1 ,1 } ( authors tend to have higher weights ) . it gives an mae of 0 .0432 ( 23 .8 % relatively ) and an rmse of 0 .1243 ( 24 .6 % relatively ) . // the traditional dimensionality reduction technique pca is able to capture 28 .4 % of total varianc . // its performance is not signicantly di erent from the performance of line
h-	417	[1]	comparing network embedding methods . // first , deepwalk and node2vec perform better than line in this task . it shows preserving random walk based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration . // second , metapath2vec learns the low dimensional representations of nodes from rich meta path based features
ho	52	[212, 213]	in almost all networks , nodes tend to have one or more functions that greatly determine their role in the system . // for example , individuals in a social network have a social role or social position , while proteins in a protein protein interaction ( ppi ) network exert specific functions . // intuitively , different nodes in such networks may perform similar functions , such as interns in the social network of a corporation or catalysts in the ppi network of a cell .
b	52	[217, 215, 218]	in the former , a distance function that leverages the neighborhood of the nodes is used to measure the distance between all node pairs , with clustering or matching then performed to place nodes into equivalent classes . // in the later , a recursion with respect to neighboring nodes is constructed and then iteratively unfolded until convergence , with final values used to determine the equivalent classes . // while such approaches have advantages and disadvantages , we provide an alternative methodology , one based on unsupervised learning of representations for the structural identity of nodes ( to be presented ) .
h-	52	[6]	such context can be leveraged by language models to learn latent representation for the nodes . // we implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks , comparing its performance with deepwalk and node2vec – two state of the art techniques for learning latent representations for nodes , and with rolx – a recent approach to identify roles of nodes . // our results indicate that while deepwalk and node2vec fail to capture the notion of structural identity , struc2vec excels on this task – even when the original network is subject to strong random noise ( random edge removal ) .
h+	52	[35, 36]	in natural language processing , generating dense embeddings for sparse data has a long history . // recently , skip gram was proposed as an effcient technique to learn embeddings for text data ( e .g . , sentences ) . // among other properties , the learned language model places semantically similar words near each other in space .
ho	52	[220]	additionally , subgraph2vec captures structural equivalence by embedding nodes with the same local structure to the same point in space . // nonetheless , the notion of structural equivalence is very rigid since it is defined as a binary property dictated by the weisfeiler lehman isomorphism test . // thus , two nodes that are structurally very similar ( but fail the test ) and have non overlapping neighbors may not be close in space .
b	52	[221]	karate network . // the zachary ’ s karate club is a network composed of 34 nodes and 78 edges , where each node represents a club member and edges denote if two members have interacted outside the club . // in this network , edges are commonly interpreted as indications of friendship between members .
h+	111	[6]	in particular , by mapping the way that people choose friends and maintain connections as a “ social language , ” recent advances in natural language processing ( nlp ) can be naturally applied to network representation learning , most notably the group of nlp models known as word2vec . // a number of recent research publications have proposed word2vec based network representation learning frameworks , such as deepwalk , line , and node2vec . // instead of handcrafted network feature design , these representation learning methods enable the automatic discovery of useful and meaningful ( latent ) features from the “ raw networks . ”
h+	111	[224]	can we directly apply homogeneous network oriented embedding architectures ( e .g . , skip gram ) to heterogeneous networks ? // by solving these challenges , the latent heterogeneous network embeddings can be further applied to various network mining tasks , such as node classification , clustering , and similarity search . // in contrast to conventional meta path based methods , the advantage of latent space representation learning lies in its ability to model similarities between nodes without connected meta paths .
b	111	[224]	in contrast to conventional meta path based methods , the advantage of latent space representation learning lies in its ability to model similarities between nodes without connected meta paths . // for example , if authors have never published papersin the same venue—imagine one publishes 10 papers all in nips and the other has 10 publications all in icml ; their “ apcpa ” based pathsim similarity would be zero—this will be naturally overcome by network representation learning . // contributions .
h-	111	[201]	specically , conventional models suer from the identical treatment of dierent types of nodes and relations , leading to the production of indistinguishable representations for heterogeneous nodes—as evident through our evaluation . // further , the metapath2vec and metapath2vec++ models also differ from the predictive text embedding ( pte ) model in several ways . // first , pte is a semi supervised learning model that incorporates label information for text data .
ho	111	[228]	problem definition . // we formalize the representation learning problem in heterogeneous networks , which was first briefly introduced in . // in specific , we leverage the definition of heterogeneous networks in and present the learning problem with its inputs and outputs .
b	111	[6]	given a text corpus , mikolov et al . proposed word2vec to learn the distributed representations of words in a corpus . // inspired by it , deepwalk and node2vec aim to map the word context concept in a text corpus into a network . // both methods leverage random walks to achieve this and utilize the skip gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network .
ho	111	[37]	meta path based random walks . how to effectively transform the structure of a network into skip gram ? // in deepwalk and node2vec , this is achieved by incorporating the node paths traversed by random walkers over a network into the neighborhood function . // naturally , we can put random walkers in a heterogeneous network to generate paths of multiple types of nodes .
ho	111	[37]	deepwalk / node2vec : with the same random walk path input ( p=1 & q=1 in node2vec ) , we find that the choice between hierarchical softmax ( deepwalk ) and negative sampling ( node2vec ) techniques does not yield significant differences . // therefore we use p=1 and q=1 in node2vec for comparison . // line : we use the advanced version of line by considering both the 1st and 2nd order of node proximity .
h-	111	[154]	pte : we construct three bipartite heterogeneous networks ( author–author , author–venue , venue–venue ) and restrain it as an unsupervised embedding method . // spectral clustering / graph factorization : with the same treatment to these methods in node2vec , we exclude them from our comparison , as previous studies have demonstrated that they are outperformed by deepwalk and line . // for all embedding methods , we use the same parameters listed below .
ho	111	[224]	our empirical results also show that this simple meta path scheme “ apvpa ” can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks , suggesting its applicability to potential applications for academic search services . // we evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks , including multi class node classification , node clustering , and similarity search . // in addition , we also use the embedding projector in tensorflow to visualize the node embeddings learned from the heterogeneous academic networks .
b	111	[33]	related work . // network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks , such as the application of factorization models for recommendation systems , node classification , relational mining , and role discovery . // this rich line of research focuses on factorizing the matrix/tensor format ( e .g . , the adjacency matrix ) of a network , generating latent dimension features for nodes or edges in this network .
b	111	[35, 36]	with the advent of deep learning techniques , signicant effort has been devoted to designing neural network based representation learning models . // for example , mikolov et al . proposed the word2vec framework—a two layer neural network—to learn the distributed representations of words in natural language . // building on word2vec , perozzi et al . suggested that the “ context ” of a node can be denoted by their co occurrence in a random walk path .
b	111	[1]	in addition , several other methods have been proposed for learning representations in networks . // in particular , to learn network embeddings , tang et al . decomposed a node ’ s context into first order ( friends ) and second order ( friends ’ friends ) proximity , which was further developed into a semi supervised model pte for embedding text data . // conclusion .
b	26	[6]	recently , there has been a surge of interest in learning graph representations from data . // for example , deepwalk , one recent model , transforms a graph structure into a sample collection of linear sequences consisting of vertices using uniform sampling ( which is also called truncated random walk ) . // the skip gram model , originally designed for learning word representations from linear sequences , can also be used to learn the representations of vertices from such samples .
h+	26	[167]	models like skipgram are proposed , which provide an efficient approach to learning word representations . // while these methods may yield good performances on some tasks , they can poorly capture useful information since they use separate local context windows , instead of global co occurrence counts . // on the other hand , the family of matrix factorization methods can utilize global statistics .
b	26	[235]	lund et al . put forward hyperspace analogue to language ( hal ) , factorizing a word word co occurrence counts matrix to generate word representations . // levy et al . presented matrix factorization over shifted positive pointwise mutual information ( pmi ) matrix for learning word representations and showed that the skip gram model with negative sampling ( sgns ) can be regarded as a model that implicitly such a matrix . // graph representation approaches
b	26	[4]	graph representation approaches . // there exist several classical approaches to learning low dimensional graph representations , such as multidimensional scaling ( mds ) , isomap , lle , and laplacian eigenmaps . // recently , tang et al . presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification .
b	26	[6]	hmed et al . proposed a graph factorization method , which used stochastic gradient descent to optimize matrices from large graphs . // perozzi et al . presented an approach , which transformed graph structure into several linear vertex sequences by using a truncated random walk algorithm and generated vertex representations by using skip gram model . // this is considered as an equally weighted linear combination of k step information .
ho	26	[160]	motivated by the skip gram model by mikolov et al . , we employ noise contrastive estimation ( nce ) , which is proposed by gutmann et al . , to define our objective function . // following , we define e = ~w · ~c , and setting ∂lk ∂e = 0 . this yields the following : ~w · ~c = log a k p w , c w0 ak w0 , c ! − log ( β ) where β = λ/n . // here pk ( c|w ) describes the k step relationship between w and c ( the k step transition probability from w to c ) , σ ( · ) is sigmoid function defined as σ ( x ) = ( 1 + e−x ) −1 , λ is a hyper parameter indicating the number of negative samples , and pk ( v ) is the distribution over the vertices in the graph .
h-	26	[240]	note that here we are essentially finding a projection from the row space of xk to the row space of wk with a lower rank . // thus alternative approaches other than the popular svd can also be exploited . examples include incremental svd , independent component analysis ( ica ) , and deep neural networks . // our focus in this work is on the novel model for learning graph representations , so we do not pursue any alternative methods .
b	26	[1]	baseline algorithms . // line . line is a recently proposed method for learning graph representations on large scale information networks . // line defines a loss function based on 1 step and 2 step relational information between vertices .
ho	26	[6]	we also concatenate both 1 step and 2 step relational information to form the representations and employ the reconstruction strategy for vertices with small degrees to achieve the optimal performance . // as mentioned in , for deepwalk and e sgns , we set window size as 10 , walk length as 40 , walks per vertex as 80 . // according to , line yielded better results when the learned graph representations are l2 normalized , while deepwalk and e sgns can achieve optimal performance without normalization .
b	48	[49, 244]	in undirected graphs , if there is an edge between vertices u and w , and one between w and v , then it is likely that u and v are connected by an edge . // transitivity plays a key role in graph inference and analysis tasks , such as calculating similarities between nodes and measuring the importance of nodes . // transitivity is symmetric in undirected graphs .
b	48	[247, 4, 3, 5, 248]	graph embedding . // graph embedding technology has been widely studied in the fields of dimensionality reduction , natural language processing , network analysis and so on . // for dimensionality reduction , adjacency matrices of graphs are constructed from the feature similarity ( distance ) between samples . and the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space .
b	48	[5]	and the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space . // for example , laplacian eigenmaps ( le ) aims to learn the low dimensional representation to expand the manifold where data lie . // locality preserving projections ( lpp ) is a linearization variant of le which learns a linear projection from feature space to embedding space .
h-	48	[250]	locality preserving projections ( lpp ) is a linearization variant of le which learns a linear projection from feature space to embedding space . // besides , there are many other graph embedding algorithms for dimensionality reduction , including non linear , linear , kernlized and tensorized algorithms . all of these algorithms are based on undirected graphs derived from symmetric similarities . // thus , they can not preserve asymmetric transitivity .
b	48	[160]	in the field of natural language processing , the graph of words is often used to learn the representation of words . // mikolov et . al . propose to ultilize the context of words to learn representation , which has been proved equivalent to factorizing word context matrix . // pennington et . al . exploit a word word co occurrance matrix .
b	48	[10]	handcock et . al . propose to apply the latent space approach to clustering in graph . // and zhu et . al . propose to address the classification problem in graph with graph embedding model . // while early graph embedding works focus on modeling the observed first order relationship ( i .e . edges in graph ) between vertexes , some recent works try to model the directed higher order relationships between vertexes in sparse graphs .
b	48	[255]	holland et . al . propose the p1 distribution model to capture the structural properties in directed graph , including the atrractiveness and expansiveness of vertexes and the reciprocation of edges . // besides these properties , wang et . al . take the group information of vertexes into consideration . // recently , some works adopt graph embedding to model directed graphs .
h-	48	[259]	chen et . al . learn the embedding vectors in euclidean space with locality property preserved . // perrault joncas et . al . and mousazadeh et . al . learn the embedding vectors based on laplacian type operators and preserve the asymmetry property of edges in a vector field . however , all of these methods can not preserve asymmetry property in embedding vector space . // high order proximity preserved embedding
b	48	[263]	the vertexes are academic papers and the directed edges are the citation relationship between papers . // twitter social network2 ( sn twitter ) : this dataset is a subnetwork of twitter . // the vertexes are users of twitter , and the directed edges are following relationships between users .
b	586	[109, 19]	networks are ubiquitous in our daily lives and many real life applications focus on mining information from the networks . // a fundamental problem in network mining is how to learn the desirable network representations . // to address this problem , network embedding is presented to learn the distributed representations of nodes in the network .
b	586	[6]	representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne . // the basic assumption of these topology driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space .
h-	586	[7]	since the node properties potentially encode diﬀerent typesof information from the network topology , integrating them into the embeddingprocess is expected to achieve a better performance . // as the first attempt , tadw incorporates the text features of nodes into network embedding process under a framework of matrix factorization . // however , there are two limitations of tadw . firstly , the very time andmemory consuming matrixfactorization processof tadwmakes it infeasible to scale up to large networks . secondly , tadw only considersthe texts associated to each node , and it is diﬃcult to apply tadw to handle thenode properties of rich types in general .
b	586	[36]	recently neural network based models are introduced to solve the networkembedding problem . // as the first attempt , deepwalk introduces an word embedding algorithm ( skip gram ) to learn the representation vectors of nodes . // tang et al . propose line , which optimizes a carefully designed objec tive function that preserves both the local and global structure .
h+	586	[7]	node2vec learns a mapping of nodes to a low dimensional space of features that maximizes the likelihood of preserving distances between network neighborhoods of nodes . // tadw incorporates the text features of nodes into network embedding process under a framework of matrix factorization . // compared to the matrix factorization based methods , neural network based methods are easier to generalize and own strong representation ability .
ho	586	[268]	in this subsection we present the details of the property derived objective func tion . // in natural language processing area , swe and rc net incorporate the semantic knowledges into the word embedding process . // inspired by the above works , we propose two ways to extract constraints from the property similarity matrix p .
b	586	[184]	in order to thoroughly evaluate the proposed methods , we conductexperiments on four paper citation networks and one social network with diﬀer ent scale of nodes . // the four paper citation networks are citeseer2 , cora ( see footnote 2 ) , pubmed ( see footnote 2 ) and dblp3 . // in the paper citation networks , nodes refer to papers and links refer to the citation relationships among papers .
ho	586	[208]	in line method , the parameters are set as follows : negative =5andsamples = 10 million . // in property features method , we reduce the dimension of node property features to 160 via svd algorithm . // in tadw method the parameters are set to the same as given in the original paper .
h+	586	[70, 273]	area under curve ( auc ) is used to evaluate the consistency between the labels and the simi larity scores of the samples . // we also choose common neighbors as a baseline method because it has been proved as an effective method . // table 3 shows the experimental results .
b	29	[279]	in social science , social influence theories have been studied that attributes of individuals can both reflect and affect their community structures . // in addition , a number of data mining applications , such as sentiment analysis and trust prediction , have been benefited by exploiting the correlations between geometrical structure and node attributes . // network embedding , as an efficient computational tool for graph mining , aims at mapping the topological proximities of all nodes in a network into a continuous low dimensional matrix representation .
b	29	[13, 282]	network embedding , as an efficient computational tool for graph mining , aims at mapping the topological proximities of all nodes in a network into a continuous low dimensional matrix representation . // the learned embedding representation paves the way for numerous applications such as node classification , link prediction , and network visualization . // while this has been extensively studied , research on attributed network embedding ( ane ) is still in its early stage . in contrast to network embedding that learns from pure networks , ane targets at leveraging both network proximity and node attribute affinity .
ho	29	[285]	there are two main challenges . first , the attributed network and label information could be sparse , incomplete and noisy . // for instance , in social networks , the number of single user ’ s friends is always exceedingly limited compared with the total number of users . // the proportion of active users who have specified their labels might also be quite small .
b	29	[6]	the concatenating original feature space is used for both training and test group . // deepwalk : it employs truncated random walks on the plain graph and involves language modeling techniques , i .e . , word2vec , to analyze the walking tracks . // line : it is one of the state of the art embedding algorithms for large scale networks . it preserves both first and second order proximities between the nodes .
ho	29	[6, 1]	experimental settings . // following a commonly adopted way , we validate the effectiveness of different learned representations on node classification task . // this task is to predict which category or categories a new node belongs to based on the model learned from training data .
b	29	[3]	a family of more general graph embedding approaches were developed around the 2000s . // they target at generating lowdimensional manifolds which can model the nonlinear geometry of data , including isomap , laplacian eigenmaps and spectral techniques . // up till now , due to the pervasiveness of networked data , a variety of network embedding algorithms have been implemented .
b	29	[294]	up till now , due to the pervasiveness of networked data , a variety of network embedding algorithms have been implemented . // iwata et al . applied probabilistic latent semantic analysis to embed document networks . // tang et al . investigated the advantage of employing temporal information to analyze dynamic multi mode networks .
b	29	[154]	mei et al . designed a harmonic regularization based embedding framework to tackle the problem of topic modeling with network structure . // ahmed et al . proposed an asynchronous distributed matrix factorization algorithm for large scale graphs . // bourigault et al . projected the observed temporal dynamic into a latent space to better model the information diffusion in networks .
b	29	[66]	to embed heterogeneous networks , jacob et al . extended the transductive models and deep learning techniques into the problem . // yang et al . exploited a probabilistic model to conduct network embedding in a semisupervised manner . // most recently , several deep learning based embedding algorithms were proposed to further enhance the performance of learned representations . attributed network analysis is put forward due to the fact that numerous networks are often associated with abundant content describing attributes of each node .
h+	29	[300]	therefore , algorithms exploiting them together could improve the overall learning performance . // for instance , tsur and rappoport advanced the prediction of spread of ideas by analyzing both social graph topology and content . // in order to tackle the complex data structures , several efforts have been devoted to jointly embedding the two information sources into a unified space .
b	29	[301]	they achieved this via a joint probabilistic model . // li et al . exploited the possibility of jointly learning latent factors in high dimensional content data and link information via a streaming feature selection framework . // chang et al . transformed content into another network and exploited a nonlinear multi layered embedding model to learn the complex interactions between the constructed content network and original network .
b	29	[304]	lou et al . applied a two side multimodal neural network to embed words based on multiple data sources . // a more detailed review of multi view learning can be referred to . // the main differences between our work and multi view learning are the facts that an attributed network can be seen as one specially constructed da ta source , and ane itself is a challenging problem .
h+	42	[36, 35]	many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
b	42	[306]	many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
b	42	[10, 109]	hence , the i .i .d . assumption of documents does not hold . // additional link information of such documents has been shown to be useful in various text mining tasks such as document classification , document clustering and feature selection . // therefore , we propose to study the novel problem of linked document embedding following the distributional hypothesis .
h+	42	[307]	however , they may not be optimal for some specialized tasks where label information is available such as y2 for d2 and y5 for d5 in figure 1 ( a ) . // for example , deep learning algorithms such as convolutional neural networks , which use label information , often outperform text embeddings for classification tasks . // hence , in this paper we study the novel problem of linked document embedding for classification and investigate two specific problems : ( 1 ) how to capture link and label information mathematically ; and ( 2 ) how to exploit them for document embedding .
b	42	[64]	document representation . // document representation is an important research area that receives great attention lately and can benefit many machine learning and data mining tasks such as document classification , information retrieval and sentiment analysis . // many different types of models have been proposed for document representation .
h+	42	[36, 35]	it is a generative model that assumes that each document has topic distribution and each word in the document is drawn from a topic with probability . // recently , mikolov et al . proposed the distributed representations of words , skip gram and cbow , which learn the embeddings of words by utilizing word cooccurrence in the local context . // it has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy , parsing , pos tagging , and sentiment analysis .
h+	42	[307]	recently , mikolov et al . proposed the distributed representations of words , skip gram and cbow , which learn the embeddings of words by utilizing word cooccurrence in the local context . // it has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // it is also scalable and can handle millions of documents . based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document .
b	42	[201]	document embedding has also been proven to be powerful in many tasks such as sentiment analysis , machine translation and information retrieve . // recently , predictive text embedding algorithm ( pte ) is proposed in , which also utilizes label information to learn predictive text embeddings . // the proposed framework lde is inherently different from pte : ( 1 ) lde is developed for linked documents while pte still assumes documents to be i .i .d . ; ( 2 ) lde captures label information via modeling document label information while pte uses label information via word label information ; ( 3 ) in addition to label information , lde also models link information among documents to learn document embeddings ; and ( 4 ) the proposed formulations and optimization problems of lde are also different from those of pte .
b	42	[319, 320, 7]	link information has been proven to be very effective for machine learning and data mining such as feature selection , recommender systems , and document classification/clustering . // based on the idea that two linked documents are likely to share similar topics , several works have been proposed to utilize link information for better document representations . // for example , rtm extends lda by considering link information for topic modeling ; pmtlm combines topic modeling with a variant of mixed membership block model to model linked documents and tadw learns linked document representations based on matrix factorization .
h-	42	[321]	graph based classification . // graph based classification is to utilize the link information to design classifier for classification . various graphbased classification algorithms are proposed . label propagation is a classical graph based methods , which performs classification by propagating label information from labeled data to unlabeled data through the graph . // however , label propagation doesn ’ t utilize the features of documents .
b	42	[36]	the proposed method lde is inherently different form the existing graph based classification . // first , lde learns both word embedding and document embedding , which can be used for other tasks , such as word analogy and visualization ; while existing graph based classification methods don ’ t learn word/document representation . // second , lde utilizes the distributional hypothesis idea and considers word word document relations , while existing graph based classification methods usually use bow without considering the word word relationship .
h+	42	[36]	rtm : relational topic model is an extension of topic modeling that models document content and links between documents . // skip gram : one of the state of the art word embedding model and its training objective is to find word representations that are useful for predicting the surrounding words of a selected word in a sentence . after obtaining word embeddings by skip gram , we use eq . ( 21 ) to get document representations . // cbow : another state of the art word embedding model . unlike skip gram , the training objective of cbow is to find word representations that are useful for predicting the center word by its neighbors .
b	42	[64]	pv dm : the distributed memory version of paragraph vector which considers the order of the words . it aims at learning document embeddings that are good at predicting the next given context . // pv dbow . // unlike pv dm , the word order is ignored in pv dbow . it aims to learn document representations that are good at predicting words in the document .
b	42	[7]	cnn : convolution neural network for classification . it uses word embeddings as input to train convolution neural network with label information3 . // tadw : text associated deepwalk is a matrix factorization based method that utilizes both link and document data4 . // pte : predictive text embedding which considers label information to learn word embedding but can not handle link information among documents .
ho	42	[201]	for skip gram , cbow , pv dm , pv dbow and lde , following the parameter setting suggestions in , we set the window size to be 7 and the number of negative samples also to be 7 . // we follow the setting in for pte and we use the default setting in the code of tadw . // for the proposed model , we choose γ to be 0 .0001 .
b	28	[329]	// representation learning , which aims at learning low dimensional semantic representations of high dimensional data , has proven to facilitate many machine learning and data mining tasks such as classification , clustering and information retrieval . // in terms of the label availability , representation learning methods can be broadly classified into supervised and unsupervised methods .
h+	28	[331, 332]	restricted boltzmann machine ( rbm ) is one of the most widely used unsupervised representation learning methods . // rbm is very powerful in learning meaningful nonlinear latent features , which has powered many applications such as collaborative filtering , link prediction , document representation and social behavior prediction . // in recent years , linked data has become pervasively available in various domains .
b	28	[314]	in recent years , linked data has become pervasively available in various domains . // for example , social media data is inherently linked via social context , web data is networked via hyperlinks , and biological data is embedded in correlation or interaction networks . // link information can be represented as a network as shown in figure 1 ( a ) , where nodes are data instances .
h+	28	[279]	in addition , linked data provides link information as demonstrated in figure 1 ( c ) . // linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection , sentiment analysis , topic modeling and document classification . // therefore , it has potential to advance rbms for better representation learning .
b	28	[337]	it is a popular and effective linear feature learning algorithm . // dae : denoising autoencoder is a variant of autoencoder that is to learn a feature representation that is able to reconstruct the input data . // specifically , dae is trained to reconstruct a clean “ repaired ” input from a corrupted version , which makes it able to extract more robust features .
b	28	[7]	the learned topic distributions of documents are treated as the representations . // tade : text associated deepwalk incorporates both attribute and link information into the matrix factorization framework to learn representations of each nodes . // it is state of the art representation learning algorithm for network with rich attributes .
h+	28	[341]	however , cd learning has a problem that it provides biased estimates of the gradient . // the persistent contrastive divergence ( pcd ) addressed this problem . instead of running a new chain for each parameter , pcd maintains a single persistent chain . the update at time t takes the state of gibbs chain at time t − 1 , perform one round of gibbs sampling and uses this state in the negative gradient estimate . // pcd has been proven to have good performance in practice .
h+	28	[328]	rbm is very powerful for unsupervised representation learning , which has powered many applications such as collaborative filtering , document representation and social behavior prediction . // in , rbm is used for representation learning of documents by considering the diversity . // in , rbm and conditional rbm are applied to the task of learning drug target relations on multidimensional networks .
h-	28	[174]	rbm is also used for modeling networks . // in , an advanced model named ctrbm is proposed to do link prediction on dynamic data . // however , they only use the link structure without considering the attributes of the nodes .
h-	39	[5]	along with the increasing requirements , a variety of researchers have studied the network embedding construction problem from different aspects . // classical methods ( e .g . , isomap and laplacian eigenmaps ) usually transform this task into a constrained optimization problem . // hence , the usefulness of these methods may be heavily impacted by the computation consumption of processing hundreds of millions of nodes .
h-	39	[343]	previous literatures also show that this kind of network is common in real world social , collaboration , information , and many other kinds of networks . // more than 200 different kinds of large real world networks where nodes explicitly state their group memberships were studied in the work done by yang and leskovec . // although communities or groups in networks can provide valuable information , previous network embedding studies rarely took this information into consideration .
ho	39	[343]	we use two large real world datasets for evaluating the proposed methods . // amazon : this network is provided by yang and collected by crawling amazon website . // in this network , if a product i is frequently co purchased with product j , the graph contains an undirected edge from i to j . each product category provided by amazon defines each ground truth community .
b	9	[235, 36]	one of effective means towards organizing such information associated with the potentially large and complex graph is to learn the graph representations , which assign to each vertex of the graph a low dimensional dense vector representation , encoding meaningful information conveyed by the graph . // recently , there has been significant interest in the work of learning word embeddings . // their goal is to learn for each natural language word a low dimensional vector representation based on their contexts , from a large amount of natural language texts .
h-	9	[33, 56, 55, 137]	next , they utilized the skip gram model proposed by to learn low dimensional representations for vertices from such linear structures . // the learned vertex representations were shown to be effective across a few tasks , outperforming several previous approaches such as spectral clustering and modularity method . //
b	9	[235]	to answer the secproceedings of the thirtieth aaai conference on artificial intelligence and question , we first revisit one popular existing method used for learning vertex representations for linear structures . // a recent study by showed that optimizing the objective function associated with the skipgram with negative sampling method has a intrinsic relation with factorzing a shifted positive pointwise mutual information ( ppmi ) matrix of the words and their contexts . // specifically , they showed that it was possible to use the standard singular value decomposition ( svd ) method to factorize the ppmi matrix to induce the vertex/word representations from the decomposed matrices .
h+	9	[149]	since our final goal is to learn good vertex representations that are effective in capturing the graph ’ s information , it is essential to investigate better ways of recovering the vertex representations from the ppmi matrix , where potentially complex , non linear relations amongst different vertices can be captured . // deep learning sheds light on the path of modeling nonlinear complex phenomena , which has many successful applications in different domains , such as speech recognition and computer vision . // deep neural networks ( dnn ) , e .g . , the stacked autoencoders , can be regarded as an effective method for learning high level abstractions from low level features .
h+	9	[235]	a major shortcoming of such an approach and related methods is that frequent words with relatively little semantic value such as stop words have a disproportionate effect on the word representations generated . // church and hanks ’ s pointwise mutual information matrix was proposed to address this problem and has since been proven to provide better word representations . // a common approach to improving performance is to assign each negative value to 0 .
h+	9	[36]	thus , it is reasonable to weigh the importance of contextual nodes based on their relative distance to the current node . // such weighting strategies were implemented in both word2vec and glove and were found important for achieving good empirical results . // based on this fact , we can see that ideally the representation for the i th vertex should be constructed in the following way : r = k k=1 w ( k ) · p∗ k where w ( · ) is a decreasing function , i .e . , w ( t + 1 ) < w ( t ) .
ho	9	[350]	to demonstrate the effectiveness of deep learning as compared to svd , we chose a dictionary of 10 ,000 most frequent words ( 10 ,000 selected due to the time complexity of the svd algorithm ) . // to evaluate the performance of word representations generated by each algorithm , we conducted experiments on word similarities on 4 datasets , including the popular wordsim353 , wordsim similarity and wordsim relatedness and mc , as used in . //
b	9	[6]	// deepwalk is a recently proposed method to learn representations of networks . // it transforms a graph structure into linear sequences by truncated random walks and processes the sequences using skip gram with hierarchical softmax .
ho	9	[160]	ppmi was used for word representations in and is a sparse high dimensional representation . // svd is a common matrix factorization method that is used to reduce dimensions or extract features . // following , we used svd to compress the ppmi matrix to obtain low dimensional representations .
h+	32	[26, 37, 353, 1]	a recent advancement in graph representation learning , deepwalk proposed online learning methods using neural networks to address this scalability limitation . // much work has since followed . // these neural network based methods have proven both highly scalable and performant , achieving strong results on classification and link prediction tasks in large networks .
ho	32	[354]	new representation learning paradigm . // we propose harp , a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing and graph representation learning communities to build substantially better graph embeddings . // improved optimization primitives .
b	32	[68]	the labels represent the categories a blogger publishes in . // citeseer – citeseer is a citation network between publications in computer science . // the labels indicate the research areas a paper belongs to .
b	32	[1]	first , several methods use different strategies for sampling neighboring nodes . // line learns graph embeddings which preserve both the first order and second order proximities in a graph . // walklets captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix .
b	32	[354, 356, 357]	graph drawing . // multilevel layout algorithms are popular methods in the graph drawing community , where a hierarchy of approximations is used to solve the original layout problem . // using an approximation of the original graph has two advantages not only is the approximation usually simpler to solve , it can also be extended as a good initialization for solving the original problem .
b	44	[13]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[18]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[37]	learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity .
b	44	[29]	learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity .
b	44	[48]	recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity . // arguably , most existing methods of graph representation learning can be classified into two categories . the first is generative graph representation learning model .
b	44	[6]	the edges in the graph can thus be viewed as observed samples generated by these conditional distributions , and these generative models learn vertex embeddings by maximizing the likelihood of edges in the graph . // for example , deepwalk uses random walk to sample “ context ” vertices for each vertex , and tries to maximize the log likelihood of observing context vertices for the given vertex . // node2vec further extends the idea by proposing a biased random walk procedure , which provides more flexibility when generating the context for a given vertex .
b	44	[30]	for instance , sdne uses the sparse adjacency vector of vertices as raw features for each vertex , and applies an autoencoder to extract short and condense features for vertices under the supervision of edge existence . // ppne directly learns vertex embeddings with supervised learning on positive samples and negative samples , also preserving the inherent properties of vertices during the learning process . // although generative and discriminative models are generally two disjoint classes of graph representation learning methods , they can be considered two sides of the same coin .
h+	44	[363]	recently , generative adversarial nets have received a great deal of attention . // by designing a game theoretical minimax game to combine generative and discriminative models , gan and its variants achieve success in various applications , such as image generation , sequence generation , dialogue generation , information retrieval , and domain adaption . // inspired by gan , in this paper we propose graphgan , a novel framework that unifies generative and discriminative thinking for graph representation learning .
h+	44	[366]	recently , generative adversarial nets have received a great deal of attention . // by designing a game theoretical minimax game to combine generative and discriminative models , gan and its variants achieve success in various applications , such as image generation , sequence generation , dialogue generation , information retrieval , and domain adaption . // inspired by gan , in this paper we propose graphgan , a novel framework that unifies generative and discriminative thinking for graph representation learning .
b	44	[52]	node2vec is a variant of deepwalk and designs a biased random walk to learn vertex embeddings . // struc2vec captures the structural identity of vertices in a graph . //
b	24	[6]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[70]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[6]	however , as mentioned before , directly conducting these tasks based on network topology has a series of problems , and thus poses a question that whether we can learn a network embedding space purely based on the network topology information , such that these tasks can be well supported in this low dimensional space . // motivated by this , attempts are proposed to preserve rich structural information into network embedding , from nodes and links to neighborhood structure , high order proximities of nodes , and community structures . // all these types of structural information have been demonstrated useful and necessary in various network analysis tasks .
ho	24	[370]	besides this structural information , network properties in the original network space are not ignorable in modeling the formation and evolution of networks . // to name a few , network transitivity ( i .e . triangle closure ) is the driving force of link formation in networks , and structural balance property plays an important role in the evolution of signed networks . preserving these properties in a network embedding space is , however , challenging due to the inhomogeneity between the network space and the embedding vector space . // some recent studies begin to look into this fig . 3 . an overview of different settings of network embedding . cui et al . : a survey on network embedding 835 problem and demonstrate the possibility of aligning these two spaces at the property level , .
b	24	[7]	network embedding with side information . // besides network topology , some types of networks are accompanied with rich side information , such as node content or labels in information networks , node and edge attributes in social networks , as well as node types in heterogeneous networks . // side information provides useful clues for characterizing relationships among network nodes , and thus is helpful in learning embedding vector spaces .
b	24	[371]	realizing this idea leads to supervised or pseudo supervised information ( i .e . the advanced information ) in the target scenarios . // directly designing a framework of representation learning for a particular target scenario is also known as an end to end solution , where highquality supervised information is exploited to learn the latent representation space from scratch . // end to end solutions have demonstrated their advantages in some fields , such as computer vision and natural language processing ( nlp ) . similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification .
b	24	[374]	similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification . // some recent works demonstrate the feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction . // in general , network structures and properties are the fundamental factors that need to be considered in network embedding .
h+	24	[30]	in the series of matrix factorization models , singular value decomposition ( svd ) is commonly used in network embedding due to its optimality for low rank approximation . // non negative matrix factorization is often used because of its advantages as an additive model . // random walk .
b	24	[19]	the key challenges are how to make deep models fit network data , and how to impose network structure and property level constraints on deep models . // some representative methods , such as sdne , sdae , and sine , propose deep learning models for network embedding to address these challenges . // at the same time , deep neural networks are also well known for their advantages in providing end to end solutions .
b	24	[375]	therefore , in the problems where advanced information is available , it is natural to exploit deep models to come up with an end to end network embedding solution . // for instance , some deep model based end to end solutions are proposed for cascade prediction and network alignment . // the network embedding models are not limited to those mentioned in this subsection .
b	24	[4]	indeed , isomap learns the representation ui of entry i , which approximately preserves the geodesic distances of the entry pairs in the low dimensional space . // the key problem of isomap is its high complexity due to the computing of pair wise shortest pathes . locally linear embedding ( lle ) is proposed to eliminate the need to estimate the pairwise distances between widely separated entries . // lle assumes that each entry and its neighbors lie on or close to a locally linear patch of a mainfold . t
b	24	[36]	deepwalk discovers that the distribution of nodes appearing in short random walks is similar to the distribution of words in natural language . // motivated by this observation , skip gram model , a widely used word representation learning model , is adopted by deepwalk to learn the representations of nodes . // specifically , as shown in fig .
b	24	[1]	node2vec is able to learn the representations that embed nodes with same network community closely , and to learn representations where nodes sharing similar roles have similar embeddings . // line is proposed for large scale network embedding , and can preserve the first and second order proximities . // the first order proximity is the observed pairwise proximity between two nodes , such as the observed edge between nodes 6 and 7 in fig . 5 .
b	24	[27]	by concentrating the representations learned from each function , the global representations can be obtained . // wang et al . propose a modularized nonnegative matrix factorization ( m nmf ) model for network embedding , which aims to preserve both the microscopic structure , i .e . , the first order and second order proximities of nodes , and the mesoscopic community structure . // to preserve the microscopic structure , they adopt the nmfmodel to factorize the pairwise node similarity matrix and learn the representations of nodes .
b	24	[5]	to impose more penalty to the reconstruction error of the non zero elements than that of zero elements , sdne introduces the penalty vector bi ¼ fbijgn j¼1 ( bij is larger than a threshold if there is an edge between nodes i and j ) and gives rise to the following function that can preserve the second order proximity l2nd ¼x i kð^xi  xiþ  bik2 : // to preserve the first order proximity of nodes , the idea of laplacian eigenmaps is adopted . // by exploiting the firstorder and second order proximities jointly into the learning process , the representations of nodes can be finally obtained .
b	24	[380]	finally , the stacked denoisingautoencoders that partially corrupt the input data before taking the training step are applied to learn the latent representations . // in order to make a general framework on network embedding , chen et al . propose a network embedding framework that unifies some of the previous algorithms , such as le , deepwalk and node2vec . // the proposed framework , denoted by gem d ½hðþ ; gðþ ; dð ; þ , involves three important building blocks : hðþ is a node proximity function based on the adjacency matrix ; gðþ is a warping function that warps the inner products of network embeddings ; and dð ; þ measures the differences between h and g .
b	24	[48]	finally they approximate the aggregated similarity to the semantic similarity based on the observation that if two nodes have a large semantic similarity , at least one of the similarities sm ij from the hash tables is large , otherwise , all of the similarities are small . // preserving the asymmetric transitivity property of directed network is considered by hope . // asymmetric transitivity indicates that , if there is a directed edge from node i to node j and a directed edge from j to v , there is likely a directed edge from i to v , but not from v to i .
b	24	[185]	asymmetric transitivity indicates that , if there is a directed edge from node i to node j and a directed edge from j to v , there is likely a directed edge from i to v , but not from v to i . // in order to measure this high order proximity , hope summarizes four measurements in a general formulation , that is , katz index , rooted pagerank , common neighbors , and adamic adar . // with the high order proximity , svd can be directly applied to obtain the low dimensional representations .
b	24	[383]	sine is proposed for signed network embedding , which considers both positive and negative edges in a network . // due to the negative edges , the social theories on signed network , such as structural balance theory , , are very different from the unsigned network . // the structural balance theory demonstrates that users in a signed social network should be able to have their “ friends ” closer than their “ foes ” .
b	24	[319]	for each node , they learn its low rankrepresentation ui in a low dimensional vector space , which can reconstruct the network structure . // also , they learn the representation of nodes in the topic space based on the relational topic model ( rtm ) , where each topic z is associated with a probability distribution over words . // to integrate the two aspects , they associate each topic z with a representation z in the same low dimensional vector space and then have the following function : pðzjviþ ¼expð 12 kui  ’ zk2 p þz expð 12 kui  ’ zk2þ : ( 11 ) finally , in a unified generative process , the representations of nodes u can be learned .
b	24	[41]	they use a logistic function to model the relationship in the new augmented network , and by combining with negative sampling , they can learn the representations of nodes in a joint objective function , such that the representations can preserve the network structure as well as the relationship between the node and content . // pan et al . propose a coupled deep model that incorporates network structure , node attributes and node labels into network embedding . // the architecture of the proposed model is shown in fig . 10 .
b	24	[386]	aane provides a distributed optimization algorithm to process each node efficiently . // wei et al . study the problem of cross view link prediction ( cvlp ) based on attributed network embedding , i .e . , to recommend nodes with only links to nodes with only attributes ( or vice versa ) . // the proposed model learns the link based and attribute based representations , and utilize the consensus to establish the relations between them .
b	24	[224]	uang and mamoulis propose a meta path similarity preserving heterogeneous information network embedding algorithm . // to model a particular relationship , a meta path is a sequence of object types with edge types in between . // they develop a fast dynamic programming approach to calculate the truncated meta path based proximities , whose time complexity is linear to the size of the network .
b	24	[297]	most of the previous studies on information diffusion are conducted in original network spaces . // recently , simon et al . propose a social network embedding algorithm for predicting information diffusion . // the basic idea is to map the observed information diffusion process into a heat diffusion process modeled by a diffusion kernel in the continuous space .
b	24	[389]	similar to deepwalk , they perform a random walk over a cascade graph to sample a set of paths . // then the gated recurrent unite ( gru ) , a specific type of recurrent neural network , is applied to these paths and learn the embeddings for these paths . // the attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph .
b	24	[374]	anomaly detection has been widely investigated in previous work . // anomaly detection in networks aims to infer the structural inconsistencies , which means the anomalous nodes that connect to various diverse influential communities , , such as the red node in fig . 13 . // hu et al . propose a network embedding based method for anomaly detection .
b	24	[375]	as illustrated in fig . 14 , the anchor link prediction problem is , given source network gs and target network gt and a set of observed anchor links t , to identify the hidden anchor links across gs and gt . // first , man et al . extend the original sparse networks gs and gt to the denser networks . // the basic idea is that given a pair of users with anchor links , if they have a connection in one network , so do their counterparts in the other network , in this way , more links will be added to the original networks .
b	24	[56]	one instance of the network can be downloaded at http : socialcomputing .asu .edu/datasets/flickr . // youtube . this is a network between users of the popular video sharing website , youtube . // one instance of the network can be found at http : socialcomputing .asu .edu/datasets/youtube2 .
b	24	[395]	one instance of the data set can be found at https : linqs .soe .ucsc .edu/node/236 . // citeseer . this network , similar to cora , also consists of scientific publications and their citation relationships . // one instance of the data set can be downloaded at https : linqs .soe .ucsc .edu/node/236 .
b	24	[1]	given some nodes with known labels in a network , the node classification problem is to classify the rest nodes into different classes . // node classification is one of most primary applications for network embedding , . // essentially , node classification based on network embedding for can be divided into three steps .
b	24	[33]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[395]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[26]	specifically , a social network usually is a communication network among users on online platforms . // deepwalk , grarep , sdne , node2vec , and lane conduct classification on blogcatalog to evaluate the performance . // also , the classification performance on flickr has been assessed in , , , .
h+	24	[6]	also , the classification performance on flickr has been assessed in , , , . // some studies , , apply their algorithms to the youtube network , which also achieves promising classification results . // a citation network usually represents the citation relationships between authors or between papers .
b	24	[41]	a citation network usually represents the citation relationships between authors or between papers . // for example , , use the dblp network to test the classification performance . // cora is used in , . citeseer is used in , , .
b	24	[12]	link prediction . // link prediction , as one of the most fundamental problems on network analysis , has received a considerable amount of attention , . // it aims to estimate the likelihood of the existence of an edge between two nodes based on observed network structure .
b	24	[263]	generally , precision @ k and mean average precision ( map ) are used to evaluate the link prediction performance . // the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , and dblp1 ) , social networks ( sn tweibo2 , sn twitter , facebook , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv .
b	24	[37]	the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , and dblp1 ) , social networks ( sn tweibo2 , sn twitter , facebook , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv . // hope applies network embedding to link prediction on two directed networks sntwitter , which is a subnetwork of twitter6 , and sn tweibo , which is a subnetwork of the social network in tencent weibo7 .
h+	24	[118]	node2vec tests the performance of link prediction on a social network facebook and a biological network ppi . // eoe uses dblp to demonstrate the effectiveness on citation networks . based on two social networks , epinions and slashdot , sine shows the superior performance of signed network embedding on link prediction . // to sum up , network embedding is able to capture inherent network structures , and thus naturally it is suitable for link prediction applications .
b	24	[224]	accuracy ( ac ) and normalized mutual information ( nmi ) are frequently used to assess the clustering performance on graphs and networks . // the node clustering performance is tested on three types of networks : social networks ( e .g . , facebook and yelp ) , citation networks ( e .g . , dblp ) , and document networks ( e .g . , 20 newsgroup ) . // in particular , extracts a social network from a social blogging site . it uses the tf idf features extracted from the blogs as the features of blog users and the “ following ” behaviors to construct the linkages .
b	24	[110]	uses the facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering . // is applied to more social networks including movie , a network extracted from yago that contains knowledge about movies , yelp , a network extracted from yelp that is about reviews given to restaurants , and game , extracted from freebase that is related to video games . // tests the node clustering performance on a document network , 20 newsgroup network , which consists of documents .
b	24	[19]	more often than not , the quality of network visualizationby different network embedding algorithms is evaluated visually . // fig .15 is an example by sdne where sdne is applied to 20 newsgroup . // in fig . 15 , each document is mapped into a two dimensional space as a point , and different colors on the points represent the labels .
b	24	[41]	also , line , grarep , and eoe are applied to a citation network dblp and generate meaningful layout of the network . // pan et al . show the visualization of another citation network citeseer m10 consisting of scientific publications from ten distinct research areas . // open source software .
b	105	[26, 37, 6, 1, 19]	introduction . // low dimensional vector embeddings of nodes in large graphs have proved extremely useful as feature inputs for a wide variety of prediction and graph analysis tasks . // the basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high dimensional information about a node ’ s neighborhood into a dense vector embedding .
b	105	[266]	these approaches can be modified to operate in an inductive setting ( e .g . , ) , but these modifications tend to be computationally expensive , requiring additional rounds of gradient descent before new predictions can be made . // there are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology . // so far , graph convolutional networks ( gcns ) have only been applied in the transductive setting with fixed graphs .
b	105	[26, 37, 6, 1, 19]	factorization based embedding approaches . // there are a number of recent node embedding approaches that learn low dimensional embeddings using random walk statistics and matrix factorization based learning objectives . // these methods also bear close relationships to more classic approaches to spectral clustering , multi dimensional scaling , as well as the pagerank algorithm .
h-	105	[37, 6, 1, 19]	these methods also bear close relationships to more classic approaches to spectral clustering , multi dimensional scaling , as well as the pagerank algorithm . // in addition , for many of these approaches ( e .g . , ) the objective function is invariant to orthogonal transformations of the embeddings , which means that the embedding space does not naturally generalize between graphs and can drift during re training . // one notable exception to this trend is the planetoid i algorithm introduced by yang et al . , which is an inductive , embeddingbased approach to semi supervised learning .
h-	105	[409, 410, 411, 266, 381]	graph convolutional networks . // in recent years , several convolutional neural network architectures for learning over graphs have been proposed ( e .g . , ) . // the majority of these methods do not scale to large graphs or are designed for whole graph classification ( or both ) .
h-	105	[389]	lstm aggregator . // we also examined a more complex aggregator based on an lstm architecture . // compared to the mean aggregator , lstms have the advantage of larger expressive capability . however , it is important to note that lstms are not inherently symmetric ( i .e . , they are not permutation invariant ) , since they process their inputs in a sequential manner .
ho	105	[413]	in the multi graph setting , we can not apply deepwalk , since the embedding spaces generated by running the deepwalk algorithm on different disjoint graphs can be arbitrarily rotated with respect to each other ( appendix ) . // all models were implemented in tensorflow with the adam optimizer ( except deepwalk , which performed better with the vanilla gradient descent optimizer ) . // we designed our experiments with the goals of ( i ) verifying the improvement of graphsage over the baseline approaches ( i .e . , raw features and deepwalk ) and ( ii ) providing a rigorous comparison of the different graphsage aggregator architectures .
h+	584	[36]	we assume that the positive examples ( i .e . , paper records in the datasets ) have an effectiveness of 1 ( and thus a distance ε of 0 ) . // we adopt the negative sampling strategy , which has widely been applied to word embeddings , to generate negative examples and assume they have an effectiveness of a small value . // figure 1 presents a 2 d visualization of ten behavior goals ( i .e . , conferences ) and two successful behavior plans ( a kdd ’ 15 paper and a nips ’ 11 paper ) losing some dimensions in the embedding results .
b	584	[416]	we also considered the deepwalk model here . since deepwalk can be seen as a special case of node2vec that uses truncated uniform random walks , we only report the better performance among them in experiments . // verse : it is able to preserves the distributions of a selected vertex to vertex similarity measure in homogeneous network such as personalized pagerank , simrank and etc . // bine : this method aims at learning the representations of vertices in a bipartite network . it conducts biased random walks to preserve the long tail distribution of vertices .
b	584	[417]	// harmonic mean of ranks ( hmr ) : this metric is to see whether the method ranks the true venue at the top . a smaller value of hmr indicates better performance . //
b	584	[416]	node2vec extended it to use biased random walks to capture the homophily and structural equivalence properties of network . // verse was designed to preserve the distributions of a selected pair wise similarity measure in network such as personalized pagerank or simrank . // besides methods focusing on homogeneous networks , bine was able to learn the representations of vertices in a bipartite network by conducting biased random walks to preserve the long tail distribution of vertices .
h-	584	[417]	however , none of these existing methods has an effective formulation of behavior and does not preserve the outcome information of behaviors . // a recent work learnsuc formulated behavior as a multi type itemset structure instead of nodes in network and preserved the behavioral success property . // but this model does not have an explicit definition of behavior outcomes .
b	584	[421]	to predict the success in art , fraiberger et al . used a markov model to predict the career trajectory of individual artists and documents the strong path and history dependence of valuation in art . // yucesoy et al . proposed a model aiming at predicting which books will become bestsellers . // and , deville et al . studied quantifying the career choices such as changing institutions affecting scientific outcomes .
h-	415	[6]	to date , existing works have primarily focused on embedding homogeneous networks where vertices are of the same type . // following the pioneering work of deepwalk , these methods typically apply a two step solution : first performing random walks on the network to obtain a “ corpus ” of vertices , and then applying word embedding methods such as word2vec to obtain the embeddings for vertices . // despite effectiveness and prevalence , we argue that these methods can be suboptimal for embedding bipartite networks due to two primary reasons .
b	415	[192]	to our knowledge , none of the existing works has paid special attention to embed bipartite networks . // this can be evidenced by existing recommendation works that usually assign varying weights on different sources of information to allow a flexible tuning on the learning process . // in this work , we focus on the problem of learning vertex representations for bipartite networks .
h-	415	[430]	the former employs the linear transformations to embed network vertices into a low dimensional embedding space , such as singular value decomposition ( svd ) and multiple dimensional scaling ( mds ) . // however , the latter maps network vertices into a low dimensional latent space by utilizing the nonlinear transformations , e .g . , kernel pca , spectral embedding , marginal fisher analysis ( mfa ) , and manifold learning approaches include lle and isomap . // generally speaking , mf based methods have two main drawbacks : ( 1 ) they are usually computationally expensive due to the eigen decomposition operations on data matrices , making them difficult to handle large scale networks ; ( 2 ) their performance are rather sensitive to the predefined proximity measures for calculating the affinity matrix .
b	415	[1]	there are some follow up works exploiting both 1st order and 2nd order proximities between vertices to embed homogeneous networks . // specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others .
h-	415	[39]	specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others . // it is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks , for which there is only one type of vertices .
h-	415	[111]	thus , these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network . // metapath2vec++ , hne and eoe are representative vertex embedding methods for heterogeneous networks . // although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks , they are not tailored for learning on bipartite networks .
b	415	[425]	for example , hits learns to rank vertices by capturing some semantic relations within a bipartite network . // co hits incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network . // birank ranks vertices by taking into account both the network structure and prior knowledge .
b	415	[428, 192]	modeling implicit relations . // as illustrated in existing recommedation works , both explicit and implicit relations are helpful to reveal different semantic in bipartite networks . // to be comprehensive , it is crucial to also account for the implicit relation between two vertices of the same type , even though they are not explicitly connected .
h-	415	[111]	we assign a probability to stop a random walk in each step . // in contrast to deepwalk and other work that apply a fixed length on the random walk , we allow the generated vertex sequences have a variable length , in order to have a close analogy to the variable length sentences in natural languages . // generally speaking , the above generation process follows the principle of “ rich gets richer ” , which is a physical phenomena existing in many real networks , i .e . , the vertex connectivities follow a scalefree power law distribution .
ho	415	[436]	here we propose a more grounded sampling method that caters the network data . // first we employ locality sensitive hashing ( lsh ) to block vertices after shingling each vertex by its ws hop neighbors with respect to the topological structure in the input bipartite network . // given a center vertex , we then randomly choose the negative samples from the buckets that are different from the bucket contained the center vertex .
b	415	[37]	we use the line ( 1st+2nd ) method which has shown the best results in their paper . // node2vec : this method extends deepwalk by performing biased random walks to generate the corpus of vertex sequences . // the hyper parameters p and q are set to 0 .5 which has empirically shown good results .
b	415	[438]	this method has been widely used in recommendation literature as a highly competitive baseline . // rankals : this method also optimizes the mf model for the ranking task , by towards a different pairwise regressionbased loss . // fismauc : distinct to mf , factored item similarity model ( fism ) is an item based collaborative filtering method . we employ the auc based objective to optimize fism for the top k task .
b	54	[442]	network ( or graph ) is a group of interconnected nodes and contains a wealth of information on the relationships between every pair of nodes . // the analysis of graph is required in almost every field , for instance , online social network , biological research , credit rating and so on . // birds of a feather flock together and people always have certain characteristics in common with their friends surrounded by .
b	54	[444]	unsupervised network embedding algorithms try to preserve the local relationships of each node in the graph . // isomap , lle and laplacian eigenmaps are three classic dimensionality reduction and data representation algorithms . // finding the k nearest neighbors is the key step of these three algorithms .
b	54	[445]	the most sensible approach is to make a balance between these two strategies . // structural equivalence is being discussed not only in social role mining task , but in recent network embedding algorithms as well . // we support the motivation that both kinds of similar nodes should be taken into consideration .
b	54	[26]	deepwalk uses depth first search in order to sample the neighborhood of the target node . the depth is set to 2 by default . // grarep also uses dfs but the depth is larger . // line uses breadth first search as well as depth first search . the number of step are all constrained below two .
h-	54	[26]	. it has the same goal as line does , considering both first order proximity and second order proximity . // grarep and node2vec discuss local structure and structural equivalence . // however , it is questionable whether the structural similarity is actually used during the learning process .
ho	54	[448]	there are many graphlet counting algorithms that can provide precise results on small graph and approximate results on big graph with a quick speed . // in this paper , we use orca to help us calculate gdv of each node . // the code is available on the authors ’ website .
ho	54	[6]	experiments . // in this section we first visualize a small network using deepwalk , node2vec and our proposed algorithm separately , which demonstrates the arguments in section 3 visually . // moreover , we show the performance of different network embedding algorithms on multi label classification task .
b	54	[180]	they all exist a mix of homophily and structural equivalences . // blogcatalog is the social blog directory which manages the bloggers and their blogs . // the network depicts the contact between users and the user labels represent their interests .
b	54	[6]	we set d = 500 , the same setting in . // deepwalk is the first embedding algorithm that brings the deep learning technology . // it is an unsupervised learning algorithm .
b	54	[37]	in our experiments , we try the unsupervised mode of line and set d = 128 , r = 10 , l = 80 , k = 10 , the same setting in . // node2vec simulates biased random walks over the underlying network . // it uses parameter p and q to balance the bfs strategy with dfs strategy .
ho	54	[37]	multi label classification . // this task uses the exact same datasets and experimental procedure as presented in . // the node vectors are the input to a one vsrest logistic regression implemented by sklearn .
ho	585	[455]	this way , the structural information is contained in how the diffusion spreads over the network rather than where it spreads . // in order to provide vector valued embeddings , we embed these wavelet distributions using the empirical characteristic function . // the advantage of empirical characteristic functions is that they capture all the moments ( including higher order moments ) of a given distribution .
h-	585	[406]	in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters . // recent neural representation learning methods ( structure2vec , neural fingerprints , graph convolutional networks ( gcns ) , message passing networks , etc . ) are a related line of research . // however , these graph embedding methods do not apply in our setting , since they solve a ( supervised ) graph classification task and/or embed entire graphs while we embed individual nodes .
ho	585	[452]	however , these graph embedding methods do not apply in our setting , since they solve a ( supervised ) graph classification task and/or embed entire graphs while we embed individual nodes . // another line of related work are graph diffusion kernels which have been utilized for various graph modeling purposes . // however , to the best of our knowledge , our paper is the first to apply graph diffusion kernels for determining structural roles in graphs .
ho	585	[453, 454]	spectral graph wavelets . // in this section , we provide background on the spectral graph waveletbased model that we will use in the rest of the paper . // let u be the eigenvector decomposition of the unnormalized graph laplacian l = d − a = uλu t and let λ1 < λ2 ≤ · · · ≤ λn ( λ = diag ( λ1 , . . . , λn ) ) denote the eigenvalues of l .
b	585	[52]	baseline methods . // we evaluate the performance of graphwave1 against two state of the art baselines for learning structural embeddings : struc2vec , a method which discovers structural embeddings at different scales through a sequence of walks on a multilayered graph , and rolx , a method based on non negative matrix factorization of a node feature matrix ( number of neighbors , triangles , etc . ) that describes each node based on this given set of latent features . // while in , the authors develop a method for automatically selecting the number of roles in rolx , we use rolx as an oracle estimator , providing it with the correct number of classes .
h-	585	[410]	for all baselines , we use the default parameter values in the available solvers , and for graphwave , we use the multiscale version ( section 4 ) , set d = 50 and use evenly spaced sampling points ti in range . // we again note that graph embedding methods ( structure2vec , neural fingerprints , gcns , etc . ) do not apply in these settings , since they embed entire graphs while we embed individual nodes . // barbell graph
b	19	[109]	therefore , mining the information in the network is very important . // one of the fundamental problems is how to learn useful network representations . // an effective way is to embed networks into a low dimensional space , i .e . learn vector representations for each vertex , with the goal of reconstructing the network in the learned embedding space .
ho	19	[468]	learning network representations faces the following great challenges . // high non linearity : as stated , the underlying structure of the network is highly non linear . // therefore , how to design a model to capture the highly non linear structure is rather difficult .
h-	19	[444]	sparsity : many real world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance . // in the past decades , many network embedding methods have been proposed , which adopted shallow models , such as isomap , laplacian eigenmaps ( le ) and line . // however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure .
h-	19	[220]	however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure . // although some methods adopt kernel techniques , as stated , kernel methods are also shallow models and can not capture the highly non linear structure well . // in order to capture the highly non linear structure well , in this paper we propose a new deep model to learn vertex representations for networks .
h+	19	[471]	in order to capture the highly non linear structure well , in this paper we propose a new deep model to learn vertex representations for networks . // this is motivated by the recent success of deep learning , which has been demonstrated to have a powerful representation ability to learn complex structures of the data and has achieved substantial success in dealing with images , text and audio data . // in particular , in our proposed model we design a multilayer architecture which consists of multiple non linear functions .
h+	19	[43]	representation learning has long been an important problem of machine learning and many works aim at learning representations for samples . // recent advances in deep neural networks have witnessed that they have powerful representations abilities and can generate very useful representations for many types of data . // for example , proposed a seven layer convolutional neural network to generate image representations for classification .
b	19	[176]	in , restricted boltzmann machines were adopted to do collaborative filtering . // adopted deep autoencoder to do graph clustering . // proposed a heterogeneous deep model to do heterogeneous data embedding .
b	19	[1]	some earlier works like local linear embedding ( lle ) , isomap first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations . // more recently , designed two loss functions attempting to capture the local and global network structure respectively . // furthermore , extended the work to utilize high order information .
ho	19	[476]	such an assumption has been proved reasonable in many fields . // for example , in linguistics words will be similar if they are always surrounded by similar contexts . // people will be friends if they have many common friends .
ho	19	[478]	note that due to the high nonlinearity of the model , it suffers from many local optimal in the parameter space . // therefore , in order to find a good region of parameter space , we use deep belief network to pretrain the parameters at first , which has been demonstrated as an essential initialization of parameters for deep learning in literature . // the full algorithm is presented in alg . 1 .
b	19	[56]	the detailed descriptions are listed as follows . // blogcatalog , flickr and youtube : they are social network of online users . // each user is labelled by at least one category .
b	19	[26]	after optimizing the loss functions , it concatenates these representations . // grarep : it extends to high order proximity and uses the svd to train the model . // it also directly concatenates the representations of first order and high order .
ho	19	[61]	the representations for the vertexes are generated from the network embedding methods and are used as features to classify each vertex into a set of labels . // specifically , we adopt the liblinear package to train the classifiers . // when training the classifier , we randomly sample a portion of the labeled nodes as the training data and the rest as the test .
b	382	[480, 481]	// with the explosive growth of data , similarity search is becoming increasingly important for a wide range of large scale applications , including image retrieval , document search , and recommendation systems . // due to the simplicity and efficiency , hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search .
ho	382	[489, 490]	in literature , most of the supervised hashing techniques use the pairwise relationship links without identifying the true similarity components , and neglect the non transitive property of the semantic metric . // in general , it is difficult to capture the true non transitive triangle relationship in a single metric space . // as an example illustrated in figure 2 , we have a non transitive relationship between three images { a , b , c } .
b	382	[490]	note that the similarity components discovered in our work are entity wise , and each entity is represented by a combination of similarity components . // multiple maps t sne aims to represent non transitive similarity and central objects in two dimensional visualizations . // in social networks , many works focus on extracting the multiple types of relationship or finding the overlapping communities .
h-	382	[498]	locality sensitive hashing ( i .e . lsh ) methods are first proposed , which generate hash codes with random projections or permutations . // moreover , mu et . al . apply lsh to non metric similarity . // while locality sensitive hashing is independent with data , and has to generate long hash code to achieve high accuracy , spectral hashing is proposed to learn hash functions based on the data distribution , and achieves much compact hash code and higher accuracy .
h-	382	[506, 507, 508, 509, 510, 511]	semi supervised or supervised hashing methods exploit the labeled pairwise simiarity relationship between entities to capture the high level semantics . // moreover , some multi modal hashing methods are proposed to exploit multiple features for hashing to get higher accuracy . // however , these hashing methods can not discover the latent similarity components and capture the non transitive similarity .
b	382	[513, 514]	since these methods treat similarity and dissimilarity relationships in the same way , these multitable hashing methods are not designed to identify latent similarity components and can not capture non transitive similarity . // hashing on multi label data learn different hash tables for different known labels . // heterogeneous hashing generates a variant of original hash table in each domain to search , in order to capture the specific characteristics of target domain .
b	382	[500]	that is , when hrs is relatively small , reduction by 1 on hrs will save most of the search space . // we select three state of art hashing methods , i .e . kernelbased supervised hashing ( ksh ) , semi supervised hashing ( splh ) , iterative quantization ( itq ) , as baselines . // ksh is a supervised method , splh is a semi supervised method , itq is a unsupervised methods . for ksh , we randomly select 300 anchors in nus wide , and 50 anchors in dblp and the synthetic dataset .
b	109	[516, 176, 517]	they are easier to handle since each data can be viewed as a point residing in an euclidean space . // thus , similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification , clustering and retrieval . // as shown in , learning good representations is one of the fundamental problems in data mining and web search , and it often has a stronger impact on performance than designing a more sophisticated model .
b	109	[520]	a combination of graphs and relational data is commonly used to represent these social networks and social media data . // current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on .
b	109	[523]	current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on . // the development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links .
h-	109	[526, 521, 10]	hne maps different heterogeneous objects into a unified latent space so that objects from different spaces can be directly compared . // unlike to traditional linear embedding models , the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture . // both deep neural networks ( dnn ) and convolutional neural networks ( cnn ) are aggregated to handle vectorized data ( e .g . , text documents ) or tensor based multimedia objects ( e .g . , color images and videos ) .
b	109	[10]	these models often transfer the problem as learning an embedding of the entities , which corresponds algebraically to a matrix factorization problem of observed relationships . // zhu et . al . proposed a joint factorization approach on both the linkage adjacency matrix and document term frequency for web page categorization . // similar concepts also include .
h-	109	[529, 526, 282]	however , these models focus only on single relations that do not adapt to heterogeneous settings and most of them are hardly to generate to other unseen samples . // a natural extension of these methods to heterogeneous settings is by stacking multiple relational matrices together , and then applying a conventional tensor factorization . // the disadvantage of such multi relational embeddings is the inherent sharing of parameters between different terms , which does not scale to large graphs .
b	109	[43]	unsupervised deep learning , often referred to as “ pre training , ” provides robust initialization and regularization with the help of unlabeled data , which is copiously available . // for example , hinton and salakhutdinov first employed layer wise initialization of deep neural networks with the use of rbms . // a similar approach is weight initialized with autoencoders , as proposed by bengio et . al . . .
h+	109	[532]	the method of dropout has shown particular promise . // a seven layer convolutional network developed by krizhevsky et . al . achieved state of the art performance on the imagenet large scale visual recognition challenge , one of the most challenging tasks in computer vision . // later on , features from one of network ’ s intermediate layers proved to be a superior feature representation for other vision tasks , such as object detection .
b	109	[125]	unsupervised deep learning methods such as auto encoders and rbms are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network , and successfully applied to several multimodal tasks . // alternatively , there has also been effort on learning the joint representation with multiple tasks . // for image and text , a particular useful scenario is zero shot learning of image classification on unseen labels achieved by incorporating the semantically meaningful embedding space to image labels .
b	109	[225]	heterogeneous networks . // a heterogeneous network is defined as a network with multiple types of objects and/or multiple types of links . // as a mathematical abstraction , we define an undirected graph g “ pv , eq , where v “ tv1 , . . . , vnu is a set of vertices and e is a set of edges .
b	109	[515]	some detailed statistics are summarized in table 1 . // nus wide : the dataset was originally collected by the lab for media search in the national university of singapore in the year 2009 . // the dataset includes 269 ,648 unique images with associate tags from flickr .
b	387	[539, 71]	as a result , the network representation is inevitable in many data mining applications . // on the one hand , many data mining applications are designed for networks , such as community detection and link prediction . // on the other hand , other data mining applications can benefit from the analysis of networks , such as collective classification and dimension reduction . all of above applications rely on the analysis of network interactions or edges .
b	387	[33, 55, 154, 6, 1]	all of above applications rely on the analysis of network interactions or edges . // nowadays , network embedding is increasingly employed to assist network analysis as it is effective to learn latent features that encode linkage information . // the basic idea of network embedding is to preserve the network structure by presenting pairs of vertices with edges to be close in the latent space .
b	387	[1]	since deepwalk is only applicable to unweighted edges , all weights are set to 1 as inputs to deepwalk . // line : line is proposed to embed large scale information networks , and is applicable to directed , undirected , weighted and unweighted networks . // the proposed eoe is applicable to large scale network as well since it can be solved in polynomial time .
b	387	[6]	the latter regulation is important because it would preserve the information that certain vertices are not likely to interact , which is a part of network structure information as well . // a recent network embedding model called deepwalk embeds local link information obtained from random walks . // it is motivated by the connection between degree distribution of networks and word frequency distribution of natural languages .
h-	8	[139, 543]	to ensure satisfactory performance , this densely labeled subgraph should be sufficiently large so that dependency relationships among network structure and node labels can be accurately learned . // however , acquiring labels of inter connected nodes in a network can be very expensive and time consuming . // therefore , very often the provided network can be only sparsely labeled , with a very small portion of labeled nodes that may not comprise a connected subgraph .
h-	8	[142, 545, 546]	our main idea of tackling node label sparsity is twofold : ( 1 ) leverage random walks to find path dependencies between unlabeled nodes and sparsely labeled nodes in the network ; and ( 2 ) learn a latent network representation that fully embeds nodes ’ content information and augmented network structure to minimize classification loss . // intuitively , the proposed dmf is motivated by observations that , for sparsely labeled networks , an unlabeled node in the network may not have labeled neighbors or have very low structure similarity to labeled nodes , so existing semi supervised and connectivity based methods would not have satisfactory performance . // alternatively , we can use network walks to examine unlabeled nodes ’ correlations to labeled nodes .
b	8	[543]	a recent study by mcdowell and aha provided a comprehensive comparison of different semi supervised cc algorithms and found that learning a hybrid classifier from content features and relational features can best boost the performance of semi supervised cc algorithms . // in a follow up work , mcdowell and aha pointed out that training discriminative classifiers additionally with neighbors ’ attributes can yield further accuracy gains for semi supervised collective classification . // another line of research has focused more on increasing network connectivity between unlabeled nodes and labeled nodes .
h-	8	[546]	gallagher et al . proposed to add “ ghost edges ” created via random walk with restart , to a network , which enables information propagation from labeled nodes to unlabeled ones . // in , a latent network propagation model ( lnp ) was proposed that constructs a latent graph from different information sources and uses label propagation to predict labels of unlabeled nodes . // however , most of these methods consider either local dependencies between network structure and labels , or correlations between nodes ’ content features and labels , but ignore the important role of their interplay for jointly classifying labels of related nodes .
ho	8	[1]	it was showed to outperform other latent representation learning methods , especially when labeled nodes are scarce . // despite of its empirical effectiveness , deepwalk does not provide a clear interpretation about what network properties are preserved , as pointed out in . // instead , tang et al . proposed a new algorithm called line that learns node representations by explicitly modeling two local pairwise node proximities , i .e . , the first order proximity between pairs of nodes directly linked to each other , and the second order proximity between pairs of nodes sharing common neighbors .
b	8	[6]	text associated deepwalk . // motivated by deepwalk , text associated deepwalk ( tadw ) incorporates node text features into network representation learning . // deepwalk originally generalizes the skip gram model from word representation learning to node representation learning . in deepwalk , nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes .
b	8	[546]	a collective inference is then performed with the learned classifier to predict labels of unlabeled nodes . // lnp : lnp is a recently proposed algorithm for performing collective classification in sparsely labeled networks . // it builds a latent graph by linearly combining five graphs : kstep graph , label similarity graph , attribute similarity graph , prediction confidence graph , and structure similarity graph .
b	110	[549]	search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces . // heterogeneous information networks ( hins ) , such as dblp , yago , dbpedia and freebase , are networks with nodes and edges that may belong to multiple types . // these graph data sources contain a vast number of interrelated facts , and they can facilitate the discovery of interesting knowledge .
b	110	[224]	for example , in the hin in figure 1 , author a1 is close to both a2 and v1 , but these relationships have different semantics . a2 is a co author of a1 , while v1 is a venue where a1 has a paper published . // meta path is a recently proposed proximity model in hins . // a meta path is a sequence of object types with edge types in between modeling a particular relationship .
h+	110	[555, 556, 522]	for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarmance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering . // although there are a few works on embedding hins , none of them is designed for meta path based proximity in general hins .
h-	110	[109, 41]	these measures have been shown to have better perforarxiv:1701 .05291v1 19 jan 2017 mance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering . // although there are a few works on embedding hins , none of them is designed for meta path based proximity in general hins . // to fill this gap , in this paper , we propose hine , which learns a transformation of the objects ( i .e . , vertices ) in a hin to a low dimensional space , such that the meta path based proximities between objects are preserved .
h-	110	[6]	we also investigate the use of negative sampling in order to accelerate the optimization process . // we conduct extensive experiments on four real hin datasets to compare our proposed hine method with state of the art network embedding methods ( i .e . , line and deepwalk ) , which do not consider meta path based proximity . // our experimental results show that our hine method with pcrw as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used .
b	110	[555, 556, 522]	pathselclus is a link based clustering algorithm for hins , in which a user can specify her clustering preference by providing some examples as seeds . // the problem of link prediction on hins has been extensively studied , due to its important applications ( e .g . , in recommender systems ) . // a related problem is entity recommendation in hins , which takes advantage of the different types of relationships in hins to provide better recommendations .
b	110	[552]	pathcount measures the number of meta path instances connecting the two objects , and pathsim is a normalized version of it . // path constrained random walk ( pcrw ) was firstly proposed for the task of relationship retrieval over bibliographic networks . // later , proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on pcrw .
h-	110	[154]	traditional dimensionality reduction techniques typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph . // graph factorization finds a low dimensional representation of a graph through matrix factorization , after representing the graph as an adjacency matrix . // however , since these general techniques are not designed for networks , they do not necessarily preserve the global network structure , as pointed out in .
ho	110	[1]	in this paper , we use pcrw , which extends random walk based proximity to be applied for hins . // second , as pointed out in , deepwalk can only preserve secondorder proximity , leading to poor performance in some tasks , such as link recover and classification , which require first order proximity to be well preserved . // line is a recently proposed embedding approach for largescale networks .
b	110	[41]	the source code of sdne is not available , so this approach can not be reproduced and compared to ours . // similarly , embeds entities in knowledge bases using an innovative neural network architecture and tridnr extends this embedding model to consider features from three aspects of the network : 1 ) network structure , 2 ) node content , and 3 ) label information . // our current work focuses on meta path based proximities , so it does not consider any other information in the hin besides the network structure and the types of nodes and edges .
b	110	[6]	we compare the following network embedding approaches . // deepwalk is a recently proposed social network embedding method ( see section 2 .3 for details ) . // in our experiment settings , we ignore the heterogeneity and directly feed the hins for embedding .
b	371	[560]	can cascades be predicted ? // while many believe that cascades are inherently unpredictable , recent work has shown that some key properties of information cascades , such as size , growth , and shape , can be predicted through a mixture of signals . // indeed , cascades of microblogs/tweets , photos , videos and academic papers are proved to be predictable to some extent .
b	371	[568]	while many believe that cascades are inherently unpredictable , recent work has shown that some key properties of information cascades , such as size , growth , and shape , can be predicted through a mixture of signals . // indeed , cascades of microblogs/tweets , photos , videos and academic papers are proved to be predictable to some extent . // in most of these studies , cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features .
b	371	[560, 561, 562]	these features are indicative but can not be generalized to other platforms or to other types of cascades . // on the other hand , a common set of features , those extracted from the network structure of the cascade , are reported to be predictive by multiple studies . // many of these features are carefully designed based on the prior knowledge from network theory and empirical analyses , such as centrality of nodes , community structures , tie strength , and structural holes .
b	371	[561, 562, 564, 565, 566, 563]	cascade prediction . // cascades of particular types of information are empirically proved to be predictable to some extent , including tweets/microblogs , photos , videos and academic papers . // one treats cascade prediction as a classification problem , which predicts whether or not a piece of information will become popular and wide spread ( above a certain threshold ) .
b	371	[562, 564, 560, 565]	cascades of particular types of information are empirically proved to be predictable to some extent , including tweets/microblogs , photos , videos and academic papers . // one treats cascade prediction as a classification problem , which predicts whether or not a piece of information will become popular and wide spread ( above a certain threshold ) . // the other formulates cascade prediction as a regression problem , which predicts the numerical properties ( e .g . , size ) of a cascade in the future .
b	371	[574, 575]	these factors are utilized for cascade prediction in two fashions . // the first mainly designs generative models of the cascade process based on temporal or structural features , which can be as simple as certain macroscopic distributions ( e .g . , of cascade size over time ) , or stochastic processes that explain the microscopic actions of passing along the information . // these generative models make various strong assumptions and oversimplify the reality .
h-	371	[560, 566]	some of the most predictive features are tied to particular platforms or particular cascades and are hard to be generalized , such as the ones mentioned in the section 1 . // some features are closely related to the structural properties of the social network , such as degree , density , and community structures . // these features could generalize over domains and platforms , but many may still involve arbitrary and hard decisions in computation , such as what to choose from hundreds of community detection algorithms available and how to detect structural holes .
b	371	[6, 1, 37]	our work is also related to the literature of representation learning for graphs . networks are traditionally represented as affiliation matrices or discrete sets of nodes and edges . // modern representation learning methods attempt to represent nodes as high dimensional vectors in a continuous space ( a .k .a . , node embeddings ) so that nodes with similar embedding vectors share similar structural properties ( e .g . , ) . // rather than learning the representation of each node , recent work also attempts to learn the representation of subgraph structures .
b	371	[6]	much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text and image . // for example , deepwalk makes an analogy between the nodes in networks and the words in natural language and uses fixed length random walk paths to stimulate the “ context ” of a node so that node representations can be learned using the same method of learning word representations . // the representation of a graph can then be calculated by averaging the embeddings of all nodes .
ho	371	[560, 580, 565, 581]	features . // we include all structural features that could be generalized across data sets from recent studies of cascade prediction . // these features include : centrality and density . degree of nodes in the cascade graph g and the global network g , average and 90th percentile of the local and global degrees of nodes in g , number of leaf nodes in g , edge density of g , and the number of nodes and edges in the frontier graph of the cascade , which is composed of nodes that are not in g but are neighbors of nodes in g .
h-	371	[1]	node2vec is selected as a representative of node embedding methods . // node2vec is a generalization of deepwalk , which is reported to be outperforming alternative methods such as deepwalk and line . // we generate walks from two sources : ( 1 ) the set of cascade graphs { g } ( 2 ) the global network g .
b	371	[578]	graph kernels . // there are a set of state of the art graph kernels : the shortest path kernel ( sp ) , the random walk kernel ( rw ) , and the weisfeiler lehman subtree kernel ( wl ) . // the rw kernel and the sp kernel are too computationally inefficient , which did not complete after 10 days for a single data set in our experiment .
ho	371	[583]	these features characterize either global or local network properties , and are listed in figure 4 . // in each subfigure , we layout the cascade graphs as data points in the test set to a 2 d space by feeding their vector representations output by the last hidden layer of deepcas to t sne , a commonly used visualization algorithm . // cascade graphs with similar vector representations are placed closely .
b	680	[722]	since then , the nnlm has gradually become the mainstream lm and has rapidly developed . // long short term memory rnn language model ( lstm rnnlm ) was proposed for the difficulty of learning long term dependence . // various improvements were proposed for reducing the cost of training and evaluation and ppl such wt−n+1 wt−2 wt−1 … h u w tanh softmax p ( wt|context ) c ( wt−n+1 ) c ( wt−2 ) c ( wt−1 ) .
h+	680	[36]	the word representation is a by product of lms , which is used to improve other nlp tasks . // based on ffnnlm , two word representation models , cbow and skip gram , were proposed by . // ffnnlm overcomes the curse of dimensions by converting words into low dimensional vectors .
b	680	[722]	lstm rnn language models long short term memory ( lstm ) rnn solved this problem . // introduced lstm into lm and proposed lstm rnnlm . // except for the memory unit and the part of nn , the architecture of lstm rnnlm is almost the same as rnnlm .
b	680	[733]	miyamoto and cho suggested interpolating word feature vectors with character feature vectors extracted from words by bilstm and inputting interpolation vectors into lstm . // proposed a character word lstm rnnlm that directly concatenated character and word level feature vectors and input concatenations into the network . // character aware lm directly uses character level lm as character feature extractor for word level lm .
b	680	[617]	contextual information also was explored . // for example , used the distribution of topics calculated from fixed length blocks of previous words . // wang and cho proposed a new approach to incorporating corpus bag of words ( bow ) context into language modeling .
b	680	[599]	the birnns utilize past and future contexts by processing the input data in both directions . // one of the most popular works of the bidirectional model is the elmo model , a new deep contextualized word representation based on bilstm rnnlms . // the vectors of the embedding layer of a pre trained elmo model is the learned representation vector of words in the vocabulary .
b	680	[734]	similar to human beings , lm with the attention mechanism uses the long history efficiently by selecting useful word representations from them . // first proposed the application of the attention mechanism to nlp tasks ( machine translation in this paper ) . // tran et al . , 2016 and mei et al . , proved that the attention mechanism could improve the performance of rnnlms .
b	680	[670]	transformer is a novel structure based entirely on the attention mechanism , which consists of an encoder and a decoder . // since then , gpt and bert have been proposed . // the main difference is that gpt uses transformer ’ s decoder , and bert uses transformer ’ s encoder .
b	680	[766]	it was proved that the model with brown clustering performed better . // proposed a speed optimal classification , i .e . , a dynamic programming algorithm that determines the classes by minimizing the running time of the model . // hierarchical softmax significantly reduces model parameters without increasing ppl .
b	746	[620]	in comparison , ae based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input . // a notable example is bert , which has been the state of the art pretraining approach . // given the input token sequence , a certain portion of tokens are replaced by a special symbol , and the model is trained to recover the original tokens from the corrupted version .
h-	746	[602, 621]	under a set of fair comparison experiments , xlnet consistently outperforms bert on multiple benchmarks . // related work the idea of permutation based ar modeling has been explored in , but there are several key differences . // previous models are orderless , while xlnet is essentially order aware with positional encodings .
h-	746	[620]	specifically , the input to our model is similar to bert : , where “ sep ” and “ cls ” are two special symbols and “ a ” and “ b ” are the two segments . // although we follow the two segment data format , xlnet large does not use the objective of next sentence prediction as it does not show consistent improvement in our ablation study ( see section 3 .7 ) . // architecturally , different from bert that adds an absolute segment embedding to the word embedding at each position , we extend the idea of relative encodings from transformer xl to also encode the segments .
h-	746	[599]	in comparison , xlnet is able to cover all dependencies in expectation . // approaches like elmo concatenate forward and backward language models in a shallow manner , which is not sufficient for modeling deep interactions between the two directions . // comparison with state of the art results on the test set of race , a reading comprehension task .
ho	746	[700]	experiments 3 .1 pretraining and implementation following bert , we use the bookscorpus and english wikipedia as part of our pretraining data , which have 13gb plain text combined . // in addition , we include giga5 ( 16gb text ) , clueweb 2012 b ( extended from ) , and common crawl for pretraining . // we use heuristics to aggressively filter out short or low quality articles for clueweb 2012 b and common crawl , which results in 19gb and 78gb text respectively .
b	746	[770]	squad is a large scale reading comprehension dataset with two tasks . // squad 1 .1 contains questions that always have a corresponding answer in the given passages , while squad 2 .0 introduces unanswerable questions . // to finetune an xlnet on squad 2 .0 , we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering .
b	746	[724]	according to table 3 , xlnet achieves new state of the art results on all the considered datasets , reducing the error rate by 16 % , 18 % , 5 % , 9 % and 5 % on imdb , yelp 2 , yelp 5 , amazon 2 , and amazon 5 respectively compared to bert . // the glue dataset is a collection of 9 natural language understanding tasks . // the test set labels are removed from the publicly released version , and all the practitioners must submit their predictions on the evaluation server to obtain test set results .
ho	746	[686]	since document ranking , or ad hoc retrieval , mainly concerns the low level representations instead of high level semantics , this dataset serves as a testbed for evaluating the quality of word embeddings . // we use a pretrained xlnet to extract word embeddings for the documents and queries without finetuning , and employ a kernel pooling network to rank the documents . // according to table 5 , xlnet substantially outperforms the other methods , including a bert model that uses the same training procedure as ours .
b	606	[751, 742]	language model pre training has been shown to be effective for improving many natural language processing tasks . // these include sentence level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token level tasks such as named entity recognition and question answering , where models are required to produce fine grained output at the token level . // there are two existing strategies for applying pre trained language representations to downstream tasks : feature based and fine tuning .
b	606	[707]	the major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre training . // for example , in openai gpt , the authors use a left toright architecture , where every token can only attend to previous tokens in the self attention layers of the transformer . // such restrictions are sub optimal for sentence level tasks , and could be very harmful when applying finetuning based approaches to token level tasks such as question answering , where it is crucial to incorporate context from both directions .
b	606	[717, 36]	pre trained word embeddings are an integral part of modern nlp systems , offering significant improvements over embeddings learned from scratch . // to pretrain word embedding vectors , left to right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context . // these approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .
b	606	[599]	to train sentence representations , prior work has used objectives to rank candidate next sentences , left to right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives . // elmo and its predecessor generalize traditional word embedding research along a different dimension . // they extract context sensitive features from a left to right and a right to left language model .
ho	606	[658]	a “ sequence ” refers to the input token sequence to bert , which may be a single sentence or two sentences packed together . // we use wordpiece embeddings with a 30 ,000 token vocabulary . // the first token of every sequence is always a special classification token ( ) .
b	606	[591]	the effect of model size is explored more thoroughly in section 5 .2 . // 4 .2 squad v1 .1 the stanford question answering dataset ( squad v1 .1 ) is a collection of 100k crowdsourced question/answer pairs . // given a question and a passage from 9the glue data set distribution does not include the test labels , and we only made a single glue evaluation server submission for each of bertbase and bertlarge .
b	606	[608]	// for example , the largest transformer explored in vaswani et al . ( 2017 ) is ( l=6 , h=1024 , a=16 ) with 100m parameters for the encoder , and the largest transformer we have found in the literature is ( l=64 , h=512 , a=2 ) with 235m parameters . // by contrast , bert base contains 110m parameters and bert large contains 340m parameters .
b	684	[306, 614, 687, 307, 713, 772, 641]	suffixes likewise refer to character n grams which end with the end of word character . // , and we posit that further gains could be achieved by employing highway layers on top of existing cnn architectures . // we also anecdotally note that ( 1 ) having one to two highway layers was important , but more highway layers generally resulted in similar performance ( though this may depend on the size of the datasets ) , ( 2 ) having more convolutional layers before max pooling did not help , and ( 3 ) highway layers did not improve models that only used word embeddings as inputs .
h+	684	[617, 664]	character level models obviate the need for morphological tagging or manual feature engineering , and have the attractive property of being able to generate novel words . // however they are generally outperformed by word level models . // outside of language modeling , improvements have been reported on part of speech tagging and named entity recognition by representing a word as a concatenation of its word embedding and an output from a characterlevel cnn , and using the combined representation as features in a conditional random field ( crf ) .
b	670	[599]	first , it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer . // recent research has looked at various objectives such as language modeling , machine translation , and discourse coherence , with each method outperforming the others on different tasks . // second , there is no consensus on the most effective way to transfer these learned representations to the target task . existing techniques involve a combination of making task specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives .
h-	670	[732]	second , there is no consensus on the most effective way to transfer these learned representations to the target task . // existing techniques involve a combination of making task specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives . // these uncertainties have made it difficult to develop effective semi supervised learning approaches for language processing .
h+	670	[773]	subsequently , we adapt these parameters to a target task using the corresponding supervised objective . // for our model architecture , we use the transformer , which has been shown to perform strongly on various tasks such as machine translation , document generation , and syntactic parsing . // this model choice provides us with a more structured memory for handling long term dependencies in text , compared to alternatives like recurrent networks , resulting in robust transfer performance across diverse tasks .
b	670	[739]	our general task agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task , significantly improving upon the state of the art in 9 out of the 12 tasks studied . // for instance , we achieve absolute improvements of 8 .9 % on commonsense reasoning ( stories cloze test ) , 5 .7 % on question answering ( race ) , 1 .5 % on textual entailment ( multinli ) and 5 .5 % on the recently introduced glue multi task benchmark . // we also analyzed zero shot behaviors of the pre trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks .
b	670	[589, 632]	semi supervised learning for nlp our work broadly falls under the category of semi supervised learning for natural language . // this paradigm has attracted significant interest , with applications to tasks like sequence labeling or text classification . // the earliest approaches used unlabeled data to compute word level or phrase level statistics , which were then used as features in a supervised model .
b	670	[779, 64, 668, 663, 728, 675, 744, 709]	recent approaches have investigated learning and utilizing more than word level semantics from unlabeled data . // phrase level or sentence level embeddings , which can be trained using an unlabeled corpus , have been used to encode text into suitable vector representations for various target tasks . // unsupervised pre training unsupervised pre training is a special case of semi supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective .
h+	670	[667]	subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks . // in recent work , the method has been used to help train deep neural networks on various tasks like image classification , speech recognition , entity disambiguation and machine translation . // the closest line of work to ours involves pre training a neural network using a language modeling objective and then fine tuning it on a target task with supervision .
h-	670	[636]	// dai et al . and howard and ruder follow this method to improve text classification . // however , although the pre training phase helps capture some linguistic information , their usage of lstm models restricts their prediction ability to a short range .
b	670	[707]	these parameters are trained using stochastic gradient descent . // in our experiments , we use a multi layer transformer decoder for the language model , which is a variant of the transformer . // this model applies a multi headed self attention operation over the input context tokens followed by position wise feedforward layers to produce an output distribution over target tokens : h0 = uwe + wp hl = transformer_block ( hl−1 ) ∀i ∈ p ( u ) = softmax ( hnwt e ) ( 2 ) where u = ( u−k , .
b	670	[599]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[644]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[731]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[603]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
ho	670	[619]	we also employed a modified version of l2 regularization proposed in , with w = 0 .01 on all non bias or gain weights . // for the activation function , we used the gaussian error linear unit ( gelu ) . // we used learned position embeddings instead of the sinusoidal version proposed in the original work .
b	670	[603]	classification finally , we also evaluate on two different text classification tasks . // the corpus of linguistic acceptability ( cola ) contains expert judgements on whether a sentence is grammatical or not , and tests the innate linguistic bias of trained models . // the stanford sentiment treebank ( sst 2 ) , on the other hand , is a standard binary classification task .
h-	599	[721]	the addition of elmo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions . // for tasks where direct comparisons are possible , elmo outperforms cove , which computes contextualized representations using a neural machine translation encoder . // finally , an analysis of both elmo and cove reveals that deep representations outperform those derived from just the top layer of an lstm .
h-	599	[631, 633]	finally , an analysis of both elmo and cove reveals that deep representations outperform those derived from just the top layer of an lstm . our trained models and code are publicly available , and we expect that elmo will provide similar gains for many other nlp problems . // related work due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state ofthe art nlp architectures , including for question answering , textual entailment and semantic role labeling . // however , these approaches for learning word vectors only allow a single contextindependent representation for each word .
b	599	[763]	context2vec uses a bidirectional long short term memory ( lstm ) to encode the context around a pivot word . // other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( mt ) system or an unsupervised language model . // both of these approaches benefit from large datasets , although the mt approach is limited by the size of parallel corpora .
b	599	[745, 699, 748]	// recent state of the art neural language models compute a context independent token representation via token embeddings or a cnn over characters then pass it through l layers of forward lstms . // at each position k , each lstm layer outputs a context dependent representation .
b	599	[717]	in contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary . // after training for 10 epochs on the 1b word benchmark , the average forward and backward perplexities is 39 .7 , compared to 30 .0 for the forward cnn big lstm . // generally , we found the forward and backward perplexities to be approximately equal , with the backward value slightly lower .
ho	599	[635]	// to do so , we first use the bilm to compute representations for all words in semcor 3 .0 , our training corpus , and then take the average representation for each sense . // at test time , we again use the bilm to compute representations for a given target word and take the nearest neighbor sense from the training set , falling back to the first sense from wordnet for lemmas not observed during training .
b	167	[36]	this evaluation scheme favors models that produce dimensions of meaning , thereby capturing the multi clustering idea of distributed representations . // the two main model families for learning word vectors are : 1 ) global matrix factorization methods , such as latent semantic analysis ( lsa ) and 2 ) local context window methods , such as the skip gram model of . //
b	167	[695]	when x is symmetric , w are equivalent and differ only as a result of their random initializations ; the two sets of vectors should perform equivalently . // on the other hand , there is evidence that for certain types of neural networks , training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results . // to demonstrate the scalability of the model , we also trained it on a much larger sixth corpus , containing 840 billion tokens of web data , but in this case we did not lowercase the vocabulary , so the results are not directly comparable .
b	36	[125, 781, 622, 676, 628, 776, 126]	this idea has since been applied to statistical language modeling with considerable success . // the follow up work includes applications to automatic speech recognition and machine translation , and a wide range of nlp tasks . // recently , mikolov et al . introduced the skip gram model , an efficient method for learning highquality vector representations of words from large amounts of unstructured text data .
b	36	[622]	therefore , using vectors to represent the whole phrases makes the skip gram model considerably more expressive . // other techniques that aim to represent meaning of sentences by composing the word vectors , such as the recursive autoencoders , would also benefit from using phrase vectors instead of the word vectors . // the extension from word based to phrase based models is relatively simple .
h+	36	[716]	it has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models . // negative sampling an alternative to the hierarchical softmax is noise contrastive estimation ( nce ) , which was introduced by gutmann and hyvarinen and applied to language modeling by mnih and teh . // nce posits that a good model should be able to differentiate data from noise by means of logistic regression .
ho	36	[690]	combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text , while having minimal computational complexity . // our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix vector operations . // we made the code for training the word and phrase vectors based on the techniques described in this paper available as an open source project4 .
b	733	[703]	the total number of epochs is fixed to 13/39 . // during training , 25 % of the neurons are dropped for the small model and 50 % for the large model . // the weights are randomly initialized to small values ( between 0 .1 and 0 .1 for the small model and between 0 .05 and 0 .05 for the large model ) based on a uniform distribution .
b	777	[740, 762, 655]	the primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge related words . // in particular , for applications such as question answering and dialogue modeling , these words are of our main interest . // specifically , in the recurrent neural network language model ( rnnlm ) the computational complexity is linearly dependent on the number of vocabulary words .
b	777	[760, 613]	kgs are managed and updated in a similar way that wikipedia pages are managed to date . // the kg embedding methods , which is an extension of word embedding techniques from neural language models , provide distributed representations for the entities in the kg . // in addition , the graph can be traversed for reasoning .
b	777	[674, 648]	related work language modeling is among the most important challenges in natural language processing and understanding . // beyond its usage as a standalone application , it has been an indispensable component in many language/speech tasks such as speech recognition , machine translation , and dialogue systems . // incorporating a language model into such downstream tasks leads to remarkable performance improvements , e .g . , by filtering out many grammatically correct but statistically unlikely outcomes .
h+	777	[390]	there have been remarkable advances in language modeling research based on neural networks . // in particular , the rnnlms are interesting for their ability to take advantage of longer term temporal dependencies without a strong conditional independence assumption . // we do not investigate the reasoning ability in this paper but highlight this example because explicit representation of facts would help handling such examples .
h-	777	[716]	there have been many efforts to speedup the language models so that it can cover a larger vocabulary . // these methods approximate the softmax output using hierarchical softmax , importance sampling , noise contrastive estimation , etc . // however , although helpful to mitigate the computational problem , this approaches still suffer from the statistical problem due to rare or unknown words .
b	777	[305]	modeled the topic as a latent variable and proposed an em based approach . // in , the topic features are learned by latent dirichlet allocation ( lda ) . // our knowledge memory is also related to the recent literature on neural networks with external memory which is applied to question answering and reading comprehension .
