h-	587	[2]	as a result , such a traditional routine often makes many network analytic tasks computationally expensive and intractable over largescale networks . // taking community detection as an example , most existing algorithms involve calculating the spectral decomposition of a matrix with at least quadratic time complexity with respect to the number of vertices . // this computational overhead makes algorithms hard to scale to large scale networks with millions of vertices .
h-	587	[3]	the idea is to find a low dimensional manifold structure hidden in the high dimensional data geometry reflected by the constructed graph , so that connected nodes are kept closer to each other in the new embedding space . // isomap , locally linear embedding ( lle ) and laplacian eigenmap are examples of algorithms based on this rationale . // however , graph embedding algorithms are designed on i .i .d . data mainly for dimensionality reduction purpose .
h-	587	[4]	the idea is to find a low dimensional manifold structure hidden in the high dimensional data geometry reflected by the constructed graph , so that connected nodes are kept closer to each other in the new embedding space . // isomap , locally linear embedding ( lle ) and laplacian eigenmap are examples of algorithms based on this rationale . // however , graph embedding algorithms are designed on i .i .d . data mainly for dimensionality reduction purpose .
h+	587	[6]	since 2008 , significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks . // many nrl algorithms , e .g . , , , , , have been proposed to embed existing networks , showing promising performance for various applications . // these algorithms embed a network into a latent , lowdimensional space that preserves structure proximity and attribute affinity , such that the original vertices of the network can be represented as low dimensional vectors .
h+	587	[7]	since 2008 , significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks . // many nrl algorithms , e .g . , , , , , have been proposed to embed existing networks , showing promising performance for various applications . // these algorithms embed a network into a latent , lowdimensional space that preserves structure proximity and attribute affinity , such that the original vertices of the network can be represented as low dimensional vectors .
h+	587	[8]	since 2008 , significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks . // many nrl algorithms , e .g . , , , , , have been proposed to embed existing networks , showing promising performance for various applications . // these algorithms embed a network into a latent , lowdimensional space that preserves structure proximity and attribute affinity , such that the original vertices of the network can be represented as low dimensional vectors .
b	587	[10]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[11]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[12]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[2]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[14]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[15]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[17]	the resulting compact , low dimensional vector representations can be then taken as features to any vector based machine learning algorithms . // this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems .
b	587	[15]	this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems . // challenges .
b	587	[18]	this paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space , such as node classification , , link prediction , , clustering , recommendation , , similarity search , and visualization . // using vector representation to represent complex networks has now been gradually advanced to many other domains , such as point of interest recommendation in urban computing , and knowledge graph search in knowledge engineering and database systems . // challenges .
b	587	[20]	to learn informative vertex representations , network representation learning should preserve network structure , such that vertices similar/close to each other in the original structure space should also be represented similarly in the learned vector space . // however , as stated in , , the structure level similarity between vertices is reflected not only at the local neighborhood structure but also at the more global community structure . // therefore , the local and global structure should be simultaneously preserved in network representation learning .
b	587	[21]	a few graph embedding and representation learning related surveys exist in the recent literature . // the first is , which reviews a few representative methods for network representation learning and visits some key concepts aroundthe idea of representation learning and its connections to other related field such as dimensionality reduction , deep learning , and network science . // the other two surveys , focus on reviewing graph embedding techniques aiming to preserve only network structure .
b	587	[22]	the first is , which reviews a few representative methods for network representation learning and visits some key concepts aroundthe idea of representation learning and its connections to other related field such as dimensionality reduction , deep learning , and network science . // the other two surveys , focus on reviewing graph embedding techniques aiming to preserve only network structure . // recently , , extend to cover work leveraging other side information , such as vertex attributes and/or vertex labels , to harness representation learning .
b	587	[24]	the other two surveys , focus on reviewing graph embedding techniques aiming to preserve only network structure . // recently , , extend to cover work leveraging other side information , such as vertex attributes and/or vertex labels , to harness representation learning . // in summary , existing surveys have the following limitations .
b	587	[25]	the other two surveys , focus on reviewing graph embedding techniques aiming to preserve only network structure . // recently , , extend to cover work leveraging other side information , such as vertex attributes and/or vertex labels , to harness representation learning . // in summary , existing surveys have the following limitations .
b	587	[1]	definition 2 ( first order proximity ) . // the first order proximity is the local pairwise proximity between two connected vertices . // for each vertex pair ( vi , vj ) , if ( vi , vj ) ∈ e , the first order proximity between vi and vj is wij ; otherwise , the first order proximity between vi and vj is 0 .
b	587	[26]	for each vertex pair ( vi , vj ) , the second order proximity is determined by the number of common neighbors shared by the two vertices , which can also be measured by the 2 step transition probability from vi to vj equivalently . // compared with the second order proximity , the highorder proximity captures more global structure , which explores k step ( k ≥ 3 ) relations between each pair of vertices . // for each vertex pair ( vi , vj ) , the higherorder proximity is measured by the k step ( k ≥ 3 ) transition probability from vertex vi to vertex vj , which can also be reflected by the number of k step ( k ≥ 3 ) paths from vi to vj .
b	587	[27]	the intracommunity proximity is the pairwise proximity between vertices in a same community . // many networks have community structure , where vertex vertex connections within the same community are dense , but connections to vertices outside the community are sparse . // as cluster structure , a community preserves certain kinds of common properties of vertices within it .
b	587	[27]	the intracommunity proximity is the pairwise proximity between vertices in a same community . // many networks have community structure , where vertex vertex connections within the same community are dense , but connections to vertices outside the community are sparse . // as cluster structure , a community preserves certain kinds of common properties of vertices within it .
ho	587	[20]	in addition to network structure , vertex attributes can provide direct evidence to measure contentlevel similarity between vertices . // as shown in , , , vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations . // vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations .
ho	587	[28]	in addition to network structure , vertex attributes can provide direct evidence to measure contentlevel similarity between vertices . // as shown in , , , vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations . // vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations .
ho	587	[29]	vertex labels provide direct information about the semantic categorization of each network vertex to certain classes or groups . // vertex labels are strongly influenced by and inherently correlated to both network structure and vertex attributes . // though vertex labels are usually partially observed , when coupled with network structure and vertex attributes , they encourage a network structure and vertex attribute consistent labeling , and help learn informative and discriminative vertex representations .
b	587	[8]	in general , there are three main types of information sources : network structure , vertex attributes , and vertex labels . // most of the unsupervised nrl algorithms focus on preserving network structure for learning vertex representations , and only a few algorithms ( e .g . , tadw , hsca ) attempt to leverage vertex attributes . // by contrast , under the semi supervised learning setting , half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations .
b	587	[30]	most of the unsupervised nrl algorithms focus on preserving network structure for learning vertex representations , and only a few algorithms ( e .g . , tadw , hsca ) attempt to leverage vertex attributes . // by contrast , under the semi supervised learning setting , half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations . on both settings , most of the algorithms focus on preserving microscopic structure , while very few algorithms ( e .g . , mnmf , dp , harp ) attempt to take advantage of the mesoscopic and macroscopic structure . // approaches to network representation learning in the above two different settings can be summarized into five categories from algorithmic perspectives .
b	587	[31]	most of the unsupervised nrl algorithms focus on preserving network structure for learning vertex representations , and only a few algorithms ( e .g . , tadw , hsca ) attempt to leverage vertex attributes . // by contrast , under the semi supervised learning setting , half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations . on both settings , most of the algorithms focus on preserving microscopic structure , while very few algorithms ( e .g . , mnmf , dp , harp ) attempt to take advantage of the mesoscopic and macroscopic structure . // approaches to network representation learning in the above two different settings can be summarized into five categories from algorithmic perspectives .
b	587	[7]	matrix factorization based methods represent the connections between network vertices in the form of a matrix and use matrix factorization to obtain the embeddings . // different types of matrices are constructed to preserve network structure , such as the k step transition probability matrix , the modularity matrix , or the vertex context matrix . // by assuming that such high dimensional vertex representations are only affected by a small quantity of latent factors , matrix factorization is used to embed the highdimensional vertex representations into a latent , low dimensional structure preserving space .
h-	587	[33]	factorization strategies vary across different algorithms according to their objectives . // for example , in the modularity maximization method , eigen decomposition is performed on the modularity matrix to learn community indicative vertex representations ; in the tadw algorithm , inductive matrix factorization is carried out on the vertexcontext matrix to simultaneously preserve vertex textual features and network structure in the learning of vertex representations . // although matrix factorization based methods have been proved effective in learning informative vertex representations , the scalability is a major bottleneck because carrying out factorization on a matrix with millions of rows and columns is memory intensive and computationally expensive or , sometime , even infeasible .
h-	587	[7]	factorization strategies vary across different algorithms according to their objectives . // for example , in the modularity maximization method , eigen decomposition is performed on the modularity matrix to learn community indicative vertex representations ; in the tadw algorithm , inductive matrix factorization is carried out on the vertexcontext matrix to simultaneously preserve vertex textual features and network structure in the learning of vertex representations . // although matrix factorization based methods have been proved effective in learning informative vertex representations , the scalability is a major bottleneck because carrying out factorization on a matrix with millions of rows and columns is memory intensive and computationally expensive or , sometime , even infeasible .
b	587	[35]	by performing truncated random walks , an information network is transformed into a collection of vertex sequences , in which , the occurrence frequency of a vertex context pair measures the structural distance between them . // borrowing the idea of word representation learning , , vertex representations are then learned by using each vertex to predict its contexts . // deepwalk is the pioneer work in using random walks to learn vertex representations .
b	587	[36]	by performing truncated random walks , an information network is transformed into a collection of vertex sequences , in which , the occurrence frequency of a vertex context pair measures the structural distance between them . // borrowing the idea of word representation learning , , vertex representations are then learned by using each vertex to predict its contexts . // deepwalk is the pioneer work in using random walks to learn vertex representations .
b	587	[6]	borrowing the idea of word representation learning , , vertex representations are then learned by using each vertex to predict its contexts . // deepwalk is the pioneer work in using random walks to learn vertex representations . // node2vec further exploits a biased random walk strategy to capture more global structure .
b	587	[38]	node2vec further exploits a biased random walk strategy to capture more global structure . // as the extensions of the structure preserving only version , algorithms like ddrw , gene and semne incorporate vertex labels with network structure to harness representation learning , ppne imports vertex attributes , and tri dnr enforces the model with both vertex labels and attributes . // as these models can be trained in an online manner , they have great potential to scale up .
b	587	[39]	node2vec further exploits a biased random walk strategy to capture more global structure . // as the extensions of the structure preserving only version , algorithms like ddrw , gene and semne incorporate vertex labels with network structure to harness representation learning , ppne imports vertex attributes , and tri dnr enforces the model with both vertex labels and attributes . // as these models can be trained in an online manner , they have great potential to scale up .
b	587	[40]	node2vec further exploits a biased random walk strategy to capture more global structure . // as the extensions of the structure preserving only version , algorithms like ddrw , gene and semne incorporate vertex labels with network structure to harness representation learning , ppne imports vertex attributes , and tri dnr enforces the model with both vertex labels and attributes . // as these models can be trained in an online manner , they have great potential to scale up .
b	587	[41]	node2vec further exploits a biased random walk strategy to capture more global structure . // as the extensions of the structure preserving only version , algorithms like ddrw , gene and semne incorporate vertex labels with network structure to harness representation learning , ppne imports vertex attributes , and tri dnr enforces the model with both vertex labels and attributes . // as these models can be trained in an online manner , they have great potential to scale up .
b	587	[1]	different from approaches that use matrix or random walk to capture network structure , the edge modeling based methods directly learn vertex representations from vertex vertex connections . // for capturing the firstorder and second order proximity , line models a joint probability distribution and a conditional probability distribution , respectively , on connected vertices . // to learn the representations of linked documents , lde models the document document relationships by maximizing the conditional probability between connected documents .
b	587	[42]	for capturing the firstorder and second order proximity , line models a joint probability distribution and a conditional probability distribution , respectively , on connected vertices . // to learn the representations of linked documents , lde models the document document relationships by maximizing the conditional probability between connected documents . // prbm adapts the rbm model to linked data by making the hidden rbm representations of connected vertices similar to each other .
b	587	[43]	to learn the representations of linked documents , lde models the document document relationships by maximizing the conditional probability between connected documents . // prbm adapts the rbm model to linked data by making the hidden rbm representations of connected vertices similar to each other . // graphgan adopts generative adversarial nets ( gan ) to accurately model the vertex connectivity probability .
h+	587	[44]	prbm adapts the rbm model to linked data by making the hidden rbm representations of connected vertices similar to each other . // graphgan adopts generative adversarial nets ( gan ) to accurately model the vertex connectivity probability . // edge modeling based methods are more efficient compared to matrix factorization and random walk based methods .
h+	587	[45]	prbm adapts the rbm model to linked data by making the hidden rbm representations of connected vertices similar to each other . // graphgan adopts generative adversarial nets ( gan ) to accurately model the vertex connectivity probability . // edge modeling based methods are more efficient compared to matrix factorization and random walk based methods .
b	587	[47]	deep learning based methods . // to extract complex structure features and learn deep , highly nonlinear vertex representations , the deep learning techniques , are also applied to network representation learning . // for example , dngr applies the stacked denoising autoencoders ( sdae ) on the high dimensional matrix representations to learn deep low dimensional vertex representations .
b	587	[9]	to extract complex structure features and learn deep , highly nonlinear vertex representations , the deep learning techniques , are also applied to network representation learning . // for example , dngr applies the stacked denoising autoencoders ( sdae ) on the high dimensional matrix representations to learn deep low dimensional vertex representations . // sdne uses a semi supervised deep autoencoder model to model non linearity in network structure .
b	587	[47]	to extract complex structure features and learn deep , highly nonlinear vertex representations , the deep learning techniques , are also applied to network representation learning . // for example , dngr applies the stacked denoising autoencoders ( sdae ) on the high dimensional matrix representations to learn deep low dimensional vertex representations . // sdne uses a semi supervised deep autoencoder model to model non linearity in network structure .
h-	587	[46]	for example , dngr applies the stacked denoising autoencoders ( sdae ) on the high dimensional matrix representations to learn deep low dimensional vertex representations . // sdne uses a semi supervised deep autoencoder model to model non linearity in network structure . // deep learning based methods have theability to capture non linearity in networks , but their computational time cost is usually high .
b	587	[31]	hybrid methods . // some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network .
b	587	[5]	hybrid methods . // some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network .
b	587	[32]	some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network . // we summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in table 3 .
b	587	[6]	some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network . // we summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in table 3 .
b	587	[37]	some other methods make use of a mixture of above methods to learn vertex representations . for example , dp enhances spectral embedding and deepwalk with the degree penalty principle to preserve the macroscopic scale free property . // harp takes advantage of random walk based methods ( deepwalk and node2vec ) and edge modeling based method ( line ) to learn vertex representations from small sampled networks to the original network . // we summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in table 3 .
b	587	[6]	deepwalk . // deepwalk generalizes the idea of the skip gram model , that utilizes word context in sentences to learn latent representations of words , to the learning of latent vertex representations in networks , by making an analogy between natural language sentence and short random walk sequence . // given a random walk sequence with length l , { v1 , v2 , · · · , vl } , following skipgram , deepwalk learns the representation of vertex vi by using it to predict its context vertices , which is achieved by the optimization problem : min f − log pr ( { vi−t , · · · , vi+t } \ vi|f ( vi ) ) , ( 1 ) where { vi−t , · · · , vi+t } \ vi are the context vertices of vertex vi within t window size
b	587	[35]	deepwalk . // deepwalk generalizes the idea of the skip gram model , that utilizes word context in sentences to learn latent representations of words , to the learning of latent vertex representations in networks , by making an analogy between natural language sentence and short random walk sequence . // given a random walk sequence with length l , { v1 , v2 , · · · , vl } , following skipgram , deepwalk learns the representation of vertex vi by using it to predict its context vertices , which is achieved by the optimization problem : min f − log pr ( { vi−t , · · · , vi+t } \ vi|f ( vi ) ) , ( 1 ) where { vi−t , · · · , vi+t } \ vi are the context vertices of vertex vi within t window size
b	587	[36]	deepwalk . // deepwalk generalizes the idea of the skip gram model , that utilizes word context in sentences to learn latent representations of words , to the learning of latent vertex representations in networks , by making an analogy between natural language sentence and short random walk sequence . // given a random walk sequence with length l , { v1 , v2 , · · · , vl } , following skipgram , deepwalk learns the representation of vertex vi by using it to predict its context vertices , which is achieved by the optimization problem : min f − log pr ( { vi−t , · · · , vi+t } \ vi|f ( vi ) ) , ( 1 ) where { vi−t , · · · , vi+t } \ vi are the context vertices of vertex vi within t window size
b	587	[6]	grarep . // following the idea of deepwalk , grarep extends the skip gram model to capture the high order proximity , i .e . , vertices sharing common k step neighbors ( k ≥ 1 ) should have similar latent representations . // specifically , for each vertex , grarep defines its kstep neighbors ( k ≥ 1 ) as context vertices , and for each 1 ≤ k ≤ k , to learn k step vertex representations , grarep employs the matrix factorization version of skip gram : h u k , σ k , v k i = sv d ( x k ) . ( 5 ) where xk is the log k step transition probability matrix . the k step representation for vertex vi is constructed as the ith row of matrix u k d ( σk d ) 1 2 , where u k d is the first d columns of u k and σ k d is the diagonal matrix composed of the top d singular values .
b	587	[26]	grarep . // following the idea of deepwalk , grarep extends the skip gram model to capture the high order proximity , i .e . , vertices sharing common k step neighbors ( k ≥ 1 ) should have similar latent representations . // specifically , for each vertex , grarep defines its kstep neighbors ( k ≥ 1 ) as context vertices , and for each 1 ≤ k ≤ k , to learn k step vertex representations , grarep employs the matrix factorization version of skip gram : h u k , σ k , v k i = sv d ( x k ) . ( 5 ) where xk is the log k step transition probability matrix . the k step representation for vertex vi is constructed as the ith row of matrix u k d ( σk d ) 1 2 , where u k d is the first d columns of u k and σ k d is the diagonal matrix composed of the top d singular values .
b	587	[9]	deep neural networks for graph representations ( dngr ) . // to overcome the weakness of truncated random walks in exploiting vertex contextual information , i .e . , the difficulty in capturing correct contextual information for vertices at the boundary of sequences and the difficulty in determining the walk length and the number of walks , dngr utilizes the random surfing model to capture contextual relatedness between each pair of vertices and preserves them into |v | dimensional vertex representations x . // to extract complex features and model non linearities , dngr applies the stacked denoising autoencoders ( sdae ) to the high dimensional vertex representations x to learn deep low dimensional vertex representations .
b	587	[19]	structural deep network embedding ( sdne ) . // sdne is a deep learning based approach that uses a semi supervised deep autoencoder model to capture non linearity in network structure . // in the unsupervised component , sdne learns the second order proximity preserving vertex representations via reconstructing the |v | dimensional vertex adjacent matrix representations , which tries to minimize l2nd = x |v | i=1 k ( r ( 0 ) vi − rˆ ( 0 ) vi ) bik , ( 6 ) where r ( 0 ) vi = si : is the input representation and rˆ ( 0 ) vi is the reconstructed representation
h+	587	[37]	node2vec . // in contrast to the rigid strategy of defining neighborhood ( context ) for each vertex , node2vec designs a flexible neighborhood sampling strategy , i .e . , biased random walk , which smoothly interpolates between two extreme sampling strategies , i .e . , breadth first sampling ( bfs ) and depth first sampling ( dfs ) . // the biased random walk exploited in node2vec can better preserve both the secondorder and high order proximity
b	587	[48]	high order proximity preserved embedding ( hope ) . // hope learns vertex representations that capture the asymmetric high order proximity in directed networks . // in undirected networks , the transitivity is symmetric , but it is asymmetric in directed networks .
b	587	[50]	to preserve the asymmetric transitivity , hope learns two vertex embedding vectors u s , ut ∈ r |v |×d , which is called source and target embedding vectors , respectively . // after constructing the high order proximity matrix s from four proximity measures , i .e . , katz index , rooted pagerank , common neighbors and adamicadar . hope learns vertex embeddings by solving the following matrix factorization problem : min us , ut ks − u s · u tt k 2 f . // asymmetric proximity preserving graph embedding ( app ) .
b	587	[51]	after constructing the high order proximity matrix s from four proximity measures , i .e . , katz index , rooted pagerank , common neighbors and adamicadar . hope learns vertex embeddings by solving the following matrix factorization problem : min us , ut ks − u s · u tt k 2 f . // asymmetric proximity preserving graph embedding ( app ) . app is another nrl algorithm designed to capture asymmetric proximity , by using a monte carlo approach to approximate the asymmetric rooted pagerank proximity . // similar to hope , app has two representations for each vertex vi , the one as a source role r s vi and the other as a target role rtvi .
b	587	[50]	after constructing the high order proximity matrix s from four proximity measures , i .e . , katz index , rooted pagerank , common neighbors and adamicadar . hope learns vertex embeddings by solving the following matrix factorization problem : min us , ut ks − u s · u tt k 2 f . // asymmetric proximity preserving graph embedding ( app ) . app is another nrl algorithm designed to capture asymmetric proximity , by using a monte carlo approach to approximate the asymmetric rooted pagerank proximity . // similar to hope , app has two representations for each vertex vi , the one as a source role r s vi and the other as a target role rtvi .
b	587	[45]	graphgan learns vertex representations by modeling the connectivity behavior through an adversarial learning framework . // inspired by gan ( generative adversarial nets ) , graphgan works through two components : ( i ) generator g ( v|vc ) , which fits the distribution of the vertices connected to vc across v and generates the likely connected vertices , and ( ii ) discriminator d ( v , vc ) , which outputs a connecting probability for the vertex pair ( v , vc ) , to differentiate the vertex pairs generated by g ( v|vc ) from the ground truth . // g ( v|vc ) and d ( v , vc ) competes in a way that g ( v|vc ) tries to fit the true connecting distribution as much as possible and generates fake connected vertex pairs to fool d ( v , vc ) , while d ( v , vc ) tries to increase its discriminative power to distinguish the vertex pairs generated by g ( v|vc ) from the ground truth .
b	587	[1]	the proximity preserved by microscopic structure preserving nrl algorithms is summarized in table 4 . // most algorithms in this category preserve the secondorder and high order proximity , whereas only line , sdne and graphgan consider the first order proximity . // from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure .
b	587	[19]	the proximity preserved by microscopic structure preserving nrl algorithms is summarized in table 4 . // most algorithms in this category preserve the secondorder and high order proximity , whereas only line , sdne and graphgan consider the first order proximity . // from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure .
b	587	[6]	most algorithms in this category preserve the secondorder and high order proximity , whereas only line , sdne and graphgan consider the first order proximity . // from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure . // grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up .
b	587	[37]	most algorithms in this category preserve the secondorder and high order proximity , whereas only line , sdne and graphgan consider the first order proximity . // from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure . // grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up .
b	587	[51]	most algorithms in this category preserve the secondorder and high order proximity , whereas only line , sdne and graphgan consider the first order proximity . // from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure . // grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up .
h-	587	[48]	from the methodology perspective , deepwalk , node2vec and app employ random walks to capture vertex neighborhood structure . // grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up . // line and graphgan directly model the connectivity behavior , while deep learning based methods ( dngr and sdne ) learn non linear vertex representations .
b	587	[1]	grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up . // line and graphgan directly model the connectivity behavior , while deep learning based methods ( dngr and sdne ) learn non linear vertex representations . // structural role proximity preserving nrl .
b	587	[44]	grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up . // line and graphgan directly model the connectivity behavior , while deep learning based methods ( dngr and sdne ) learn non linear vertex representations . // structural role proximity preserving nrl .
b	587	[19]	grarep and hope are realized by performing factorization on a |v | × |v | scale matrix , making them hard to scale up . // line and graphgan directly model the connectivity behavior , while deep learning based methods ( dngr and sdne ) learn non linear vertex representations . // structural role proximity preserving nrl .
b	587	[52]	struct2vec . // struct2vec first encodes the vertex structural role similarity into a multilayer graph , where the weights of edges at each layer are determined by the structural role difference at the corresponding scale . // deepwalk is then performed on the multilayer graph to learn vertex representations , such that vertices close to each other in the multilayer graph ( with high structural role similarity ) are embedded closely in the new representation space .
b	587	[6]	struct2vec first encodes the vertex structural role similarity into a multilayer graph , where the weights of edges at each layer are determined by the structural role difference at the corresponding scale . // deepwalk is then performed on the multilayer graph to learn vertex representations , such that vertices close to each other in the multilayer graph ( with high structural role similarity ) are embedded closely in the new representation space . // for each vertex pair ( vi , vj ) , considering their k hop neighborhood formed by their neighbors within k steps , their structural distance at scale k , dk ( vi , vj ) , is defined as dk ( vi , vj ) = dk−1 ( vi , vj ) + g ( s ( rk ( vi ) ) , s ( rk ( vj ) ) ) , ( 14 ) where rk ( vi ) is the set of vertices in its k hop neighborhood , s ( rk ( vi ) ) is the ordered degree sequence of the vertices in rk ( vi ) , and g ( s ( rk ( vi ) ) , s ( rk ( vj ) ) ) is the distance between the ordered degree sequences s ( rk ( vi ) ) and s ( rk ( vj ) ) .
b	587	[54]	structural and neighborhood similarity preserving network embedding ( sns ) . // sns enhances the random walk based method with structural role proximity . // to preserve vertex structural roles , sns represents each vertex as a graphlet degree vector with each element being the number of times the given vertex is touched by the corresponding orbit of graphlets .
b	587	[33]	thus , the problem boils down to one classical network analytic task—community detection—that aims to discover a set of communities with denser within group connections than between group connections . // three clustering techniques , including modularity maximization , spectral clustering and edge clustering are employed to discover latent social dimensions . each social dimension describes the likelihood of a vertex belonging to a plausible affiliation . //
b	587	[55]	thus , the problem boils down to one classical network analytic task—community detection—that aims to discover a set of communities with denser within group connections than between group connections . // three clustering techniques , including modularity maximization , spectral clustering and edge clustering are employed to discover latent social dimensions . each social dimension describes the likelihood of a vertex belonging to a plausible affiliation . //
b	587	[30]	modularized nonnegative matrix factorization ( mnmf ) . // m nmf augments the second order and highorder proximity with broader community structure to learn more informative vertex embeddings u ∈ r |v |×d using the following objective : min m , u , h , c ks − mu t k 2 f + αkh − uct k 2 f − βtr ( h tbh ) s .t . , m ≥ 0 , u ≥ 0 , h ≥ 0 , c ≥ 0 , tr ( h th ) = |v | , where vertex embedding u is learned by minimizing ks − mu tk 2 f , with s ∈ r |v |×|v | being the vertex pairwise proximity matrix , which captures the second order and the high order proximity when taken as representations . //
h-	587	[33]	summary . // the algorithms of learning latent social dimensions , , only consider the community structure to learn vertex representation , while m nmf integrates microscopic structure preserving ( the secondorder and high order proximity ) with the intra community proximity . // these methods primarily rely on matrix factorization to detect community structure , which makes them hard to scale to large scale networks .
h-	587	[55]	summary . // the algorithms of learning latent social dimensions , , only consider the community structure to learn vertex representation , while m nmf integrates microscopic structure preserving ( the secondorder and high order proximity ) with the intra community proximity . // these methods primarily rely on matrix factorization to detect community structure , which makes them hard to scale to large scale networks .
h-	587	[30]	summary . // the algorithms of learning latent social dimensions , , only consider the community structure to learn vertex representation , while m nmf integrates microscopic structure preserving ( the secondorder and high order proximity ) with the intra community proximity . // these methods primarily rely on matrix factorization to detect community structure , which makes them hard to scale to large scale networks .
b	587	[31]	many real world networks present the macroscopic scale free property , which depicts the phenomenon that vertex degree follows a longtailed distribution , i .e . , most vertices are sparsely connected and only few vertices have dense edges . // to capture the scale free property , proposes the degree penalty principle ( dp ) : penalizing the proximity between high degree vertices . this principle is then coupled with two nrl algorithms ( i .e . , spectral embedding and deepwalk ) to learn scale free property preserving vertex representations . // hierarchical representation learning for networks ( harp ) .
b	587	[5]	many real world networks present the macroscopic scale free property , which depicts the phenomenon that vertex degree follows a longtailed distribution , i .e . , most vertices are sparsely connected and only few vertices have dense edges . // to capture the scale free property , proposes the degree penalty principle ( dp ) : penalizing the proximity between high degree vertices . this principle is then coupled with two nrl algorithms ( i .e . , spectral embedding and deepwalk ) to learn scale free property preserving vertex representations . // hierarchical representation learning for networks ( harp ) .
b	587	[32]	hierarchical representation learning for networks ( harp ) . // to capture the global patterns in networks , harp samples small networks to approximate the global structure . // the vertex representations learned from sampled networks are taken as the initialization for inferring the vertex representations of the original network .
b	587	[6]	to obtain smooth solutions , a series of smaller networks are successively sampled from the original network by coalescing edges and vertices , and the vertex representations are hierarchically inferred back from the smallest network to the original network . // in harp , deepwalk and line are used to learn vertex representations . // summary : dp and harp are both realized by adapting the existing nrl algorithms to capture the macroscopic structure .
b	587	[1]	to obtain smooth solutions , a series of smaller networks are successively sampled from the original network by coalescing edges and vertices , and the vertex representations are hierarchically inferred back from the smallest network to the original network . // in harp , deepwalk and line are used to learn vertex representations . // summary : dp and harp are both realized by adapting the existing nrl algorithms to capture the macroscopic structure .
b	587	[32]	in harp , deepwalk and line are used to learn vertex representations . // summary : dp and harp are both realized by adapting the existing nrl algorithms to capture the macroscopic structure . // the former tries to preserve the scale free property , while the latter makes the learned vertex representations respect the global network structure .
b	587	[7]	text associated deepwalk ( tadw ) . // tadw firstly proves the equivalence between deepwalk and the following matrix factorization : min w , h km − wthk 2 f + λ 2 ( kwk 2 f + khk 2 f ) , ( 20 ) where w and h are learned latent embeddings and m is the vertex context matrix carrying transition probability between each vertex pair within k steps . //
b	587	[6]	text associated deepwalk ( tadw ) . // tadw firstly proves the equivalence between deepwalk and the following matrix factorization : min w , h km − wthk 2 f + λ 2 ( kwk 2 f + khk 2 f ) , ( 20 ) where w and h are learned latent embeddings and m is the vertex context matrix carrying transition probability between each vertex pair within k steps . //
b	587	[20]	despite its ability to incorporate textural features , tadw only considers structural context of network vertices , i .e . , the second order and high order proximity , but ignores the important homophily property ( the first order proximity ) in its learning framework . // hsca is proposed to simultaneously integrates homophily , structural context , and vertex content to learn effective network representations . for tadw , the learned representation for the i th vertex vi is wt i , ( hti : ) t t , where wi : and ti : is the i th row of w and t , respectively . //
b	587	[43]	paired restricted boltzmann machine ( prbm ) . // by leveraging the strength of restricted boltzmann machine ( rbm ) , designs a novel model called paired rbm ( prbm ) to learn vertex representations by combining vertex attributes and link information . // the prbm considers the networks with vertices associated with binary attributes . for each edge ( vi , vj ) ∈ e , the attributes for vi and vj are v ( i ) and v ( j ) ∈ { 0 , 1 } m , and their hidden representations are h ( i ) and h ( j ) ∈ { 0 , 1 } d .
b	587	[28]	paired restricted boltzmann machine ( prbm ) . // by leveraging the strength of restricted boltzmann machine ( rbm ) , designs a novel model called paired rbm ( prbm ) to learn vertex representations by combining vertex attributes and link information . // the prbm considers the networks with vertices associated with binary attributes . for each edge ( vi , vj ) ∈ e , the attributes for vi and vj are v ( i ) and v ( j ) ∈ { 0 , 1 } m , and their hidden representations are h ( i ) and h ( j ) ∈ { 0 , 1 } d .
b	587	[58]	compared with textural content features , user profiles have two unique properties : ( 1 ) user profiles are noise , sparse and incomplete and ( 2 ) different dimensions of user profile features are topic inconsistent . // to filter out noise and extract useful information from user profiles , upp sne constructs user representations by performing a non linear mapping on user profile features , which is guided by network structure . the approximated kernel mapping is used in uppsne to construct user embedding from user profile features : f ( vi ) = ϕ ( xi ) = 1 √ d h cos ( µ t 1 xi ) , · · · , cos ( µ t d xi ) , sin ( µ t 1 xi ) , · · · , sin ( µ t d xi ) it , ( 26 ) where xi is the user profile feature vector of vertex vi and µi is the corresponding coefficient vector . //
b	587	[6]	// to supervise the learning of the non linear mapping and make user profiles and network structure complement each other , the objective of deepwalk is used : min f − log pr ( { vi−t , · · · , vi+t } \ vi|f ( vi ) ) , ( 27 ) where { vi−t , · · · , vi+t } \vi is the context vertices of vertex vi within t window size in the given random walk sequence . // property preserving network embedding ( ppne )
b	587	[30]	property preserving network embedding ( ppne ) . // to learn content augmented vertex representations , ppne jointly optimizes two objectives : ( i ) the structuredriven objective and ( ii ) the attribute driven objective . // following deepwalk , the structure driven objective aims to make vertices sharing similar context vertices represented closely .
b	587	[20]	the above unsupervised content augmented nrl algorithms incorporate vertex content features in three ways . // the first , used by tadw and hsca , is to couple the network structure with vertex content features via inductive matrix factorization . // this process can be considered as a linear transformation on vertex attributes constrained by network structure .
b	587	[34]	the above unsupervised content augmented nrl algorithms incorporate vertex content features in three ways . // the first , used by tadw and hsca , is to couple the network structure with vertex content features via inductive matrix factorization . // this process can be considered as a linear transformation on vertex attributes constrained by network structure .
b	587	[43]	the second is to perform a non linear mapping to construct new vertex embeddings that respect network structure . // for example , rbm and the approximated kernel mapping is used by prbm and upp sne , respectively , to achieve this goal . // the third used by ppne is to add an attribute preserving constraint to the structure preserving optimization objective .
b	587	[28]	the second is to perform a non linear mapping to construct new vertex embeddings that respect network structure . // for example , rbm and the approximated kernel mapping is used by prbm and upp sne , respectively , to achieve this goal . // the third used by ppne is to add an attribute preserving constraint to the structure preserving optimization objective .
b	587	[57]	the second is to perform a non linear mapping to construct new vertex embeddings that respect network structure . // for example , rbm and the approximated kernel mapping is used by prbm and upp sne , respectively , to achieve this goal . // the third used by ppne is to add an attribute preserving constraint to the structure preserving optimization objective .
b	587	[30]	for example , rbm and the approximated kernel mapping is used by prbm and upp sne , respectively , to achieve this goal . // the third used by ppne is to add an attribute preserving constraint to the structure preserving optimization objective . // semi supervised network representation learning
b	587	[60]	discriminative deep random walk ( ddrw ) . // inspired by the discriminative representation learning , , ddrw proposes to learn discriminative network representations through jointly optimizing the objective of deepwalk together with the following l2 loss support vector classification classification objective : lc = c x |v | i=1 ( σ ( 1 − yikβ t rvi ) ) 2 + 1 2 β t β , ( 30 ) where σ ( x ) = x , if x > 0 and otherwise σ ( x ) = 0 . //
b	587	[38]	discriminative deep random walk ( ddrw ) . // inspired by the discriminative representation learning , , ddrw proposes to learn discriminative network representations through jointly optimizing the objective of deepwalk together with the following l2 loss support vector classification classification objective : lc = c x |v | i=1 ( σ ( 1 − yikβ t rvi ) ) 2 + 1 2 β t β , ( 30 ) where σ ( x ) = x , if x > 0 and otherwise σ ( x ) = 0 . //
b	587	[6]	discriminative deep random walk ( ddrw ) . // inspired by the discriminative representation learning , , ddrw proposes to learn discriminative network representations through jointly optimizing the objective of deepwalk together with the following l2 loss support vector classification classification objective : lc = c x |v | i=1 ( σ ( 1 − yikβ t rvi ) ) 2 + 1 2 β t β , ( 30 ) where σ ( x ) = x , if x > 0 and otherwise σ ( x ) = 0 . //
b	587	[62]	max margin deepwalk ( mmdw ) . // similarly , mmdw couples the objective of the matrix factorization version deepwalk with the following multi class support vector machine objective with { ( rv1 , y1 : ) , · · · , ( rvt , yt : ) } training set : min w , ξ lsv m = min w , ξ 1 2 kwk 2 2 + c x t i=1 ξi , s .t . w t li rvi − w t j rvi ≥ e j i − ξi , ∀i , j , ( 32 ) where e j i = 1 , if yij = −1 . otherwise , e j i = 0 , if yij = 1 . //
b	587	[7]	max margin deepwalk ( mmdw ) . // similarly , mmdw couples the objective of the matrix factorization version deepwalk with the following multi class support vector machine objective with { ( rv1 , y1 : ) , · · · , ( rvt , yt : ) } training set : min w , ξ lsv m = min w , ξ 1 2 kwk 2 2 + c x t i=1 ξi , s .t . w t li rvi − w t j rvi ≥ e j i − ξi , ∀i , j , ( 32 ) where e j i = 1 , if yij = −1 . otherwise , e j i = 0 , if yij = 1 . //
b	587	[63]	transductive line ( tline ) . // along similar lines , tline is proposed as a semisupervised extension of line that simultaneously learns line ’ s vertex representations and an svm classifier . given a set of labeled vertices { v1 , v2 , · · · , vl } and { vl+1 , · · · , v|v | } , tline trains a multi class svm classifier on { v1 , v2 , · · · , vl } by optimizing the objective : osvm = xl i=1 xk k=1 max ( 0 , 1 − yikwk t rvi ) + λkwkk 2 2 . // based on line ’ s formulations that preserve the firstorder and second order proximity , tline optimizes two objective functions .
b	587	[39]	group enhanced network embedding ( gene ) . // gene integrates group ( label ) information with network structure in a probabilistic manner . // gene assumes that vertices should be embedded closely in lowdimensional space , if they share similar neighbors or join similar groups .
b	587	[6]	gene assumes that vertices should be embedded closely in lowdimensional space , if they share similar neighbors or join similar groups . // inspired by deepwalk and document modeling , , the mechanism of gene for learning group label informed vertex representations is achieved by maximizing the following log probability : l = x gi∈y  α x w∈wgi x vj∈w log pr ( vj |vj−t , · · · , vj+t , gi ) + β x vˆj∈wˆ gj log pr ( ˆvj |gi )    , ( 37 ) where y is the set of different groups , wgi is the set of random walk sequences labeled with gi , wˆ gi is the set of vertices randomly sampled from group gi . //
b	587	[64]	gene assumes that vertices should be embedded closely in lowdimensional space , if they share similar neighbors or join similar groups . // inspired by deepwalk and document modeling , , the mechanism of gene for learning group label informed vertex representations is achieved by maximizing the following log probability : l = x gi∈y  α x w∈wgi x vj∈w log pr ( vj |vj−t , · · · , vj+t , gi ) + β x vˆj∈wˆ gj log pr ( ˆvj |gi )    , ( 37 ) where y is the set of different groups , wgi is the set of random walk sequences labeled with gi , wˆ gi is the set of vertices randomly sampled from group gi . //
b	587	[40]	semi supervised network embedding ( semine ) . // semine learns semi supervised vertex representations in two stages . // in the first stage , semine exploits the deepwalk framework to learn vertex representations in an unsupervised manner
b	587	[41]	tri party deep network representation ( tridnr ) . // using a coupled neural network framework , tridnr learns vertex representations from three information sources : network structure , vertex content and vertex labels . // to capture the vertex content and label information , tridnr adapts the paragraph vector model to describe the vertex word correlation and the label word correspondence by maximizing the following objective : lp v = x i∈l log pr ( w−b : wb|ci ) +x |v | i=1 log pr ( w−b : wb|vi ) , ( 39 ) where { w−b : wb } is a sequence of words inside a contextual window of length 2b , ci is the class label of vertex vi , and l is the set of indices of labeled vertices .
b	587	[64]	using a coupled neural network framework , tridnr learns vertex representations from three information sources : network structure , vertex content and vertex labels . // to capture the vertex content and label information , tridnr adapts the paragraph vector model to describe the vertex word correlation and the label word correspondence by maximizing the following objective : lp v = x i∈l log pr ( w−b : wb|ci ) +x |v | i=1 log pr ( w−b : wb|vi ) . // ( 39 ) where { w−b : wb } is a sequence of words inside a contextual window of length 2b , ci is the class label of vertex vi , and l is the set of indices of labeled vertices .
b	587	[41]	lde is proposed to learn representations for linked documents , which are actually the vertices of citation or webpage networks . // similar to tridnr , lde learns vertex representations by modeling three kinds of relations , i .e . , word word document relations , document document relations , and document label relations . // lde is realized by solving the following optimization problem :
b	587	[8]	discriminative matrix factorization ( dmf ) . // to empower vertex representations with discriminative ability , dmf enforces the objective of tadw ( 21 ) with an empirical loss minimization for a linear classifier trained on labeled vertices : min w , h , η 1 2 x |v | i , j=1 ( mij − w t i htj ) 2 + µ 2 x n∈l ( yn1 − η txn ) 2 + λ1 2 ( khk 2 f + kηk 2 2 ) + λ2 2 kwk 2 f , ( 42 ) where wi is the i th column of vertex representation matrix w and tj is j th column of vertex textual feature matrix t , and l is the set of indices of labeled vertices . //
b	587	[66]	predictive labels and neighbors with embeddings transductively or inductively from data ( planetoid ) . // planetoid leverages network embedding together with vertex attributes to carry out semi supervised learning . // planetoid learns vertex embeddings by minimizing the loss for predicting structural context , which is formulated as
b	587	[62]	we now summarize and compare the discriminative learning strategies used by semi supervised nrl algorithms in table 5 in terms of their advantages and disadvantages . // three strategies are used to achieve discriminative learning . the first strategy ( i .e . , ddrw , mmdw , tline , dmf , semine ) is to enforce classification loss minimization on vertex representations , i .e . , fitting the vertex representations to a classifier . // this provides a direct way to separate vertices of different categories from each other in the new embedding space .
b	587	[63]	we now summarize and compare the discriminative learning strategies used by semi supervised nrl algorithms in table 5 in terms of their advantages and disadvantages . // three strategies are used to achieve discriminative learning . the first strategy ( i .e . , ddrw , mmdw , tline , dmf , semine ) is to enforce classification loss minimization on vertex representations , i .e . , fitting the vertex representations to a classifier . // this provides a direct way to separate vertices of different categories from each other in the new embedding space .
b	587	[8]	we now summarize and compare the discriminative learning strategies used by semi supervised nrl algorithms in table 5 in terms of their advantages and disadvantages . // three strategies are used to achieve discriminative learning . the first strategy ( i .e . , ddrw , mmdw , tline , dmf , semine ) is to enforce classification loss minimization on vertex representations , i .e . , fitting the vertex representations to a classifier . // this provides a direct way to separate vertices of different categories from each other in the new embedding space .
b	587	[39]	this provides a direct way to separate vertices of different categories from each other in the new embedding space . // the second strategy ( used by gene , tridnr , lde and planetoid ) is achieved by modeling vertex label relation , such that vertices with same labels have similar vector representations . // the third strategy used by lane is to jointly embed vertices and labels into a common space .
b	587	[41]	this provides a direct way to separate vertices of different categories from each other in the new embedding space . // the second strategy ( used by gene , tridnr , lde and planetoid ) is achieved by modeling vertex label relation , such that vertices with same labels have similar vector representations . // the third strategy used by lane is to jointly embed vertices and labels into a common space .
b	587	[42]	this provides a direct way to separate vertices of different categories from each other in the new embedding space . // the second strategy ( used by gene , tridnr , lde and planetoid ) is achieved by modeling vertex label relation , such that vertices with same labels have similar vector representations . // the third strategy used by lane is to jointly embed vertices and labels into a common space .
b	587	[29]	the second strategy ( used by gene , tridnr , lde and planetoid ) is achieved by modeling vertex label relation , such that vertices with same labels have similar vector representations . // the third strategy used by lane is to jointly embed vertices and labels into a common space . // fitting vertex representations to a classifier can take advantage of the discriminative power in vertex labels . algorithms using this strategy only require a small number of labeled vertices ( e .g . , 10 % ) to achieve significant performance gain over their unsupervised counterparts . they are thus more effective for discriminative learning in sparsely labeled scenarios .
b	587	[67]	they are thus more effective for discriminative learning in sparsely labeled scenarios . however , fitting vertex representations to a classifier is more prone to overfitting . // regularization and dropout are often introduced to overcome this problem . // by contrast , modeling vertex label relation and joint vertex embedding requires more vertex labels to make vertex representations more discriminative , but they can better capture intra class proximity , i .e . , vertices belonging to the same class are kept closer to each other in the new embedding space .
b	587	[10]	often , because network vertices are partially or sparsely labeled due to high labeling costs , a large portion of vertices in networks have unknown labels . // the problem of vertex classification aims to predict the labels of unlabeled vertices given a partially labeled network , . // since vertices are not independent but connected to each other in the form of a network via links , vertex classification should exploit these dependencies for jointly classifying the labels of vertices .
h+	587	[68]	since vertices are not independent but connected to each other in the form of a network via links , vertex classification should exploit these dependencies for jointly classifying the labels of vertices . // among others , collective classification proposes to construct a new set of vertex features that summarize label dependencies in the neighborhood , which has been shown to be most effective in classifying many real world networks , . // network representation learning follows the same principle that automatically learns vertex features based on network structure .
h+	587	[69]	since vertices are not independent but connected to each other in the form of a network via links , vertex classification should exploit these dependencies for jointly classifying the labels of vertices . // among others , collective classification proposes to construct a new set of vertex features that summarize label dependencies in the neighborhood , which has been shown to be most effective in classifying many real world networks , . // network representation learning follows the same principle that automatically learns vertex features based on network structure .
b	587	[1]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[7]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[20]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[37]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[29]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[38]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[62]	network representation learning follows the same principle that automatically learns vertex features based on network structure . // existing studies have evaluated the discriminative power of the learned vertex representations under two settings : unsupervised settings ( e .g . , , , , , ) , where vertex representations are learned separately , followed by applying discriminative classifiers like svm or logistic regression on the new embeddings , and semisupervised settings ( e .g . , , , , , ) , where representation learning and discriminative learning are simultaneously tackled , so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations . // these studies have proved that better vertex representations can contribute to high classification accuracy .
b	587	[13]	link prediction . // another important application of network representation learning is link prediction , , which aims to infer the existence of new relationships or emerging interactions between pairs of entities based on the currently observed links and their properties . // the approaches developed to solve this problem can enable the discovery of implicit or missing interactions in the network , the identification of spurious links , as well as understanding the network evolution mechanism .
b	587	[70]	link prediction . // another important application of network representation learning is link prediction , , which aims to infer the existence of new relationships or emerging interactions between pairs of entities based on the currently observed links and their properties . // the approaches developed to solve this problem can enable the discovery of implicit or missing interactions in the network , the identification of spurious links , as well as understanding the network evolution mechanism .
b	587	[19]	good network representations should be able to capture explicit and implicit connections between network vertices thus enabling application to link prediction . // and predict missing links based on the learned vertex representations on social networks . // also applies network representation learning to collaboration networks and protein protein interaction networks . they demonstrate that on these networks links predicted using the learned representations achieve better performance than traditional similarity based link prediction approaches .
h+	587	[37]	and predict missing links based on the learned vertex representations on social networks . // also applies network representation learning to collaboration networks and protein protein interaction networks . // they demonstrate that on these networks links predicted using the learned representations achieve better performance than traditional similarity based link prediction approaches .
b	587	[71]	clustering . // network clustering refers to the task of partitioning network nodes into a set of clusters , such that vertices are densely connected to each other within the same cluster , but connected to few vertices from other clusters . // such cluster structures , or communities widely occur in a wide spectrum of networked systems from bioinformatics , computer science , physics , sociology , etc . , and have strong implications .
b	587	[72]	researchers have proposed a large body of network clustering algorithms based on various metrics of similarity or strength of connection between vertices . // min max cut and normalized cut methods , seek to recursively partition a graph into two clusters that maximize the number of intra cluster connections and minimize the number of intercluster connections . // modularity based methods ( e .g . , , ) aim to maximize the modularity of a clustering , which is the fraction of intra cluster edges minus the expected fraction assuming the edges were randomly distributed . a network partitioning with high modularity would have dense intra cluster connections but sparse inter cluster connections .
b	587	[74]	min max cut and normalized cut methods , seek to recursively partition a graph into two clusters that maximize the number of intra cluster connections and minimize the number of intercluster connections . // modularity based methods ( e .g . , , ) aim to maximize the modularity of a clustering , which is the fraction of intra cluster edges minus the expected fraction assuming the edges were randomly distributed . // a network partitioning with high modularity would have dense intra cluster connections but sparse inter cluster connections .
b	587	[75]	min max cut and normalized cut methods , seek to recursively partition a graph into two clusters that maximize the number of intra cluster connections and minimize the number of intercluster connections . // modularity based methods ( e .g . , , ) aim to maximize the modularity of a clustering , which is the fraction of intra cluster edges minus the expected fraction assuming the edges were randomly distributed . // a network partitioning with high modularity would have dense intra cluster connections but sparse inter cluster connections .
b	587	[76]	a network partitioning with high modularity would have dense intra cluster connections but sparse inter cluster connections . // some other methods ( e .g . , ) try to identify nodes with similar structural roles like bridges and outliers . // recent nrl methods ( e .g . , grarep , dngr , mnmf , and prbm ) used the clustering performance to evaluate the quality of the learned network representations on different networks . intuitively , better representations would lead to better clustering performance .
b	587	[9]	some other methods ( e .g . , ) try to identify nodes with similar structural roles like bridges and outliers . // recent nrl methods ( e .g . , grarep , dngr , mnmf , and prbm ) used the clustering performance to evaluate the quality of the learned network representations on different networks . intuitively , better representations would lead to better clustering performance . // intuitively , better representations would lead to better clustering performance .
b	587	[30]	some other methods ( e .g . , ) try to identify nodes with similar structural roles like bridges and outliers . // recent nrl methods ( e .g . , grarep , dngr , mnmf , and prbm ) used the clustering performance to evaluate the quality of the learned network representations on different networks . intuitively , better representations would lead to better clustering performance . // intuitively , better representations would lead to better clustering performance .
b	587	[28]	some other methods ( e .g . , ) try to identify nodes with similar structural roles like bridges and outliers . // recent nrl methods ( e .g . , grarep , dngr , mnmf , and prbm ) used the clustering performance to evaluate the quality of the learned network representations on different networks . intuitively , better representations would lead to better clustering performance . // intuitively , better representations would lead to better clustering performance .
ho	587	[77]	visualization techniques play critical roles in managing , exploring , and analyzing complex networked data . // surveys a range of methods used to visualize graphs from an information visualization perspective . // this work compares various traditional layouts used to visualize graphs , such as tree , 3d , and hyperbolic based methods , and shows that classical visualization techniques are proved effective for small or intermediate sized networks ; they however confront a big challenge when applied to large scale networks .
b	587	[17]	few systems can claim to deal effectively with thousands of vertices , although networks with this order of magnitude often occur in a wide variety of applications . // consequently , a first step in the visualization process is often to reduce the size of the network to display . one common approach is essentially to find an extremely low dimensional representation of a network that preserves the intrinsic structure , i .e . , keeping similar vertices close and dissimilar vertices far apart , in the low dimensional space . // network representation learning has the same objective that embeds a large network into a new latent space of low dimensionality
b	587	[78]	network representation learning has the same objective that embeds a large network into a new latent space of low dimensionality . // after new embeddings are obtained in the vector space , popular methods such as t distributed stochastic neighbor embedding ( t sne ) can be applied to visualize the network in a 2 d or 3 d space . // by taking the learned vertex representations as input , line used the t sne package to visualize the dblp co author network after the authors are mapped into a 2 d space , and showed that line is able to cluster authors in the same field to the same community .
b	587	[63]	hsca illustrated the advantages of the content augmented nrl algorithm by visualizing the citations networks . // semi supervised algorithms ( e .g . , tline , tridnr , and dmf ) demonstrated that the visualization results have better clustering structures with vertex labels properly imported . // recommendation
b	587	[41]	hsca illustrated the advantages of the content augmented nrl algorithm by visualizing the citations networks . // semi supervised algorithms ( e .g . , tline , tridnr , and dmf ) demonstrated that the visualization results have better clustering structures with vertex labels properly imported . // recommendation
b	587	[8]	hsca illustrated the advantages of the content augmented nrl algorithm by visualizing the citations networks . // semi supervised algorithms ( e .g . , tline , tridnr , and dmf ) demonstrated that the visualization results have better clustering structures with vertex labels properly imported . // recommendation
b	587	[14]	however , because each user ’ s check in records are very sparse , finding similar users or calculating transition probability between users and locations is a significant challenge . // recently , spatial temporal embedding , , has emerged to learn low dimensional dense vectors to represent users , locations , and point of interests etc . // as a result , each user , location , and poi can be represented as a lowdimensional vector , respectively , for similarity search and many other analysis .
b	587	[15]	however , because each user ’ s check in records are very sparse , finding similar users or calculating transition probability between users and locations is a significant challenge . // recently , spatial temporal embedding , , has emerged to learn low dimensional dense vectors to represent users , locations , and point of interests etc . // as a result , each user , location , and poi can be represented as a lowdimensional vector , respectively , for similarity search and many other analysis .
b	587	[80]	however , because each user ’ s check in records are very sparse , finding similar users or calculating transition probability between users and locations is a significant challenge . // recently , spatial temporal embedding , , has emerged to learn low dimensional dense vectors to represent users , locations , and point of interests etc . // as a result , each user , location , and poi can be represented as a lowdimensional vector , respectively , for similarity search and many other analysis .
b	587	[82]	in addition to using vector representation to represent knowledge graph entities , research has also proposed to use such representation to further enhance and complete the knowledge graph itself . // for example , knowledge graph completion intends to discover complete relationships between entities , and a recent work has proposed to use graph context to find missing links between entities . // this is similar to link prediction in social networks , but the entities are typically heterogeneous and a pair of entities may also have different types of relationships .
b	587	[83]	the facebook network is a combination of 10 facebook ego networks , where each vertex contains user profile attributes . // the amherst , hamilton , mich and rochester datasets are the facebook networks formed by users from the corresponding us universities , where each user has six user profile features . // often , user profile features are noisy , incomplete , and long tail distributed .
b	587	[1]	language network . // the language network wikipedia is a word co occurrence network constructed from the entire set of english wikipedia pages . there is no class label on this network . // the word embeddings learned from this network is evaluated by word analogy and document classification .
b	587	[1]	the citation networks are directed information networks formed by author author citation relationships or paper paper citation relationships . they are collected from different databases of academic papers , such as dblp and citeseer . // among the commonly used citation networks , dblp ( authorcitation ) , cora , citeseer , pubmed and citeseer m10 are the binary paper citation networks , which are also attached with vertex text attributes as the content of papers . // compared with user profile features in social network , the vertex text features here are more topic centric , informative and can better complement network structure to learn effective vertex representations .
b	587	[84]	collaboration network . // the collaboration network arxiv gr qc describes the co author relationships for papers in the research field of general relativity and quantum cosmology . // in this network , vertices represent authors and edges indicate co author relationships between authors . because there is no category information for vertices , this network is used for the link prediction task to evaluate the quality of learned vertex representations .
b	587	[85]	webpage network . // webpage networks ( wikipedia , webkb and political blog ) are composed of real world webpages and hyperlinks between them , where the vertex represents a webpage and the edge indicates that there is a hyperlink from one webpage to another . // webpage text content is often collected as vertex features .
b	587	[87]	biological network . // as a typical biological network , the protein protein interaction network is a subgraph of the ppi network for homo sapiens . the vertex here represents a protein and the edge indicates that there is an interaction between proteins . the labels of vertices are obtained from the hallmark gene sets and represent biological states . // communication network .
b	587	[53]	traffic network . // european airline networks used in are constructed from 6 airlines operating flights between european airports : 4 commercial airlines ( air france , easyjet , lufthansa , and ryanair ) and 2 cargo airlines ( tap portugal , and european airline transport ) . // for each airline network , vertices are airports and edges represent the direct flights between airports . in all , 45 airports are labeled as hub airports , regional hubs , commercial hubs , and focus cities , according to their structural roles .
b	587	[19]	the known links in the original network are served as the ground truth for evaluating reconstruction performance . // precision @ k and map are often used as evaluation metrics . // this evaluation method can check whether the learned vertex representations well preserve network structure and support network formation .
b	587	[78]	visualization provides a straightforward way to visually evaluate the quality of the learned vertex representations . // often , t distributed stochastic neighbor embedding ( t sne ) is applied to project the learned vertex representation vectors into a 2 d space , where the distribution of vertex 2 d mappings can be easily visualized . // if vertex representations are of good quality , in the 2 d space , vertices within a same class or community should be embedded closely , and the 2 d mappings of vertices in different classes or communities should be far apart from each other .
b	587	[6]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
b	587	[1]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
b	587	[30]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
b	587	[7]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
b	587	[20]	note that , because semi supervised nrl algorithms are task dependent : the target task may be binary or multi class , or multi label classification , or because they use different classification strategies , it would be difficult to assess the effectiveness of network embedding under the same settings . // therefore , our empirical study focuses on comparing seven unsupervised nrl algorithms ( deepwalk , line , node2vec , mnmf , tadw , hsca , upp sne ) on vertex classification and vertex clustering , which are the two most commonly used evaluation methods in the literature . // our empirical studies are based on seven benchmark datasets : amherst , hamilton , mich , rochester , citeseer , cora and facebook .
h-	587	[89]	on citeseer and cora , node2vec performs much better than other only structure preserving nrl algorithms , including its equivalent version deepwalk . // for each vertex context pair ( vi , vj ) , deepwalk and node2vec use two different strategies to approximate the probability pr ( vj |vi ) : hierarchical softmax , and negative sampling . // the better clustering performance of node2vec over deepwalk proves the advantage of negative sampling over hierarchical softmax , which is consistent with the word embedding results as reported in .
h-	587	[90]	on citeseer and cora , node2vec performs much better than other only structure preserving nrl algorithms , including its equivalent version deepwalk . // for each vertex context pair ( vi , vj ) , deepwalk and node2vec use two different strategies to approximate the probability pr ( vj |vi ) : hierarchical softmax , and negative sampling . // the better clustering performance of node2vec over deepwalk proves the advantage of negative sampling over hierarchical softmax , which is consistent with the word embedding results as reported in .
h+	587	[91]	on citeseer and cora , node2vec performs much better than other only structure preserving nrl algorithms , including its equivalent version deepwalk . // for each vertex context pair ( vi , vj ) , deepwalk and node2vec use two different strategies to approximate the probability pr ( vj |vi ) : hierarchical softmax , and negative sampling . // the better clustering performance of node2vec over deepwalk proves the advantage of negative sampling over hierarchical softmax , which is consistent with the word embedding results as reported in .
b	587	[7]	the corresponding nrl algorithms usually perform factorization on |v |×|v | structure preserving matrix , which is quite time consuming . // efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition .
b	587	[8]	the corresponding nrl algorithms usually perform factorization on |v |×|v | structure preserving matrix , which is quite time consuming . // efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition .
b	587	[20]	the corresponding nrl algorithms usually perform factorization on |v |×|v | structure preserving matrix , which is quite time consuming . // efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition .
b	587	[53]	efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition . // future research directions .
b	587	[92]	efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition . // future research directions .
b	587	[93]	efforts have been made to reduce the complexity of matrix factorization . for example , tadw , dmf and hsca take advantage of the sparsity of the original vertex context matrix . // hope and graphwave adopt advanced techniques to perform matrix eigen decomposition . // future research directions .
b	587	[94]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
b	587	[95]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
b	587	[96]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
b	587	[98]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
b	587	[99]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
b	587	[100]	to date , most existing nrl algorithms are task independent , and task specific nrl algorithms have primarily focused on vertex classification under the semi supervised setting . // only very recently , a few studies have started to design task specific nrl algorithms for link prediction , community detection , , , , class imbalance learning , active learning , and information retrieval . // the advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task .
ho	587	[1]	there is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results . // to better understand deepwalk , line , and node2vec , discovers the theoretical connections between them and graph laplacians . // however , in depth theoretical analysis about network representation learning is necessary , as it provides a deep understanding of algorithms and helps interpret empirical results .
ho	587	[37]	there is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results . // to better understand deepwalk , line , and node2vec , discovers the theoretical connections between them and graph laplacians . // however , in depth theoretical analysis about network representation learning is necessary , as it provides a deep understanding of algorithms and helps interpret empirical results .
h-	587	[101]	there is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results . // to better understand deepwalk , line , and node2vec , discovers the theoretical connections between them and graph laplacians . // however , in depth theoretical analysis about network representation learning is necessary , as it provides a deep understanding of algorithms and helps interpret empirical results .
b	587	[103]	the vertices/edges may also be described by some time varying information . // dynamic networks have unique characteristics that make static network embedding fail to work : ( i ) vertex content features may drift over time ; ( ii ) the addition of new vertices/edges requires learning or updating vertex representations to be efficient ; and ( iii ) network size is not fixed . the work on dynamic network embedding is rather limited ; the majority of existing approaches ( e .g . , , , ) assume that the node set is fixed and deal with the dynamics caused by the deletion/addition of edges only . // however , a more challenging problem is to predict the representations of new added vertices , which is referred to as “ out of sample ” problem .
b	587	[104]	the vertices/edges may also be described by some time varying information . // dynamic networks have unique characteristics that make static network embedding fail to work : ( i ) vertex content features may drift over time ; ( ii ) the addition of new vertices/edges requires learning or updating vertex representations to be efficient ; and ( iii ) network size is not fixed . the work on dynamic network embedding is rather limited ; the majority of existing approaches ( e .g . , , , ) assume that the node set is fixed and deal with the dynamics caused by the deletion/addition of edges only . // however , a more challenging problem is to predict the representations of new added vertices , which is referred to as “ out of sample ” problem .
b	587	[66]	however , a more challenging problem is to predict the representations of new added vertices , which is referred to as “ out of sample ” problem . // a few attempts such as , , are made to exploit inductive learning to address this issue . // they learn an explicit mapping function from a network at a snapshot , and use this function to infer the representations of out ofsample vertices , based on their available information such as attributes or neighborhood structure .
b	587	[106]	however , a more challenging problem is to predict the representations of new added vertices , which is referred to as “ out of sample ” problem . // a few attempts such as , , are made to exploit inductive learning to address this issue . // they learn an explicit mapping function from a network at a snapshot , and use this function to infer the representations of out ofsample vertices , based on their available information such as attributes or neighborhood structure .
h-	587	[107]	deep learning based methods can capture non linearity in networks , but their computational cost is usually high . // traditional deep learning architectures take advantage of gpu to speed up training on euclidean structured data . // however , networks do not have such a structure , and therefore require new solutions to improve the scalability .
ho	587	[108]	traditional deep learning architectures take advantage of gpu to speed up training on euclidean structured data . // however , networks do not have such a structure , and therefore require new solutions to improve the scalability . // afew attempts such as , , are made to exploit inductive learning to address this issue .
b	587	[109]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[110]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[111]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[113]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[114]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[115]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[117]	an hin is composed of different types of entities , such as text , images , or videos , and the interdependencies between entities are very complex . // this makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space . recent studies by , , , , , , , , , have investigated the use of various descriptors ( e .g . , metapath or meta structure ) to capture semantic proximity between distant hin vertices for representation learning . // however , the research along this line is still at early stage .
b	587	[118]	the existence of negative links makes the traditional homophily based network representation learning algorithms unable to be directly applied . // some studies , , tackles signed network representation learning through directly modeling the polar of links . // how to fully encode network structure and vertex attributes for signed network embedding remains an open question .
b	587	[70]	the existence of negative links makes the traditional homophily based network representation learning algorithms unable to be directly applied . // some studies , , tackles signed network representation learning through directly modeling the polar of links . // how to fully encode network structure and vertex attributes for signed network embedding remains an open question .
b	587	[119]	real world networks are often noisy and uncertain , which makes traditional nrl algorithms unable to produce stable and robust representations . // ane ( adversarial network embedding ) and arga ( adversarially regularized graph autoencoder ) learn robust vertex representations via enforcing an adversarial learning regularizer . // to deal with the uncertainty in the existence of edges , urge ( uncertain graph embedding ) encodes the edge existence probability into the vertex representation learning process .
b	587	[120]	real world networks are often noisy and uncertain , which makes traditional nrl algorithms unable to produce stable and robust representations . // ane ( adversarial network embedding ) and arga ( adversarially regularized graph autoencoder ) learn robust vertex representations via enforcing an adversarial learning regularizer . // to deal with the uncertainty in the existence of edges , urge ( uncertain graph embedding ) encodes the edge existence probability into the vertex representation learning process .
b	587	[45]	real world networks are often noisy and uncertain , which makes traditional nrl algorithms unable to produce stable and robust representations . // ane ( adversarial network embedding ) and arga ( adversarially regularized graph autoencoder ) learn robust vertex representations via enforcing an adversarial learning regularizer . // to deal with the uncertainty in the existence of edges , urge ( uncertain graph embedding ) encodes the edge existence probability into the vertex representation learning process .
ho	6	[121, 68]	sparsity enables the design of efficient discrete algorithms , but can make it harder to generalize in statistical learning . // machine learning applications in networks ( such as network classification , content recommendation , anomaly detection , and missing link prediction ) must be able to deal with this sparsity in order to survive . // in this paper we introduce deep learning ( unsupervised feature learning ) techniques , which have proven successful in natural language processing , into network analysis for the first time . we develop an algorithm ( deepwalk ) that learns social representations of a graph ’ s vertices , by modeling a stream of short random walks .
ho	6	[122]	sparsity enables the design of efficient discrete algorithms , but can make it harder to generalize in statistical learning . // machine learning applications in networks ( such as network classification , content recommendation , anomaly detection , and missing link prediction ) must be able to deal with this sparsity in order to survive . // in this paper we introduce deep learning ( unsupervised feature learning ) techniques , which have proven successful in natural language processing , into network analysis for the first time . we develop an algorithm ( deepwalk ) that learns social representations of a graph ’ s vertices , by modeling a stream of short random walks .
ho	6	[123]	sparsity enables the design of efficient discrete algorithms , but can make it harder to generalize in statistical learning . // machine learning applications in networks ( such as network classification , content recommendation , anomaly detection , and missing link prediction ) must be able to deal with this sparsity in order to survive . // in this paper we introduce deep learning ( unsupervised feature learning ) techniques , which have proven successful in natural language processing , into network analysis for the first time . we develop an algorithm ( deepwalk ) that learns social representations of a graph ’ s vertices , by modeling a stream of short random walks .
h+	6	[124]	machine learning applications in networks ( such as network classification , content recommendation , anomaly detection , and missing link prediction ) must be able to deal with this sparsity in order to survive . // in this paper we introduce deep learning ( unsupervised feature learning ) techniques , which have proven successful in natural language processing , into network analysis for the first time . // we develop an algorithm ( deepwalk ) that learns social representations of a graph ’ s vertices , by modeling a stream of short random walks .
b	6	[125]	deepwalk generalizes neural language models to process a special language composed of a set of randomly generated walks . // these neural language models have been used to capture the semantic and syntactic structure of human language , and even logical analogies . // deepwalk takes a graph as input and produces a latent representation as an output .
b	6	[126]	deepwalk generalizes neural language models to process a special language composed of a set of randomly generated walks . // these neural language models have been used to capture the semantic and syntactic structure of human language , and even logical analogies . // deepwalk takes a graph as input and produces a latent representation as an output .
h-	6	[33, 55]	our representation quality is not influenced by the choice of labeled vertices , so they can be shared among tasks . // deepwalk outperforms other latent representation methods for creating social dimensions , especially when labeled nodes are scarce . // strong performance with our representations is possible with very simple linear classifiers ( e .g . logistic regression ) .
ho	6	[68]	in our case , we can utilize the significant information about the dependence of the examples embedded in the structure of g to achieve superior performance . // in the literature , this is known as the relational classification ( or the collective classification problem ) . // traditional approaches to relational classification pose the problem as an inference in an undirected markov network , and then use iterative approximate inference algorithms ( such as the iterative classification algorithm , gibbs sampling , or label relaxation ) to compute the posterior distribution of labels given the network structure . we propose a different approach to capture the network topology information .
h-	6	[127]	in the literature , this is known as the relational classification ( or the collective classification problem ) . // traditional approaches to relational classification pose the problem as an inference in an undirected markov network , and then use iterative approximate inference algorithms ( such as the iterative classification algorithm , gibbs sampling , or label relaxation ) to compute the posterior distribution of labels given the network structure . // we propose a different approach to capture the network topology information .
h-	6	[130]	in the literature , this is known as the relational classification ( or the collective classification problem ) . // traditional approaches to relational classification pose the problem as an inference in an undirected markov network , and then use iterative approximate inference algorithms ( such as the iterative classification algorithm , gibbs sampling , or label relaxation ) to compute the posterior distribution of labels given the network structure . // we propose a different approach to capture the network topology information .
h-	6	[131]	instead of mixing the label space as part of the feature space , we propose an unsupervised method which learns features that capture the graph structure independent of the labels ’ distribution . // this separation between the structural representation and the labeling task avoids cascading errors , which can occur in iterative methods . // moreover , the same representation can be used for multiple classification problems concerning that network .
b	6	[122]	it is a stochastic process with random variables w1 vi , w2 vi , . . . , wk vi such that wk+1 vi is a vertex chosen at random from the neighbors of vertex vk . // random walks have been used as a similarity measure for a variety of problems in content recommendation and community detection . // they are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph .
ho	6	[133]	random walks have been used as a similarity measure for a variety of problems in content recommendation and community detection . // they are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph . // it is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network .
b	6	[35, 36]	however as the walk length grows , computing this objective function becomes unfeasible . // a recent relaxation in language modeling turns the prediction problem on its head . // first , instead of using the context to predict a missing word , it uses one word to predict the context .
ho	6	[35]	for each vertex vi we generate a random walk |wvi | = t , and then use it to update our representations ( line 7 ) . // we use the skipgram algorithm to update these representations in accordance with our objective function in eq . 2 . // skipgram .
b	6	[134]	the model parameter set is { φ , t } where the size of each is o ( d|v | ) . // stochastic gradient descent ( sgd ) is used to optimize these parameters ( line 4 , algorithm 2 ) . // the derivatives are estimated using the back propagation algorithm .
h+	6	[135]	this allows us to use asynchronous version of stochastic gradient descent ( asgd ) , in the multi worker case . // given that our updates are sparse and we do not acquire a lock to access the model shared parameters , asgd will achieve an optimal rate of convergence . // while we run experiments on one machine using multiple threads , it has been demonstrated that this technique is highly scalable , and can be used in very large scale machine learning . figure 4 presents the effects of parallelizing deepwalk .
h+	6	[136]	given that our updates are sparse and we do not acquire a lock to access the model shared parameters , asgd will achieve an optimal rate of convergence . // while we run experiments on one machine using multiple threads , it has been demonstrated that this technique is highly scalable , and can be used in very large scale machine learning . // figure 4 presents the effects of parallelizing deepwalk .
b	6	[33]	the labels represent the topic categories provided by the authors . // flickr is a network of the contacts between users of the photo sharing website . // the labels represent the interest groups of the users such as ‘ black and white photos ’ .
b	6	[56]	the labels represent the interest groups of the users such as ‘ black and white photos ’ . // youtube is a social network between users of the popular video sharing website . // the labels here represent groups of viewers that enjoy common video genres ( e .g . anime and wrestling ) .
b	6	[55]	to validate the performance of our approach we compare it against a number of baselines . // spectralclustering : this method generates a representation in r d from the d smallest eigenvectors of le , the normalized graph laplacian of g . // utilizing the eigenvectors of le implicitly assumes that graph cuts will be useful for classification .
b	6	[75]	utilizing the eigenvectors of le implicitly assumes that graph cuts will be useful for classification . // modularity : this method generates a representation in r d from the top d eigenvectors of b , the modularity matrix of g . the eigenvectors of b encode information about modular graph partitions of g . // using them as features assumes that modular graph partitions will be useful for classification .
b	6	[56]	using them as features assumes that modular graph partitions will be useful for classification . // edgecluster : this method uses k means clustering to cluster the adjacency matrix of g . // its has been shown to perform comparably to the modularity method , with the added advantage of scaling to graphs which are too large for spectral decomposition .
b	6	[137]	its has been shown to perform comparably to the modularity method , with the added advantage of scaling to graphs which are too large for spectral decomposition . // wvrn : the weighted vote relational neighbor is a relational classifier . // given the neighborhood ni of vertex vi , wvrn estimates pr ( yi|ni ) with the ( appropriately normalized ) weighted mean of its neighbors ( i .e pr ( yi|ni ) = 1 zpvj∈ni wij pr ( yj | nj ) ) .
ho	6	[33, 56]	multi label classification . // to facilitate the comparison between our method and the relevant baselines , we use the exact same datasets and experimental procedure as in . // specifically , we randomly sample a portion ( tr ) of the labeled nodes , and use them as training data .
ho	6	[33, 56]	we repeat this process 10 times , and report the average performance in terms of both macro f1 and micro f1 . // when possible we report the original results here directly . // for all models we use a one vs rest logistic regression implemented by liblinear for classification .
ho	6	[61]	when possible we report the original results here directly . // for all models we use a one vs rest logistic regression implemented by liblinear for classification . // we present results for deepwalk with ( γ = 80 , w = 10 , d = 128 ) . the results for ( spectralclustering , modularity , edgecluster ) use tang and liu ’ s preferred dimensionality , d = 500 .
h-	6	[55]	the main differences between our proposed method and previous work can be summarized as follows . // we learn our latent social representations , instead of computing statistics related to centrality or partitioning . // we do not attempt to extend the classification procedure itself ( through collective inference or graph kernels )
h-	6	[68]	we learn our latent social representations , instead of computing statistics related to centrality or partitioning . // we do not attempt to extend the classification procedure itself ( through collective inference or graph kernels ) . // we propose a scalable online method which uses only local information . most methods require global information and are offline .
h-	6	[140]	we learn our latent social representations , instead of computing statistics related to centrality or partitioning . // we do not attempt to extend the classification procedure itself ( through collective inference or graph kernels ) . // we propose a scalable online method which uses only local information . most methods require global information and are offline .
h-	6	[68]	relational classification ( or collective classification ) methods use links between data items as part of the classification process . // exact inference in the collective classification problem is np hard , and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge . // the most relevant relational classification algorithms to our work incorporate community information by learning clusters , by adding edges between nearby nodes , by using pagerank , or by extending relational classification to take additional features into account .
h-	6	[141]	exact inference in the collective classification problem is np hard , and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge . // the most relevant relational classification algorithms to our work incorporate community information by learning clusters , by adding edges between nearby nodes , by using pagerank , or by extending relational classification to take additional features into account . // our work takes a substantially different approach .
h-	6	[142]	exact inference in the collective classification problem is np hard , and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge . // the most relevant relational classification algorithms to our work incorporate community information by learning clusters , by adding edges between nearby nodes , by using pagerank , or by extending relational classification to take additional features into account . // our work takes a substantially different approach .
h-	6	[144]	exact inference in the collective classification problem is np hard , and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge . // the most relevant relational classification algorithms to our work incorporate community information by learning clusters , by adding edges between nearby nodes , by using pagerank , or by extending relational classification to take additional features into account . // our work takes a substantially different approach .
h-	6	[6]	in contrast to these methods , we frame the feature creation procedure as a representation learning problem . // graph kernels have been proposed as a way to use relational data as part of the classification process , but are quite slow unless approximated . // our approach is complementary ; instead of encoding the structure as part of a kernel function , we learn a representation which allows them to be used directly as features for any classification method .
h+	6	[145]	in contrast to these methods , we frame the feature creation procedure as a representation learning problem . // graph kernels have been proposed as a way to use relational data as part of the classification process , but are quite slow unless approximated . // our approach is complementary ; instead of encoding the structure as part of a kernel function , we learn a representation which allows them to be used directly as features for any classification method .
ho	6	[147]	computational costs and numerical instability led to these techniques to be abandoned for almost a decade . // recently , distributed computing allowed for larger models to be trained , and the growth of data for unsupervised learning algorithms to emerge . // distributed representations usually are trained through neural networks , these networks have made advancements in diverse fields such as computer vision , speech recognition , and natural language processing .
ho	6	[148]	computational costs and numerical instability led to these techniques to be abandoned for almost a decade . // recently , distributed computing allowed for larger models to be trained , and the growth of data for unsupervised learning algorithms to emerge . // distributed representations usually are trained through neural networks , these networks have made advancements in diverse fields such as computer vision , speech recognition , and natural language processing .
ho	6	[149]	recently , distributed computing allowed for larger models to be trained , and the growth of data for unsupervised learning algorithms to emerge . // distributed representations usually are trained through neural networks , these networks have made advancements in diverse fields such as computer vision , speech recognition , and natural language processing . // conclusions
ho	6	[125]	recently , distributed computing allowed for larger models to be trained , and the growth of data for unsupervised learning algorithms to emerge . // distributed representations usually are trained through neural networks , these networks have made advancements in diverse fields such as computer vision , speech recognition , and natural language processing . // conclusions
h+	1	[78]	this paper studies the problem of embedding information networks into lowdimensional spaces , in which every vertex is represented as a low dimensional vector . // such a low dimensional embedding is very useful in a variety of applications such as visualization , node classification , link prediction , and recommendation . // various methods of graph embedding have been proposed in the machine learning literature ( e .g . , ) .
h+	1	[11]	this paper studies the problem of embedding information networks into lowdimensional spaces , in which every vertex is represented as a low dimensional vector . // such a low dimensional embedding is very useful in a variety of applications such as visualization , node classification , link prediction , and recommendation . // various methods of graph embedding have been proposed in the machine learning literature ( e .g . , ) .
h+	1	[151]	this paper studies the problem of embedding information networks into lowdimensional spaces , in which every vertex is represented as a low dimensional vector . // such a low dimensional embedding is very useful in a variety of applications such as visualization , node classification , link prediction , and recommendation . // various methods of graph embedding have been proposed in the machine learning literature ( e .g . , ) .
h-	1	[152, 3, 5]	such a low dimensional embedding is very useful in a variety of applications such as visualization , node classification , link prediction , and recommendation . // various methods of graph embedding have been proposed in the machine learning literature ( e .g . , ) . // they generally perform well on smaller networks , but problem becomes much more challenging when a real world information network is concerned , which typically contains millions of nodes and billions of edges .
ho	1	[153]	the problem becomes much more challenging when a real world information network is concerned , which typically contains millions of nodes and billions of edges . // for example , the twitter followee follower network contains 175 million active users and around twenty billion edges in 2012 . // most existing graph embedding algorithms do not scale for networks of this size .
h-	1	[3]	most existing graph embedding algorithms do not scale for networks of this size . // for example , the time complexity of classical graph embedding algorithms such as mds , isomap , laplacian eigenmap are at least quadratic to the number of vertices , which is too expensive for networks with millions of nodes . // although a few very recent studies approach the embedding of large scale networks , these methods either use an indirect approach that is not designed for networks ( e .g . , ) or lack a clear objective function tailored for network embedding ( e .g . , ) .
h-	1	[5]	most existing graph embedding algorithms do not scale for networks of this size . // for example , the time complexity of classical graph embedding algorithms such as mds , isomap , laplacian eigenmap are at least quadratic to the number of vertices , which is too expensive for networks with millions of nodes . // although a few very recent studies approach the embedding of large scale networks , these methods either use an indirect approach that is not designed for networks ( e .g . , ) or lack a clear objective function tailored for network embedding ( e .g . , ) .
h-	1	[154]	for example , the time complexity of classical graph embedding algorithms such as mds , isomap , laplacian eigenmap are at least quadratic to the number of vertices , which is too expensive for networks with millions of nodes . // although a few very recent studies approach the embedding of large scale networks , these methods either use an indirect approach that is not designed for networks ( e .g . , ) or lack a clear objective function tailored for network embedding ( e .g . , ) . // we anticipate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the embedding of millions of nodes .
h-	1	[3]	naturally , the local structures are represented by the observed links in the networks , which capture the first order proximity between the vertices . // most existing graph embedding algorithms are designed to preserve this first order proximity , e .g . , isomap and laplacian eigenmap , even if they do not scale . // we observe that in a real world network many ( if not the majority of ) legitimate links are actually not observed .
h-	1	[5]	naturally , the local structures are represented by the observed links in the networks , which capture the first order proximity between the vertices . // most existing graph embedding algorithms are designed to preserve this first order proximity , e .g . , isomap and laplacian eigenmap , even if they do not scale . // we observe that in a real world network many ( if not the majority of ) legitimate links are actually not observed .
b	1	[155]	the general notion of the second order proximity can be interpreted as nodes with shared neighbors being likely to be similar . such an intuition can be found in the theories of sociology and linguistics . // for example , “ the degree of overlap of two people ’ s friendship networks correlates with the strength of ties between them , ” in a social network ; and “ you shall know a word by the company it keeps ” ( firth , j . r . 1957:11 ) in text corpora . // indeed , people who share many common friends are likely to share the same interest and become friends , and words that are used together with many similar words are likely to have similar meanings .
b	1	[152]	related work . // our work is related to classical methods of graph embedding or dimension reduction in general , such as multidimensional scaling ( mds ) , isomap , lle and laplacian eigenmap . // these approaches typically first construct the affinity graph using the feature vectors of the data points , e .g . , the k nearest neighbor graph of data , and then embed the affinity graph into a low dimensional space .
b	1	[3]	related work . // our work is related to classical methods of graph embedding or dimension reduction in general , such as multidimensional scaling ( mds ) , isomap , lle and laplacian eigenmap . // these approaches typically first construct the affinity graph using the feature vectors of the data points , e .g . , the k nearest neighbor graph of data , and then embed the affinity graph into a low dimensional space .
b	1	[4]	related work . // our work is related to classical methods of graph embedding or dimension reduction in general , such as multidimensional scaling ( mds ) , isomap , lle and laplacian eigenmap . // these approaches typically first construct the affinity graph using the feature vectors of the data points , e .g . , the k nearest neighbor graph of data , and then embed the affinity graph into a low dimensional space .
b	1	[157]	our work is related to classical methods of graph embedding or dimension reduction in general , such as multidimensional scaling ( mds ) , isomap , lle and laplacian eigenmap . // these approaches typically first construct the affinity graph using the feature vectors of the data points , e .g . , the k nearest neighbor graph of data , and then embed the affinity graph into a low dimensional space . // however , these algorithms usually rely on solving the leading eigenvectors of the affinity matrices , the complexity of which is at least quadratic to the number of nodes , making them inefficient to handle large scale networks .
b	1	[154]	however , these algorithms usually rely on solving the leading eigenvectors of the affinity matrices , the complexity of which is at least quadratic to the number of nodes , making them inefficient to handle large scale networks . // among the most recent literature is a technique called graph factorization . // it finds the low dimensional embedding of a large graph through matrix factorization , which is optimized using stochastic gradient descent .
h-	1	[6]	practically , the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs . // the most recent work related with ours is deepwalk , which deploys a truncated random walk for social network embedding . // although empirically effective , the deepwalk does not provide a clear objective that articulates what network properties are preserved .
ho	1	[158]	therefore , we minimize the following objective function : o2 = x i∈v λid ( ˆp2 ( ·|vi ) , p2 ( ·|vi ) ) , ( 5 ) where d ( · , · ) is the distance between two distributions . // as the importance of the vertices in the network may be different , we introduce λi in the objective function to represent the prestige of vertex i in the network , which can be measured by the degree or estimated through algorithms such as pagerank . // the empirical distribution ˆp2 ( ·|vi ) is defined as ˆp2 ( vj |vi ) = wij di , where wij is the weight of the edge ( i , j ) and di is the out degree of vertex i , i .e . di = p k∈n ( i ) wik , where n ( i ) is the set of out neighbors of vi .
ho	1	[36]	optimizing objective ( 6 ) is computationally expensive , which requires the summation over the entire set of vertices when calculating the conditional probability p2 ( ·|vi ) . // to address this problem , we adopt the approach of negative sampling proposed in , which samples multiple negative edges according to some noisy distribution for each edge ( i , j ) . // more specifically , it specifies the following objective function for each edge ( i , j ) : log σ ( ~u0 j t · ~ui ) +xk i=1 evn∼pn ( v ) , ( 7 ) where σ ( x ) = 1/ ( 1 + exp ( −x ) ) is the sigmoid function .
ho	1	[135]	to avoid the trivial solution , we can still utilize the negative sampling approach ( 7 ) by just changing ~u0t j to ~ut j . // we adopt the asynchronous stochastic gradient algorithm ( asgd ) for optimizing eqn ( 7 ) . // in each step , the asgd algorithm samples a mini batch of edges and then updates the model parameters .
ho	1	[159]	two types of citation networks are used : an author citation network and a paper citation network . // we use the dblp data set to construct the citation networks between authors and between papers . // the author citation network records the number of papers written by one author and cited by another author .
b	1	[154]	we do not compare with some classical graph embedding algorithms such as mds , isomap , and laplacian eigenmap , as they can not handle networks of this scale . // graph factorization ( gf ) : we compare with the matrix factorization techniques for graph factorization . // an information network can be represented as an affinity matrix , and is able to represent each vertex with a low dimensional vector through matrix factorization . graph factorization is optimized through stochastic gradient descent and is able to handle large networks . it only applies to undirected networks .
b	1	[6]	graph factorization is optimized through stochastic gradient descent and is able to handle large networks . it only applies to undirected networks . // deepwalk . deepwalk is an approach recently proposed for social network embedding , which is only applicable for networks with binary edges . // for each vertex , truncated random walks starting from the vertex are used to obtain the contextual information , and therefore only second order proximity is utilized .
ho	1	[6]	for fair comparisons , the dimensionality of the embeddings of the language network is set to 200 , as used in word embedding . // for other networks , the dimension is set as 128 by default , as used in . // other default settings include : the number of negative samples k = 5 for line and line sgd ; the total number of samples t = 10 billion for line ( 1st ) and line ( 2nd ) , t = 20 billion for gf ; window size win = 10 , walk length t = 40 , walks per vertex γ = 40 for deepwalk . all the embedding vectors are finally normalized by setting || ~w||2 = 1 .
ho	1	[35]	we start with the results on the language network , which contains two million nodes and a billion edges . // two applications are used to evaluate the effectiveness of the learned embeddings : word analogy and document classification . // word analogy .
b	1	[35]	word analogy . // this task is introduced by mikolov et al . . // given a word pair ( a , b ) and a word c , the task aims to find a word d , such that the relation between c and d is similar to the relation between a and b , or denoted as : a : b → c : ? .
b	1	[160]	for deepwalk , di erent cuto thresholds are tried to convert the language network into a binary network , and the best performance is achieved when all the edges are kept in the network . // we also compare with the state of the art word embedding model skipgram , which learns the word embeddings directly from the original wikipedia pages and is also implicitly a matrix factorization approach . // the window size is set as 5 , the same as used for constructing the language network .
ho	1	[161]	all document vectors are used to train a one vs rest logistic regression classi er using the liblinear package4 . // we report the classification metrics micro f1 and macro f1 . // the results are averaged over 10 different runs by sampling different training data .
ho	1	[78]	laying out this co author network is very challenging as the three research elds are very close to each other . // we first map the co author network into a low dimensional space with different embedding approaches and then further map the lowdimensional vectors of the vertices to a 2 d space with the t sne package . // fig . 2 compares the visualization results with different embedding approaches .
b	37	[163, 164]	in a typical node classification task , we are interested in predicting the most probable labels of nodes in a network . // for example , in a social network , we might be interested in predicting interests of users , or in a protein protein interaction network we might be interested in predicting functional labels of proteins . //
b	37	[70]	for example , in a social network , we might be interested in predicting interests of users , or in a protein protein interaction network we might be interested in predicting functional labels of proteins . // similarly , in link prediction , we wish to predict whether a pair of nodes in a network should have an edge connecting them . // link prediction is useful in a wide variety of domains ; for instance , in genomics , it helps us discover novel interactions between genes , and in social networks , it can identify real world friends .
b	37	[165, 166]	similarly , in link prediction , we wish to predict whether a pair of nodes in a network should have an edge connecting them . // link prediction is useful in a wide variety of domains ; for instance , in genomics , it helps us discover novel interactions between genes , and in social networks , it can identify real world friends . // any supervised machine learning algorithm requires a set of informative , discriminating , and independent features .
h-	37	[35, 167]	at the other extreme , the objective function can be defined to be independent of the downstream prediction task and the representations can be learned in a purely unsupervised way . // this makes the optimization computationally efficient and with a carefully designed objective , it results in task independent features that closely match task specific approaches in predictive accuracy . // however , current techniques fail to satisfactorily define and optimize a reasonable objective required for scalable unsupervised feature learning in networks .
h-	37	[168, 4, 3, 157]	however , current techniques fail to satisfactorily define and optimize a reasonable objective required for scalable unsupervised feature learning in networks . // classic approaches based on linear and non linear dimensionality reduction techniques such as principal component analysis , multi dimensional scaling and their extensions optimize an objective that transforms a representative data matrix of the network such that it maximizes the variance of the data representation . // consequently , these approaches invariably involve eigende composition of the appropriate data matrix which is expensive for large real world networks .
h-	37	[6, 1]	the objective can be efficiently optimized using stochastic gradient descent ( sgd ) akin to backpropogation on just single hidden layer feedforward neural networks . // recent attempts in this direction propose efficient algorithms but rely on a rigid notion of a network neighborhood , which results in these approaches being largely insensitive to connectivity patterns unique to networks . // specifically , nodes in networks could be organized based on communities they belong to ( i .e . , homophily ) .
ho	37	[35]	we propose node2vec , a semi supervised algorithm for scalable feature learning in networks . // we optimize a custom graph based objective function using sgd motivated by prior work on natural language processing . // intuitively , our approach returns feature representations that maximize the likelihood of preserving network neighborhoods of nodes in a d dimensional feature space .
h-	37	[6, 1]	we achieve this by developing a family of biased random walks , which efficiently explore diverse neighborhoods of a given node . // the resulting algorithm is flexible , giving us control over the search space through tunable parameters , in contrast to rigid search procedures in prior work . // consequently , our method generalizes prior work and can model the full spectrum of equivalences observed in networks .
b	37	[6, 1]	our experiments focus on two common prediction tasks in networks : a multi label classification task , where every node is assigned one or more class labels and a link prediction task , where we predict the existence of an edge given a pair of nodes . // we contrast the performance of node2vec with state of the art feature learning algorithms . // we experiment with several real world networks from diverse domains , such as social networks , information networks , as well as networks from systems biology .
h-	37	[168, 4, 3, 157]	under this linear algebra perspective , these methods can be viewed as dimensionality reduction techniques .under this linear algebra perspective , these methods can be viewed as dimensionality reduction techniques . // several linear ( e .g . , pca ) and non linear ( e .g . , isomap ) dimensionality reduction techniques have been proposed . // these methods suffer from both computational and statistical performance drawbacks . in terms of computational efficiency , eigendecomposition of a data matrix is expensive unless the solution quality is significantly compromised with approximations , and hence , these methods are hard to scale to large networks .
h-	37	[55]	secondly , these methods optimize for objectives that are not robust to the diverse patterns observed in networks ( such as homophily and structural equivalence ) and make assumptions about the relationship between the underlying network structure and the prediction task . // for instance , spectral clustering makes a strong homophily assumption that graph cuts will be useful for classification . // such assumptions are reasonable in many scenarios , but unsatisfactory in effectively generalizing across diverse networks .
b	37	[35]	recent advancements in representational learning for natural language processing opened new ways for feature learning of discrete objects such as words . // in particular , the skip gram model aims to learn continuous feature representations for words by optimizing a neighborhood preserving likelihood objective . // the algorithm proceeds as follows : it scans over the words of a document , and for every word it aims to embed it such that the word ’ s features can predict nearby words ( i .e . , words inside some context window ) .
b	37	[172]	the word feature representations are learned by optmizing the likelihood objective using sgd with negative sampling . // the skip gram objective is based on the distributional hypothesis which states that words in similar contexts tend to have similar meanings . // that is , similar words tend to appear in similar word neighborhoods
b	37	[6, 1]	that is , similar words tend to appear in similar word neighborhoods . // inspired by the skip gram model , recent research established an analogy for networks by representing a network as a “ document ” . // the same way as a document is an ordered sequence of words , one could sample sequences of nodes from the underlying network and turn a network into a ordered sequence of nodes .
h-	37	[6, 1]	in fact , as we shall show , there is no clear winning sampling strategy that works across all networks and all prediction tasks . // this is a major shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network . // our algorithm node2vec overcomes this limitation by designing a flexible objective that is not tied to a particular sampling strategy and provides parameters to tune the explored search space ( see section 3 ) .
ho	37	[35, 6]	for every source node u ∈ v , we define ns ( u ) ⊂ v as a network neighborhood of node u generated through a neighborhood sampling strategy s . // we proceed by extending the skip gram architecture to networks . // we seek to optimize the following objective function , which maximizes the log probability of observing a network neighborhood ns ( u ) for a node u conditioned on its feature representation , given by f : max f x u∈v log p r ( ns ( u ) |f ( u ) ) .
h+	37	[36]	with the above assumptions , the objective in eq . 1 simplifies to . // the per node partition function , zu = p v∈v exp ( f ( u ) · f ( v ) ) , is expensive to compute for large networks and we approximate it using negative sampling . // we optimize eq . 2 using stochastic gradient ascent over the model parameters defining the features f .
b	37	[178]	the breadth first and depth first sampling represent extreme scenarios in terms of the search space they explore leading to interesting implications on the learned representations . // in particular , prediction tasks on nodes in networks often shuttle between two kinds of similarities : homophily and structural equivalence . // under the homophily hypothesis nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together ( e .g . , nodes s1 and u in figure 1 belong to the same network community ) .
b	37	[169]	under the homophily hypothesis nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together ( e .g . , nodes s1 and u in figure 1 belong to the same network community ) . // in contrast , under the structural equivalence hypothesis nodes that have similar structural roles in networks should be embedded closely together ( e .g . , nodes u and s6 in figure 1 act as hubs of their corresponding communities ) . // importantly , unlike homophily , structural equivalence does not emphasize connectivity ; nodes could be far apart in the network and still have the same structural role .
ho	37	[179]	we now aim to empirically demonstrate this fact and show that node2vec in fact can discover embeddings that obey both principles . // we use a network where nodes correspond to characters in the novel les misérables and edges connect coappearing characters . // the network has 77 nodes and 254 edg
b	37	[55]	// spectral clustering : this is a matrix factorization approach in which we take the top d eigenvectors of the normalized laplacian matrix of graph g as the feature vector representations for nodes . // deepwalk : this approach learns d dimensional feature representations by simulating uniform random walks . the sampling strategy in deepwalk can be seen as a special case of node2vec with p = 1 and q = 1 .
b	37	[1]	deepwalk : this approach learns d dimensional feature representations by simulating uniform random walks . the sampling strategy in deepwalk can be seen as a special case of node2vec with p = 1 and q = 1 . // line : this approach learns d dimensional feature representations in two separate phases . // in the first phase , it learns d/2 dimensions by bfs style simulations over immediate neighbors of nodes .
ho	37	[6]	in the second phase , it learns the next d/2 dimensions by sampling nodes strictly at a 2 hop distance from the source nodes . // we exclude other matrix factorization approaches which have already been shown to be inferior to deepwalk . // we also exclude a recent approach , grarep , that generalizes line to incorporate information from network neighborhoods beyond 2 hops , but is unable to efficiently scale to large networks .
h-	37	[26]	we exclude other matrix factorization approaches which have already been shown to be inferior to deepwalk . // we also exclude a recent approach , grarep , that generalizes line to incorporate information from network neighborhoods beyond 2 hops , but is unable to efficiently scale to large networks . // in contrast to the setup used in prior work for evaluating samplingbased feature learning algorithms , we generate an equal number of samples for each method and then evaluate the quality of the obtained features on the prediction task .
b	37	[180]	this is a challenging task especially if l is large . // blogcatalog : this is a network of social relationships of the bloggers listed on the blogcatalog website . // the labels represent blogger interests inferred through the metadata provided by the bloggers , the network has 10 ,312 nodes , 333 ,983 edges , and 39 different labels .
ho	37	[181]	the network has 10 ,312 nodes , 333 ,983 edges , and 39 different labels . // protein protein interactions ( ppi ) : we use a subgraph of the ppi network for homo sapiens . // the subgraph corresponds to the graph induced by nodes for which we could obtain labels from the hallmark gene sets and represent biological states .
ho	37	[87]	protein protein interactions ( ppi ) : we use a subgraph of the ppi network for homo sapiens . // the subgraph corresponds to the graph induced by nodes for which we could obtain labels from the hallmark gene sets and represent biological states . // the network has 3 ,890 nodes , 76 ,584 edges , and 50 different labels .
b	37	[183]	wikipedia : this is a cooccurrence network of words appearing in the first million bytes of the wikipedia dump . // the labels represent the part of speech ( pos ) tags inferred using the stanford pos tagger . // the network has 4 ,777 nodes , 184 ,812 edges , and 40 different labels .
b	37	[184]	we test our benchmarks on the following datasets . // facebook : in the facebook network , nodes represent users , and edges represent a friendship relation between any two users . // the network has 4 ,039 nodes and 88 ,234 edges .
b	37	[181]	the network has 4 ,039 nodes and 88 ,234 edges . // protein protein interactions ( ppi ) : in the ppi network for homo sapiens , nodes represent proteins , and an edge indicates a biological interaction between a pair of proteins . // the network has 19 ,706 nodes and 390 ,633 edges .
h-	37	[185]	the best p and q parameter settings for each node2vec entry are omitted for ease of presentation . // a general observation we can draw from the results is that the learned feature representations for node pairs significantly outperform the heuristic benchmark scores with node2vec achieving the best auc improvement on 12 .6 % on the arxiv dataset over the best performing baseline ( adamic adar ) . // amongst the feature learning algorithms , node2vec outperforms both deepwalk and line in all networks with gain up to 3 .8 % and 6 .5 % respectively in the auc scores for the best possible choices of the binary operator for each algorithm .
h-	37	[6]	both deepwalk and line can be seen as rigid search strategies over networks . // deepwalk proposes search using uniform random walks . // the obvious limitation with such a strategy is that it gives us no control over the explored neighborhoods .
h-	37	[1]	the obvious limitation with such a strategy is that it gives us no control over the explored neighborhoods . // line proposes primarily a breadth first strategy , sampling nodes and optimizing the likelihood independently over only 1 hop and 2 hop neighbors . // the effect of such an exploration is easier to characterize , but it is restrictive and provides no flexibility in exploring nodes at further depths .
h-	417	[187, 188]	contextual behavior modeling has been broadly used for pattern discovery and predictive analysis . // tensor methods decompose multidimensional counts ( e .g . , numbers of co occurrence among one author , one conference , and one reference ) into lowdimensional latent vectors , however , real behaviors do not guarantee one context item per dimension/type – a paper can have multiple authors and multiple references . // jamali et al . proposed a multicontextual factor model to learn latent factors of users and items for product recommendation , but neither tensor methods nor factor models can scale for massive behavior data .
ho	417	[189]	contextual behavior modeling has been broadly used for pattern discovery and predictive analysis . // tensor methods decompose multidimensional counts ( e .g . , numbers of co occurrence among one author , one conference , and one reference ) into lowdimensional latent vectors , however , real behaviors do not guarantee one context item per dimension/type – a paper can have multiple authors and multiple references . // jamali et al . proposed a multicontextual factor model to learn latent factors of users and items for product recommendation , but neither tensor methods nor factor models can scale for massive behavior data .
h-	417	[190]	tensor methods decompose multidimensional counts ( e .g . , numbers of co occurrence among one author , one conference , and one reference ) into lowdimensional latent vectors , however , real behaviors do not guarantee one context item per dimension/type – a paper can have multiple authors and multiple references . // jamali et al . proposed a multicontextual factor model to learn latent factors of users and items for product recommendation , but neither tensor methods nor factor models can scale for massive behavior data . // recently embedding methods have been proposed to e ciently learn low dimensional vectors of nodes from large heterogeneous networks .
b	417	[194]	there has been a wide line of research on learning latent representations of context items in behavior data towards various applications . // agarwal et al . proposed localized factor models combining multi context information to improve predictive accuracy in recommender systems . // jamali et al . proposed context dependent factor models to learn latent factors of users and items for recommendation .
b	417	[190]	agarwal et al . proposed localized factor models combining multi context information to improve predictive accuracy in recommender systems . // jamali et al . proposed context dependent factor models to learn latent factors of users and items for recommendation . // besides factor models , tensor decompositions have been widely used for modeling multi contextual data .
b	417	[195]	jamali et al . proposed context dependent factor models to learn latent factors of users and items for recommendation . // besides factor models , tensor decompositions have been widely used for modeling multi contextual data . // jiang et al . proposed a tensor sequence decomposition approach for discovering multifaceted behavioral patterns .
b	417	[196]	jiang et al . proposed a tensor sequence decomposition approach for discovering multifaceted behavioral patterns . // ermiş et al . studied various alternative tensor models for link prediction in heterogeneous data . // lian et al . proposed regularized tensor factorization for spatiotemporal recommendation .
b	417	[188]	ermiş et al . studied various alternative tensor models for link prediction in heterogeneous data . // lian et al . proposed regularized tensor factorization for spatiotemporal recommendation . // yang et al . developed a predictive task guided tensor decomposition model for representation learning from electronic health records .
b	417	[197]	lian et al . proposed regularized tensor factorization for spatiotemporal recommendation . // yang et al . developed a predictive task guided tensor decomposition model for representation learning from electronic health records . // perros et al . designed a scalable parafac2 tensor model for large and sparse datasets .
b	417	[26, 24, 199, 7]	network data representation learning . // network embedding methods learn node representations that preserve node proximities ( e .g . , one hop or two hop connections ) in network data . // deepwalk used random walks to expand the neighborhood of a node and expected nodes with higher proximity yield similar representations .
b	417	[6]	network embedding methods learn node representations that preserve node proximities ( e .g . , one hop or two hop connections ) in network data . // deepwalk used random walks to expand the neighborhood of a node and expected nodes with higher proximity yield similar representations . // node2vec presented biased random walkers to diversify the neighborhood .
b	417	[37]	deepwalk used random walks to expand the neighborhood of a node and expected nodes with higher proximity yield similar representations . // node2vec presented biased random walkers to diversify the neighborhood . // line provided clear objectives for homogeneous network embedding that articulates what network properties are preserved .
b	417	[109, 111, 200, 110, 19]	line provided clear objectives for homogeneous network embedding that articulates what network properties are preserved . // we have spotted a series of heterogeneous network embedding work that capture heterogeneous structural properties in network data . // if we explicitly represent behavior entries as nodes and thus behavior datasets are represented as behavior item heterogeneous bipartite networks , existing network embedding methods can be applied to learn representations of both items and behaviors .
b	417	[201]	therefore , it finds complementary items that will maximize the success of a target behavior . // there are other network embedding methods designed to learn task specific node representations by utilizing label information in supervised fashion such as pte . // chen et al . proposed a task guided and path augmented heterogeneous network embedding model to identify author names given anonymized paper ’ s title .
b	417	[202]	there are other network embedding methods designed to learn task specific node representations by utilizing label information in supervised fashion such as pte . // chen et al . proposed a task guided and path augmented heterogeneous network embedding model to identify author names given anonymized paper ’ s title . // the most recent work pne decomposes a partially labeled network into two bipartite networks and encodes the node label information by learning label and context vectors from the label context network . in our case , we only have the success rate for behaviors , which can be binary or real values ; and , we aims at learning the item representations preserving behavior success .
b	417	[204, 35, 36]	with the success of deep learning techniques , representation learning becomes popular starting from practices on text data . // mikolov et al . proposed the word2vec framework to learn the distributed representations of words in natural language . // pennington et al . proposed glove to learn word vectors from nonzero elements in a word word cooccurrence matrix .
b	417	[167]	mikolov et al . proposed the word2vec framework to learn the distributed representations of words in natural language . // pennington et al . proposed glove to learn word vectors from nonzero elements in a word word cooccurrence matrix . // le et al . extended the embedded objects from words or phrases to paragraphs .
b	417	[64]	pennington et al . proposed glove to learn word vectors from nonzero elements in a word word cooccurrence matrix . // le et al . extended the embedded objects from words or phrases to paragraphs . // recently , nichel et al . proposed poincaré embedding based on a non euclidean space to preserve hierarchical semantic structures .
b	417	[206]	le et al . extended the embedded objects from words or phrases to paragraphs . // recently , nichel et al . proposed poincaré embedding based on a non euclidean space to preserve hierarchical semantic structures . // our work focuses on representation learning from behavior data .
ho	417	[207]	for tweet posting behaviors , if the success is measured as the existence of their context itemsets in tweet data , we have tons of positive behaviors but no real negative ones . // if the success is measured as the behavior ’ s popularity , the success rate is the number of views , likes , retweets , or shares . // in this case , positive behaviors are popular posts and negative behaviors are unpopular but still real posts .
h-	417	[6]	in this way , the contributions of each context item towards behavior ’ s success are preserved . // unlike network embedding models such as deepwalk , line and node2vec that preserve proximities and were evaluated on clustering tasks , our behavior data embedding model , also a multi type itemset embedding model , preserves success property . // we will evaluate it on the two tasks of behavior modeling we have introduced in section 1 and compete with existing works in experiments .
h-	417	[37]	in this way , the contributions of each context item towards behavior ’ s success are preserved . // unlike network embedding models such as deepwalk , line and node2vec that preserve proximities and were evaluated on clustering tasks , our behavior data embedding model , also a multi type itemset embedding model , preserves success property . // we will evaluate it on the two tasks of behavior modeling we have introduced in section 1 and compete with existing works in experiments .
b	417	[208]	since there is not any itemset embedding method in the literature , we compare our learnsuc with the dimensionality reduction methods and the state of the art network embedding models . // pca : it learns data representations that describe as much of the variance in the data as possible . // line : this homogeneous network embedding method preserves the 1st and 2nd order of node proximity .
b	417	[1]	pca : it learns data representations that describe as much of the variance in the data as possible . // line : this homogeneous network embedding method preserves the 1st and 2nd order of node proximity . // deepwalk : it uses local information obtained from truncated random walks to learn latent representations of vertices in a network .
b	417	[37]	deepwalk : it uses local information obtained from truncated random walks to learn latent representations of vertices in a network . // node2vec : it learns continuous feature representations for nodes in networks by maximizing the likelihood of preserving network neighborhoods of nodes . // metapath2vec : this method learns node representations based on meta path based random walks from heterogeneous networks .
b	417	[111]	node2vec : it learns continuous feature representations for nodes in networks by maximizing the likelihood of preserving network neighborhoods of nodes . // metapath2vec : this method learns node representations based on meta path based random walks from heterogeneous networks . // we evaluate learnsuc ’ s alternatives such as learnsuc pn and learnsuc pt to compare the e ectiveness of di erent negative sampling strategies .
h+	417	[111]	overall performance . // the best baseline method is a heterogeneous network embedding method metapath2vec which gives an mae of 0 .0567 and an rmse of 0 .1648 that are much better than the error by random guess ( 0 .5 ) , because pair wise similarity plays an important role in generating behaviors . // for example , co authors are often working in very similar research elds
ho	417	[1]	the traditional dimensionality reduction technique pca is able to capture 28 .4 % of total varianc . // its performance is not significantly different from the performance of line . // the network embedding methods show very high auc and f1 because the pairwise similarities between the items ( e .g . , authors and keywords ) do have impact on the chance of collaboration
h+	417	[6]	comparing network embedding methods . // first , deepwalk and node2vec perform better than line in this task . it shows preserving random walk based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration . // second , metapath2vec learns the low dimensional representations of nodes from rich meta path based features
h+	417	[37]	comparing network embedding methods . // first , deepwalk and node2vec perform better than line in this task . it shows preserving random walk based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration . // second , metapath2vec learns the low dimensional representations of nodes from rich meta path based features
b	417	[111]	first , deepwalk and node2vec perform better than line in this task . it shows preserving random walk based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration . // second , metapath2vec learns the low dimensional representations of nodes from rich meta path based features . // it models the heterogeneity of the network .
b	417	[209]	all the papers have been successful ( of a success rate above 0 .6 ) but higher success rate indicates more likely to be impactful . // one of the good examples is the paper “ inferring social ties across heterogeneous networks ” published in wsdm 2012 . // the leading author prof . jie tang ( tsinghua university ) is a data mining expert on a subset of the keywords such as “ factor graph ” , “ heterogeneous network ” , “ predictive model ” ; and the co author prof . jon kleinberg ( cornell university ) has world level reputation in computational social science of the keywords such as “ social theory ” , “ social influence ” , and “ social ties ” .
ho	52	[210, 211]	in almost all networks , nodes tend to have one or more functions that greatly determine their role in the system . // for example , individuals in a social network have a social role or social position , while proteins in a protein protein interaction ( ppi ) network exert specific functions . // intuitively , different nodes in such networks may perform similar functions , such as interns in the social network of a corporation or catalysts in the ppi network of a cell .
b	52	[210, 214, 211]	in this context , not even the labels of the nodes matter but just their relationship to other nodes ( edges ) . // indeed , mathematical sociologists have worked on this problem since the 1970s , defining and computing structural identity of individuals in social networks . // beyond sociology , the role of webpages in the webgraph is another example of identity ( in this case , hubs and authorities ) emerging from the network structure , as dened by the celebrated work of kleinberg .
b	52	[215]	indeed , mathematical sociologists have worked on this problem since the 1970s , defining and computing structural identity of individuals in social networks . // beyond sociology , the role of webpages in the webgraph is another example of identity ( in this case , hubs and authorities ) emerging from the network structure , as defined by the celebrated work of kleinberg . // the most common practical approaches to determine the structural identity of nodes are based on distances or recursions .
b	52	[122, 216]	the most common practical approaches to determine the structural identity of nodes are based on distances or recursions . // in the former , a distance function that leverages the neighborhood of the nodes is used to measure the distance between all node pairs , with clustering or matching then performed to place nodes into equivalent classes . // in the later , a recursion with respect to neighboring nodes is constructed and then iteratively unfolded until convergence , with final values used to determine the equivalent classes .
h+	52	[37, 219, 6, 1]	while such approaches have advantages and disadvantages , we provide an alternative methodology , one based on unsupervised learning of representations for the structural identity of nodes ( to be presented ) . // recent efforts in learning latent representations for nodes in networks have been quite successful in performing classification and prediction tasks . // in particular , these efforts encode nodes using as context a generalized notion of their neighborhood ( e .g . , w steps of a random walk , or nodes with neighbors in common ) .
h-	52	[6]	. figure 1 illustrates the problem , where nodes u and v play similar roles ( i .e . , have similar local structures ) but are very far apart in the network . since their neighborhoods have no common nodes , recent approaches can not capture their structural similarity ( as we soon show ) . // it is worth noting why recent approaches for learning node representations such as deepwalk and node2vec succeed in classification tasks but tend to fail in structural equivalence tasks . // the key point is that many node features in most real networks exhibit a strong homophily ( e .g . , two blogs with the same political inclination are much more likely to be connected than at random ) .
h-	52	[37]	. figure 1 illustrates the problem , where nodes u and v play similar roles ( i .e . , have similar local structures ) but are very far apart in the network . since their neighborhoods have no common nodes , recent approaches can not capture their structural similarity ( as we soon show ) . // it is worth noting why recent approaches for learning node representations such as deepwalk and node2vec succeed in classification tasks but tend to fail in structural equivalence tasks . // the key point is that many node features in most real networks exhibit a strong homophily ( e .g . , two blogs with the same political inclination are much more likely to be connected than at random ) .
h-	52	[37]	such context can be leveraged by language models to learn latent representation for the nodes . // we implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks , comparing its performance with deepwalk and node2vec – two state of the art techniques for learning latent representations for nodes , and with rolx – a recent approach to identify roles of nodes . // our results indicate that while deepwalk and node2vec fail to capture the notion of structural identity , struc2vec excels on this task – even when the original network is subject to strong random noise ( random edge removal ) .
b	52	[169]	such context can be leveraged by language models to learn latent representation for the nodes . // we implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks , comparing its performance with deepwalk and node2vec – two state of the art techniques for learning latent representations for nodes , and with rolx – a recent approach to identify roles of nodes . // our results indicate that while deepwalk and node2vec fail to capture the notion of structural identity , struc2vec excels on this task – even when the original network is subject to strong random noise ( random edge removal ) .
b	52	[147]	the technique is instrumental for machine learning applications that leverage network data , as node embeddings can be directly used in tasks such as classification and clustering . // in natural language processing , generating dense embeddings for sparse data has a long history . //
b	52	[6]	among other properties , the learned language model places semantically similar words near each other in space . // learning a language model from a network was first proposed by deepwalk . // it uses random walks to generate sequences of nodes from the network , which are then treated as sentences by skip gram .
b	52	[37]	intuitively , nodes close in the network will tend to have similar contexts ( sequences ) and thus have embeddings that are near one another . // this idea was later extended by node2vec . // by proposing a biased second order random walk model , node2vec provides more flexibility when generating the context of a vertex .
b	52	[219]	however , a fundamental limitation is that structurally similar nodes will never share the same context if their distance ( hop count ) is larger than the skip gram window . // subgraph2vec is another recent approach for learning embeddings for rooted subgraphs , and unlike the previous techniques it does not use random walks to generate context . // alternatively , the context of a node is simply dened by its neighbors .
b	52	[9, 19]	thus , two nodes that are structurally very similar ( but fail the test ) and have non overlapping neighbors may not be close in space . // similarly to subgraph2vec , considerable effort has recently been made on learning richer representations for network nodes . // however , building representations that explicitly capture structural identity is a relative orthogonal problem that has not received much attention .
b	52	[169]	however , building representations that explicitly capture structural identity is a relative orthogonal problem that has not received much aention . is is the focus of struc2vec . // this is the focus of struc2vec . a recent approach to explicitly identify the role of nodes using just the network structure is rolx . // this unsupervised approach is based on enumerating various structural features for nodes , finding the more suited basis vector for this joint feature space , and then assigning for every node a distribution over the identified roles ( basis ) , allowing for mixed membership across the roles .
h+	52	[35]	informally , the task can be defined as learning word probabilities given a context . // in particular , skip gram has proven to be effective at learning meaningful representations for a variety of data . // in order to apply it to networks , it suffices to use artificially generated node sequences instead of word sentence
ho	111	[222]	// neural network based learning models can represent latent embeddings that capture the internal relations of rich , complex data across various modalities , such as image , audio , and language . // social and information networks are similarly rich and complex data that encode the dynamics and types of human interactions , and are similarly amenable to representation learning using neural networks .
b	111	[124]	social and information networks are similarly rich and complex data that encode the dynamics and types of human interactions , and are similarly amenable to representation learning using neural networks . // in particular , by mapping the way that people choose friends and maintain connections as a “ social language , ” recent advances in natural language processing ( nlp ) can be naturally applied to network representation learning , most notably the group of nlp models known as word2vec . // a number of recent research publications have proposed word2vec based network representation learning frameworks , such as deepwalk , line , and node2vec .
h+	111	[35, 36]	social and information networks are similarly rich and complex data that encode the dynamics and types of human interactions , and are similarly amenable to representation learning using neural networks . // in particular , by mapping the way that people choose friends and maintain connections as a “ social language , ” recent advances in natural language processing ( nlp ) can be naturally applied to network representation learning , most notably the group of nlp models known as word2vec . // a number of recent research publications have proposed word2vec based network representation learning frameworks , such as deepwalk , line , and node2vec .
h+	111	[1]	in particular , by mapping the way that people choose friends and maintain connections as a “ social language , ” recent advances in natural language processing ( nlp ) can be naturally applied to network representation learning , most notably the group of nlp models known as word2vec . // a number of recent research publications have proposed word2vec based network representation learning frameworks , such as deepwalk , line , and node2vec . // instead of handcrafted network feature design , these representation learning methods enable the automatic discovery of useful and meaningful ( latent ) features from the “ raw networks . ”
h+	111	[37]	in particular , by mapping the way that people choose friends and maintain connections as a “ social language , ” recent advances in natural language processing ( nlp ) can be naturally applied to network representation learning , most notably the group of nlp models known as word2vec . // a number of recent research publications have proposed word2vec based network representation learning frameworks , such as deepwalk , line , and node2vec . // instead of handcrafted network feature design , these representation learning methods enable the automatic discovery of useful and meaningful ( latent ) features from the “ raw networks . ”
ho	111	[223]	however , these work has thus far focused on representation learning for homogeneous networks—representative of singular type of nodes and relationships . // yet a large number of social and information networks are heterogeneous in nature , involving diversity of node types and/or relationships between nodes . // these heterogeneous networks present unique challenges that can not be handled by representation learning models that are specifically designed for homogeneous networks .
h+	111	[225, 226]	can we directly apply homogeneous network oriented embedding architectures ( e .g . , skip gram ) to heterogeneous networks ? // by solving these challenges , the latent heterogeneous network embeddings can be further applied to various network mining tasks , such as node classification , clustering , and similarity search . // in contrast to conventional meta path based methods , the advantage of latent space representation learning lies in its ability to model similarities between nodes without connected meta paths .
h+	111	[224, 227]	can we directly apply homogeneous network oriented embedding architectures ( e .g . , skip gram ) to heterogeneous networks ? // by solving these challenges , the latent heterogeneous network embeddings can be further applied to various network mining tasks , such as node classification , clustering , and similarity search . // in contrast to conventional meta path based methods , the advantage of latent space representation learning lies in its ability to model similarities between nodes without connected meta paths .
h-	111	[223]	by solving these challenges , the latent heterogeneous network embeddings can be further applied to various network mining tasks , such as node classification , clustering , and similarity search . // in contrast to conventional meta path based methods , the advantage of latent space representation learning lies in its ability to model similarities between nodes without connected meta paths . // for example , if authors have never published papers in the same venue—imagine one publishes 10 papers all in nips and the other has 10 publications all in icml ; their “ apcpa ” based pathsim similarity would be zero—this will be naturally overcome by network representation learning .
ho	111	[223]	the goal of metapath2vec is to maximize the likelihood of preserving both the structures and semantics of a given heterogeneous network . // in metapath2vec , we first propose meta path based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes . // second , we extend the skip gram model to facilitate the modeling of geographically and semantically close nodes .
ho	111	[36]	in metapath2vec , we first propose meta path based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes . // second , we extend the skip gram model to facilitate the modeling of geographically and semantically close nodes . // finally , we develop a heterogeneous negative sampling based method , referred to as metapath2vec++ , that enables the accurate and effecient prediction of a node ’ s heterogeneous neighborhood .
h-	111	[37, 6, 1]	finally , we develop a heterogeneous negative sampling based method , referred to as metapath2vec++ , that enables the accurate and effecient prediction of a node ’ s heterogeneous neighborhood . // the proposed metapath2vec and metapath2vec++ models are different from conventional network embedding models , which focus on homogeneous networks . // specifically , conventional models suffer from the identical treatment of different types of nodes and relations , leading to the production of indistinguishable representations for heterogeneous nodes—as evident through our evaluation .
b	111	[224]	essentially , the raw input of pte is words and its output is the embedding of each word , rather than multiple types of objects . // we summarize the differences of these methods in table 1 , which lists their input to learning algorithms , as well as the top five similarity search results in the dbis network for the same two queries used in ( see section 4 for details ) . // by modeling the heterogeneous neighborhood and further leveraging the heterogeneous negative sampling technique , metapath2vec++ is able to achieve the best top five similar results for both types of queries .
ho	111	[159]	data . // we use two heterogeneous networks , including the aminer computer science ( cs ) dataset and the database and information systems ( dbis ) dataset . // both datasets and code are publicly available .
ho	111	[224]	data . // we use two heterogeneous networks , including the aminer computer science ( cs ) dataset and the database and information systems ( dbis ) dataset . // both datasets and code are publicly available .
ho	111	[223, 225]	we formalize the representation learning problem in heterogeneous networks , which was first briefly introduced in . // in specific , we leverage the definition of heterogeneous networks in and present the learning problem with its inputs and outputs . // definition .
ho	111	[37, 6, 1]	the main challenge of this problem comes from the network heterogeneity , wherein it is difficult to directly apply homogeneous language and network embedding methods . // the premise of network embedding models is to preserve the proximity between a node and its neighborhood ( context ) . // in a heterogeneous environment , how do we define and model this ‘ node–neighborhood ’ concept ?
b	111	[35, 36]	we , rst , briey introduce the word2vec model and its application to homogeneous network embedding tasks . // given a text corpus , mikolov et al . proposed word2vec to learn the distributed representations of words in a corpus . // inspired by it , deepwalk and node2vec aim to map the word context concept in a text corpus into a network . both methods leverage random walks to achieve this and utilize the skip gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network .
b	111	[37]	given a text corpus , mikolov et al . proposed word2vec to learn the distributed representations of words in a corpus . // inspired by it , deepwalk and node2vec aim to map the word context concept in a text corpus into a network . // both methods leverage random walks to achieve this and utilize the skip gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network .
b	111	[37, 36, 6]	both methods leverage random walks to achieve this and utilize the skip gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network . // usually , given a network g = ( v , e ) , the objective is to maximize the network probability in terms of local structures , that is : arg max θ y v ∈v y c ∈n ( v ) p ( c|v ; θ ) ( 1 ) where n ( v ) is the neighborhood of node v in the network g , which can be defined in different ways such as v ’ s one hop neighbors , and p ( c|v ; θ ) defines the conditional probability of having a context node c given a node v . // 3 .2 heterogeneous network embedding : metapath2vec .
ho	111	[6]	meta path based random walks . how to effectively transform the structure of a network into skip gram ? // in deepwalk and node2vec , this is achieved by incorporating the node paths traversed by random walkers over a network into the neighborhood function . // naturally , we can put random walkers in a heterogeneous network to generate paths of multiple types of nodes .
b	111	[224]	the links represent dierent types of relationships among three sets of nodes— such as collaboration relationships on a paper . // the dbis dataset was constructed and used by sun et al . . // it covers 464 venues , their top 5000 authors , and corresponding 72 ,902 publications .
ho	111	[6]	we compare metapath2vec and metapath2vec++ with several recent network representation learning methods . // deepwalk / node2vec : with the same random walk path input ( p=1 & q=1 in node2vec ) , we find that the choice between hierarchical softmax ( deepwalk ) and negative sampling ( node2vec ) techniques does not yield significant differences . // therefore we use p=1 and q=1 in node2vec for comparison .
ho	111	[37]	we compare metapath2vec and metapath2vec++ with several recent network representation learning methods . // deepwalk / node2vec : with the same random walk path input ( p=1 & q=1 in node2vec ) , we find that the choice between hierarchical softmax ( deepwalk ) and negative sampling ( node2vec ) techniques does not yield significant differences . // therefore we use p=1 and q=1 in node2vec for comparison .
ho	111	[1]	therefore we use p=1 and q=1 in node2vec for comparison . // line : we use the advanced version of line by considering both the 1st and 2nd order of node proximity . // pte : we construct three bipartite heterogeneous networks ( author–author , author–venue , venue–venue ) and restrain it as an unsupervised embedding method .
ho	111	[201]	line : we use the advanced version of line by considering both the 1st and 2nd order of node proximity . // pte : we construct three bipartite heterogeneous networks ( author–author , author–venue , venue–venue ) and restrain it as an unsupervised embedding method . // spectral clustering / graph factorization : with the same treatment to these methods in node2vec , we exclude them from our comparison , as previous studies have demonstrated that they are outperformed by deepwalk and line .
h-	111	[55]	pte : we construct three bipartite heterogeneous networks ( author–author , author–venue , venue–venue ) and restrain it as an unsupervised embedding method . // spectral clustering / graph factorization : with the same treatment to these methods in node2vec , we exclude them from our comparison , as previous studies have demonstrated that they are outperformed by deepwalk and line . // for all embedding methods , we use the same parameters listed below .
ho	111	[37]	pte : we construct three bipartite heterogeneous networks ( author–author , author–venue , venue–venue ) and restrain it as an unsupervised embedding method . // spectral clustering / graph factorization : with the same treatment to these methods in node2vec , we exclude them from our comparison , as previous studies have demonstrated that they are outperformed by deepwalk and line . // for all embedding methods , we use the same parameters listed below .
ho	111	[224]	our empirical results also show that this simple meta path scheme “ apvpa ” can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks , suggesting its applicability to potential applications for academic search services . // we evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks , including multi class node classification , node clustering , and similarity search . // in addition , we also use the embedding projector in tensorflow to visualize the node embeddings learned from the heterogeneous academic networks .
ho	111	[225]	our empirical results also show that this simple meta path scheme “ apvpa ” can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks , suggesting its applicability to potential applications for academic search services . // we evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks , including multi class node classification , node clustering , and similarity search . // in addition , we also use the embedding projector in tensorflow to visualize the node embeddings learned from the heterogeneous academic networks .
ho	111	[111]	we evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks , including multi class node classification , node clustering , and similarity search . // in addition , we also use the embedding projector in tensorflow to visualize the node embeddings learned from the heterogeneous academic networks . // multi class classication .
b	111	[178, 157]	related work . // network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks , such as the application of factorization models for recommendation systems , node classification , relational mining , and role discovery . // this rich line of research focuses on factorizing the matrix/tensor format ( e .g . , the adjacency matrix ) of a network , generating latent dimension features for nodes or edges in this network .
b	111	[229, 230]	related work . // network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks , such as the application of factorization models for recommendation systems , node classification , relational mining , and role discovery . // this rich line of research focuses on factorizing the matrix/tensor format ( e .g . , the adjacency matrix ) of a network , generating latent dimension features for nodes or edges in this network .
b	111	[141]	related work . // network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks , such as the application of factorization models for recommendation systems , node classification , relational mining , and role discovery . // this rich line of research focuses on factorizing the matrix/tensor format ( e .g . , the adjacency matrix ) of a network , generating latent dimension features for nodes or edges in this network .
b	111	[169]	related work . // network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks , such as the application of factorization models for recommendation systems , node classification , relational mining , and role discovery . // this rich line of research focuses on factorizing the matrix/tensor format ( e .g . , the adjacency matrix ) of a network , generating latent dimension features for nodes or edges in this network .
ho	111	[37]	this rich line of research focuses on factorizing the matrix/tensor format ( e .g . , the adjacency matrix ) of a network , generating latent dimension features for nodes or edges in this network . // however , the computational cost of decomposing a large scale matrix/tensor is usually very expensive , and also suffers from its statistical performance drawback , making it neither practical nor effective for addressing tasks in big networks . // with the advent of deep learning techniques , significant effort has been devoted to designing neural network based representation learning models .
b	111	[6]	for example , mikolov et al . proposed the word2vec framework—a two layer neural network—to learn the distributed representations of words in natural language . // building on word2vec , perozzi et al . suggested that the “ context ” of a node can be denoted by their co occurrence in a random walk path . // formally , they put random walkers over networks to record their walking paths , each of which is composed of a chain of nodes that could be considered as a “ sentence ” of words in a text corpus .
b	111	[37]	formally , they put random walkers over networks to record their walking paths , each of which is composed of a chain of nodes that could be considered as a “ sentence ” of words in a text corpus . // more recently , in order to diversify the neighborhood of a node , grover & leskovec presented biased random walkers—a mixture of breadth first and width first search procedures—over networks to produce paths of nodes . // with node paths generated , both works leveraged the skip gram architecture in word2vec to model the structural correlations between nodes in a path .
b	111	[109, 202, 29, 48, 231]	with node paths generated , both works leveraged the skip gram architecture in word2vec to model the structural correlations between nodes in a path . // in addition , several other methods have been proposed for learning representations in networks . // in particular , to learn network embeddings , tang et al . decomposed a node ’ s context into first order ( friends ) and second order ( friends ’ friends ) proximity , which was further developed into a semi supervised model pte for embedding text data .
b	111	[201]	in addition , several other methods have been proposed for learning representations in networks . // in particular , to learn network embeddings , tang et al . decomposed a node ’ s context into first order ( friends ) and second order ( friends ’ friends ) proximity , which was further developed into a semi supervised model pte for embedding text data . // conclusion .
b	26	[6]	this model learns low dimensional vectors to represent vertices appearing in a graph and , unlike existing work , integrates global structural information of the graph into the learning process . // we also formally analyze the connections between our work and several previous research efforts , including the deepwalk model of perozzi et al . as well as the skip gram model with negative sampling of mikolov et al . // we conduct experiments on a language network , a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering , classification and visualization .
b	26	[36]	this model learns low dimensional vectors to represent vertices appearing in a graph and , unlike existing work , integrates global structural information of the graph into the learning process . // we also formally analyze the connections between our work and several previous research efforts , including the deepwalk model of perozzi et al . as well as the skip gram model with negative sampling of mikolov et al . // we conduct experiments on a language network , a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering , classification and visualization .
h-	26	[36]	for example , deepwalk , one recent model , transforms a graph structure into a sample collection of linear sequences consisting of vertices using uniform sampling ( which is also called truncated random walk ) . // the skip gram model , originally designed for learning word representations from linear sequences , can also be used to learn the representations of vertices from such samples . // although this method is empirically effective , it is not well understood what is the exact loss function defined over the graph involved in their learning process .
b	26	[1]	the above limitation is overcome in our proposed model through the preservation of different k step relational information in distinct subspaces . // another recently proposed work is line , which has a loss function to capture both 1 step and 2 step local relational information . // to capture certain complex relations in such local information , they also learn non linear transformations from such data .
h-	26	[36]	currently , there are two mainstream methods for learning word representations : neural embedding methods and matrix factorization based approaches . neural embedding methods employ a fixed slide window capturing context words of current word . // models like skipgram are proposed , which provide an efficient approach to learning word representations . // while these methods may yield good performances on some tasks , they can poorly capture useful information since they use separate local context windows , instead of global co occurrence counts .
b	26	[232]	while these methods may yield good performances on some tasks , they can poorly capture useful information since they use separate local context windows , instead of global co occurrence counts . // on the other hand , the family of matrix factorization methods can utilize global statistics . // previous work include latent semantic analysis ( lsa ) , which decomposes termdocument matrix and yields latent semantic representations .
b	26	[233]	on the other hand , the family of matrix factorization methods can utilize global statistics . // previous work include latent semantic analysis ( lsa ) , which decomposes termdocument matrix and yields latent semantic representations . // lund et al . put forward hyperspace analogue to language ( hal ) , factorizing a word word co occurrence counts matrix to generate word representations .
b	26	[234]	previous work include latent semantic analysis ( lsa ) , which decomposes termdocument matrix and yields latent semantic representations . // lund et al . put forward hyperspace analogue to language ( hal ) , factorizing a word word co occurrence counts matrix to generate word representations . // levy et al . presented matrix factorization over shifted positive pointwise mutual information ( pmi ) matrix for learning word representations and showed that the skip gram model with negative sampling ( sgns ) can be regarded as a model that implicitly such a matrix .
b	26	[160]	lund et al . put forward hyperspace analogue to language ( hal ) , factorizing a word word co occurrence counts matrix to generate word representations . // levy et al . presented matrix factorization over shifted positive pointwise mutual information ( pmi ) matrix for learning word representations and showed that the skip gram model with negative sampling ( sgns ) can be regarded as a model that implicitly such a matrix . // graph representation approaches
b	26	[152]	graph representation approaches . // there exist several classical approaches to learning low dimensional graph representations , such as multidimensional scaling ( mds ) , isomap , lle , and laplacian eigenmaps . // recently , tang et al . presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification .
b	26	[3]	graph representation approaches . // there exist several classical approaches to learning low dimensional graph representations , such as multidimensional scaling ( mds ) , isomap , lle , and laplacian eigenmaps . // recently , tang et al . presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification .
b	26	[5]	graph representation approaches . // there exist several classical approaches to learning low dimensional graph representations , such as multidimensional scaling ( mds ) , isomap , lle , and laplacian eigenmaps . // recently , tang et al . presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification .
b	26	[33]	there exist several classical approaches to learning low dimensional graph representations , such as multidimensional scaling ( mds ) , isomap , lle , and laplacian eigenmaps . // recently , tang et al . presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification . // ahmed et al . proposed a graph factorization method , which used stochastic gradient descent to optimize matrices from large graphs .
b	26	[154]	recently , tang et al . presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification . // ahmed et al . proposed a graph factorization method , which used stochastic gradient descent to optimize matrices from large graphs . // perozzi et al . presented an approach , which transformed graph structure into several linear vertex sequences by using a truncated random walk algorithm and generated vertex representations by using skip gram model . this is considered as an equally weighted linear combination of k step information .
b	26	[1]	this is considered as an equally weighted linear combination of k step information . // tang et al . later proposed a large scale information network embedding , which optimizes a loss function where both 1 step and 2 step relational information can be captured in the learning process . // grarep model
ho	26	[36]	our objective aims to maximize : 1 ) the probability that these pairs come from the graph , and 2 ) the probability that all other pairs do not come from the graph . // motivated by the skip gram model by mikolov et al . , we employ noise contrastive estimation ( nce ) , which is proposed by gutmann et al . , to define our objective function . // following a similar discussion presented in , we first introduce our k step loss function defined over the complete graph as follows : lk = x w∈v lk ( w ) where lk ( w ) = x c∈v pk ( c|w ) log σ ( ~w · ~c ) ! +λec 0∼pk ( v ) here pk ( c|w ) describes the k step relationship between w and c ( the k step transition probability from w to c ) , σ ( · ) is sigmoid function defined as σ ( x ) = ( 1 + e −x ) −1 , λ is a hyper parameter indicating the number of negative samples , and pk ( v ) is the distribution over the vertices in the graph .
ho	26	[91]	our objective aims to maximize : 1 ) the probability that these pairs come from the graph , and 2 ) the probability that all other pairs do not come from the graph . // motivated by the skip gram model by mikolov et al . , we employ noise contrastive estimation ( nce ) , which is proposed by gutmann et al . , to define our objective function . // following a similar discussion presented in , we first introduce our k step loss function defined over the complete graph as follows : lk = x w∈v lk ( w ) where lk ( w ) = x c∈v pk ( c|w ) log σ ( ~w · ~c ) ! +λec 0∼pk ( v ) here pk ( c|w ) describes the k step relationship between w and c ( the k step transition probability from w to c ) , σ ( · ) is sigmoid function defined as σ ( x ) = ( 1 + e −x ) −1 , λ is a hyper parameter indicating the number of negative samples , and pk ( v ) is the distribution over the vertices in the graph .
ho	26	[160]	optimization with matrix factorization . // following the work of levy et al . , to reduce noise , we replace all negative entries in y k with 0 . // this gives us a positive k step log probabilistic matrix x k , where x k i , j = max ( y k i , j , 0 ) while various techniques for matrix factorization exist , in this work we focus on the popular singular value decomposition ( svd ) method due to its simplicity .
h+	26	[236, 237]	while various techniques for matrix factorization exist , in this work we focus on the popular singular value decomposition ( svd ) method due to its simplicity . // svd has been shown successful in several matrix factorization tasks , and is regarded as one of the important methods that can be used for dimensionality reduction . // for the matrix xk , svd factorizes it as : xk = u kσk ( vk ) t where u and v are orthonormal matrices and σ is a diagonal matrix consisting of an ordered list of singular values .
b	26	[238, 232, 239]	this way , we can factorize our matrix x kas : xk ≈ xkd = wkckwherewk = ukd ( σkd ) 12 , ck = ( σkd ) 12 vkd t . // the resulting wk gives representations of current vertices as its column vectors , and ck gives the representations of context vertices as its column vectors . // the final matrix wk is returned from the algorithm as the low d representations of the vertices which capture k step global structural information in the graph .
h-	26	[241, 242]	note that here we are essentially finding a projection from the row space of xk to the row space of wk with a lower rank . // thus alternative approaches other than the popular svd can also be exploited . examples include incremental svd , independent component analysis ( ica ) , and deep neural networks . // our focus in this work is on the novel model for learning graph representations , so we do not pursue any alternative methods .
h-	26	[43]	note that here we are essentially finding a projection from the row space of xk to the row space of wk with a lower rank . // thus alternative approaches other than the popular svd can also be exploited . examples include incremental svd , independent component analysis ( ica ) , and deep neural networks . // our focus in this work is on the novel model for learning graph representations , so we do not pursue any alternative methods .
ho	26	[160]	we plug these expected counts into the equation of y e−sgns i , j , and we arrive at : ye−sgns w , c = log  # ( w , c ) · |d| # ( w ) · # ( c ) − log ( λ ) where d is the collection of all observed pairs in sequences , that is , |d| = γk . // to maintain the consistency with levy et al . , we only employed svd in this work . // this shows sgns is essentially a special version of our grarep model that deals with linear sequences which can be sampled from graphs .
b	26	[6]	line will get the best performance , if concatenating the representation of 1 step and 2 step relational information and tuning the threshold of maximum number of vertices . // deepwalk . deepwalk is a method that learns the representation of social networks . // the original model only works for unweighted graph .
b	26	[73]	this method can be regarded as a special case of our model , where different representational vector of each k step information is averaged . // spectral clustering . spectral clustering is a reasonable baseline algorithm , which aims at minimizing normalized cut ( ncut ) . // spectral clustering also factorize a matrix , but it focuses on a different matrix of the graphs – the laplacian matrix . essentially , the difference between spectral clustering and e sgns lies on their different loss function .
ho	26	[1]	parameter settings . // as suggested in , for line , we set the mini batch size of stochastic gradient descent ( sgd ) as 1 , learning rate of starting value as 0 .025 , the number of negative samples as 5 , and the total number of samples as 10 billion . // we also concatenate both 1 step and 2 step relational information to form the representations and employ the reconstruction strategy for vertices with small degrees to achieve the optimal performance .
ho	26	[1]	as mentioned in , for deepwalk and e sgns , we set window size as 10 , walk length as 40 , walks per vertex as 80 . // according to , line yielded better results when the learned graph representations are l2 normalized , while deepwalk and e sgns can achieve optimal performance without normalization . // for grarep , we found the l2 normalization yielded better results .
b	48	[157]	however , graph representation in general remains an open problem . // recently , the graph embedding paradigm is proposed to represent vertices of a graph in a low dimensional vector space while the structures ( i .e . edges and other high order structures ) of the graph can be reconstructed in the vector space . // with proper graph embedding , we can easily apply classic vector based machine learning techniques to process graph data .
ho	48	[243]	the answer is no due to a fundamentally different characteristic of directed graphs : asymmetric transitivity . // transitivity is a common characteristic of undirected and directed graphs ( see figure 1 ) . // in undirected graphs , if there is an edge between vertices u and w , and one between w and v , then it is likely that u and v are connected by an edge .
b	48	[10, 245, 246, 26, 6]	however , how to preserve the asymmetric transitivity of directed graphs in a vector space is much more challenging . // recently , a paucity of studies focus on directed graph embedding . // in order to reflect the asymmetric property in vector space , these methods design asymmetric metrics on the embedding vectors .
b	48	[49]	from the graph embedding pserspective , the property of asymmetric transitivity leads to the assumption that the more and the shorter paths from vi to vj , the more similar should be vi ’ s source vector and vj ’ s target vector . // this assumption coincides with high order proximities of nodes in graphs , such as katz and rooted pagerank . // what is more , these high order proximities are defined to be asymmetric in directed graphs . thus , in its place , we propose to use highorder proximities of nodes as the target measure , resulting in a novel directed graph embedding algorithm , high order proximity preserved embedding ( hope ) .
b	48	[50]	from the graph embedding pserspective , the property of asymmetric transitivity leads to the assumption that the more and the shorter paths from vi to vj , the more similar should be vi ’ s source vector and vj ’ s target vector . // this assumption coincides with high order proximities of nodes in graphs , such as katz and rooted pagerank . // what is more , these high order proximities are defined to be asymmetric in directed graphs . thus , in its place , we propose to use highorder proximities of nodes as the target measure , resulting in a novel directed graph embedding algorithm , high order proximity preserved embedding ( hope ) .
b	48	[160]	graph embedding . // graph embedding technology has been widely studied in the fields of dimensionality reduction , natural language processing , network analysis and so on . // for dimensionality reduction , adjacency matrices of graphs are constructed from the feature similarity ( distance ) between samples . and the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space .
b	48	[178]	graph embedding . // graph embedding technology has been widely studied in the fields of dimensionality reduction , natural language processing , network analysis and so on . // for dimensionality reduction , adjacency matrices of graphs are constructed from the feature similarity ( distance ) between samples . and the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space .
b	48	[157]	graph embedding technology has been widely studied in the fields of dimensionality reduction , natural language processing , network analysis and so on . // for dimensionality reduction , adjacency matrices of graphs are constructed from the feature similarity ( distance ) between samples . // and the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space .
b	48	[248]	for example , laplacian eigenmaps ( le ) aims to learn the low dimensional representation to expand the manifold where data lie . // locality preserving projections ( lpp ) is a linearization variant of le which learns a linear projection from feature space to embedding space . // besides , there are many other graph embedding algorithms for dimensionality reduction , including non linear , linear , kernlized and tensorized algorithms .
h-	48	[3, 4]	locality preserving projections ( lpp ) is a linearization variant of le which learns a linear projection from feature space to embedding space . // besides , there are many other graph embedding algorithms for dimensionality reduction , including non linear , linear , kernlized and tensorized algorithms . all of these algorithms are based on undirected graphs derived from symmetric similarities . // thus , they can not preserve asymmetric transitivity .
h-	48	[247, 249]	locality preserving projections ( lpp ) is a linearization variant of le which learns a linear projection from feature space to embedding space . // besides , there are many other graph embedding algorithms for dimensionality reduction , including non linear , linear , kernlized and tensorized algorithms . all of these algorithms are based on undirected graphs derived from symmetric similarities . // thus , they can not preserve asymmetric transitivity .
h-	48	[251]	locality preserving projections ( lpp ) is a linearization variant of le which learns a linear projection from feature space to embedding space . // besides , there are many other graph embedding algorithms for dimensionality reduction , including non linear , linear , kernlized and tensorized algorithms . all of these algorithms are based on undirected graphs derived from symmetric similarities . // thus , they can not preserve asymmetric transitivity .
b	48	[36, 160, 167]	thus , they can not preserve asymmetric transitivity . // in the field of natural language processing , the graph of words is often used to learn the representation of words . // mikolov et . al . propose to ultilize the context of words to learn representation , which has been proved equivalent to factorizing word context matrix .
b	48	[36]	in the field of natural language processing , the graph of words is often used to learn the representation of words . // mikolov et . al . propose to ultilize the context of words to learn representation , which has been proved equivalent to factorizing word context matrix . // pennington et . al . exploit a word word co occurrance matrix .
b	48	[178]	pennington et . al . exploit a word word co occurrance matrix . // in network analysis , hoff et . al . first propose to learn latent space representation of vertexes in graph and the probability of a relation depends on the distance between vertexes in the latent space , and they apply it to link prediction problem . // handcock et . al . propose to apply the latent space approach to clustering in graph .
b	48	[252]	pennington et . al . exploit a word word co occurrance matrix . // in network analysis , hoff et . al . first propose to learn latent space representation of vertexes in graph and the probability of a relation depends on the distance between vertexes in the latent space , and they apply it to link prediction problem . // handcock et . al . propose to apply the latent space approach to clustering in graph .
b	48	[253]	in network analysis , hoff et . al . first propose to learn latent space representation of vertexes in graph and the probability of a relation depends on the distance between vertexes in the latent space , and they apply it to link prediction problem . // handcock et . al . propose to apply the latent space approach to clustering in graph . // and zhu et . al . propose to address the classification problem in graph with graph embedding model .
b	48	[6, 26]	and zhu et . al . propose to address the classification problem in graph with graph embedding model . // while early graph embedding works focus on modeling the observed first order relationship ( i .e . edges in graph ) between vertexes , some recent works try to model the directed higher order relationships between vertexes in sparse graphs . // grarep is related to our work . but , it can not fully capture transitivity .
h-	48	[26]	while early graph embedding works focus on modeling the observed first order relationship ( i .e . edges in graph ) between vertexes , some recent works try to model the directed higher order relationships between vertexes in sparse graphs . // grarep is related to our work . but , it can not fully capture transitivity . // directed graph .
b	48	[254]	so , modeling directed graph is a critical problem for graph analysis . // holland et . al . propose the p1 distribution model to capture the structural properties in directed graph , including the atrractiveness and expansiveness of vertexes and the reciprocation of edges . // besides these properties , wang et . al . take the group information of vertexes into consideration .
b	48	[256, 257, 258, 259]	besides these properties , wang et . al . take the group information of vertexes into consideration . // recently , some works adopt graph embedding to model directed graphs . // chen et . al . learn the embedding vectors in euclidean space with locality property preserved .
h-	48	[256]	recently , some works adopt graph embedding to model directed graphs . // chen et . al . learn the embedding vectors in euclidean space with locality property preserved . // perrault joncas et . al . and mousazadeh et . al . learn the embedding vectors based on laplacian type operators and preserve the asymmetry property of edges in a vector field . however , all of these methods can not preserve asymmetry property in embedding vector space .
h-	48	[257, 258]	chen et . al . learn the embedding vectors in euclidean space with locality property preserved . // perrault joncas et . al . and mousazadeh et . al . learn the embedding vectors based on laplacian type operators and preserve the asymmetry property of edges in a vector field . however , all of these methods can not preserve asymmetry property in embedding vector space . // high order proximity preserved embedding
ho	48	[260]	as the calculation of s is the efficiency bottleneck and s is just the intermediate product in our problem , we propose a novel algorithm to avoid the calculation of s and learn the embedding vectors directly . // as many proximity measurements have the general formulation in equation ( 2 ) , we transform the original svd problem into a generalized svd problem for proximity measurements with the general formulation . // according to , it is easy to derive the following theorem .
ho	48	[261]	// synthetic data ( syn ) : we generate the synthetic data by the forest fire model . // the model can generate powerlaw graphs .
b	48	[262]	we randomly generate ten synthetic datasets . // cora1 : this is a citation network of academic papers . // the vertexes are academic papers and the directed edges are the citation relationship between papers .
h-	48	[1]	baseline methods . // line : this algorithm preserves the first order and second order proximity between vertexes , but it only can preserve symmetric second order proximity when applied to directed graph . // we use vertex vectors as source vectors and context vectors as target vectors .
b	48	[6]	we use line1 to represent line preserving first order proximity and line2 to represent line preserving second order proximity . // deepwalk : this algorithm first randomly walks on the graph , and assumes a pair of vertexes similar if they are close in the random path . // then , the embedding is learned to preserve these pairwise similarities in the embedding .
b	48	[50]	then , the embedding is learned to preserve these pairwise similarities in the embedding . // ppe ( partial proximity embedding ) : this algorithm first selects a small subset of vertexes as landmarks , and learns the embedding by approximating the proximity between vertexes and landmarks . // common neighbors .
b	586	[37]	the main idea of network embedding is to ﬁnda dense , continuous , and low dimensional vector for each node as its distributedrepresentation . // representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne .
b	586	[70]	the main idea of network embedding is to ﬁnda dense , continuous , and low dimensional vector for each node as its distributedrepresentation . // representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne .
b	586	[157, 264]	the main idea of network embedding is to ﬁnda dense , continuous , and low dimensional vector for each node as its distributedrepresentation . // representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne .
b	586	[1]	representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne . // the basic assumption of these topology driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space .
b	586	[37]	representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne . // the basic assumption of these topology driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space .
b	586	[19]	representing nodes into the distributed vectors can form up a potentially powerful basis to generate high quality node features for many data mining and machine learning tasks , such as node classification , link prediction and recommendation . // most related works investigate the topology information for network embedding , such as deepwalk , line , node2vec and sdne . // the basic assumption of these topology driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space .
b	586	[4]	matrix factorization based methods first express the input network with a affinity matrix in which the entries represent the relationships between nodes , and then embed the affinity matrix into a low dimensional space using matrix factorization techniques . // locally linear embedding seeks a lower dimensional projection of the input affinity matrix which preserves distances within local neighborhoods . // spectral embedding is one method to calculate the non linear embeddings .
h-	586	[265]	it ﬁnds a low dimensional representation of the input data usinga spectral decomposition of the graph laplacian . // sparse random projection reduces the dimensionality of data by projecting the original input space using a sparse random matrix . // however , matrix factorization based methods rely on the decomposition of the affinity matrix , which is too expensive to scale efficiently to large real world networks .
b	586	[6]	recently neural network based models are introduced to solve the networkembedding problem . // as the first attempt , deepwalk introduces an word embedding algorithm ( skip gram ) to learn the representation vectors of nodes . // tang et al . propose line , which optimizes a carefully designed objec tive function that preserves both the local and global structure .
b	586	[1]	as the ﬁrst attempt , deepwalk introduces an word embedding algorithm ( skip gram ) to learn the representation vectors ofnodes . // tang et al . propose line , which optimizes a carefully designed objective function that preserves both the local and global structure . // wang et al . propose sdne , a deep embedding model to capture the highly non linear network structure and preserve the global and local structures . sdne exploits the first order and second order proximities to preserve the network structure .
b	586	[19]	tang et al . propose line , which optimizes a carefully designed objective function that preserves both the local and global structure . // wang et al . propose sdne , a deep embedding model to capture the highly non linear network structure and preserve the global and local structures . // sdne exploits the first order and second order proximities to preserve the network structure .
b	586	[37]	sdne exploitsthe ﬁrst order and second order proximities to preserve the network structure . // node2vec learns a mapping of nodes to a low dimensional space of features that maximizes the likelihood of preserving distances between network neighborhoods of nodes . // tadw incorporates the text features of nodes into network embedding process under a framework of matrix factorization .
h-	586	[266, 55, 55, 66]	diﬀerent fromthe previous typology only works , our work aims to propose a general propertypreserving network embedding model which integrate the rich types of nodeproperties in the network into the embedding process . // finally , there is a body of works focus on the problem of node classification or link prediction . // however , the objective of our work is totally different from these works . we aim to learn better representation vectors for nodes , while the node classification or link prediction tasks are only utilized to evaluate the quality of the embedding results .
h-	586	[70, 267]	diﬀerent fromthe previous typology only works , our work aims to propose a general propertypreserving network embedding model which integrate the rich types of nodeproperties in the network into the embedding process . // finally , there is a body of works focus on the problem of node classification or link prediction . // however , the objective of our work is totally different from these works . we aim to learn better representation vectors for nodes , while the node classification or link prediction tasks are only utilized to evaluate the quality of the embedding results .
ho	586	[6]	topology derived objective function . // following the idea of deepwalk , we assume that nodes with similar topology context tend to be similar . // with such an assumption , we aim to maximize the likelihood of the prediction of the center node given a specific contextual node .
ho	586	[269]	in this subsection we present the details of the property derived objective func tion . // in natural language processing area , swe and rc net incorporate the semantic knowledges into the word embedding process . // inspired by the above works , we propose two ways to extract constraints from the property similarity matrix p .
h-	586	[7]	based on these constraints , we introduce the property derivedobjective functions . // related works incorporate the original property features of single type into the embedding process . // diﬀerent from such works , in our approach theproperty matrix p∈r|v|×|v|contains the property similarity scores betweeneach pair of nodes , which is calculated by a predeﬁned similarity measurement .
b	586	[68]	in order to thoroughly evaluate the proposed methods , we conductexperiments on four paper citation networks and one social network with diﬀer ent scale of nodes . // the four paper citation networks are citeseer2 , cora ( see footnote 2 ) , pubmed ( see footnote 2 ) and dblp3 . // in the paper citation networks , nodes refer to papers and links refer to the citation relationships among papers .
b	586	[6]	// deepwalk : deepwalk is a topology only network embedding method , which introduces the skip gram algorithm to learn the node representation vectors . // line : line is a popular topology only network embedding method , which considers the first order and second order proximities information .
b	586	[1]	deepwalk : deepwalk is a topology only network embedding method , which introduces the skip gram algorithm to learn the node representation vectors . // line : line is a popular topology only network embedding method , which considers the first order and second order proximities information . // tadw : tadw incorporates the text features of each node into the embedding process under a framework of matrix factorization .
b	586	[7]	line : line is a popular topology only network embedding method , which considers the first order and second order proximities information . // tadw : tadw incorporates the text features of each node into the embedding process under a framework of matrix factorization . // ppneineq : ppneineq is the ppne model with the inequality constrains .
ho	586	[270]	we utilize the representation vectors generated by various network embeddingmethods to perform multi class node classiﬁcation task . // the representation vector of each node is treated as its feature vector , and then we use a linear support vector machine model to return the most likely category . // the classification model is implemented using scikit learn .
b	586	[271, 267]	link prediction . // given a snapshot of the current network , the link prediction task refers to predicting the edges that will be added in the future time . // link prediction can show the predictability of different network embedding methods .
ho	586	[272]	given a node pair in the samples , the cosine similarityscore is calculated according to their representation vectors . // area under curve ( auc ) is used to evaluate the consistency between the labels and the similarity scores of the samples . // we also choose common neighbors as a baseline method because it has been proved as an effective method .
b	29	[274, 275]	// attributed networks are ubiquitous in a variety of real world information systems , such as academic networks and health care systems . // different from plain networks in which only node to node interactions and dependencies are observed , each node in an attributed network is often associated with a rich set of features .
b	29	[276, 277]	for instance , with the popularity of social networking services , people not only make friends with each other to form online communities but also actively share opinions and post comments . // in social science , social influence theories have been studied that attributes of individuals can both reflect and affect their community structures . // in addition , a number of data mining applications , such as sentiment analysis and trust prediction , have been benefited by exploiting the correlations between geometrical structure and node attributes .
b	29	[278]	for instance , with the popularity of social networking services , people not only make friends with each other to form online communities but also actively share opinions and post comments . // in social science , social influence theories have been studied that attributes of individuals can both reflect and affect their community structures . // in addition , a number of data mining applications , such as sentiment analysis and trust prediction , have been benefited by exploiting the correlations between geometrical structure and node attributes .
b	29	[280]	in social science , social influence theories have been studied that attributes of individuals can both reflect and affect their community structures . // in addition , a number of data mining applications , such as sentiment analysis and trust prediction , have been benefited by exploiting the correlations between geometrical structure and node attributes . // network embedding , as an efficient computational tool for graph mining , aims at mapping the topological proximities of all nodes in a network into a continuous low dimensional matrix representation .
h+	29	[1, 157]	in addition , a number of data mining applications , such as sentiment analysis and trust prediction , have been benefited by exploiting the correlations between geometrical structure and node attributes . // network embedding , as an efficient computational tool for graph mining , aims at mapping the topological proximities of all nodes in a network into a continuous low dimensional matrix representation . // the learned embedding representation paves the way for numerous applications such as node classification , link prediction , and network visualization .
b	29	[281, 10]	network embedding , as an efficient computational tool for graph mining , aims at mapping the topological proximities of all nodes in a network into a continuous low dimensional matrix representation . // the learned embedding representation paves the way for numerous applications such as node classification , link prediction , and network visualization . // while this has been extensively studied , research on attributed network embedding ( ane ) is still in its early stage . in contrast to network embedding that learns from pure networks , ane targets at leveraging both network proximity and node attribute affinity .
b	29	[17]	network embedding , as an efficient computational tool for graph mining , aims at mapping the topological proximities of all nodes in a network into a continuous low dimensional matrix representation . // the learned embedding representation paves the way for numerous applications such as node classification , link prediction , and network visualization . // while this has been extensively studied , research on attributed network embedding ( ane ) is still in its early stage . in contrast to network embedding that learns from pure networks , ane targets at leveraging both network proximity and node attribute affinity .
b	29	[109]	the learned embedding representation paves the way for numerous applications such as node classification , link prediction , and network visualization . // while this has been extensively studied , research on attributed network embedding ( ane ) is still in its early stage . in contrast to network embedding that learns from pure networks , ane targets at leveraging both network proximity and node attribute affinity // in contrast to network embedding that learns from pure networks , ane targets at leveraging both network proximity and node attribute affinity .
b	29	[283, 284]	papers published in the same research community usually share common topics . they also heavily cite others from the same community . // these facts can be explained by the homophily hypothesis , i .e . , individuals with the same label usually have similar social relations and similar node attributes . // labels are strongly influenced by and inherently correlated to both of the network structure and attribute information .
ho	29	[283]	the second term tr ( u ( y ) t u ( g ) u ( g ) t u ( y ) ) also measures the correlation between u ( g ) and u ( y ) . // it is beneficial to the label proximity learning since they are considered to be highly correlated . // third , the noise in the learned latent representation u ( y ) could also be greatly reduced and most information in the original label space is recoverable .
h+	29	[286]	it is beneficial to the label proximity learning since they are considered to be highly correlated . // third , the noise in the learned latent representation u ( y ) could also be greatly reduced and most information in the original label space is recoverable . // therefore , although label information might be incomplete and noisy , we are still able to fully capture the proximities of nodes in the label space .
h+	29	[287]	it is beneficial to the label proximity learning since they are considered to be highly correlated . // third , the noise in the learned latent representation u ( y ) could also be greatly reduced and most information in the original label space is recoverable . // therefore , although label information might be incomplete and noisy , we are still able to fully capture the proximities of nodes in the label space .
h+	29	[1]	deepwalk : it employs truncated random walks on the plain graph and involves language modeling techniques , i .e . , word2vec , to analyze the walking tracks . // line : it is one of the state of the art embedding algorithms for large scale networks . // it preserves both first and second order proximities between the nodes .
b	29	[10]	it preserves both first and second order proximities between the nodes . // lcmf : it conducts a joint matrix factorization on the linkage and attribute information , and maps them into a shared subspace . // it uses this subspace as the learned representation .
b	29	[288]	the corresponding top d eigenvectors are collected as the embedding representation . // multiview : it considers the network , attributes , and labels as three views , and applies co regularized spectral clustering on them collectively . // lane on net and lane w/o label : they are two variations of lane , which have been described in section 3 .6 .
ho	29	[281, 10]	experimental settings . // following a commonly adopted way , we validate the effectiveness of different learned representations on node classification task . // this task is to predict which category or categories a new node belongs to based on the model learned from training data .
b	29	[289]	network embedding enjoys increasing popularity in recent years . // its pioneer work can be traced back to the graph embedding problem , which was introduced by filotti et al . as a graph genus determining problem in 1979 . // a family of more general graph embedding approaches were developed around the 2000s .
b	29	[290, 3]	its pioneer work can be traced back to the graph embedding problem , which was introduced by filotti et al . as a graph genus determining problem in 1979 . // a family of more general graph embedding approaches were developed around the 2000s . // they target at generating lowdimensional manifolds which can model the nonlinear geometry of data , including isomap , laplacian eigenmaps and spectral techniques .
b	29	[168]	a family of more general graph embedding approaches were developed around the 2000s . // they target at generating lowdimensional manifolds which can model the nonlinear geometry of data , including isomap , laplacian eigenmaps and spectral techniques . // up till now , due to the pervasiveness of networked data , a variety of network embedding algorithms have been implemented .
b	29	[291, 292]	a family of more general graph embedding approaches were developed around the 2000s . // they target at generating lowdimensional manifolds which can model the nonlinear geometry of data , including isomap , laplacian eigenmaps and spectral techniques . // up till now , due to the pervasiveness of networked data , a variety of network embedding algorithms have been implemented .
b	29	[293, 1, 157]	they target at generating lowdimensional manifolds which can model the nonlinear geometry of data , including isomap , laplacian eigenmaps and spectral techniques . // up till now , due to the pervasiveness of networked data , a variety of network embedding algorithms have been implemented . // iwata et al . applied probabilistic latent semantic analysis to embed document networks .
b	29	[295]	iwata et al . applied probabilistic latent semantic analysis to embed document networks . // tang et al . investigated the advantage of employing temporal information to analyze dynamic multi mode networks . // shaw and jebara exploited a semidefinite program to learn a low dimensional representation that well preserves the global topological structure .
b	29	[293]	tang et al . investigated the advantage of employing temporal information to analyze dynamic multi mode networks . // shaw and jebara exploited a semidefinite program to learn a low dimensional representation that well preserves the global topological structure . // mei et al . designed a harmonic regularization based embedding framework to tackle the problem of topic modeling with network structure .
b	29	[296]	shaw and jebara exploited a semidefinite program to learn a low dimensional representation that well preserves the global topological structure . // mei et al . designed a harmonic regularization based embedding framework to tackle the problem of topic modeling with network structure . // ahmed et al . proposed an asynchronous distributed matrix factorization algorithm for large scale graphs .
b	29	[297]	ahmed et al . proposed an asynchronous distributed matrix factorization algorithm for large scale graphs . // bourigault et al . projected the observed temporal dynamic into a latent space to better model the information diffusion in networks . // grover and leskovec further advanced the random walk based embedding algorithms by adding flexibility in exploiting neighborhoods .
b	29	[37]	bourigault et al . projected the observed temporal dynamic into a latent space to better model the information diffusion in networks . // grover and leskovec further advanced the random walk based embedding algorithms by adding flexibility in exploiting neighborhoods . // to embed heterogeneous networks , jacob et al . extended the transductive models and deep learning techniques into the problem .
b	29	[298]	grover and leskovec further advanced the random walk based embedding algorithms by adding flexibility in exploiting neighborhoods . // to embed heterogeneous networks , jacob et al . extended the transductive models and deep learning techniques into the problem . // yang et al . exploited a probabilistic model to conduct network embedding in a semisupervised manner .
b	29	[6, 19, 299]	yang et al . exploited a probabilistic model to conduct network embedding in a semisupervised manner . // most recently , several deep learning based embedding algorithms were proposed to further enhance the performance of learned representations . // attributed network analysis is put forward due to the fact that numerous networks are often associated with abundant content describing attributes of each node .
ho	29	[276, 278]	attributed network analysis is put forward due to the fact that numerous networks are often associated with abundant content describing attributes of each node . // in these networks , it has been widely accepted that there exist correlations among geometrical structure and node attributes . // therefore , algorithms exploiting them together could improve the overall learning performance .
h+	29	[13, 279, 280]	in these networks , it has been widely accepted that there exist correlations among geometrical structure and node attributes . // therefore , algorithms exploiting them together could improve the overall learning performance . // for instance , tsur and rappoport advanced the prediction of spread of ideas by analyzing both social graph topology and content .
b	29	[275, 10]	for instance , tsur and rappoport advanced the prediction of spread of ideas by analyzing both social graph topology and content . // in order to tackle the complex data structures , several efforts have been devoted to jointly embedding the two information sources into a unified space . // qi et al . explored an effective algorithm that jointly embeds context and content in social media by constructing a latent space of semantic concepts .
b	29	[275]	in order to tackle the complex data structures , several efforts have been devoted to jointly embedding the two information sources into a unified space . // qi et al . explored an effective algorithm that jointly embeds context and content in social media by constructing a latent space of semantic concepts . // le and lauw advocated a holistic framework for handling both document linkage and textual information and finding a unified lowdimensional representation . they achieved this via a joint probabilistic model .
b	29	[274]	qi et al . explored an effective algorithm that jointly embeds context and content in social media by constructing a latent space of semantic concepts . // le and lauw advocated a holistic framework for handling both document linkage and textual information and finding a unified lowdimensional representation . // they achieved this via a joint probabilistic model .
b	29	[288, 302]	in many applications , data exhibits multiple facets of presentations , and these data are referred as multi view data . // multi view learning aims at learning a statistical model from multiple information sources . // a number of algorithms have been proposed in the literature .
b	29	[303]	a number of algorithms have been proposed in the literature . // qian et al . investigated a reconstruction error based framework for handling multi label and multi view learning , which can explicitly quantify the performance of multiple labels or views merging . // lou et al . applied a two side multimodal neural network to embed words based on multiple data sources .
b	29	[302]	qian et al . investigated a reconstruction error based framework for handling multi label and multi view learning , which can explicitly quantify the performance of multiple labels or views merging . // lou et al . applied a two side multimodal neural network to embed words based on multiple data sources . // a more detailed review of multi view learning can be referred to . the main differences between our work and multi view learning are the facts that an attributed network can be seen as one specially constructed data source , and ane itself is a challenging problem .
b	42	[233]	a meaningful and discriminative representation for documents can help many text analysis tasks such as document classification , document clustering and information retrieval . // many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis .
b	42	[305]	a meaningful and discriminative representation for documents can help many text analysis tasks such as document classification , document clustering and information retrieval . // many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis .
b	42	[36, 35, 64]	a meaningful and discriminative representation for documents can help many text analysis tasks such as document classification , document clustering and information retrieval . // many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis .
h+	42	[64]	many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
b	42	[36]	many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
b	42	[306]	many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
b	42	[307]	many document representation methods are proposed such as bag of words , n gram , latent semantic analysis , latent dirichlet allocation and word/document embedding . // among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
ho	42	[156]	among these algorithms , the recently proposed distributed representations of words and documents such as skip gram and pv dm have demonstrated superior performance in many tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // the assumption behind these document/word embedding approaches is basically the distributional hypothesis that “ you shall know a word by the company it keeps . ” // they embed words or documents into a low dimensional space , which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag of words and n gram .
ho	42	[10]	for example , web documents such as blogs and online news often contain hyperlinks to other web documents , and scientific articles commonly cite other articles . // a toy example of linked documents is illustrated in figure 1 where { d1 , d2 , . . . , d5 } are documents and { w1 , w2 , . . . , w8 } are words in documents . in addition to content information , documents are linked and links suggest the inter dependence of documents . hence , the i .i .d . assumption of documents does not hold . // additional link information of such documents has been shown to be useful in various text mining tasks such as document classification , document clustering and feature selection . therefore , we propose to study the novel problem of linked document embedding following the distributional hypothesis .
b	42	[308, 28]	hence , the i .i .d . assumption of documents does not hold . // additional link information of such documents has been shown to be useful in various text mining tasks such as document classification , document clustering and feature selection . // therefore , we propose to study the novel problem of linked document embedding following the distributional hypothesis .
b	42	[309]	hence , the i .i .d . assumption of documents does not hold . // additional link information of such documents has been shown to be useful in various text mining tasks such as document classification , document clustering and feature selection . // therefore , we propose to study the novel problem of linked document embedding following the distributional hypothesis .
b	42	[36, 64, 7]	therefore , we propose to study the novel problem of linked document embedding following the distributional hypothesis . // most existing document embedding algorithms use unsupervised learning , such as those in . // the representations learned by these algorithms are very general and can be applied to various tasks .
h-	42	[201]	however , they may not be optimal for some specialized tasks where label information is available such as y2 for d2 and y5 for d5 in figure 1 ( a ) . // for example , deep learning algorithms such as convolutional neural networks , which use label information , often outperform text embeddings for classification tasks . // hence , in this paper we study the novel problem of linked document embedding for classification and investigate two specific problems : ( 1 ) how to capture link and label information mathematically ; and ( 2 ) how to exploit them for document embedding .
b	42	[201]	document representation . // document representation is an important research area that receives great attention lately and can benefit many machine learning and data mining tasks such as document classification , information retrieval and sentiment analysis . // many different types of models have been proposed for document representation .
b	42	[310, 311]	document representation . // document representation is an important research area that receives great attention lately and can benefit many machine learning and data mining tasks such as document classification , information retrieval and sentiment analysis . // many different types of models have been proposed for document representation .
h-	42	[312]	many different types of models have been proposed for document representation . // bog of words is one of the most widely used one . // it is simple to implement , but not scalable since as the number of documents increases , the vocabulary size can become huge . at the same time , it suffers from data sparsity and curse of dimensionality problems and the semantic relatedness between different words is omitted .
b	42	[233]	at the same time , it suffers from data sparsity and curse of dimensionality problems and the semantic relatedness between different words is omitted . // to mitigate the high dimensionality and data sparsity problems of bow , latent semantic analysis uses dimensionality reduction technique , i .e . , svd , to project the document word matrix to a low dimension space . // latent dirichlet allocation is another low dimensional document representation algorithm . it is a generative model that assumes that each document has topic distribution and each word in the document is drawn from a topic with probability .
b	42	[305]	to mitigate the high dimensionality and data sparsity problems of bow , latent semantic analysis uses dimensionality reduction technique , i .e . , svd , to project the document word matrix to a low dimension space . // latent dirichlet allocation is another low dimensional document representation algorithm . // it is a generative model that assumes that each document has topic distribution and each word in the document is drawn from a topic with probability .
h+	42	[36]	recently , mikolov et al . proposed the distributed representations of words , skip gram and cbow , which learn the embeddings of words by utilizing word cooccurrence in the local context . // it has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy , parsing , pos tagging , and sentiment analysis . // it is also scalable and can handle millions of documents . based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document .
h+	42	[306]	recently , mikolov et al . proposed the distributed representations of words , skip gram and cbow , which learn the embeddings of words by utilizing word cooccurrence in the local context . // it has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy , parsing , and sentiment analysis . // it is also scalable and can handle millions of documents . based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document .
h+	42	[306]	recently , mikolov et al . proposed the distributed representations of words , skip gram and cbow , which learn the embeddings of words by utilizing word cooccurrence in the local context . // it has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy , parsing , and sentiment analysis . // it is also scalable and can handle millions of documents . based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document .
b	42	[64]	based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document . // document embedding has also been proven to be powerful in many tasks such as sentiment analysis , machine translation and information retrieve . // recently , predictive text embedding algorithm ( pte ) is proposed in , which also utilizes label information to learn predictive text embeddings .
b	42	[313]	based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document . // document embedding has also been proven to be powerful in many tasks such as sentiment analysis , machine translation and information retrieve . // recently , predictive text embedding algorithm ( pte ) is proposed in , which also utilizes label information to learn predictive text embeddings .
b	42	[311]	based on the same distributed representation idea , extended the word embedding model to document embedding ( pv dm , pv dbow ) by finding document representations that are good at predicting words in the document . // document embedding has also been proven to be powerful in many tasks such as sentiment analysis , machine translation and information retrieve . // recently , predictive text embedding algorithm ( pte ) is proposed in , which also utilizes label information to learn predictive text embeddings .
h+	42	[314, 309]	documents in many real world applications are inherently linked . for example , web pages are linked by hyperlinks and scientific papers are linked by citations . // link information has been proven to be very effective for machine learning and data mining such as feature selection , recommender systems , and document classification/clustering . // the proposed framework lde is inherently different from pte : ( 1 ) lde is developed for linked documents while pte still assumes documents to be i .i .d . ; ( 2 ) lde captures label information via modeling document label information while pte uses label information via word label information ; ( 3 ) in addition to label information , lde also models link information among documents to learn document embeddings ; and ( 4 ) the proposed formulations and optimization problems of lde are also different from those of pte .
h+	42	[315, 316]	documents in many real world applications are inherently linked . for example , web pages are linked by hyperlinks and scientific papers are linked by citations . // link information has been proven to be very effective for machine learning and data mining such as feature selection , recommender systems , and document classification/clustering . // the proposed framework lde is inherently different from pte : ( 1 ) lde is developed for linked documents while pte still assumes documents to be i .i .d . ; ( 2 ) lde captures label information via modeling document label information while pte uses label information via word label information ; ( 3 ) in addition to label information , lde also models link information among documents to learn document embeddings ; and ( 4 ) the proposed formulations and optimization problems of lde are also different from those of pte .
h+	42	[317, 318]	documents in many real world applications are inherently linked . for example , web pages are linked by hyperlinks and scientific papers are linked by citations . // link information has been proven to be very effective for machine learning and data mining such as feature selection , recommender systems , and document classification/clustering . // the proposed framework lde is inherently different from pte : ( 1 ) lde is developed for linked documents while pte still assumes documents to be i .i .d . ; ( 2 ) lde captures label information via modeling document label information while pte uses label information via word label information ; ( 3 ) in addition to label information , lde also models link information among documents to learn document embeddings ; and ( 4 ) the proposed formulations and optimization problems of lde are also different from those of pte .
h-	42	[319]	based on the idea that two linked documents are likely to share similar topics , several works have been proposed to utilize link information for better document representations . // for example , rtm extends lda by considering link information for topic modeling ; pmtlm combines topic modeling with a variant of mixed membership block model to model linked documents and tadw learns linked document representations based on matrix factorization . // however , the majority of the aforementioned works do not utilize label information ; meanwhile most of them do not learn distributed document representations based on the distributional hypothesis ; while lde employs distributional hypothesis idea for document embedding by combining link and label information with content simultaneously .
h-	42	[320]	based on the idea that two linked documents are likely to share similar topics , several works have been proposed to utilize link information for better document representations . // for example , rtm extends lda by considering link information for topic modeling ; pmtlm combines topic modeling with a variant of mixed membership block model to model linked documents and tadw learns linked document representations based on matrix factorization . // however , the majority of the aforementioned works do not utilize label information ; meanwhile most of them do not learn distributed document representations based on the distributional hypothesis ; while lde employs distributional hypothesis idea for document embedding by combining link and label information with content simultaneously .
h-	42	[7]	based on the idea that two linked documents are likely to share similar topics , several works have been proposed to utilize link information for better document representations . // for example , rtm extends lda by considering link information for topic modeling ; pmtlm combines topic modeling with a variant of mixed membership block model to model linked documents and tadw learns linked document representations based on matrix factorization . // however , the majority of the aforementioned works do not utilize label information ; meanwhile most of them do not learn distributed document representations based on the distributional hypothesis ; while lde employs distributional hypothesis idea for document embedding by combining link and label information with content simultaneously .
b	42	[322]	however , label propagation doesn ’ t utilize the features of documents . // gc is a more advanced graph based classification method , which takes into account both link structure and documents ’ content and can be combined with svm classifiers . // graffiti is proposed to perform random walk on heterogeneous networks so as to capture the mutual influence of connected nodes for classification .
b	42	[323]	gc is a more advanced graph based classification method , which takes into account both link structure and documents ’ content and can be combined with svm classifiers . // graffiti is proposed to perform random walk on heterogeneous networks so as to capture the mutual influence of connected nodes for classification . // abernethy et al . incorporates the graph information into svm classifier for web spam detection .
b	42	[324]	graffiti is proposed to perform random walk on heterogeneous networks so as to capture the mutual influence of connected nodes for classification . // abernethy et al . incorporates the graph information into svm classifier for web spam detection . // the proposed method lde is inherently different form the existing graph based classification .
b	42	[201]	the proposed method lde is inherently different form the existing graph based classification . // first , lde learns both word embedding and document embedding , which can be used for other tasks , such as word analogy and visualization ; while existing graph based classification methods don ’ t learn word/document representation . // second , lde utilizes the distributional hypothesis idea and considers word word document relations , while existing graph based classification methods usually use bow without considering the word word relationship .
b	42	[312]	the details of these algorithms are listed as follows . // bow : the classical “ bag of words ” represent each document as a m dimensional vector , where m is the size of the vocabulary and weight of each dimension is calculated by the tfidf scheme . // rtm : relational topic model is an extension of topic modeling that models document content and links between documents
b	42	[319]	bow : the classical “ bag of words ” represent each document as a m dimensional vector , where m is the size of the vocabulary and weight of each dimension is calculated by the tfidf scheme . // rtm : relational topic model is an extension of topic modeling that models document content and links between documents . // skip gram : one of the state of the art word embedding model and its training objective is to find word representations that are useful for predicting the surrounding words of a selected word in a sentence . after obtaining word embeddings by skip gram , we use eq . ( 21 ) to get document representations .
h+	42	[36]	skip gram : one of the state of the art word embedding model and its training objective is to find word representations that are useful for predicting the surrounding words of a selected word in a sentence . after obtaining word embeddings by skip gram , we use eq . ( 21 ) to get document representations . // cbow : another state of the art word embedding model . unlike skip gram , the training objective of cbow is to find word representations that are useful for predicting the center word by its neighbors . // pv dm : the distributed memory version of paragraph vector which considers the order of the words . it aims at learning document embeddings that are good at predicting the next given context .
b	42	[64]	cbow : another state of the art word embedding model . unlike skip gram , the training objective of cbow is to find word representations that are useful for predicting the center word by its neighbors . // pv dm : the distributed memory version of paragraph vector which considers the order of the words . it aims at learning document embeddings that are good at predicting the next given context . // pv dbow : the distributed bag of words version of paragraph vector model proposed in .
b	42	[64]	pv dm : the distributed memory version of paragraph vector which considers the order of the words . it aims at learning document embeddings that are good at predicting the next given context . // pv dbow . // unlike pv dm , the word order is ignored in pv dbow . it aims to learn document representations that are good at predicting words in the document .
b	42	[321]	unlike pv dm , the word order is ignored in pv dbow . it aims to learn document representations that are good at predicting words in the document . // lp : a traditional semi supervised algorithm based on label propagation , which performs classification by propagating label information from labeled data to unlabeled data through the graph . // lp denotes a traditional method that utilizes both network information and label information for classification .
b	42	[322]	lp : a traditional semi supervised algorithm based on label propagation , which performs classification by propagating label information from labeled data to unlabeled data through the graph . // gc a graph based classification method which utilizes both document contents , link and label information into a probabilistic framework for classification . // cnn : convolution neural network for classification . it uses word embeddings as input to train convolution neural network with label information3 .
b	42	[307]	gc a graph based classification method which utilizes both document contents , link and label information into a probabilistic framework for classification . // cnn : convolution neural network for classification . it uses word embeddings as input to train convolution neural network with label information3 . // tadw : text associated deepwalk is a matrix factorization based method that utilizes both link and document data4 .
h-	42	[201]	tadw : text associated deepwalk is a matrix factorization based method that utilizes both link and document data4 . // pte : predictive text embedding which considers label information to learn word embedding but can not handle link information among documents . // lde word : the proposed framework trains both wordembedding and document embedding .
ho	42	[270]	labels of testing data are held out and no algorithm can use labels of testing data during the representation learning phase . // during the classification phase , we use libsvm5 to train a svm classifier using the learned document embeddings and the training data . the trained svm classifier is then assessed on the testing data . // there are some parameters to set for the baseline algorithms .
ho	42	[36]	for a fair comparison , for skip gram , cbow , pvdm , pv dbow , cnn , rtm and lde , we set the embedding dimension to be 100 . // for skip gram , cbow , pv dm , pv dbow and lde , following the parameter setting suggestions in , we set the window size to be 7 and the number of negative samples also to be 7 . // we follow the setting in for pte and we use the default setting in the code of tadw .
ho	42	[307]	for the proposed model , we choose γ to be 0 .0001 . // as of cnn , we use the default architecture in . // for both datasets , we randomly select 60 % as training data and the remaining 40 % as testing data .
b	28	[325, 326, 109]	// representation learning , which aims at learning low dimensional semantic representations of high dimensional data , has proven to facilitate many machine learning and data mining tasks such as classification , clustering and information retrieval . // in terms of the label availability , representation learning methods can be broadly classified into supervised and unsupervised methods .
b	28	[327, 328]	// representation learning , which aims at learning low dimensional semantic representations of high dimensional data , has proven to facilitate many machine learning and data mining tasks such as classification , clustering and information retrieval . // in terms of the label availability , representation learning methods can be broadly classified into supervised and unsupervised methods .
b	28	[325, 326]	representation learning , which aims at learning low dimensional semantic representations of high dimensional data , has proven to facilitate many machine learning and data mining tasks such as classification , clustering and information retrieval . // in terms of the label availability , representation learning methods can be broadly classified into supervised and unsupervised methods . // as most data is unlabeled and it is very expensive to label the data , unsupervised representation learning has attracted increasing attention in recent years .
b	28	[327, 330]	representation learning , which aims at learning low dimensional semantic representations of high dimensional data , has proven to facilitate many machine learning and data mining tasks such as classification , clustering and information retrieval . // in terms of the label availability , representation learning methods can be broadly classified into supervised and unsupervised methods . // as most data is unlabeled and it is very expensive to label the data , unsupervised representation learning has attracted increasing attention in recent years .
b	28	[327, 328]	in terms of the label availability , representation learning methods can be broadly classified into supervised and unsupervised methods . // as most data is unlabeled and it is very expensive to label the data , unsupervised representation learning has attracted increasing attention in recent years . // restricted boltzmann machine ( rbm ) is one of the most widely used unsupervised representation learning methods .
h+	28	[174]	restricted boltzmann machine ( rbm ) is one of the most widely used unsupervised representation learning methods . // rbm is very powerful in learning meaningful nonlinear latent features , which has powered many applications such as collaborative filtering , link prediction , document representation and social behavior prediction . // in recent years , linked data has become pervasively available in various domains .
h+	28	[329, 328]	restricted boltzmann machine ( rbm ) is one of the most widely used unsupervised representation learning methods . // rbm is very powerful in learning meaningful nonlinear latent features , which has powered many applications such as collaborative filtering , link prediction , document representation and social behavior prediction . // in recent years , linked data has become pervasively available in various domains .
h+	28	[333]	restricted boltzmann machine ( rbm ) is one of the most widely used unsupervised representation learning methods . // rbm is very powerful in learning meaningful nonlinear latent features , which has powered many applications such as collaborative filtering , link prediction , document representation and social behavior prediction . // in recent years , linked data has become pervasively available in various domains .
b	28	[334]	in recent years , linked data has become pervasively available in various domains . // for example , social media data is inherently linked via social context , web data is networked via hyperlinks , and biological data is embedded in correlation or interaction networks . // link information can be represented as a network as shown in figure 1 ( a ) , where nodes are data instances .
b	28	[335]	in recent years , linked data has become pervasively available in various domains . // for example , social media data is inherently linked via social context , web data is networked via hyperlinks , and biological data is embedded in correlation or interaction networks . // link information can be represented as a network as shown in figure 1 ( a ) , where nodes are data instances .
h+	28	[314]	in addition , linked data provides link information as demonstrated in figure 1 ( c ) . // linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection , sentiment analysis , topic modeling and document classification . // therefore , it has potential to advance rbms for better representation learning .
h+	28	[319]	in addition , linked data provides link information as demonstrated in figure 1 ( c ) . // linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection , sentiment analysis , topic modeling and document classification . // therefore , it has potential to advance rbms for better representation learning .
h+	28	[42]	in addition , linked data provides link information as demonstrated in figure 1 ( c ) . // linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection , sentiment analysis , topic modeling and document classification . // therefore , it has potential to advance rbms for better representation learning .
h+	28	[336]	// pca : principle component analysis performs dimensionality reduction by seeking orthogonal projections of the data onto a low dimensional linear space such that the variance of the projected data is maximized . // it is a popular and effective linear feature learning algorithm .
h+	28	[47]	the encoded feature is used to perform clustering . // sdae : stacked denoising autoencoder is a deep network based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs . // compared with the denoising autoencoder , features learned in a purely unsupervised fashion by sdae are higher level and could boost the performance of clustering .
b	28	[338]	sdae is used as a representative deep learning algorithm for unsupervised representation learning . // rbm : restricted boltzmann machine is an undirected graphical model which defines a probability distribution over a vector of observed and a vector of latent variables . // the learned latent variable is used for clustering in our experiment .
b	28	[319]	rbm can be seen as prbm without link information . // rtm : relational topic model is a variant of latent dirichlet allocation ( lda ) , which takes attribute and link information into consideration for learning topic distributions . // the learned topic distributions of documents are treated as the representations .
b	28	[173]	it is state of the art representation learning algorithm for network with rich attributes . // lrbm : lrbm combines graph factorization and conditional rbm using four way tensor for linked data . // it is the closest work to ours and their differences will be detailed in the related work section .
b	28	[339]	as mentioned earlier , for training rbm and prbm , calculation of the negative gradient is intractable . // contrastive divergence ( cd ) is the first practical method for training rbms . // instead of running a gibbs chain until equilibrium to draw samples for negative gradient , contrastive divergence approximates the negative gradient using samples obtained by starting a gibbs chain at a training vector and running it for a few steps .
ho	28	[340]	instead of running a gibbs chain until equilibrium to draw samples for negative gradient , contrastive divergence approximates the negative gradient using samples obtained by starting a gibbs chain at a training vector and running it for a few steps . // though it has been proven that the resulting gradient estimate is not the gradient of any function , it is extensively used for training energy based models such as rbms , and the performance is good . // however , cd learning has a problem that it provides biased estimates of the gradient .
h+	28	[332, 331]	variants of rbms . // rbm is very powerful for unsupervised representation learning , which has powered many applications such as collaborative filtering , document representation and social behavior prediction . // in , rbm is used for representation learning of documents by considering the diversity .
h+	28	[329, 328]	variants of rbms . // rbm is very powerful for unsupervised representation learning , which has powered many applications such as collaborative filtering , document representation and social behavior prediction . // in , rbm is used for representation learning of documents by considering the diversity .
h+	28	[333]	variants of rbms . // rbm is very powerful for unsupervised representation learning , which has powered many applications such as collaborative filtering , document representation and social behavior prediction . // in , rbm is used for representation learning of documents by considering the diversity .
b	28	[342]	in , rbm is used for representation learning of documents by considering the diversity . // in , rbm and conditional rbm are applied to the task of learning drug target relations on multidimensional networks . // in deep boltzmann machines ( dbm ) are proposed with multiple hidden layers by stacking rbms . rbm is also used for modeling networks .
b	28	[327]	in , rbm and conditional rbm are applied to the task of learning drug target relations on multidimensional networks . // in . // rbm is also used for modeling networks .
b	28	[327]	in , rbm and conditional rbm are applied to the task of learning drug target relations on multidimensional networks . // in . // rbm is also used for modeling networks .
b	28	[173]	however , they only use the link structure without considering the attributes of the nodes . // in , lrbm is proposed to learn feature representation from both attributes and the link structure for node classification and link prediction . // lrbm combines graph factorization and conditional rbm using a four way tensor for linked data .
b	39	[3, 298, 6, 1, 201]	hence , network embedding , which is used to represent each vertex of a network with a lowdimensional vector that can preserve the similarities between them , has attracted continuous attention and has been successfully used in various applications , including image processing , knowledge graph , recommendation , etc . // along with the increasing requirements , a variety of researchers have studied the network embedding construction problem from different aspects . // classical methods ( e .g . , isomap and laplacian eigenmaps ) usually transform this task into a constrained optimization problem .
h-	39	[3]	along with the increasing requirements , a variety of researchers have studied the network embedding construction problem from different aspects . // classical methods ( e .g . , isomap and laplacian eigenmaps ) usually transform this task into a constrained optimization problem . // hence , the usefulness of these methods may be heavily impacted by the computation consumption of processing hundreds of millions of nodes .
b	39	[6]	hence , the usefulness of these methods may be heavily impacted by the computation consumption of processing hundreds of millions of nodes . // to process largescale networks , deepwalk uses a shallow neural network architecture . // line uses both first order and second order proximity to train the embedding , and negative sampling methods to reduce the computational requirement .
h-	39	[1]	to process largescale networks , deepwalk uses a shallow neural network architecture . // line uses both first order and second order proximity to train the embedding , and negative sampling methods to reduce the computational requirement . // however , most of the previous studies focused on a classical network and took only vertices and edges into consideration .
b	39	[343]	for example , in social media ( e .g . , youtube and facebook ) , users can create groups that other users can join . // previous literatures also show that this kind of network is common in real world social , collaboration , information , and many other kinds of networks . // more than 200 different kinds of large real world networks where nodes explicitly state their group memberships were studied in the work done by yang and leskovec .
ho	39	[6]	the proposed methods . // inspired by the work of deepwalk and the idea of modelling document in natural language processing , our model contains two main stages , sampling and training . // we will then illustrate the two steps in details .
ho	39	[64, 65]	the proposed methods . // inspired by the work of deepwalk and the idea of modelling document in natural language processing , our model contains two main stages , sampling and training . // we will then illustrate the two steps in details .
ho	39	[36]	millions of vertices , so it is impractical to compute these gradients directly . // to address this problem , we adopt the approach of negative sampling proposed in to reduce the computational requirement . // we then give a brief explanation to our proposed model . since we adopt random walks starting from different vertices in the network , and there should be similar walks for the vertices sharing similar neighbours , thus , those vertices will be placed closely
b	39	[344]	each product category provided by amazon defines each ground truth community . // youtube : this network is provided by alan mislove . // youtube is a video sharing web site that includes a social network . in the youtube social network , users form friendship each other and users can create groups which other users can join .
b	39	[6]	// deepwalk : it uses information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences . // line : it is a network embedding method , which tries to preserve both the local and global network structures .
b	39	[1]	deepwalk : it uses information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences . // line : it is a network embedding method , which tries to preserve both the local and global network structures . // groupwalk : it only takes the labeled random walks cross groups mentioned above into consideration .
h-	9	[36, 167]	the resulting compact , low dimensional vector representations are believed to be able to capture rich semantic information and are proven to be useful for various natural language processing tasks . // while efficient and effective methods for learning good representations for linear structures have been identified , dealing with general graph structures with rich topologies is more complicated . // to this end , one natural approach is to identify effective means to cast the task of learning vertex representations for general graph structures into the task of learning representations from linear structures .
b	9	[167]	to this end , one natural approach is to identify effective means to cast the task of learning vertex representations for general graph structures into the task of learning representations from linear structures . // deepwalk proposed by presented an idea to transform unweighted graphs into collections of linear sequences by a uniform sampling method known as truncated random walk . // in their approach , sampled vertex sequences characterize the connections between vertices in a graph .
h+	9	[36]	this step can be understood as a process for converting a general graph structure into a large collection of linear structures . // next , they utilized the skip gram model proposed by to learn low dimensional representations for vertices from such linear structures . // the learned vertex representations were shown to be effective across a few tasks , outperforming several previous approaches such as spectral clustering and modularity method .
b	9	[235]	to answer the first question , we design a random surfing model suitable for weighted graphs , which can directly yield a probabilistic co occurrence matrix . // such a matrix is similar to the cooccurrence matrix obtained by sampling linear sequences from graphs , but no sampling process is required in our approach . // to answer the secproceedings of the thirtieth aaai conference on artificial intelligence and question , we first revisit one popular existing method used for learning vertex representations for linear structures .
b	9	[160]	to answer the secproceedings of the thirtieth aaai conference on artificial intelligence and question , we first revisit one popular existing method used for learning vertex representations for linear structures . // a recent study by showed that optimizing the objective function associated with the skipgram with negative sampling method has a intrinsic relation with factorzing a shifted positive pointwise mutual information ( ppmi ) matrix of the words and their contexts . // specifically , they showed that it was possible to use the standard singular value decomposition ( svd ) method to factorize the ppmi matrix to induce the vertex/word representations from the decomposed matrices .
b	9	[36]	to answer the secproceedings of the thirtieth aaai conference on artificial intelligence and question , we first revisit one popular existing method used for learning vertex representations for linear structures . // a recent study by showed that optimizing the objective function associated with the skipgram with negative sampling method has a intrinsic relation with factorzing a shifted positive pointwise mutual information ( ppmi ) matrix of the words and their contexts . // specifically , they showed that it was possible to use the standard singular value decomposition ( svd ) method to factorize the ppmi matrix to induce the vertex/word representations from the decomposed matrices .
h-	9	[26]	specifically , they showed that it was possible to use the standard singular value decomposition ( svd ) method to factorize the ppmi matrix to induce the vertex/word representations from the decomposed matrices . // our recent approach called grarep has been shown to achieve good empirical results on the task of learning graph representations . // however , the approach employs svd for performing linear dimension reduction , while better non linear dimension reduction techniques were not explored .
ho	9	[345]	while the svd step for inducing the final word representations was shown effective , levy et al . ( 2015 ) also demonstrated the effectiveness of using the ppmi matrix itself as the word representations . // interestingly , as shown by the authors , the representations learned from the svd method can not completely outperform the representations from the ppmi matrix itself . // since our final goal is to learn good vertex representations that are effective in capturing the graph ’ s information , it is essential to investigate better ways of recovering the vertex representations from the ppmi matrix , where potentially complex , non linear relations amongst different vertices can be captured .
h+	9	[150]	since our final goal is to learn good vertex representations that are effective in capturing the graph ’ s information , it is essential to investigate better ways of recovering the vertex representations from the ppmi matrix , where potentially complex , non linear relations amongst different vertices can be captured . // deep learning sheds light on the path of modeling nonlinear complex phenomena , which has many successful applications in different domains , such as speech recognition and computer vision . // deep neural networks ( dnn ) , e .g . , the stacked autoencoders , can be regarded as an effective method for learning high level abstractions from low level features .
h+	9	[6]	we apply the deep learning method to the ppmi matrix instead of the laplacian matrix used in their model . // the former has been shown to have the potential to yield better representations than the latter . // to enhance our model ’ s robustness , we also employ the stacked denoising autoencoders for learning multiple layers of representations .
h-	9	[234]	such approaches learn representations based on statistics of global co occurrence counts , and can outperform neural network method based on separate local context windows in certain prediction tasks . // an example of a matrix factorization method is hyperspace analogue analysis which factorizes a word word co occurrence matrix to yield word representations . // a major shortcoming of such an approach and related methods is that frequent words with relatively little semantic value such as stop words have a disproportionate effect on the word representations generated .
h+	9	[346]	a major shortcoming of such an approach and related methods is that frequent words with relatively little semantic value such as stop words have a disproportionate effect on the word representations generated . // church and hanks ’ s pointwise mutual information matrix was proposed to address this problem and has since been proven to provide better word representations . // a common approach to improving performance is to assign each negative value to 0 .
h+	9	[150, 149]	deep neural networks . // deep neural networks , which can be used to learn multiple levels of feature representations , has achieved successful results in different fields . // training such networks were shown difficult .
h+	9	[43, 347]	training such networks were shown difficult . // one effective solution , proposed in , is to use the greedy layer wise unsupervised pre training . // this strategy aims at learning useful representations of each layer at a time .
b	9	[348, 349]	neural networks typically employ non linear activation function such as sigmoid or tanh to capture complex , non linear projections from the inputs to the outputs . // to train a deep architecture that involves multiple layers of feature representations , autoencoders have emerged as one of the commonly used building blocks . // an autoencoder performs two actions – an encoding step , followed by a decoding step .
h+	9	[167]	thus , it is reasonable to weigh the importance of contextual nodes based on their relative distance to the current node . // such weighting strategies were implemented in both word2vec and glove and were found important for achieving good empirical results . // based on this fact , we can see that ideally the representation for the i th vertex should be constructed in the following way : r = k k=1 w ( k ) · p∗ k where w ( · ) is a decreasing function , i .e . , w ( t + 1 ) < w ( t ) .
h+	9	[345]	thus , it is reasonable to weigh the importance of contextual nodes based on their relative distance to the current node . // such weighting strategies were implemented in both word2vec and glove and were found important for achieving good empirical results . // based on this fact , we can see that ideally the representation for the i th vertex should be constructed in the following way : r = k k=1 w ( k ) · p∗ k where w ( · ) is a decreasing function , i .e . , w ( t + 1 ) < w ( t ) .
ho	9	[36]	to demonstrate the effectiveness of deep learning as compared to svd , we chose a dictionary of 10 ,000 most frequent words ( 10 ,000 selected due to the time complexity of the svd algorithm ) . // to evaluate the performance of word representations generated by each algorithm , we conducted experiments on word similarities on 4 datasets , including the popular wordsim353 , wordsim similarity and wordsim relatedness and mc , as used in . //
ho	9	[351]	to demonstrate the effectiveness of deep learning as compared to svd , we chose a dictionary of 10 ,000 most frequent words ( 10 ,000 selected due to the time complexity of the svd algorithm ) . // to evaluate the performance of word representations generated by each algorithm , we conducted experiments on word similarities on 4 datasets , including the popular wordsim353 , wordsim similarity and wordsim relatedness and mc , as used in . //
ho	9	[352]	to demonstrate the effectiveness of deep learning as compared to svd , we chose a dictionary of 10 ,000 most frequent words ( 10 ,000 selected due to the time complexity of the svd algorithm ) . // to evaluate the performance of word representations generated by each algorithm , we conducted experiments on word similarities on 4 datasets , including the popular wordsim353 , wordsim similarity and wordsim relatedness and mc , as used in . //
ho	9	[167, 345]	to demonstrate the effectiveness of deep learning as compared to svd , we chose a dictionary of 10 ,000 most frequent words ( 10 ,000 selected due to the time complexity of the svd algorithm ) . // to evaluate the performance of word representations generated by each algorithm , we conducted experiments on word similarities on 4 datasets , including the popular wordsim353 , wordsim similarity and wordsim relatedness and mc , as used in . //
h+	9	[36]	it transforms a graph structure into linear sequences by truncated random walks and processes the sequences using skip gram with hierarchical softmax . // sgns is proposed in word2vec . // it is suitable for capturing linear structures . sgns has been proven to be an effective model to learn word representations both theoretically and empircally
b	9	[235]	sgns has been proven to be an effective model to learn word representations both theoretically and empircally . // ppmi is a measure often used in information theory . // ppmi was used for word representations in and is a sparse high dimensional representation .
b	9	[345]	ppmi is a measure often used in information theory . // ppmi was used for word representations in and is a sparse high dimensional representation . // svd is a common matrix factorization method that is used to reduce dimensions or extract features .
ho	9	[160]	svd is a common matrix factorization method that is used to reduce dimensions or extract features . // following , we used svd to compress the ppmi matrix to obtain low dimensional representations . // parameters .
h-	32	[5, 4, 3]	these representations can then be used as features for common tasks on graphs such as multi label classification , clustering , and link prediction . // traditional methods for graph dimensionality reduction perform well on small graphs . // however , the time complexity of these methods are at least quadratic in the number of graph nodes , makes them impossible to run on large scale networks .
b	32	[6]	however , the time complexity of these methods are at least quadratic in the number of graph nodes , makes them impossible to run on large scale networks . // a recent advancement in graph representation learning , deepwalk proposed online learning methods using neural networks to address this scalability limitation . // much work has since followed . these neural network based methods have proven both highly scalable and performant , achieving strong results on classification and link prediction tasks in large networks .
h-	32	[204, 36]	this focus on local structure implicitly ignores long distance global relationships , and the learned representations can fail to uncover important global structural patterns . // secondly , they all rely on a non convex optimization goal solved using stochastic gradient descent which can become stuck in a local minima ( e .g . perhaps as a result of a poor initialization ) . // in other words , all previously proposed techniques for graph representation learning can accidentally learn embedding configurations which disregard important structural features of their input graph .
ho	32	[354]	new representation learning paradigm . // we propose harp , a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing and graph representation learning communities to build substantially better graph embeddings . // improved optimization primitives .
ho	32	[6, 1, 37]	new representation learning paradigm . // we propose harp , a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing and graph representation learning communities to build substantially better graph embeddings . // improved optimization primitives .
ho	32	[6, 1, 37]	new representation learning paradigm . // we propose harp , a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing and graph representation learning communities to build substantially better graph embeddings . // improved optimization primitives .
b	32	[353]	table 1 : statistics of the graphs used in our experiments . // dblp – dblp is a co author graph of researchers in computer science . // the labels indicate the research areas a researcher publishes his work in .
b	32	[56]	the labels indicate the research areas a researcher publishes his work in . // blogcatalog – blogcatalog is a network of social relationships between users on the blogcatalog website . // the labels represent the categories a blogger publishes in .
h-	32	[5, 355, 3]	graph representation learning . // most early methods treated representation learning as performing dimension reduction on the laplacian and adjacency matrices . // these methods work well on small graphs , but the time complexity of these algorithms is too high for the large scale graphs commonly encountered today .
b	32	[6]	these methods work well on small graphs , but the time complexity of these algorithms is too high for the large scale graphs commonly encountered today . // deepwalk presents a two phase algorithm for graph representation learning . in the first phase , deepwalk samples sequences of neighboring nodes of each node by random walking on the graph . // in the first phase , deepwalk samples sequences of neighboring nodes of each node by random walking on the graph .
b	32	[36]	in the first phase , deepwalk samples sequences of neighboring nodes of each node by random walking on the graph . // then , the node representation is learned by training a skip gram model on the random walks . // a number of methods have been proposed which extend this idea .
b	32	[353]	line learns graph embeddings which preserve both the first order and second order proximities in a graph . // walklets captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix . // node2vec combines dfs like and bfs like exploration within the random walk framework .
b	32	[37]	walklets captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix . // node2vec combines dfs like and bfs like exploration within the random walk framework . // second , matrix factorization methods and deep neural networks have also been proposed as alternatives to the skip gram model for learning the latent representations .
h-	32	[26, 48, 19, 353]	node2vec combines dfs like and bfs like exploration within the random walk framework . // second , matrix factorization methods and deep neural networks have also been proposed as alternatives to the skip gram model for learning the latent representations . // although these methods are highly scalable , they all rely on optimizing a non convex objective function . with no prior knowledge of the graph , the latent representations are usually initialized with random numbers or zero .
h+	32	[358]	using an approximation of the original graph has two advantages not only is the approximation usually simpler to solve , it can also be extended as a good initialization for solving the original problem . // in addition to force directed graph drawing , the multilevel framework has been proved successful in various graph theory problems , including the traveling salesman problem , and graph partitioning . // harp extends the idea of the multilevel layout to neural representation learning methods .
h+	32	[359]	using an approximation of the original graph has two advantages not only is the approximation usually simpler to solve , it can also be extended as a good initialization for solving the original problem . // in addition to force directed graph drawing , the multilevel framework has been proved successful in various graph theory problems , including the traveling salesman problem , and graph partitioning . // harp extends the idea of the multilevel layout to neural representation learning methods .
h+	32	[360]	using an approximation of the original graph has two advantages not only is the approximation usually simpler to solve , it can also be extended as a good initialization for solving the original problem . // in addition to force directed graph drawing , the multilevel framework has been proved successful in various graph theory problems , including the traveling salesman problem , and graph partitioning . // harp extends the idea of the multilevel layout to neural representation learning methods .
b	44	[281]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[151]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[78]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[176]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[1]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[361]	graph representation learning , also known as network embedding , aims to represent each vertex in a graph ( network ) as a low dimensional vector , which could facilitate tasks of network analysis and prediction over vertices and edges . // learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs .
b	44	[40]	learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity .
b	44	[30]	learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity .
b	44	[117]	learned embeddings are capable to benefit a wide range of real world applications such as link prediction , node classification , recommendation , visualization , knowledge graph representation , clustering , text embedding , and social network analysis . // recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity .
b	44	[19]	recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity . // arguably , most existing methods of graph representation learning can be classified into two categories . the first is generative graph representation learning model .
b	44	[30]	recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity . // arguably , most existing methods of graph representation learning can be classified into two categories . the first is generative graph representation learning model .
b	44	[39]	recently , researchers have examined applying representation learning methods to various types of graphs , such as weighted graphs , directed graphs , signed graphs , heterogeneous graphs , and attributed graphs . // in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity . // arguably , most existing methods of graph representation learning can be classified into two categories . the first is generative graph representation learning model .
b	44	[6, 37, 40, 111, 40]	in addition , several prior works also try to preserve specific properties during the learning process , such as global structures , community structures , group information , and asymmetric transitivity . // arguably , most existing methods of graph representation learning can be classified into two categories . the first is generative graph representation learning model . // similar to classic generative models such as gaussian mixture model or latent dirichlet allocation , generative graph representation learning models assume that , for each vertex vc , there exists an underlying true connectivity distribution ptrue , which implies vc ’ s connectivity preference over all other vertices in the graph .
b	44	[362]	the first is generative graph representation learning model . // similar to classic generative models such as gaussian mixture model or latent dirichlet allocation , generative graph representation learning models assume that , for each vertex vc , there exists an underlying true connectivity distribution ptrue , which implies vc ’ s connectivity preference over all other vertices in the graph . // the edges in the graph can thus be viewed as observed samples generated by these conditional distributions , and these generative models learn vertex embeddings by maximizing the likelihood of edges in the graph .
b	44	[305]	the first is generative graph representation learning model . // similar to classic generative models such as gaussian mixture model or latent dirichlet allocation , generative graph representation learning models assume that , for each vertex vc , there exists an underlying true connectivity distribution ptrue , which implies vc ’ s connectivity preference over all other vertices in the graph . // the edges in the graph can thus be viewed as observed samples generated by these conditional distributions , and these generative models learn vertex embeddings by maximizing the likelihood of edges in the graph .
b	44	[37]	for example , deepwalk uses random walk to sample “ context ” vertices for each vertex , and tries to maximize the log likelihood of observing context vertices for the given vertex . // node2vec further extends the idea by proposing a biased random walk procedure , which provides more flexibility when generating the context for a given vertex . // the second kind of graph representation learning method is the discriminative model .
b	44	[117, 9, 19, 30]	node2vec further extends the idea by proposing a biased random walk procedure , which provides more flexibility when generating the context for a given vertex . // the second kind of graph representation learning method is the discriminative model . // different from generative models , discriminative graph representation learning models do not treat edges as generated from an underlying conditional distribution , but aim to learn a classifier for predicting the existence of edges directly .
b	44	[19]	typically , discriminative models consider two vertices vi and vj jointly as features , and predict the probability of an edge existing between the two vertices , i .e . , p edge| ( vi , vj ) , based on the training data in the graph . // for instance , sdne uses the sparse adjacency vector of vertices as raw features for each vertex , and applies an autoencoder to extract short and condense features for vertices under the supervision of edge existence . // ppne directly learns vertex embeddings with supervised learning on positive samples and negative samples , also preserving the inherent properties of vertices during the learning process .
ho	44	[40]	ppne directly learns vertex embeddings with supervised learning on positive samples and negative samples , also preserving the inherent properties of vertices during the learning process . // although generative and discriminative models are generally two disjoint classes of graph representation learning methods , they can be considered two sides of the same coin . // in fact , line has done a preliminary trial on implicitly combining these two objectives .
b	44	[1]	although generative and discriminative models are generally two disjoint classes of graph representation learning methods , they can be considered two sides of the same coin . // in fact , line has done a preliminary trial on implicitly combining these two objectives . // recently , generative adversarial nets have received a great deal of attention .
b	44	[45]	in fact , line has done a preliminary trial on implicitly combining these two objectives . // recently , generative adversarial nets have received a great deal of attention . // by designing a game theoretical minimax game to combine generative and discriminative models , gan and its variants achieve success in various applications , such as image generation , sequence generation , dialogue generation , information retrieval , and domain adaption .
h+	44	[364]	recently , generative adversarial nets have received a great deal of attention . // by designing a game theoretical minimax game to combine generative and discriminative models , gan and its variants achieve success in various applications , such as image generation , sequence generation , dialogue generation , information retrieval , and domain adaption . // inspired by gan , in this paper we propose graphgan , a novel framework that unifies generative and discriminative thinking for graph representation learning .
h+	44	[365]	recently , generative adversarial nets have received a great deal of attention . // by designing a game theoretical minimax game to combine generative and discriminative models , gan and its variants achieve success in various applications , such as image generation , sequence generation , dialogue generation , information retrieval , and domain adaption . // inspired by gan , in this paper we propose graphgan , a novel framework that unifies generative and discriminative thinking for graph representation learning .
h+	44	[40]	recently , generative adversarial nets have received a great deal of attention . // by designing a game theoretical minimax game to combine generative and discriminative models , gan and its variants achieve success in various applications , such as image generation , sequence generation , dialogue generation , information retrieval , and domain adaption . // inspired by gan , in this paper we propose graphgan , a novel framework that unifies generative and discriminative thinking for graph representation learning .
b	44	[6]	movielens 1m6 is a bipartite graph consisting of approximately 1 million ratings ( edges ) with 6 ,040 users and 3 ,706 movies in movielens website . // deepwalk adopts random walk and skip gram to learn vertex embeddings . // line preserves the first order and second order proximity among vertices in the graph .
b	44	[1]	deepwalk adopts random walk and skip gram to learn vertex embeddings . // line preserves the first order and second order proximity among vertices in the graph . // node2vec is a variant of deepwalk and designs a biased random walk to learn vertex embeddings .
b	44	[37]	line preserves the first order and second order proximity among vertices in the graph . // node2vec is a variant of deepwalk and designs a biased random walk to learn vertex embeddings . // struc2vec captures the structural identity of vertices in a graph .
h-	24	[367]	thus , distributing different nodes in different shards or servers often causes demandingly high communication cost among servers , and holds back speed up ratio . // although some limited progress is made on graph parallelization by subtly segmenting large scale graphs , the luck of these methods heavily depends on the topological characteristics of the underlying graphs . inapplicability of machine learning methods . // inapplicability of machine learning methods .
b	24	[6]	fig . 1 . // an example of network embedding on a karate network . images are extracted from deepwalk . // ( i .e . , the nodes ) are dependant to each other to some degree determined by e .
b	24	[68]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[30]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[77]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[19]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[48]	after embedding the karate club network into a two dimensional space , the similar nodes marked by the same color are close to each other in the embedding space , demonstrating that the network structure can be well modeled in the two dimensional embedding space . // network embedding , as a promising way of network representation , is capable of supporting subsequent network processing and analysis tasks such as node classification , , node clustering , network visualization , and link prediction , . // if this goal is fulfilled , the advantages of network embedding over traditional network representation methods are apparent , as shown in fig . 2 .
b	24	[368]	we will briefly review those methods in section 3 . // fu and ma present a more detailed survey . // in this paper , we focus on the recently proposed network embedding methods aiming to address the goal of network inference .
b	24	[1]	however , as mentioned before , directly conducting these tasks based on network topology has a series of problems , and thus poses a question that whether we can learn a network embedding space purely based on the network topology information , such that these tasks can be well supported in this low dimensional space . // motivated by this , attempts are proposed to preserve rich structural information into network embedding , from nodes and links to neighborhood structure , high order proximities of nodes , and community structures . // all these types of structural information have been demonstrated useful and necessary in various network analysis tasks .
b	24	[19]	however , as mentioned before , directly conducting these tasks based on network topology has a series of problems , and thus poses a question that whether we can learn a network embedding space purely based on the network topology information , such that these tasks can be well supported in this low dimensional space . // motivated by this , attempts are proposed to preserve rich structural information into network embedding , from nodes and links to neighborhood structure , high order proximities of nodes , and community structures . // all these types of structural information have been demonstrated useful and necessary in various network analysis tasks .
b	24	[30]	however , as mentioned before , directly conducting these tasks based on network topology has a series of problems , and thus poses a question that whether we can learn a network embedding space purely based on the network topology information , such that these tasks can be well supported in this low dimensional space . // motivated by this , attempts are proposed to preserve rich structural information into network embedding , from nodes and links to neighborhood structure , high order proximities of nodes , and community structures . // all these types of structural information have been demonstrated useful and necessary in various network analysis tasks .
ho	24	[369]	besides this structural information , network properties in the original network space are not ignorable in modeling the formation and evolution of networks . // to name a few , network transitivity ( i .e . triangle closure ) is the driving force of link formation in networks , and structural balance property plays an important role in the evolution of signed networks . preserving these properties in a network embedding space is , however , challenging due to the inhomogeneity between the network space and the embedding vector space . // some recent studies begin to look into this fig . 3 . an overview of different settings of network embedding . cui et al . : a survey on network embedding 835 problem and demonstrate the possibility of aligning these two spaces at the property level , .
b	24	[48]	to name a few , network transitivity ( i .e . triangle closure ) is the driving force of link formation in networks , and structural balance property plays an important role in the evolution of signed networks . preserving these properties in a network embedding space is , however , challenging due to the inhomogeneity between the network space and the embedding vector space . // some recent studies begin to look into this fig . 3 . an overview of different settings of network embedding . cui et al . : a survey on network embedding 835 problem and demonstrate the possibility of aligning these two spaces at the property level , . // network embedding with side information
b	24	[118]	to name a few , network transitivity ( i .e . triangle closure ) is the driving force of link formation in networks , and structural balance property plays an important role in the evolution of signed networks . preserving these properties in a network embedding space is , however , challenging due to the inhomogeneity between the network space and the embedding vector space . // some recent studies begin to look into this fig . 3 . an overview of different settings of network embedding . cui et al . : a survey on network embedding 835 problem and demonstrate the possibility of aligning these two spaces at the property level , . // network embedding with side information
b	24	[62]	network embedding with side information . // besides network topology , some types of networks are accompanied with rich side information , such as node content or labels in information networks , node and edge attributes in social networks , as well as node types in heterogeneous networks . // side information provides useful clues for characterizing relationships among network nodes , and thus is helpful in learning embedding vector spaces .
b	24	[109]	network embedding with side information . // besides network topology , some types of networks are accompanied with rich side information , such as node content or labels in information networks , node and edge attributes in social networks , as well as node types in heterogeneous networks . // side information provides useful clues for characterizing relationships among network nodes , and thus is helpful in learning embedding vector spaces .
b	24	[7]	in the cases where the network topology is relatively sparse , the importance of the side information as complementary information sources is even more substantial . methodologically , the main challenge is how to integrate and balance the topological and side information in network embedding . // some multimodal and multisource fusion techniques are explored in this line of research , . // advanced information preserving network embedding
b	24	[34]	in the cases where the network topology is relatively sparse , the importance of the side information as complementary information sources is even more substantial . methodologically , the main challenge is how to integrate and balance the topological and side information in network embedding . // some multimodal and multisource fusion techniques are explored in this line of research , . // advanced information preserving network embedding
b	24	[372]	directly designing a framework of representation learning for a particular target scenario is also known as an end to end solution , where highquality supervised information is exploited to learn the latent representation space from scratch . // end to end solutions have demonstrated their advantages in some fields , such as computer vision and natural language processing ( nlp ) . similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification . // some recent works demonstrate the feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction . in general , network structures and properties are the fundamental factors that need to be considered in network embedding .
b	24	[373]	directly designing a framework of representation learning for a particular target scenario is also known as an end to end solution , where highquality supervised information is exploited to learn the latent representation space from scratch . // end to end solutions have demonstrated their advantages in some fields , such as computer vision and natural language processing ( nlp ) . similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification . // some recent works demonstrate the feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction . in general , network structures and properties are the fundamental factors that need to be considered in network embedding .
b	24	[371]	similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification . // some recent works demonstrate the feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction . // in general , network structures and properties are the fundamental factors that need to be considered in network embedding .
b	24	[375]	similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification . // some recent works demonstrate the feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction . // in general , network structures and properties are the fundamental factors that need to be considered in network embedding .
b	24	[202]	similar ideas are also feasible for network applications . taking the network node classification problem as an example , if we have the labels of some network nodes , we can design a solution with network structure as input , node labels as supervised information , and embedding representation as latent middle layer , and the resulted network embedding is specific for node classification . // some recent works demonstrate the feasibility in applications such as cascading prediction , anomaly detection , network alignment and collaboration prediction . // in general , network structures and properties are the fundamental factors that need to be considered in network embedding .
b	24	[48]	in this sense , matrix factorization methods , with the same goal of learning lowrank space for the original matrix , can naturally be applied to solve this problem . // in the series of matrix factorization models , singular value decomposition ( svd ) is commonly used in network embedding due to its optimality for low rank approximation . // non negative matrix factorization is often used because of its advantages as an additive model .
h+	24	[36]	in the field of natural language processing , the word representation also suffers from similar drawbacks . // the development of word2vector significantly improves the effectiveness of word representation by transforming sparse , discrete and high dimensional vectors into dense , continuous and low dimensional vectors . // the intuition of word2vector is that a word vector should be able to reconstruct the vectors of its neighborhood words which are defined by co occurence rate .
b	24	[6]	by regarding a node as a word , we can regard a random path as a sentence , and the node neighborhood can be identified by co occurence rate as in word2vector . // some representative methods include deepwalk and node2vec . // deep neural networks
b	24	[37]	by regarding a node as a word , we can regard a random path as a sentence , and the node neighborhood can be identified by co occurence rate as in word2vector . // some representative methods include deepwalk and node2vec . // deep neural networks
b	24	[9]	the key challenges are how to make deep models fit network data , and how to impose network structure and property level constraints on deep models . // some representative methods , such as sdne , sdae , and sine , propose deep learning models for network embedding to address these challenges . // at the same time , deep neural networks are also well known for their advantages in providing end to end solutions .
b	24	[118]	the key challenges are how to make deep models fit network data , and how to impose network structure and property level constraints on deep models . // some representative methods , such as sdne , sdae , and sine , propose deep learning models for network embedding to address these challenges . // at the same time , deep neural networks are also well known for their advantages in providing end to end solutions .
b	24	[371]	therefore , in the problems where advanced information is available , it is natural to exploit deep models to come up with an end to end network embedding solution . // for instance , some deep model based end to end solutions are proposed for cascade prediction and network alignment . // the network embedding models are not limited to those mentioned in this subsection .
b	24	[376]	network embedding versus graph embedding . // the goal of graph embedding is similar as network embedding , that is , to embed a graph into a low dimensional vector space . // there is a rich literature in graph embedding .
b	24	[368]	there is a rich literature in graph embedding . // fu and ma provide a thorough review on the traditional graph embedding methods . // here we only present some representative and classical methods on graph embedding , aiming to demonstrate the critical differences between graph embedding and the current network embedding .
b	24	[3]	graph embedding methods are originally studied as dimensionreduction techniques . // a graph is usually constructed from a feature represented data set , like image data set . isomap first constructs a neighborhood graph g using connectivity algorithms such as k nearest neighbors ( knn ) , i .e . , connecting data entries i and j if i is one of the k nearest neighbors of j . // then based on g , the shortest path dg ij of entries i and j in g can be computed . consequently , for all the n data entries in the data set , we have the matrix of graph distances dg ¼ fdg ijg .
b	24	[5]	by optimizing the above function , the low dimensional representation matrix u , which preserves the neighborhood structure , can be obtained . // laplacian eigenmaps ( le ) also begins with constructing a graph using neighborhoods or k nearest neighbors . // then the heat kernel is utilized to choose the weight wij of nodes i and j in the graph .
b	24	[368]	( le ) also begins with constructing a graph using neighborhoods or k nearest neighbors . // these methods are extended in the rich literature of graph embedding by considering different characteristics of the constructed graphs . // major differences
b	24	[377]	// furthermore , the locality preserving projection ( lpp ) , a linear approximation of the nonlinear le , is proposed . // also , it introduces a transformation matrix a such that the representation ui of entry xi is ui ¼ at xi .
b	24	[6]	the commonly exploited network structures in network embedding include neighborhood structure , high order node proximity and network communities . // deepwalk is proposed for learning the representations of nodes in a network , which is able to preserve the neighbor structures of nodes . // deepwalk discovers that the distribution of nodes appearing in short random walks is similar to the distribution of words in natural language .
h+	24	[35]	where w is the window size , fðviþ is the current representation of vi and fviw ; : : : ; viþwgnvi is the local context nodes of vi . // finally , hierarchical soft max is used to efficiently infer the embeddings . // node2vec demonstrates that deepwalk is not expressive enough to capture the diversity of connectivity patterns in a network .
ho	24	[37]	finally , hierarchical soft max is used to efficiently infer the embeddings . // node2vec demonstrates that deepwalk is not expressive enough to capture the diversity of connectivity patterns in a network . // node2vec defines a flexible notion of a node ’ s network neighborhood and designs a second order random walk strategy to sample the neighborhood nodes , which can smoothly interpolate between breadth first sampling ( bfs ) and depth first sampling ( dfs ) .
b	24	[26]	by minimizing the kl divergence of the two distributions and the empirical distributions respectively , the representations of nodes that are able to preserve the first and second order proximities can be obtained . // considering that line only preserves the first order and second order proximities , grarep demonstrates that k step ( k > 2 ) proximities should also be captured when constructing the global representations of nodes . // given the adjacency matrix a , the k step probability transition matrix can be computed by ak ¼ a : : : a |fflffl { zfflffl } k , whose element ak ij refers to the transition probability pkðjjiþ from a current node i to a context node j and the transition consists of k steps .
b	24	[36]	given the adjacency matrix a , the k step probability transition matrix can be computed by ak ¼ |afflffl { : :z : afflffl } k , whose element ak ij refers to the transition probability pkðjjiþ from a current node i to a context node j and the transition consists of k steps . // moreover , motivated by the skip gram model , the k step loss function of node i is defined aswhere sðxþ¼ð1 þ exþ 1 , pkðv þ is the distribution over the nodes in the network and j0 is the node obtained from negative sampling . // furthermore , grarep reformulates the loss function as the matrix factorization problem , for each k step loss function , svd can be directly used to infer the representations of nodes .
b	24	[30]	by concentrating the representations learned from each function , the global representations can be obtained . // wang et al . propose a modularized nonnegative matrix factorization ( m nmf ) model for network embedding , which aims to preserve both the microscopic structure , i .e . , the first order and second order proximities of nodes , and the mesoscopic community structure . // to preserve the microscopic structure , they adopt the nmfmodel to factorize the pairwise node similarity matrix and learn the representations of nodes .
b	24	[378]	wang et al . propose a modularized nonnegative matrix factorization ( m nmf ) model for network embedding , which aims to preserve both the microscopic structure , i .e . , the first order and second order proximities of nodes , and the mesoscopic community structure . // to preserve the microscopic structure , they adopt the nmfmodel to factorize the pairwise node similarity matrix and learn the representations of nodes . // meanwhile , the community structure is detected by modularity maximization .
b	24	[379]	to preserve the microscopic structure , they adopt the nmfmodel to factorize the pairwise node similarity matrix and learn the representations of nodes . // meanwhile , the community structure is detected by modularity maximization . // then , based on the assumption that if the representation of a node is similar to that of a community , the node may have a high propensity to be in this community , they introduce an auxiliary community representation matrix to bridge the representations of nodes with the community structure .
b	24	[19]	the aforementioned methods mainly adopt the shallow models , consequently , the representation ability is limited . // sdne proposes a deep model for network embedding , so as to address the high non linearity , structure preserving , and sparsity issues . // the framework is shown in fig . 6 .
b	24	[9]	laplacian eigenmaps is adopted . by exploiting the firstorder and second order proximities jointly into the learning process , the representations of nodes can be finally obtained . // cao et al . propose a network embedding method to capture the weighted graph structure and represent nodes of non linear structures . // as shown in fig . 7 , instead of adopting the previous sampling strategy that needs to determine certain hyper parameters , they considers a random surfing model motivated by the pagerank model .
b	24	[160]	as shown in fig . 7 , instead of adopting the previous sampling strategy that needs to determine certain hyper parameters , they considers a random surfing model motivated by the pagerank model . based on this random surfing model , the representation of a node can be initiatively constructed by combining the weighted transition probability matrix . // after that , the ppmi matrix can be computed . // finally , the stacked denoisingautoencoders that partially corrupt the input data before taking the training step are applied to learn the latent representations .
b	24	[47]	after that , the ppmi matrix can be computed . // finally , the stacked denoisingautoencoders that partially corrupt the input data before taking the training step are applied to learn the latent representations . // in order to make a general framework on network embedding , chen et al . propose a network embedding framework that unifies some of the previous algorithms , such as le , deepwalk and node2vec .
b	24	[381]	therefore , the representations of these subgraphs can be learned and the similarities of different networks can be captured . // patchy san is proposed to learn the embedding for a whole graph based on convolutional neural network ( cnn ) , so as to deal with the whole graph related tasks . // in order to make the traditional cnn compatible with the network data , they elaborately design several network data preprocessing steps , such as node sequence selection and graph normalization . in this way , the network topology can be transformed to the formation for cnn .
b	24	[222]	therefore , the representations of these subgraphs can be learned and the similarities of different networks can be captured . // patchy san is proposed to learn the embedding for a whole graph based on convolutional neural network ( cnn ) , so as to deal with the whole graph related tasks . // in order to make the traditional cnn compatible with the network data , they elaborately design several network data preprocessing steps , such as node sequence selection and graph normalization . in this way , the network topology can be transformed to the formation for cnn .
b	24	[382]	specifically , most of the existing property preserving network embedding methods focus on network transitivity in all types of networks and the structural balance property in signed networks . // ou et al . aim to preserve the non transitivity property via latent similarity components . // the non transitivity property declares that , for nodes a , b and c in a network where ða ; bþ and ðb ; cþ are similar pairs , ða ; cþ may be a dissimilar pair .
b	24	[49]	asymmetric transitivity indicates that , if there is a directed edge from node i to node j and a directed edge from j to v , there is likely a directed edge from i to v , but not from v to i . // in order to measure this high order proximity , hope summarizes four measurements in a general formulation , that is , katz index , rooted pagerank , common neighbors , and adamic adar . // with the high order proximity , svd can be directly applied to obtain the low dimensional representations .
b	24	[70]	asymmetric transitivity indicates that , if there is a directed edge from node i to node j and a directed edge from j to v , there is likely a directed edge from i to v , but not from v to i . // in order to measure this high order proximity , hope summarizes four measurements in a general formulation , that is , katz index , rooted pagerank , and adamic adar . // with the high order proximity , svd can be directly applied to obtain the low dimensional representations .
b	24	[70]	asymmetric transitivity indicates that , if there is a directed edge from node i to node j and a directed edge from j to v , there is likely a directed edge from i to v , but not from v to i . // in order to measure this high order proximity , hope summarizes four measurements in a general formulation , that is , katz index , rooted pagerank , and adamic adar . // with the high order proximity , svd can be directly applied to obtain the low dimensional representations .
b	24	[260]	with the high order proximity , svd can be directly applied to obtain the low dimensional representations . // furthermore , the general formulation of high order proximity enables hope to transform the original svd problem into a generalized svd problem , such that the time complexity of hope is largely reduced , which means hope is scalable for large scale networks . // sine is proposed for signed network embedding , which considers both positive and negative edges in a network .
b	24	[118]	furthermore , the general formulation of high order proximity enables hope to transform the original svd problem into a generalized svd problem , such that the time complexity of hope is largely reduced , which means hope is scalable for large scale networks . // sine is proposed for signed network embedding , which considers both positive and negative edges in a network . // due to the negative edges , the social theories on signed network , such as structural balance theory , , are very different from the unsigned network .
b	24	[370]	sine is proposed for signed network embedding , which considers both positive and negative edges in a network . // due to the negative edges , the social theories on signed network , such as structural balance theory , , are very different from the unsigned network . // the structural balance theory demonstrates that users in a signed social network should be able to have their “ friends ” closer than their “ foes ” .
b	24	[62]	how to combine them with the network topology in network embedding arouses considerable research interests . // tu et al . propose a semi supervised network embedding algorithm , mmdw , by leveraging labeling information of nodes . mmdw is also based on the deepwalk derived matrix factorization . // mmdw is also based on the deepwalk derived matrix factorization .
b	24	[24]	mmdw is also based on the deepwalk derivedmatrix factorization . // mmdw adopts support vector machines ( svm ) and incorporates the label information to find an optimal classifying boundary . // by optimizing the max margin classifier of svm and matrix factorization based deepwalk simultaneously , the representations of nodes that have more discriminative ability can be learned .
b	24	[274]	by optimizing the max margin classifier of svm and matrix factorization based deepwalk simultaneously , the representations of nodes that have more discriminative ability can be learned . // le et al . propose a generative model for document network embedding , where the words associated with each documents and the relationships between documents are both considered . // for each node , they learn its low rank representation ui in a low dimensional vector space , which can reconstruct the network structure .
b	24	[7]	finally , in a unified generative process , the representations of nodes u can be learned . // besides network structures , yang et al . propose tadw that takes the rich information ( e .g . , text ) associated with nodes into account when they learn the low dimensional representations of nodes . // deepwalk is equivalent to factorizing the matrix m whose element mij ¼ log ð½eiða þ a2 þ : : : þ atþj=tþ , where a is the adjacency matrix , t denotes the t steps in a random walk and ei is a row vector where all entries are 0 except the i th entry is 1 .
b	24	[34]	deepwalk is equivalent to factorizing the matrix m whose element mij ¼ log ð½eiða þ a2 þ : : : þ atþj=tþ , where a is the adjacency matrix , t denotes the t steps in a random walk and ei is a row vector where all entries are 0 except the i th entry is 1 . // then , based on the deepwalk derived matrix factorization and motivated by the inductive matrix completion , they incorporate rich text information t into network embedding as follows : min w ; h km wthtk2 f þ 2 ðkwk2 f þ khk2 f þ : ( 12 ) finally , they concatenate the optimal w and ht as the representations of nodes . // tadw suffers from high computational cost and the node attributes just simply incorporated as unordered features lose the much semantic information .
b	24	[384]	tadw suffers from high computational cost and the node attributes just simply incorporated as unordered features lose the much semantic information . // sun et al . consider the content as a special kind of nodes , and give rise to an augmented network , as shown in fig . 9 . // with this augmented network , they are able to model the node node links and node content links in the latent vector space .
b	24	[24]	as a result , the learned representations is enhanced by network structure , node content , and node labels . // lane is also proposed to incorporate the label information into the attributed network embedding . // lane is mainly based on spectral techniques . lane adopts the cosine similarity to construct the corresponding affinity matrices of the node attributes , network structure , and labels .
b	24	[292]	lane is also proposed to incorporate the label information into the attributed network embedding . // lane is mainly based on spectral techniques . lane adopts the cosine similarity to construct the corresponding affinity matrices of the node attributes , network structure , and labels . // then , based on the corresponding laplacian matrices , lane is able to map the three different sources into different latent representations , respectively .
b	24	[385]	the learned representations of nodes are able to capture the structure proximities as well as the correlations in the label informed attributed network . // huang et al . pay more attentions on the scalability of attributed network embedding . // the proposed method , named aane , is based on the decomposition of attribute affinity matrix and the penalty of embedding difference between linked nodes .
b	24	[298]	howto unify the heterogeneous types of nodes and links in network embedding is also an interesting and challenging problem . // yann et al . propose a heterogeneous social network embedding algorithm for classifying nodes . // they learn the representations of all types of nodes in a common vector space , and perform the inference in this space .
b	24	[109]	a stochastic gradient descent method is used here to learn the representations of nodes in a heterogeneous network for classifying . // chang et al . propose a deep embedding algorithm for heterogeneous networks , whose nodes have various types . // the main goal of the heterogeneous network embedding is to learn the representations of nodes with different types such that the heterogeneous network structure can be well preserved .
b	24	[110]	in the common space , the similarities between data from different modalities can be directly measured , so that if there is an edge in the original heterogeneous network , the pair of data has similar representations . // huang and mamoulis propose a meta path similarity preserving heterogeneous information network embedding algorithm . // to model a particular relationship , a meta path is a sequence of object types with edge types in between . they develop a fast dynamic programming approach to calculate the truncated meta path based proximities , whose time complexity is linear to the size of the network .
b	24	[1]	they develop a fast dynamic programming approach to calculate the truncated meta path based proximities , whose time complexity is linear to the size of the network . // they adopt a similar strategy as line to preserve the proximity in the low dimensional space . // xu et al . propose a network embedding method for coupled heterogeneous network . the coupled heterogeneous network consists of two different but related homogeneous networks .
b	24	[387]	they adopt a similar strategy as line to preserve the proximity in the low dimensional space . // xu et al . propose a network embedding method for coupled heterogeneous network . the coupled heterogeneous network consists of two different but related homogeneous networks . // the coupled heterogeneous network consists of two different but related homogeneous networks .
b	24	[388]	information diffusion . // information diffusion is an ubiquitous phenomenon on the web , especially in social networks . // many real applications , such as marketing , public opinion formation , epidemics , are related to information diffusion .
b	24	[371]	by minimizing the eq . ( 18 ) and reformulating it as a ranking problem , the optimal representations u of nodes can be obtained . // the cascade prediction problem here is defined as predicting the increment of cascade size after a given time interval . // li et al . argue that the previous work on cascade prediction all depends on the bag of hand crafting features to represent the cascade and network structures . instead , they present an end to end deep learning model to solve this problem using the idea of network embedding , as illustrated in fig . 12 .
b	24	[371]	the cascade prediction problem here is defined as predicting the increment of cascade size after a given time interval . // li et al . argue that the previous work on cascade prediction all depends on the bag of hand crafting features to represent the cascade and network structures . // instead , they present an end to end deep learning model to solve this problem using the idea of network embedding , as illustrated in fig . 12 .
b	24	[6]	instead , they present an end to end deep learning model to solve this problem using the idea of network embedding , as illustrated in fig . // similar to deepwalk , they perform a random walk over a cascade graph to sample a set of paths . // then the gated recurrent unite ( gru ) , a specific type of recurrent neural network , is applied to these paths and learn the embeddings for these paths . the attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph .
b	24	[390]	similar to deepwalk , they perform a random walk over a cascade graph to sample a set of paths . // then the gated recurrent unite ( gru ) , a specific type of recurrent neural network , is applied to these paths and learn the embeddings for these paths . // the attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph .
b	24	[391]	the attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph . // once the representation of this cascade is known , a multi layer perceptron can be adopted to output the final predicted size of this cascade . // the whole procedure is able to learn the representation of cascade graph in an end to end manner .
b	24	[392]	anomaly detection . // anomaly detection has been widely investigated in previous work . // anomaly detection in networks aims to infer the structural inconsistencies , which means the anomalous nodes that connect to various diverse influential communities , , such as the red node in fig . 13 .
b	24	[393]	anomaly detection has been widely investigated in previous work . // anomaly detection in networks aims to infer the structural inconsistencies , which means the anomalous nodes that connect to various diverse influential communities , , such as the red node in fig . 13 . // hu et al . propose a network embedding based method for anomaly detection .
b	24	[374]	anomaly detection in networks aims to infer the structural inconsistencies , which means the anomalous nodes that connect to various diverse influential communities , , such as the red node in fig . 13 . // hu et al . propose a network embedding based method for anomaly detection . // in particular , in the proposed model , the kth element uki in the embedding ui of node i represents the correlation between node i and community k .
b	24	[375]	the goal of network alignment is to establish the correspondence between the nodes from two networks . // man et al . propose a network embedding algorithm to predict the anchor links across social networks . // the same users who are shared by different social networks naturally form the anchor links , and these links bridge the different networks .
b	24	[394]	first , man et al . extend the original sparse networks gs and gt to the denser networks . // the basic idea is that given a pair of users with anchor links , if they have a connection in one network , so do their counterparts in the other network , in this way , more links will be added to the original networks . // for a pair of nodes i and j whose representations are ui and uj , respectively , by combining the negative sampling strategy , they use the following function to preserve the structures of gs and gt in a vector space : log sðuti ujþ þxkk¼1 evk/pnðvþ½log ð1  sðuti ukþþ ; ( 20 ) where sðxþ ¼ 1=ð1 þ expðxþþ .
b	24	[391]	the loss function is defined as : kfðui ; uþ  ujkf : // the mapping function can be linear or non linear via multilayer perceptron ( mlp ) . // by optimizing eq . ( 20 ) and eq . ( 21 ) simultaneously , the representations that can preserve the network structure and respect the observed anchor links can be learned .
b	24	[33]	one instance of this data set can be found at http : socialcomputing .asu .edu/datasets/blogcatalog3 . // flickr . this is a network of the contacts between users of the photo sharing websites flickr . // one instance of the network can be downloaded at http : socialcomputing .asu .edu/datasets/flickr .
b	24	[263]	one instance of the network can be found at http : socialcomputing .asu .edu/datasets/youtube2 . // twitter . this is a network between users on a social news website twitter . // one instance of the network can be downloaded at http : socialcomputing .asu .edu/datasets/twitter .
b	24	[159]	citation networks . // dblp . this network represents the citation relationships between authors and papers . // one instance of the data set can be found at http : arnetminer .org/citation .
b	24	[395]	one instance of the data set can be found at http : arnetminer .org/citation . // cora . this network represents the citation relationships between scientific publications . // besides the link information , each publication is also associated with a word vector indicating the absence/presence of the corresponding words from the dictionary .
b	24	[84]	one instance of the data set can be downloadedat https : linqs .soe .ucsc .edu/node/236 . // arxiv , . this is the collaboration network constructed from the arxiv website . // one instance of the data set can be found at http : snap .stanford .edu/data/ca astroph .html .
b	24	[184]	one instance of the data set can be downloadedat https : linqs .soe .ucsc .edu/node/236 . // arxiv , . this is the collaboration network constructed from the arxiv website . // one instance of the data set can be found at http : snap .stanford .edu/data/ca astroph .html .
b	24	[6]	given some nodes with known labels in a network , the node classification problem is to classify the rest nodes into different classes . // node classification is one of most primary applications for network embedding , . // essentially , node classification based on network embedding for can be divided into three steps .
b	24	[61]	then , the nodes with known labels are used as the training set . // last , a classifier , such as liblinear , is learned from the training set . // using the trained classifier , we can infer the labels of the rest nodes .
b	24	[33]	using the trained classifier , we can infer the labels of the rest nodes . // the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) .
b	24	[33]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[56]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[159]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[395]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[182]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[86]	the popularly used evaluation metrics for multi label classification problem include micro f1 and macro f1 . // the multi label classification application has been successfully tested on four categories of data sets , namely social networks ( blogcatalog , flickr , and youtube ) , citation networks ( dblp , cora , and citeseer ) , language networks ( wikipedia ) , and biological networks ( ppi ) . // specifically , a social network usually is a communication network among users on online platforms .
b	24	[6]	specifically , a social network usually is a communication network among users on online platforms . // deepwalk , grarep , sdne , node2vec , and lane conduct classification on blogcatalog to evaluate the performance . // also , the classification performance on flickr has been assessed in , , , .
b	24	[19]	specifically , a social network usually is a communication network among users on online platforms . // deepwalk , grarep , sdne , node2vec , and lane conduct classification on blogcatalog to evaluate the performance . // also , the classification performance on flickr has been assessed in , , , .
b	24	[37]	specifically , a social network usually is a communication network among users on online platforms . // deepwalk , grarep , sdne , node2vec , and lane conduct classification on blogcatalog to evaluate the performance . // also , the classification performance on flickr has been assessed in , , , .
b	24	[24]	specifically , a social network usually is a communication network among users on online platforms . // deepwalk , grarep , sdne , node2vec , and lane conduct classification on blogcatalog to evaluate the performance . // also , the classification performance on flickr has been assessed in , , , .
h+	24	[19]	also , the classification performance on flickr has been assessed in , , , . // some studies , , apply their algorithms to the youtube network , which also achieves promising classification results . // a citation network usually represents the citation relationships between authors or between papers .
h+	24	[1]	also , the classification performance on flickr has been assessed in , , , . // some studies , , apply their algorithms to the youtube network , which also achieves promising classification results . // a citation network usually represents the citation relationships between authors or between papers .
b	24	[1]	a citation network usually represents the citation relationships between authors or between papers . // for example , , use the dblp network to test the classification performance . // cora is used in , . citeseer is used in , , .
h+	24	[396]	the protein protein interactions ( ppi ) is used in . // based on nuswide , a heterogeneous network extracted from flickr , chang et al . validated the superior classification performance of network embedding on heterogeneous networks . // to summarize , network embedding algorithms havebeen widely used on various networks and have been well demonstrated their effectiveness on node classification .
h+	24	[109]	the protein protein interactions ( ppi ) is used in . // based on nuswide , a heterogeneous network extracted from flickr , chang et al . validated the superior classification performance of network embedding on heterogeneous networks . // to summarize , network embedding algorithms havebeen widely used on various networks and have been well demonstrated their effectiveness on node classification .
b	24	[70]	link prediction . // link prediction , as one of the most fundamental problems on network analysis , has received a considerable amount of attention , . // it aims to estimate the likelihood of the existence of an edge between two nodes based on observed network structure .
b	24	[397]	link prediction , as one of the most fundamental problems on network analysis , has received a considerable amount of attention , . // it aims to estimate the likelihood of the existence of an edge between two nodes based on observed network structure . // since network embedding algorithms are able to learn the vector based features for each node , the similarity between nodes can be easily estimated , for example , by the inner product or the cosine similarity .
b	24	[84]	generally , precision @ k and mean average precision ( map ) are used to evaluate the link prediction performance . // the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , and dblp1 ) , social networks ( sn tweibo2 , sn twitter , facebook , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv .
b	24	[184]	generally , precision @ k and mean average precision ( map ) are used to evaluate the link prediction performance . // the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv .
b	24	[184]	generally , precision @ k and mean average precision ( map ) are used to evaluate the link prediction performance . // the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv .
b	24	[86]	generally , precision @ k and mean average precision ( map ) are used to evaluate the link prediction performance . // the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , and dblp1 ) , social networks ( sn tweibo2 , sn twitter , facebook , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv .
b	24	[19]	the popularly used real networks for the link prediction task can be divided into three categories : citation networks ( arxiv , and dblp1 ) , social networks ( sn tweibo2 , sn twitter , facebook , epinions , and slashdot ) , and biological networks ( ppi ) . // specifically , and test the effectiveness on arxiv . // hope applies network embedding to link prediction on two directed networks sntwitter , which is a subnetwork of twitter6 , and sn tweibo , which is a subnetwork of the social network in tencent weibo7 .
b	24	[48]	specifically , and test the effectiveness on arxiv . // hope applies network embedding to link prediction on two directed networks sntwitter , which is a subnetwork of twitter6 , and sn tweibo , which is a subnetwork of the social network in tencent weibo . // node2vec tests the performance of link prediction on a social network facebook and a biological network ppi .
b	24	[37]	hope applies network embedding to link prediction on two directed networks sntwitter , which is a subnetwork of twitter6 , and sn tweibo , which is a subnetwork of the social network in tencent weibo . // node2vec tests the performance of link prediction on a social network facebook and a biological network ppi . // eoe uses dblp to demonstrate the effectiveness on citation networks . based on two social networks , epinions and slashdot , sine shows the superior performance of signed network embedding on link prediction .
h+	24	[387]	node2vec tests the performance of link prediction on a social network facebook and a biological network ppi . // eoe uses dblp to demonstrate the effectiveness on citation networks . based on two social networks , epinions and slashdot , sine shows the superior performance of signed network embedding on link prediction . // to sum up , network embedding is able to capture inherent network structures , and thus naturally it is suitable for link prediction applications .
b	24	[398]	many evaluation criteria have been proposed for clustering evaluation . // accuracy ( ac ) and normalized mutual information ( nmi ) are frequently used to assess the clustering performance on graphs and networks . // the node clustering performance is tested on three types of networks : social networks ( e .g . , facebook and yelp ) , citation networks ( e .g . , dblp ) , and document networks ( e .g . , 20 newsgroup ) .
b	24	[83]	accuracy ( ac ) and normalized mutual information ( nmi ) are frequently used to assess the clustering performance on graphs and networks . // the node clustering performance is tested on three types of networks : social networks ( e .g . , facebook and yelp ) , citation networks ( e .g . , dblp ) , and document networks ( e .g . , 20 newsgroup ) . // in particular , extracts a social network from a social blogging site . it uses the tf idf features extracted from the blogs as the features of blog users and the “ following ” behaviors to construct the linkages .
b	24	[110]	accuracy ( ac ) and normalized mutual information ( nmi ) are frequently used to assess the clustering performance on graphs and networks . // the node clustering performance is tested on three types of networks : social networks ( e .g . , facebook and yelp ) , citation networks ( e .g . , dblp ) , and document networks ( e .g . , 20 newsgroup ) . // in particular , extracts a social network from a social blogging site . it uses the tf idf features extracted from the blogs as the features of blog users and the “ following ” behaviors to construct the linkages .
b	24	[176]	accuracy ( ac ) and normalized mutual information ( nmi ) are frequently used to assess the clustering performance on graphs and networks . // the node clustering performance is tested on three types of networks : social networks ( e .g . , facebook and yelp ) , citation networks ( e .g . , dblp ) , and document networks ( e .g . , 20 newsgroup ) . // in particular , extracts a social network from a social blogging site . it uses the tf idf features extracted from the blogs as the features of blog users and the “ following ” behaviors to construct the linkages .
b	24	[109]	the node clustering performance is tested on three types of networks : social networks ( e .g . , facebook and yelp ) , citation networks ( e .g . , dblp ) , and document networks ( e .g . , 20 newsgroup ) . // in particular , extracts a social network from a social blogging site . it uses the tf idf features extracted from the blogs as the features of blog users and the “ following ” behaviors to construct the linkages . // it usesthe tf idf features extracted from the blogs as the features of blog users and the “ following ” behaviors to construct the linkages .
b	24	[30]	it successfully applies network embedding to thenode clustering task . // uses the facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering . // is applied to more social networks including movie , a network extracted from yago that contains knowledge about movies , yelp , a network extracted from yelp that is about reviews given to restaurants , and game , extracted from freebase that is related to video games .
b	24	[399]	uses the facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering . // is applied to more social networks including movie , a network extracted from yago that contains knowledge about movies , yelp , a network extracted from yelp that is about reviews given to restaurants , and game , extracted from freebase that is related to video games . // tests the node clustering performance on a document network , 20 newsgroup network , which consists of documents .
b	24	[400]	uses the facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering . // is applied to more social networks including movie , a network extracted from yago that contains knowledge about movies , yelp , a network extracted from yelp that is about reviews given to restaurants , and game , extracted from freebase that is related to video games . // tests the node clustering performance on a document network , 20 newsgroup network , which consists of documents .
b	24	[9]	is applied to more social networks including movie , a network extracted from yago that contains knowledge about movies , yelp , a network extracted from yelp that is about reviews given to restaurants , and game , extracted from freebase that is related to video games . // tests the node clustering performance on a document network , 20 newsgroup network , which consists of documents . // the node clustering performance on citation networks is tested by clustering authors in dblp .
b	24	[1]	as can be seen , network embedding preserves the intrinsic structure of the network , where similar nodes are closer to each other than dissimilar nodes in the low dimensional space . // also , line , grarep , and eoe are applied to a citation network dblp and generate meaningful layout of the network . // pan et al . show the visualization of another citation network citeseer m10 consisting of scientific publications from ten distinct research areas .
b	24	[26]	as can be seen , network embedding preserves the intrinsic structure of the network , where similar nodes are closer to each other than dissimilar nodes in the low dimensional space . // also , line , grarep , and eoe are applied to a citation network dblp and generate meaningful layout of the network . // pan et al . show the visualization of another citation network citeseer m10 consisting of scientific publications from ten distinct research areas .
b	24	[387]	as can be seen , network embedding preserves the intrinsic structure of the network , where similar nodes are closer to each other than dissimilar nodes in the low dimensional space . // also , line , grarep , and eoe are applied to a citation network dblp and generate meaningful layout of the network . // pan et al . show the visualization of another citation network citeseer m10 consisting of scientific publications from ten distinct research areas .
b	24	[401]	also , line , grarep , and eoe are applied to a citation network dblp and generate meaningful layout of the network . // pan et al . show the visualization of another citation network citeseer m10 consisting of scientific publications from ten distinct research areas . // open source software .
b	24	[399]	besides , in a heterogeneous information network , to measure the relevance of two objects , the meta path , a sequence of object types with edge types in between , has been widely used . // however , meta structure , which is essentially a directed acyclic graph of object and edge types , provides a higher order structure constraint . // this suggests a huge potential direction for improving heterogeneous information network embedding .
b	24	[402]	in general , the principle of network embedding can be extended to other target spaces . // for example , recently some studies assume that the underlying structure of a network is in the hyperbolic space . // under this assumption , heterogeneous degree distributions and strong clustering emerge naturally , as they are the simple reflections of the negative curvature and metric property of the underlying hyperbolic geometry .
h-	105	[37, 6, 1]	the basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high dimensional information about a node ’ s neighborhood into a dense vector embedding . // these node embeddings can then be fed to downstream machine learning systems and aid in tasks such as node classification , clustering , and link prediction . // however , previous works have focused on embedding nodes from a single fixed graph , and many real world applications require embeddings to be quickly generated for unseen nodes , or entirely new ( sub ) graphs .
h-	105	[26, 37, 290, 6, 1, 19, 30, 403]	most existing approaches to generating node embeddings are inherently transductive . // the majority of these approaches directly optimize the embeddings for each node using matrix factorization based objectives , and do not naturally generalize to unseen data , since they make predictions on nodes in a single , fixed graph . // these approaches can be modified to operate in an inductive setting ( e .g . , ) , but these modifications tend to be computationally expensive , requiring additional rounds of gradient descent before new predictions can be made .
h-	105	[6]	the majority of these approaches directly optimize the embeddings for each node using matrix factorization based objectives , and do not naturally generalize to unseen data , since they make predictions on nodes in a single , fixed graph . // these approaches can be modified to operate in an inductive setting ( e .g . , ) , but these modifications tend to be computationally expensive , requiring additional rounds of gradient descent before new predictions can be made . // there are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology .
h-	105	[266, 404]	there are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology . // so far , graph convolutional networks ( gcns ) have only been applied in the transductive setting with fixed graphs . // in this work we both extend gcns to the task of inductive unsupervised learning and propose a framework that generalizes the gcn approach to use trainable aggregation functions ( beyond simple convolutions ) .
h-	105	[6]	we use two evolving document graphs based on citation data and reddit post data ( predicting paper and post categories , respectively ) , and a multigraph generalization experiment based on a dataset of protein protein interactions ( predicting protein functions ) . // using these benchmarks , we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin : across domains , our supervised approach improves classification f1 scores by an average of 51 % compared to using node features alone and graphsage consistently outperforms a strong , transductive baseline , despite this baseline taking ∼100× longer to run on unseen nodes . // we also show that the new aggregator architectures we propose provide significant gains ( 7 .4 % on average ) compared to an aggregator inspired by graph convolutional networks .
h-	105	[266]	using these benchmarks , we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin : across domains , our supervised approach improves classification f1 scores by an average of 51 % compared to using node features alone and graphsage consistently outperforms a strong , transductive baseline , despite this baseline taking ∼100× longer to run on unseen nodes . // we also show that the new aggregator architectures we propose provide significant gains ( 7 .4 % on average ) compared to an aggregator inspired by graph convolutional networks . // lastly , we probe the expressive capability of our approach and show , through theoretical analysis , that graphsage is capable of learning structural information about a node ’ s role in a graph , despite the fact that it is inherently based on features ( section 5 ) .
b	105	[290]	there are a number of recent node embedding approaches that learn low dimensional embeddings using random walk statistics and matrix factorization based learning objectives . // these methods also bear close relationships to more classic approaches to spectral clustering , multi dimensional scaling , as well as the pagerank algorithm . // in addition , for many of these approaches ( e .g . , ) the objective function is invariant to orthogonal transformations of the embeddings , which means that the embedding space does not naturally generalize between graphs and can drift during re training .
b	105	[405]	there are a number of recent node embedding approaches that learn low dimensional embeddings using random walk statistics and matrix factorization based learning objectives . // these methods also bear close relationships to more classic approaches to spectral clustering , multi dimensional scaling , as well as the pagerank algorithm . // in addition , for many of these approaches ( e .g . , ) the objective function is invariant to orthogonal transformations of the embeddings , which means that the embedding space does not naturally generalize between graphs and can drift during re training .
b	105	[158]	there are a number of recent node embedding approaches that learn low dimensional embeddings using random walk statistics and matrix factorization based learning objectives . // these methods also bear close relationships to more classic approaches to spectral clustering , multi dimensional scaling , as well as the pagerank algorithm . // in addition , for many of these approaches ( e .g . , ) the objective function is invariant to orthogonal transformations of the embeddings , which means that the embedding space does not naturally generalize between graphs and can drift during re training .
b	105	[66]	in addition , for many of these approaches ( e .g . , ) the objective function is invariant to orthogonal transformations of the embeddings , which means that the embedding space does not naturally generalize between graphs and can drift during re training . // one notable exception to this trend is the planetoid i algorithm introduced by yang et al . , which is an inductive , embeddingbased approach to semi supervised learning . // however , planetoid i does not use any graph structural information during inference ; instead , it uses the graph structure as a form of regularization during training .
b	105	[220]	beyond node embedding approaches , there is a rich literature on supervised learning over graph structured data . // this includes a wide variety of kernel based approaches , where feature vectors for graphs are derived from various graph kernels ( see and references therein ) . // there are also a number of recent neural network approaches to supervised learning over graph structures . our approach is conceptually inspired by a number of these algorithms .
h-	105	[406, 407, 175, 408]	this includes a wide variety of kernel based approaches , where feature vectors for graphs are derived from various graph kernels ( see and references therein ) . // there are also a number of recent neural network approaches to supervised learning over graph structures . // however , whereas these previous approaches attempt to classify entire graphs ( or subgraphs ) , the focus of this work is generating useful representations for individual nodes .
h-	105	[409, 410, 411, 381]	in recent years , several convolutional neural network architectures for learning over graphs have been proposed ( e .g . , ) . // the majority of these methods do not scale to large graphs or are designed for whole graph classification ( or both ) . // however , our approach is closely related to the graph convolutional network ( gcn ) , introduced by kipf et al . .
b	105	[266, 404]	the majority of these methods do not scale to large graphs or are designed for whole graph classification ( or both ) . // however , our approach is closely related to the graph convolutional network ( gcn ) , introduced by kipf et al . . // the original gcn algorithm is designed for semi supervised learning in a transductive setting , and the exact algorithm requires that the full graph laplacian is known during training . a simple variant of our algorithm can be viewed as an extension of the gcn framework to the inductive setting , a point which we revisit in section 3 .3 .
b	105	[266]	however , our approach is closely related to the graph convolutional network ( gcn ) , introduced by kipf et al . . // the original gcn algorithm is designed for semi supervised learning in a transductive setting , and the exact algorithm requires that the full graph laplacian is known during training . // a simple variant of our algorithm can be viewed as an extension of the gcn framework to the inductive setting , a point which we revisit in section 3 .3 .
b	105	[6]	experimental set up . // to contextualize the empirical results on our inductive benchmarks , we compare against four baselines : a random classifer , a logistic regression feature based classifier ( that ignores graph structure ) , the deepwalk algorithm as a representative factorization based approach , and a concatenation of the raw features and deepwalk embeddings . // we also compare four variants of graphsage that use the different aggregator functions ( section 3 .3 ) .
ho	105	[266]	we also compare four variants of graphsage that use the different aggregator functions ( section 3 .3 ) . // since , the “ convolutional ” variant of graphsage is an extended , inductive version of kipf et al ’ s semi supervised gcn , we term this variant graphsage gcn . // we test unsupervised variants of graphsage trained according to the loss in equation ( 1 ) , as well as supervised variants that are trained directly on classification cross entropy loss .
ho	105	[412]	in the multi graph setting , we can not apply deepwalk , since the embedding spaces generated by running the deepwalk algorithm on different disjoint graphs can be arbitrarily rotated with respect to each other ( appendix ) . // all models were implemented in tensorflow with the adam optimizer ( except deepwalk , which performed better with the vanilla gradient descent optimizer ) . // we designed our experiments with the goals of ( i ) verifying the improvement of graphsage over the baseline approaches ( i .e . , raw features and deepwalk ) and ( ii ) providing a rigorous comparison of the different graphsage aggregator architectures .
b	584	[159]	the fourth point makes our prediction more positive . it turns out to be a paper called seismic published in kdd ’ 15 . // open academic datasets ( such as aminer and microsoft academic graph ) make hundreds of millions of successful examples ( i .e . , published papers ) available , and the above clues we assess the success are lying beneath the huge data . // in this work , we aim at proposing a data mining approach to predict the success of a plan towards a goal .
b	584	[414]	the fourth point makes our prediction more positive . it turns out to be a paper called seismic published in kdd ’ 15 . // open academic datasets ( such as aminer and microsoft academic graph ) make hundreds of millions of successful examples ( i .e . , published papers ) available , and the above clues we assess the success are lying beneath the huge data . // in this work , we aim at proposing a data mining approach to predict the success of a plan towards a goal .
h-	584	[111, 415, 37, 6, 1, 416]	existing approaches formulate the behavior data as an information network and the papers or components or venues as nodes . // then network embedding methods have been widely applied for learning the node ’ s feature vectors by preserving the pair wise node proximity , neighborhood , or global structure . // unfortunately , the outcomes of behavior , or say , the underlying patterns of the components forming an ( un ) successful plan towards the goal , were not preserved in the embeddings , and thus not effective for predicting the success ( as shown in the experiments ) .
b	584	[1]	we compare our tube method against the state of the art network embedding methods , and a very recent method for success prediction . // line : it preserves both local and global structure of the network by conducting edge sampling . // node2vec : this method uses biased random walks to capture the homophily and structural equivalence properties of network .
b	584	[37]	line : it preserves both local and global structure of the network by conducting edge sampling . // node2vec : this method uses biased random walks to capture the homophily and structural equivalence properties of network . // we also considered the deepwalk model here . since deepwalk can be seen as a special case of node2vec that uses truncated uniform random walks , we only report the better performance among them in experiments .
b	584	[6]	node2vec : this method uses biased random walks to capture the homophily and structural equivalence properties of network . // we also considered the deepwalk model here . // since deepwalk can be seen as a special case of node2vec that uses truncated uniform random walks , we only report the better performance among them in experiments .
b	584	[415]	verse : it is able to preserves the distributions of a selected vertex to vertex similarity measure in homogeneous network such as personalized pagerank , simrank and etc . // bine : this method aims at learning the representations of vertices in a bipartite network . it conducts biased random walks to preserve the long tail distribution of vertices . // metapath2vec : it is the state of the art method for heterogeneous network embedding .
h+	584	[111]	bine : this method aims at learning the representations of vertices in a bipartite network . it conducts biased random walks to preserve the long tail distribution of vertices . // metapath2vec : it is the state of the art method for heterogeneous network embedding . it samples meta path based random walks for learning . // we use the advanced version metapath2vec++ which also conducts heterogeneous negative sampling in network .
b	584	[417]	we use the advanced version metapath2vec++ which also conducts heterogeneous negative sampling in network . // learnsuc : it is a recent work that formulates behavior as a multi type itemset instead of a node in networks , and learns item embeddings collectively for success prediction . // parameter settings .
b	584	[1]	learning representations of network data , or network embedding , aims at learning the low dimensional vector representations of nodes in network while preserving the pair wise proximities . // line first introduced the notion of 1st and 2nd order proximity to preserve both local and global structure of the network by conducting edge sampling . // deepwalk used truncated uniform random walks to explore the neighborhood of a node and expected nodes with higher proximity yield similar representations .
b	584	[6]	line first introduced the notion of 1st and 2nd order proximity to preserve both local and global structure of the network by conducting edge sampling . // deepwalk used truncated uniform random walks to explore the neighborhood of a node and expected nodes with higher proximity yield similar representations . // node2vec extended it to use biased random walks to capture the homophily and structural equivalence properties of network .
b	584	[37]	deepwalk used truncated uniform random walks to explore the neighborhood of a node and expected nodes with higher proximity yield similar representations . // node2vec extended it to use biased random walks to capture the homophily and structural equivalence properties of network . // verse was designed to preserve the distributions of a selected pair wise similarity measure in network such as personalized pagerank or simrank .
b	584	[415]	verse was designed to preserve the distributions of a selected pair wise similarity measure in network such as personalized pagerank or simrank . // besides methods focusing on homogeneous networks , bine was able to learn the representations of vertices in a bipartite network by conducting biased random walks to preserve the long tail distribution of vertices . // for heterogeneous network embedding , metapath2vec was based on meta path based random walks and the heterogeneous research track paper kdd ’ 19 , august 4–8 , 2019 , anchorage , ak , usa 1689 skip gram model .
b	584	[111]	besides methods focusing on homogeneous networks , bine was able to learn the representations of vertices in a bipartite network by conducting biased random walks to preserve the long tail distribution of vertices . // for heterogeneous network embedding , metapath2vec was based on meta path based random walks and the heterogeneous skip gram model . // here is another line of methods that utilized deep models .
h-	584	[418, 112, 19, 44]	for heterogeneous network embedding , metapath2vec was based on meta path based random walks and the heterogeneous skip gram model . // there is another line of methods that utilized deep models . // however , none of these existing methods has an effective formulation of behavior and does not preserve the outcome information of behaviors .
b	584	[189, 419, 420, 421]	quantifying success . // there exists a wide line of research on quantifying success in various fields and areas . // wang et al . proposed a model for predicting long term scientific impact by collapsing the citation histories of papers from different journals and disciplines into a single curve to model the citation dynamics of individual papers .
b	584	[422]	there exists a wide line of research on quantifying success in various fields and areas . // wang et al . proposed a model for predicting long term scientific impact by collapsing the citation histories of papers from different journals and disciplines into a single curve to model the citation dynamics of individual papers . // to predict the success in art , fraiberger et al . used a markov model to predict the career trajectory of individual artists and documents the strong path and history dependence of valuation in art .
b	584	[423]	wang et al . proposed a model for predicting long term scientific impact by collapsing the citation histories of papers from different journals and disciplines into a single curve to model the citation dynamics of individual papers . // to predict the success in art , fraiberger et al . used a markov model to predict the career trajectory of individual artists and documents the strong path and history dependence of valuation in art . // yucesoy et al . proposed a model aiming at predicting which books will become bestsellers .
b	584	[424]	yucesoy et al . proposed a model aiming at predicting which books will become bestsellers . // and , deville et al . studied quantifying the career choices such as changing institutions affecting scientific outcomes . // however , the definition of success , or outcome , in previous studies varies greatly from paper citations number to art value .
b	415	[425, 426]	it has been widely used in many applications such as recommender systems , search engines , question answering systems and so on . // for example , in search engines , queries and webpages form a bipartite network , where the edges can indicate users ’ click behaviors that provide valuable relevance signal ; in another application of recommender systems , users and items form a bipartite network , where the edges can encode users ’ rating behaviors that contain rich collaborative filtering patterns . // to perform predictive analytics on network data , it is crucial to first obtain the representations ( i .e . , feature vectors ) for vertices .
b	415	[427]	it has been widely used in many applications such as recommender systems , search engines , question answering systems and so on . // for example , in search engines , queries and webpages form a bipartite network , where the edges can indicate users ’ click behaviors that provide valuable relevance signal ; in another application of recommender systems , users and items form a bipartite network , where the edges can encode users ’ rating behaviors that contain rich collaborative filtering patterns . // to perform predictive analytics on network data , it is crucial to first obtain the representations ( i .e . , feature vectors ) for vertices .
b	415	[36]	to date , existing works have primarily focused on embedding homogeneous networks where vertices are of the same type . // following the pioneering work of deepwalk , these methods typically apply a two step solution : first performing random walks on the network to obtain a “ corpus ” of vertices , and then applying word embedding methods such as word2vec to obtain the embeddings for vertices . // despite effectiveness and prevalence , we argue that these methods can be suboptimal for embedding bipartite networks due to two primary reasons .
h-	415	[428]	although edges exist between vertices of different types only , there are essentially implicit relations between vertices of the same type . // for example , in the useritem bipartite network built for recommendation , there exists an implicit relation between users which can indicate their preference in consuming the same item ; and importantly , it is recently reported that modeling such implicit relations can improve the recommendation performance . // however , existing network embedding methods modeled the explicit relation ( i .e . , observed edges ) only and ignored the underlying implicit relations .
h-	415	[111]	however , existing network embedding methods modeled the explicit relation ( i .e . , observed edges ) only and ignored the underlying implicit relations . // while a recent work by dong et al . proposed metapath2vec++ for embedding heterogeneous networks which can also be applied to bipartite networks , we argue that a key limitation is that it treats the explicit and implicit relations as contributing equally to the learning . // in real world bipartite networks , the explicit and implicit relations typically carry different semantics .
b	415	[429]	our work is related to vertex representation learning methods on homogeneous networks , which can be categorized into two types : matrix factorization ( mf ) based and neural network based methods . // mf based methods are either linear or nonlinear in learning vertex embeddings . // the former employs the linear transformations to embed network vertices into a low dimensional embedding space , such as singular value decomposition ( svd ) and multiple dimensional scaling ( mds ) .
b	415	[430]	our work is related to vertex representation learning methods on homogeneous networks , which can be categorized into two types : matrix factorization ( mf ) based and neural network based methods . // mf based methods are either linear or nonlinear in learning vertex embeddings . // the former employs the linear transformations to embed network vertices into a low dimensional embedding space , such as singular value decomposition ( svd ) and multiple dimensional scaling ( mds ) .
b	415	[429]	mf based methods are either linear or nonlinear in learning vertex embeddings . // the former employs the linear transformations to embed network vertices into a low dimensional embedding space , such as singular value decomposition ( svd ) and multiple dimensional scaling ( mds ) . // however , the latter maps network vertices into a low dimensional latent space by utilizing the nonlinear transformations , e .g . , kernel pca , spectral embedding , marginal fisher analysis ( mfa ) , and manifold learning approaches include lle and isomap .
h-	415	[6]	neural network based methods are the state of art vertex representation learning techniques . // the pioneer work deepwalk and node2vec extend the idea of skip gram to model homogeneous network , which is convert to a corpus of vertex sequences by performing truncated random walks . // however , they may not be effective to preserve both explicit and implicit relations of the network .
h-	415	[37]	neural network based methods are the state of art vertex representation learning techniques . // the pioneer work deepwalk and node2vec extend the idea of skip gram to model homogeneous network , which is convert to a corpus of vertex sequences by performing truncated random walks . // however , they may not be effective to preserve both explicit and implicit relations of the network .
b	415	[36]	neural network based methods are the state of art vertex representation learning techniques . // the pioneer work deepwalk and node2vec extend the idea of skip gram to model homogeneous network , which is convert to a corpus of vertex sequences by performing truncated random walks . // however , they may not be effective to preserve both explicit and implicit relations of the network .
b	415	[19]	there are some follow up works exploiting both 1st order and 2nd order proximities between vertices to embed homogeneous networks . // specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others .
b	415	[26]	there are some follow up works exploiting both 1st order and 2nd order proximities between vertices to embed homogeneous networks . // specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others .
h-	415	[103, 29]	specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others . // it is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks , for which there is only one type of vertices .
h-	415	[431]	specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others . // it is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks , for which there is only one type of vertices .
h-	415	[432]	specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others . // it is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks , for which there is only one type of vertices .
h-	415	[15]	specifically , line learns two separated embeddings for 1st order and 2nd order relations ; sdne incorporates both 1st order and 2nd order proximities to preserve the network structure ; and grarep further extends the method to capture higher order proximities . // besides capturing high order proximities , there are several proposals to incorporate side information into vertex embedding learning , such as vertex labels , community information , textual content , user profiles , location information , among others . // it is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks , for which there is only one type of vertices .
h-	415	[109]	thus , these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network . // metapath2vec++ , hne and eoe are representative vertex embedding methods for heterogeneous networks . // although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks , they are not tailored for learning on bipartite networks .
h-	415	[387]	thus , these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network . // metapath2vec++ , hne and eoe are representative vertex embedding methods for heterogeneous networks . // although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks , they are not tailored for learning on bipartite networks .
b	415	[215]	as a ubiquitous data structure , bipartite networks have been mined for many applications , among which vertex ranking is an active research problem . // for example , hits learns to rank vertices by capturing some semantic relations within a bipartite network . // co hits incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network .
b	415	[427]	co hits incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network . // birank ranks vertices by taking into account both the network structure and prior knowledge . // distributed vertex representation is an alternative way to leverage signals from bipartite network .
h-	415	[433]	and a typical implementation of lfm is based on matrix factorization . // recent advances utilize deep learning methods to learn vertex embeddings on the user item network for recommendation . // it is worth pointing out that these methods are tailored for the recommendation task , rather than for learning informative vertex embeddings .
b	415	[428, 192]	it is worth pointing out that these methods are tailored for the recommendation task , rather than for learning informative vertex embeddings . // moreover , they model the explicit relations in bipartite network only , which can be improved by incorporating implicit relations as shown in . // problem formulation
h-	415	[37, 6]	constructing corpus of vertex sequences . // it is a common way to convert a network into a corpus of vertex sequences by performing random walks on the network , which has been used in some homogeneous network embedding methods . // however , directly performing random walks on a bipartite network could fail , since there is no stationary distribution of random walks on bipartite networks due to the periodicity issue .
ho	415	[434]	it is a common way to convert a network into a corpus of vertex sequences by performing random walks on the network , which has been used in some homogeneous network embedding methods . // however , directly performing random walks on a bipartite network could fail , since there is no stationary distribution of random walks on bipartite networks due to the periodicity issue . // to address this issue , we consider performing random walks on two homogeneous networks that contain the 2nd order proximity between vertices of the same type .
ho	415	[425]	to address this issue , we consider performing random walks on two homogeneous networks that contain the 2nd order proximity between vertices of the same type . // following the idea of co hits , we define the 2nd order proximity between two vertices as : where wij is the weight of edge eij . // hence , we can use the |u | × |u | matrix wu = and the |v | × |v | matrix wv = to represent the two induced homogeneous networks , respectively .
ho	415	[36]	nevertheless , optimizing the objectives is non trivial , since each evaluation of the softmax function needs to traverse all vertices of a side , which is very time costing . // to reduce the learning complexity , we employ the idea of negative sampling . // negative sampling .
b	415	[435]	negative sampling . // the idea of negative sampling is to approximate the costly denominator term of softmax with some sampled negative instances . // then the learning can be performed by optimizing a point wise classification loss .
h-	415	[36]	for a center vertex ui , high quality negatives should be the vertices that are dissimilar from ui . // towards this goal , some heuristics have been applied , such as sampling from popularity biased non uniform distribution . // here we propose a more grounded sampling method that caters the network data .
ho	415	[436]	given a center vertex , we then randomly choose the negative samples from the buckets that are different from the bucket contained the center vertex . // through this way , we can obtain high quality and diverse negative samples , since lsh can guarantee that dissimilar vertices are located in different buckets in a probabilistic way . // let n ns s ( ui ) denote the ns negative samples for a center vertex ui in sequence s ∈ du , we can then approximate the conditional probability
b	415	[6]	for each method , we use the released implementations of the authors for our experiments . // deepwalk : as a homogeneous network embedding method , deepwalk performs uniform random walks to get a corpus of vertex sequences . // then the word2vec is applied on the corpus to learn vertex embeddings .
b	415	[1]	then the word2vec is applied on the corpus to learn vertex embeddings . // line : this approach optimizes both the 1st order and 2nd order proximities in a homogeneous network . // we use the line ( 1st+2nd ) method which has shown the best results in their paper .
b	415	[111]	the hyper parameters p and q are set to 0 .5 which has empirically shown good results . // metapath2vec++ : this is the state of the art method for embedding heterogeneous networks . // the meta path scheme chosen in our experiments are “ iui ” ( item user item ) and “ iui ” + “ uiu ” ( user item user ) , and we only report the best result between them .
b	415	[437]	we compare with several competitive methods that are designed for the top k item recommendation task . // bpr : this method optimizes the matrix factorization ( mf ) model with a pairwise ranking aware objective . // this method has been widely used in recommendation literature as a highly competitive baseline .
b	415	[433]	bpr : this method optimizes the matrix factorization ( mf ) model with a pairwise ranking aware objective . // this method has been widely used in recommendation literature as a highly competitive baseline . // rankals : this method also optimizes the mf model for the ranking task , by towards a different pairwise regressionbased loss .
b	415	[439]	rankals : this method also optimizes the mf model for the ranking task , by towards a different pairwise regressionbased loss . // fismauc : distinct to mf , factored item similarity model ( fism ) is an item based collaborative filtering method . // we employ the auc based objective to optimize fism for the top k task .
h-	415	[440]	this is due to the factors that . // one index proposed in only emphasizes one kind of network topological structure , rather than the global structure . // the neural network based methods predict the links in a data dependent supervised manner , which is more advantageous .
b	54	[441]	network ( or graph ) is a group of interconnected nodes and contains a wealth of information on the relationships between every pair of nodes . // the analysis of graph is required in almost every field , for instance , online social network , biological research , credit rating and so on . // birds of a feather flock together and people always have certain characteristics in common with their friends surrounded by .
b	54	[443]	network ( or graph ) is a group of interconnected nodes and contains a wealth of information on the relationships between every pair of nodes . // the analysis of graph is required in almost every field , for instance , online social network , biological research , credit rating and so on . // birds of a feather flock together and people always have certain characteristics in common with their friends surrounded by .
b	54	[3]	unsupervised network embedding algorithms try to preserve the local relationships of each node in the graph . // isomap , lle and laplacian eigenmaps are three classic dimensionality reduction and data representation algorithms . // finding the k nearest neighbors is the key step of these three algorithms .
b	54	[4]	unsupervised network embedding algorithms try to preserve the local relationships of each node in the graph . // isomap , lle and laplacian eigenmaps are three classic dimensionality reduction and data representation algorithms . // finding the k nearest neighbors is the key step of these three algorithms .
b	54	[6]	classic algorithms can hardly tackle real networks due to the huge computational complexity of eigen decomposition . // deepwalk first introduces deep learning techniques word2vec to the network embedding task . // the authors pioneered the similarity between random walks and natural language .
h+	54	[26, 37, 1, 20]	the authors pioneered the similarity between random walks and natural language . // later , more work is presented to improve deepwalk by extending the definition of neighborhood and capturing neighborhood information from different levels of scope , namely first order proximity , second order proximity and higher order proximity . // using random walks to capture the local structure is truly a neat idea , which makes it possible to build representations of big networks .
b	54	[37]	the strategy focuses on neighbors . // on the other hand , figure 1b shows another strategy , that is dividing nodes by their structural similarity , namely structural equivalence defined in . // there are three groups altogether , core nodes , peripheral nodes and hub nodes under this criteria for network partition . these different starting points induce different definitions of similar nodes : ( 1 ) densely connected nodes or ( 2 ) nodes with similar network positions .
b	54	[37]	the most sensible approach is to make a balance between these two strategies . // structural equivalence is being discussed not only in social role mining task , but in recent network embedding algorithms as well . // we support the motivation that both kinds of similar nodes should be taken into consideration .
b	54	[56, 55]	we refer the reader to for more comprehensive details . // traditional methods mainly focus on dimension reduction . // by the techniques of matrix factorization , traditional methods project the adjacency matrix to a low dimension space .
b	54	[6]	the difference between the stateof art neural network based algorithms lies in the neighborhood sampling strategies . // deepwalk uses depth first search in order to sample the neighborhood of the target node . the depth is set to 2 by default . // grarep also uses dfs but the depth is larger
b	54	[1]	grarep also uses dfs but the depth is larger . // line uses breadth first search as well as depth first search . the number of step are all constrained below two . // the number of step are all constrained below two .
h-	54	[37]	the number of step are all constrained below two . // node2vec also uses both two kinds of search methods and finds the balance point by semi supervised learning . // it selects the optimal balance point with grid search method , which is time consuming .
b	54	[19]	obviously , none of these algorithms take structural similarity into consideration . // sdne is not based on word2vec framework but deep autoencoder instead . // it has the same goal as line does , considering both first order proximity and second order proximity .
h-	54	[37]	. it has the same goal as line does , considering both first order proximity and second order proximity . // grarep and node2vec discuss local structure and structural equivalence . // however , it is questionable whether the structural similarity is actually used during the learning process .
b	54	[446]	computing similarities between structured objects ( network ) is a hot topic in recent years . // methods are mainly based on graphlets , subtree patterns and random walks . // in the network embedding task , we focus on nodes rather than the whole network structure , which means that we need a metric describing the structural characteristics of a single node .
b	54	[447]	in the network embedding task , we focus on nodes rather than the whole network structure , which means that we need a metric describing the structural characteristics of a single node . // this topic has been given much consideration in the real fields such as biology and social science . // we first introduce graphlets and graphlet based network distance measures .
ho	54	[449]	too many features , however , will result in over fitting and not all orbits contribute equally to a certain task . // therefore , we evaluate the importance of different orbits on the node classification task with the help of random forest . // random forest consists of a number of decision trees .
ho	54	[450]	too many features , however , will result in over fitting and not all orbits contribute equally to a certain task . // therefore , we evaluate the importance of different orbits on the node classification task with the help of random forest . // random forest consists of a number of decision trees .
ho	54	[450]	note that features with more categories and with less correlated features are considered to be more important when using the impurity based ranking . // we use random forest implemented by sklearn and calculate the orbit importance io as stated before . the experiments are repeated on several datasets . // the experiments are repeated on several datasets .
ho	54	[37]	experiments . // in this section we first visualize a small network using deepwalk , node2vec and our proposed algorithm separately , which demonstrates the arguments in section 3 visually . // moreover , we show the performance of different network embedding algorithms on multi label classification task .
b	54	[6]	nodes in this network have various structural positions , making it easier to analyze different sampling strategies thoroughly . // we compare our algorithm with deepwalk and node2vec . // both of them are classic network embedding algorithm with random walk based sampling strategy .
b	54	[37]	nodes in this network have various structural positions , making it easier to analyze different sampling strategies thoroughly . // we compare our algorithm with deepwalk and node2vec . // both of them are classic network embedding algorithm with random walk based sampling strategy .
b	54	[451]	the network contains 10 ,312 nodes , 333 ,983 edges and 39 labels . // protein protein interactions is a subgraph of the ppi network for homo sapiens . // labels stand for the protein biological states .
b	54	[182]	the network contains 3 ,890 nodes , 76 ,584 edges and 50 labels . // pos is a co occurrence network of words appearing in the first million bytes of the wikipedia dump . // a part of speech ( pos ) is a category of words which have similar grammatical properties .
b	54	[55]	we choose a classic method aiming at dimension reduction and three representative neural network embedding algorithms as baselines . // spectral clustering is based on matrix factorization and aims at minimizing normalized cut . // we set d = 500 , the same setting in .
b	54	[1]	// line defines a loss function based on 1 hop and 2 hop relational information . // it learns d/2 dimensions of the node vector by these two parts of information respectively and combines them directly as the final output .
ho	54	[1]	in a supervised learning task , it finds the weighting of dimensions based on training data and achieves a better performance . // in our experiments , we try the unsupervised mode of line . // node2vec simulates biased random walks over the underlying network . it uses parameter p and q to balance the bfs strategy with dfs strategy .
ho	54	[1]	in a supervised learning task , it finds the weighting of dimensions based on training data and achieves a better performance . // in our experiments , we try the unsupervised mode of line . // node2vec simulates biased random walks over the underlying network . it uses parameter p and q to balance the bfs strategy with dfs strategy .
ho	54	[37]	node2vec is a semi supervised algorithm and it need 10 % labeled data of the network to decide the value of p and q . // we use the value of p and q as showed in the authors ’ paper . // the settings of our algorithm sns is as follows : in the preprocessing step , k = 5 , s = 1 , o = 14 , r = 9 .
h+	54	[448]	in the learning step , we use the same parameters as deepwalk does and c = 5 . // in the pre processing step , we use orca , a very efficient algorithm for graphlet enumeration . // as reported in the paper , on a desktop computer ( intel core 2 , 2 .67 ghz ) , orca only takes 2 .5s to count the four node graphlets of a network with 25 ,368 nodes and 75 ,004 edges .
h+	54	[448]	in the pre processing step , we use orca , a very efficient algorithm for graphlet enumeration . // as reported in the paper , on a desktop computer ( intel core 2 , 2 .67 ghz ) , orca only takes 2 .5s to count the four node graphlets of a network with 25 ,368 nodes and 75 ,004 edges . // sns spends more time than deepwalk does , as more parameters are involved in the learning process .
ho	54	[450]	this task uses the exact same datasets and experimental procedure as presented in . // the node vectors are the input to a one vsrest logistic regression implemented by sklearn . // we sample a portion of the labeled nodes as training data and use the rest nodes as test data .
h-	54	[26, 52]	utilizing global information of the network is a difficult field and we have to acknowledge that the computational cost of sns is huge when s is a large value . // some related work suffers from the same drawback . // a possible way to improve the performance might be designing heuristics and limiting the search scope .
ho	585	[452, 453, 454]	here we address the problem of structure learning on graphs by developing graphwave . // building upon techniques from graph signal processing , our approach learns a multidimensional structural embedding for each node based on the diffusion of a spectral graph wavelet centered at the node . // intuitively , each node propagates a unit of energy over the graph and characterizes its neighboring topology based on the response of the network to this probe .
b	585	[456, 169]	these methods generate an exhaustive listing of each node ’ s local topological properties ( e .g . , node degree , number of triangles it participates in , number of k cliques , its pagerank score ) before computing node similarities based on such heuristic representations . // a notable example of such approaches is rolx , a matrix factorization based method which aims to recover a softclustering of nodes into a predetermined number of k distinct roles using recursive feature extraction . // similarly , struc2vec uses a heuristic to construct a multilayered graph based on topological metrics and simulates random walks on the graph to capture structural information . in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters .
b	585	[171]	these methods generate an exhaustive listing of each node ’ s local topological properties ( e .g . , node degree , number of triangles it participates in , number of k cliques , its pagerank score ) before computing node similarities based on such heuristic representations . // a notable example of such approaches is rolx , a matrix factorization based method which aims to recover a softclustering of nodes into a predetermined number of k distinct roles using recursive feature extraction . // similarly , struc2vec uses a heuristic to construct a multilayered graph based on topological metrics and simulates random walks on the graph to capture structural information . in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters .
h-	585	[52]	a notable example of such approaches is rolx , a matrix factorization based method which aims to recover a softclustering of nodes into a predetermined number of k distinct roles using recursive feature extraction . // similarly , struc2vec uses a heuristic to construct a multilayered graph based on topological metrics and simulates random walks on the graph to capture structural information . // in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters .
h-	585	[410]	in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters . // recent neural representation learning methods ( structure2vec , neural fingerprints , graph convolutional networks ( gcns ) , message passing networks , etc . ) are a related line of research . // however , these graph embedding methods do not apply in our setting , since they solve a ( supervised ) graph classification task and/or embed entire graphs while we embed individual nodes .
h-	585	[105, 266]	in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters . // recent neural representation learning methods ( structure2vec , neural fingerprints , graph convolutional networks ( gcns ) , message passing networks , etc . ) are a related line of research . // however , these graph embedding methods do not apply in our setting , since they solve a ( supervised ) graph classification task and/or embed entire graphs while we embed individual nodes .
h-	585	[457]	in contrast , our approach does not rely on heuristics ( we mathematically prove its efficacy ) and does not require explicit manual feature engineering or hand tuning of parameters . // recent neural representation learning methods ( structure2vec , neural fingerprints , graph convolutional networks ( gcns ) , message passing networks , etc . ) are a related line of research . // however , these graph embedding methods do not apply in our setting , since they solve a ( supervised ) graph classification task and/or embed entire graphs while we embed individual nodes .
b	585	[458, 459, 460, 461]	however , these graph embedding methods do not apply in our setting , since they solve a ( supervised ) graph classification task and/or embed entire graphs while we embed individual nodes . // another line of related work are graph diffusion kernels which have been utilized for various graph modeling purposes . // however , to the best of our knowledge , our paper is the first to apply graph diffusion kernels for determining structural roles in graphs .
h-	585	[462, 463, 464]	however , to the best of our knowledge , our paper is the first to apply graph diffusion kernels for determining structural roles in graphs . // kernels have been shown to efficiently capture geometrical properties and have been successfully used for shape detection in the image processing community . // however , in contrast to shape matching problems , graphwave considers these kernels as probability distributions over real world graphs .
ho	585	[453]	we consider the problem of learning , for every node ai , a structural embedding representing ai ’ s position in a continuous multidimensional space of structural roles . // we frame this as an unsupervised learning problem based on spectral graph wavelets and develop an approach called graphwave that provides mathematical guarantees on the optimality of learned structural embeddings . // spectral graph wavelets .
b	585	[465]	let дs be a filter kernel with scaling parameter s . // in this paper , we use the heat kernel дs ( λ ) = e −λs , but our results apply to any scaling wavelet . // for now , we assume thats is given ; we develop a method for selecting an appropriate value of s in section 4 .
b	585	[453, 454]	for now , we assume thats is given ; we develop a method for selecting an appropriate value of s in section 4 . // graph signal processing defines the spectral graph wavelet associated with дs as the signal resulting from the modulation in the spectral domain of a dirac signal centered around node a . // the spectral graph wavelet ψa is given by an n dimensional vector : ψa = u diag ( дs ( λ1 ) , . . . , дs ( λn ) ) u t δa , ( 1 ) where δa = 1 ( a ) is the one hot vector for node a .
b	585	[454]	the m th wavelet coefficient of this column vector is thus given by ψma = pn l=1 дs ( λl ) umlual . // in spectral graph wavelets , the kernel дs modulates the eigenspectrum such that the resulting signal is typically localized on the graph and in the spectral domain . // spectral graph wavelets are based on an analogy between temporal frequencies of a signal and the laplacian ’ s eigenvalues .
b	585	[169]	baseline methods . // we evaluate the performance of graphwave1 against two state of the art baselines for learning structural embeddings : struc2vec , a method which discovers structural embeddings at different scales through a sequence of walks on a multilayered graph , and rolx , a method based on non negative matrix factorization of a node feature matrix ( number of neighbors , triangles , etc . ) that describes each node based on this given set of latent features . // while in , the authors develop a method for automatically selecting the number of roles in rolx , we use rolx as an oracle estimator , providing it with the correct number of classes .
ho	585	[169]	we evaluate the performance of graphwave1 against two state of the art baselines for learning structural embeddings : struc2vec , a method which discovers structural embeddings at different scales through a sequence of walks on a multilayered graph , and rolx , a method based on non negative matrix factorization of a node feature matrix ( number of neighbors , triangles , etc . ) that describes each node based on this given set of latent features . // while in , the authors develop a method for automatically selecting the number of roles in rolx , we use rolx as an oracle estimator , providing it with the correct number of classes . // we note that graphwave and struc2vec learn embeddings on a continuous spectrum instead of into discrete classes ( and thus they do not require this parameter ) .
h-	585	[406]	for all baselines , we use the default parameter values in the available solvers , and for graphwave , we use the multiscale version ( section 4 ) , set d = 50 and use evenly spaced sampling points ti in range . // we again note that graph embedding methods ( structure2vec , neural fingerprints , gcns , etc . ) do not apply in these settings , since they embed entire graphs while we embed individual nodes . // barbell graph
h-	585	[105, 266]	for all baselines , we use the default parameter values in the available solvers , and for graphwave , we use the multiscale version ( section 4 ) , set d = 50 and use evenly spaced sampling points ti in range . // we again note that graph embedding methods ( structure2vec , neural fingerprints , gcns , etc . ) do not apply in these settings , since they embed entire graphs while we embed individual nodes . // barbell graph
ho	585	[52]	mirrored karate network . // we first consider a mirrored karate network , using the same experimental setup as in . // the mirrored karate network is created by taking two copies of zachary ’ s karate network and adding a given number of random edges , each edge connecting mirrored nodes from different copies representing the same individual in the karate club ( i .e . , mirrored edges ) .
b	585	[466]	data and setup . // nodes represent enron employees and edges correspond to email communication between the employees . // an employee has one of seven functions in the company ( e .g . , ceo , president , manager ) .
b	19	[467]	an effective way is to embed networks into a low dimensional space , i .e . learn vector representations for each vertex , with the goal of reconstructing the network in the learned embedding space . // as a result , mining information in networks , such as information retrieval , classification , and clustering , can be directly conducted in the low dimensional space . // learning network representations faces the following great challenges .
b	19	[149]	an effective way is to embed networks into a low dimensional space , i .e . learn vector representations for each vertex , with the goal of reconstructing the network in the learned embedding space . // as a result , mining information in networks , such as information retrieval , classification , and clustering , can be directly conducted in the low dimensional space . // learning network representations faces the following great challenges .
b	19	[290]	an effective way is to embed networks into a low dimensional space , i .e . learn vector representations for each vertex , with the goal of reconstructing the network in the learned embedding space . // as a result , mining information in networks , such as information retrieval , classification , and clustering , can be directly conducted in the low dimensional space . // learning network representations faces the following great challenges .
ho	19	[293]	to support applications analyzing networks , network embedding is required to preserve the network structure . // however , the underlying structure of the network is very complex . // the similarity of vertexes is dependent on both the local and global network structure .
ho	19	[6]	therefore , how to simultaneously preserve the local and global structure is a tough problem . // sparsity : many real world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance . // in the past decades , many network embedding methods have been proposed , which adopted shallow models , such as isomap , laplacian eigenmaps ( le ) and line .
h-	19	[3]	sparsity : many real world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance . // in the past decades , many network embedding methods have been proposed , which adopted shallow models , such as isomap , laplacian eigenmaps ( le ) and line . // however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure .
h-	19	[1]	sparsity : many real world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance . // in the past decades , many network embedding methods have been proposed , which adopted shallow models , such as isomap , laplacian eigenmaps ( le ) and line . // however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure .
h-	19	[469]	in the past decades , many network embedding methods have been proposed , which adopted shallow models , such as isomap , laplacian eigenmaps ( le ) and line . // however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure . // although some methods adopt kernel techniques , as stated , kernel methods are also shallow models and can not capture the highly non linear structure well .
ho	19	[176]	in the past decades , many network embedding methods have been proposed , which adopted shallow models , such as isomap , laplacian eigenmaps ( le ) and line . // however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure . // although some methods adopt kernel techniques , as stated , kernel methods are also shallow models and can not capture the highly non linear structure well .
ho	19	[470]	however , due to the limited representation ability of shallow models , it is difficult for them to capture the highly nonlinear network structure . // although some methods adopt kernel techniques , as stated , kernel methods are also shallow models and can not capture the highly non linear structure well . // in order to capture the highly non linear structure well , in this paper we propose a new deep model to learn vertex representations for networks .
h+	19	[469]	in order to capture the highly non linear structure well , in this paper we propose a new deep model to learn vertex representations for networks . // this is motivated by the recent success of deep learning , which has been demonstrated to have a powerful representation ability to learn complex structures of the data and has achieved substantial success in dealing with images , text and audio data . // in particular , in our proposed model we design a multilayer architecture which consists of multiple non linear functions .
h+	19	[149]	in order to capture the highly non linear structure well , in this paper we propose a new deep model to learn vertex representations for networks . // this is motivated by the recent success of deep learning , which has been demonstrated to have a powerful representation ability to learn complex structures of the data and has achieved substantial success in dealing with images , text and audio data . // in particular , in our proposed model we design a multilayer architecture which consists of multiple non linear functions .
h+	19	[472]	in order to capture the highly non linear structure well , in this paper we propose a new deep model to learn vertex representations for networks . // this is motivated by the recent success of deep learning , which has been demonstrated to have a powerful representation ability to learn complex structures of the data and has achieved substantial success in dealing with images , text and audio data . // in particular , in our proposed model we design a multilayer architecture which consists of multiple non linear functions .
ho	19	[1]	the composition of multiple layers of non linear functions can map the data into a highly non linear latent space , thereby being able to capture the highly non linear network structure . // in order to address the structure preserving and sparsity problems in the deep model , we further propose to exploit the first order and second order proximity jointly into the learning process . // the first order proximity is the local pairwise similarity only between the vertexes linked by edges , which characterizes the local network structure .
b	19	[124, 473]	deep neural network . // representation learning has long been an important problem of machine learning and many works aim at learning representations for samples . // recent advances in deep neural networks have witnessed that they have powerful representations abilities and can generate very useful representations for many types of data .
b	19	[149]	recent advances in deep neural networks have witnessed that they have powerful representations abilities and can generate very useful representations for many types of data . // for example , proposed a seven layer convolutional neural network to generate image representations for classification . // proposed a multimodal deep model to learn image text unified representations to achieve cross modality retrieval task .
b	19	[474]	for example , proposed a seven layer convolutional neural network to generate image representations for classification . // proposed a multimodal deep model to learn image text unified representations to achieve cross modality retrieval task . // however , to the best of our knowledge , there have been few deep learning works handling networks , especially learning network representations .
b	19	[475]	however , to the best of our knowledge , there have been few deep learning works handling networks , especially learning network representations . // in , restricted boltzmann machines were adopted to do collaborative filtering . // adopted deep autoencoder to do graph clustering .
h-	19	[109]	adopted deep autoencoder to do graph clustering . // proposed a heterogeneous deep model to do heterogeneous data embedding . // we differ from these works in two aspects .
b	19	[4]	network embedding . // some earlier works like local linear embedding ( lle ) , isomap first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations . // more recently , designed two loss functions attempting to capture the local and global network structure respectively .
b	19	[3]	network embedding . // some earlier works like local linear embedding ( lle ) , isomap first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations . // more recently , designed two loss functions attempting to capture the local and global network structure respectively .
h-	19	[26]	more recently , designed two loss functions attempting to capture the local and global network structure respectively . // furthermore , extended the work to utilize high order information . // despite the success of these network embedding approaches , they all adopt shallow models .
h-	19	[6]	obviously , it is sub optimal than simultaneously modeling them in a unified architecture to capture both the local and global network structure . // deepwalk combined random walk and skip gram to learn network representations . // although empirically effective , it lacks a clear objective function to articulate how to preserve the network structure .
ho	19	[476, 477]	intuitively , the second order proximity assumes that if two vertexes share many common neighbors , they tend to be similar . // such an assumption has been proved reasonable in many fields . // for example , in linguistics words will be similar if they are always surrounded by similar contexts .
ho	19	[477]	for example , in linguistics words will be similar if they are always surrounded by similar contexts . // people will be friends if they have many common friends . // the second order proximity has been demonstrated to be a good metric to define the similarity of a pair of vertexes , even if they are not linked by an edge , and thus can highly enrich the relationship of vertexes .
h+	19	[70]	people will be friends if they have many common friends . // the second order proximity has been demonstrated to be a good metric to define the similarity of a pair of vertexes , even if they are not linked by an edge , and thus can highly enrich the relationship of vertexes . // therefore , by introducing the second order proximity , it is able to characterize the global network structure and alleviate the sparsity problem .
ho	19	[46]	therefore , si describes the neighborhood structure of the vertex vi and s provides the information of the neighborhood structure of each vertex . // with s , we extend the traditional deep autoencoder to preserve the second order proximity . // for the consideration of being self contained , we briefly review the key idea of deep autoencoder .
ho	19	[148]	note that due to the high nonlinearity of the model , it suffers from many local optimal in the parameter space . // therefore , in order to find a good region of parameter space , we use deep belief network to pretrain the parameters at first , which has been demonstrated as an essential initialization of parameters for deep learning in literature . // the full algorithm is presented in alg . 1 .
b	19	[33]	the detailed descriptions are listed as follows . // blogcatalog and youtube : they are social network of online users . // each user is labelled by at least one category .
b	19	[33]	the detailed descriptions are listed as follows . // blogcatalog and youtube : they are social network of online users . // each user is labelled by at least one category .
b	19	[84]	therefore , they can be evaluated on the multi label classification task . // arxiv gr qc : it is a paper collaboration network which covers papers in the field of general relativity and quantum cosmology from arxiv . // in this network , the vertex represents an author and the edge indicates that the authors have coauthored a scientific paper in arxiv .
b	19	[6]	common neighbor directly predicts the links over the networks , which has been demonstrated to be an effective method to perform link prediction . // deepwalk : it adopts random walk and skip gram model to generate network representations . // line : it defines loss functions to preserve the first order or second order proximity separately . after optimizing the loss functions , it concatenates these representations .
b	19	[1]	deepwalk : it adopts random walk and skip gram model to generate network representations . // line : it defines loss functions to preserve the first order or second order proximity separately . // after optimizing the loss functions , it concatenates these representations .
b	19	[444]	it also directly concatenates the representations of first order and high order . // laplacian eigenmaps ( le ) : it generates network representations by factorizing the laplacian matrix of the adjacency matrix . // it only exploits the first order proximity to preserve the network structure .
b	19	[70]	it only exploits the first order proximity to preserve the network structure . // common neighbor : it only uses the number of common neighbors to measure the similarity between vertexes . // it is used as the baseline only in the task of link prediction .
b	19	[479]	multi label classification . // classification is a so important task among many applications that the related algorithm and theories have been investigated by many works . // therefore , we evaluate the effectiveness of different network representations through a multilabel classification task in this experiment .
h+	19	[70]	therefore , this task can show the performance of predictability of different network embedding methods . // in addition , we add common neighbor in this task because it has been proved as an effective method to do link prediction . // for the first experiment , we randomly hide 15 precentage of existing links ( about 4000 links ) and use the precision @ k as the evaluation metric of predicting the hidden links .
ho	19	[78]	therefore , we visualize the learned representations of the 20 newsgroup network . // we use the low dimensional network representations learned by different network embedding methods as the input to the visualization tool t sne . // as a result , each newsgroup document is mapped as a two dimensional vector .
ho	19	[26]	the visualization figure is shown in figure 7 . // besides the visualization figure , similar to we use the kullback leibler divergence as a quantitative evaluation metric . // the lower the kl divergence , the better the performance .
b	382	[46]	// with the explosive growth of data , similarity search is becoming increasingly important for a wide range of large scale applications , including image retrieval , document search , and recommendation systems . // due to the simplicity and efficiency , hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search .
b	382	[482, 483, 484]	// with the explosive growth of data , similarity search is becoming increasingly important for a wide range of large scale applications , including image retrieval , document search , and recommendation systems . // due to the simplicity and efficiency , hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search .
h+	382	[485, 486, 487, 488]	due to the simplicity and efficiency , hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search . // in particular , recent hashing methods often employ machine learning techniques to leverage supervised information like pairwise constraints to design more efficient hash codes to search semantically similar entities . // the key for the learning based hashing methods is to approximate the semantic similarity between entities in the learned hamming space .
b	382	[246]	non transitive similarity learning . // although most of existing methods assume that similarity are all transitive , non transitive similarity have been studied in different fields // in metric learning , researchers realize the limitation of metric space on capturing the nontransitive similarity , and propose some effective non metric learning algorithms for non transitive similarity .
h+	382	[489, 490]	although most of existing methods assume that similarity are all transitive , non transitive similarity have been studied in different fields // in metric learning , researchers realize the limitation of metric space on capturing the nontransitive similarity , and propose some effective non metric learning algorithms for non transitive similarity . // similarity component analysis ( sca ) proposes a probabilistic graphical model to discover latent pairwise similarity components , and aggregating them as the final similarity .
b	382	[489]	in metric learning , researchers realize the limitation of metric space on capturing the nontransitive similarity , and propose some effective non metric learning algorithms for non transitive similarity . // similarity component analysis ( sca ) proposes a probabilistic graphical model to discover latent pairwise similarity components , and aggregating them as the final similarity . // note that the similarity components discovered in our work are entity wise , and each entity is represented by a combination of similarity components .
b	382	[491, 492, 493, 494, 495]	multiple maps t sne aims to represent non transitive similarity and central objects in two dimensional visualizations . // in social networks , many works focus on extracting the multiple types of relationship or finding the overlapping communities . // mixed membership stochastic blockmodels discovers overlapping communities in networks , and represents vertices in networks with mixed membership to communities .
h-	382	[494]	in social networks , many works focus on extracting the multiple types of relationship or finding the overlapping communities . // mixed membership stochastic blockmodels discovers overlapping communities in networks , and represents vertices in networks with mixed membership to communities . // however , these methods can not work in the scenario of hashing .
b	382	[496, 497]	with the rapid increase of the data volume , hashing is proposed to solve the efficiency problem on approximate nearest neighbors search in large scale high dimensional data . // locality sensitive hashing ( i .e . lsh ) methods are first proposed , which generate hash codes with random projections or permutations . // moreover , mu et . al . apply lsh to non metric similarity .
h+	382	[467]	moreover , mu et . al . apply lsh to non metric similarity . // while locality sensitive hashing is independent with data , and has to generate long hash code to achieve high accuracy , spectral hashing is proposed to learn hash functions based on the data distribution , and achieves much compact hash code and higher accuracy . // some more unsupervised hashing methods based on data distributions are proposed later .
b	382	[499, 500, 501, 502]	while locality sensitive hashing is independent with data , and has to generate long hash code to achieve high accuracy , spectral hashing is proposed to learn hash functions based on the data distribution , and achieves much compact hash code and higher accuracy . // some more unsupervised hashing methods based on data distributions are proposed later . // for the wellknown semantic gap between low level features and semantic concepts , the performance of unsupervised hashing suffers bottleneck .
b	382	[485, 487, 503, 504, 505]	for the wellknown semantic gap between low level features and semantic concepts , the performance of unsupervised hashing suffers bottleneck . // semi supervised or supervised hashing methods exploit the labeled pairwise simiarity relationship between entities to capture the high level semantics . // moreover , some multi modal hashing methods are proposed to exploit multiple features for hashing to get higher accuracy . however , these hashing methods can not discover the latent similarity components and capture the non transitive similarity .
h-	382	[486, 512, 512]	another class of hashing methods that is related to our work is the hashing methods using multiple hash tables . // in order to improve the recall of hashing and preserve the precision at the same time , some multi table hashing methods are proposed , where complementary hashing learns multiple hash tables with boosting methods , and reciprocal hashing learns multiple hash tables by selecting hash functions from a pool of hash functions that learned by many hashing methods . // since these methods treat similarity and dissimilarity relationships in the same way , these multitable hashing methods are not designed to identify latent similarity components and can not capture non transitive similarity .
h-	382	[486]	another class of hashing methods that is related to our work is the hashing methods using multiple hash tables . // in order to improve the recall of hashing and preserve the precision at the same time , some multi table hashing methods are proposed , where complementary hashing learns multiple hash tables with boosting methods , and reciprocal hashing learns multiple hash tables by selecting hash functions from a pool of hash functions that learned by many hashing methods . // since these methods treat similarity and dissimilarity relationships in the same way , these multitable hashing methods are not designed to identify latent similarity components and can not capture non transitive similarity .
h-	382	[512, 512]	another class of hashing methods that is related to our work is the hashing methods using multiple hash tables . // in order to improve the recall of hashing and preserve the precision at the same time , some multi table hashing methods are proposed , where complementary hashing learns multiple hash tables with boosting methods , and reciprocal hashing learns multiple hash tables by selecting hash functions from a pool of hash functions that learned by many hashing methods . // since these methods treat similarity and dissimilarity relationships in the same way , these multitable hashing methods are not designed to identify latent similarity components and can not capture non transitive similarity .
b	382	[488]	hashing on multi label data learn different hash tables for different known labels . // heterogeneous hashing generates a variant of original hash table in each domain to search , in order to capture the specific characteristics of target domain . // the setting of the above two class of hashing methods are different from our setting where similarity components ( i .e . label or domain ) are latent .
b	382	[487]	that is , when hrs is relatively small , reduction by 1 on hrs will save most of the search space . // we select three state of art hashing methods , i .e . kernelbased supervised hashing ( ksh ) , semi supervised hashing ( splh ) , iterative quantization ( itq ) , as baselines . // ksh is a supervised method , splh is a semi supervised method , itq is a unsupervised methods . for ksh , we randomly select 300 anchors in nus wide , and 50 anchors in dblp and the synthetic dataset .
b	382	[485]	that is , when hrs is relatively small , reduction by 1 on hrs will save most of the search space . // we select three state of art hashing methods , i .e . kernelbased supervised hashing ( ksh ) , semi supervised hashing ( splh ) , iterative quantization ( itq ) , as baselines . // ksh is a supervised method , splh is a semi supervised method , itq is a unsupervised methods . for ksh , we randomly select 300 anchors in nus wide , and 50 anchors in dblp and the synthetic dataset .
b	382	[515]	datasets . // nus wide is an image dataset crawled from flickr with about 260 , 000 images and 81 concept categories . // a pair of images is regarded similar if they share at least one common concept , otherwise , they are dissimilar .
ho	109	[35, 167]	vectorized data representations frequently arise in many data mining applications . // they are easier to handle since each data can be viewed as a point residing in an euclidean space . // thus , similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification , clustering and retrieval .
b	109	[68]	they are easier to handle since each data can be viewed as a point residing in an euclidean space . // thus , similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification , clustering and retrieval . // as shown in , learning good representations is one of the fundamental problems in data mining and web search , and it often has a stronger impact on performance than designing a more sophisticated model .
b	109	[518]	they are easier to handle since each data can be viewed as a point residing in an euclidean space . // thus , similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification , clustering and retrieval . // as shown in , learning good representations is one of the fundamental problems in data mining and web search , and it often has a stronger impact on performance than designing a more sophisticated model .
ho	109	[519]	thus , similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification , clustering and retrieval . // as shown in , learning good representations is one of the fundamental problems in data mining and web search , and it often has a stronger impact on performance than designing a more sophisticated model . // unfortunately , many networked data sources ( e .g . facebook , youtube , flickr and twitter ) can not be naturally represented as vectorized inputs .
b	109	[309]	a combination of graphs and relational data is commonly used to represent these social networks and social media data . // current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on .
b	109	[68]	current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on . // the development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links .
b	109	[521]	current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on . // the development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links .
b	109	[522]	current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on . // the development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links .
b	109	[524]	current research has focused on either pre defined feature vectors or sophisticated graph based algorithms to solve the underlying tasks . // a significant amount of research has been accomplished on various topics , such as collective classification , community detection , link prediction , social recommendation , targeted advertising , and so on . // the development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links .
ho	109	[525]	given the varied backgrounds of the users , social media tends to be noisy . // in addition , researchers have noticed that spammers generate more data than legitimate users , which makes network mining even more difficult . // furthermore , social media data contains diverse and heterogeneous information .
ho	109	[488]	as a google search example of “ malaysia airlines mh 17 ” illustrates in figure 1 , relevant results include not only text documents but also images and videos . // moreover , social media data does not exist in isolation , but in combination with various data types . // these interactions can be formed either explicitly or implicitly with the linkages between them .
b	109	[347, 43]	unlike to traditional linear embedding models , the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture . // both deep neural networks ( dnn ) and convolutional neural networks ( cnn ) are aggregated to handle vectorized data ( e .g . , text documents ) or tensor based multimedia objects ( e .g . , color images and videos ) . // our model coordinates two mutually reinforcing parts by iteratively solving an optimization problem with respect to feature learning and objective minimization .
b	109	[527, 149]	unlike to traditional linear embedding models , the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture . // both deep neural networks ( dnn ) and convolutional neural networks ( cnn ) are aggregated to handle vectorized data ( e .g . , text documents ) or tensor based multimedia objects ( e .g . , color images and videos ) . // our model coordinates two mutually reinforcing parts by iteratively solving an optimization problem with respect to feature learning and objective minimization .
b	109	[528]	network embedding . // a branch of latent feature embeddings is motivated by applications such as collaborative filtering and link prediction in networks that model the relations between entities from latent attributes . // these models often transfer the problem as learning an embedding of the entities , which corresponds algebraically to a matrix factorization problem of observed relationships .
b	109	[526, 521]	zhu et . al . proposed a joint factorization approach on both the linkage adjacency matrix and document term frequency for web page categorization . // similar concepts also include . // in addition , shaw et .al . proposed a structure preserving embedding framwork that embeds graphs to a low dimensional euclidean space with global topological properties preserved .
b	109	[293]	similar concepts also include . // in addition , shaw et .al . proposed a structure preserving embedding framwork that embeds graphs to a low dimensional euclidean space with global topological properties preserved . // moreover , deepwalk learns latent representations of vertices in a network from truncated random walks .
h-	109	[6]	in addition , shaw et .al . proposed a structure preserving embedding framwork that embeds graphs to a low dimensional euclidean space with global topological properties preserved . // moreover , deepwalk learns latent representations of vertices in a network from truncated random walks . // however , these models focus only on single relations that do not adapt to heterogeneous settings and most of them are hardly to generate to other unseen samples .
h-	109	[530]	the disadvantage of such multi relational embeddings is the inherent sharing of parameters between different terms , which does not scale to large graphs . // a nonlinear embedding model proposed by yuan et . al . , used restricted boltzmann machines ( rbms ) for crossmodel link analysis . // however , it did not utilize all of the social information from the raw data ( required a feature vectorization step ) , which resulted in suboptimal solutions .
h-	109	[149]	however , it did not utilize all of the social information from the raw data ( required a feature vectorization step ) , which resulted in suboptimal solutions . // moreover , work in computer vision and speech has shown that layer wise rbm training is inefficient for large scale settings when compared to dnns and cnns . // deep learning .
b	109	[531]	deep learning . // in recent years , machine learning research has seen a marked switch from hand crafted features to those that are learned from raw data , mainly due to the success of deep learning . // deep learning models have become increasingly important in speech recognition , object recognition/detection , and more recently in natural language processing .
b	109	[347]	for example , hinton and salakhutdinov first employed layer wise initialization of deep neural networks with the use of rbms . // a similar approach is weight initialized with autoencoders , as proposed by bengio et . al . . . // the method of dropout has shown particular promise .
h+	109	[67]	a similar approach is weight initialized with autoencoders , as proposed by bengio et . al . . . // the method of dropout has shown particular promise . // a seven layer convolutional network developed by krizhevsky et . al . achieved state of the art performance on the imagenet large scale visual recognition challenge , one of the most challenging tasks in computer vision .
h+	109	[149]	the method of dropout has shown particular promise . // a seven layer convolutional network developed by krizhevsky et . al . achieved state of the art performance on the imagenet large scale visual recognition challenge , one of the most challenging tasks in computer vision . // later on , features from one of network ’ s intermediate layers proved to be a superior feature representation for other vision tasks , such as object detection .
h+	109	[533]	a seven layer convolutional network developed by krizhevsky et . al . achieved state of the art performance on the imagenet large scale visual recognition challenge , one of the most challenging tasks in computer vision . // later on , features from one of network ’ s intermediate layers proved to be a superior feature representation for other vision tasks , such as object detection . // there has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures .
h+	109	[534]	there has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures . // unsupervised deep learning methods such as auto encoders and rbms are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network , and successfully applied to several multimodal tasks . // alternatively , there has also been effort on learning the joint representation with multiple tasks .
h+	109	[535]	there has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures . // unsupervised deep learning methods such as auto encoders and rbms are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network , and successfully applied to several multimodal tasks . // alternatively , there has also been effort on learning the joint representation with multiple tasks .
h+	109	[536, 537]	alternatively , there has also been effort on learning the joint representation with multiple tasks . // for image and text , a particular useful scenario is zero shot learning of image classification on unseen labels achieved by incorporating the semantically meaningful embedding space to image labels . // to the best of our knowledge , there have been few previous attempts to make use of the linkage structure in a network for representation learning .
b	109	[174]	to the best of our knowledge , there have been few previous attempts to make use of the linkage structure in a network for representation learning . // in a conditional temporary rbm was used to model the dynamics of network links , and the main task was to predict future links with historical data . // the most similar work to ours is that of .
b	109	[530]	in a conditional temporary rbm was used to model the dynamics of network links , and the main task was to predict future links with historical data . // the most similar work to ours is that of . // a content rbm was trained on multimedia link data .
ho	109	[309]	datasets and experiment settings we use two publicly available datasets from real world social sites . // the first one is blogcatalog which is used in to select features in linked social media data . // the second one is a heterogeneous dataset , which is referred to as nus wide .
b	109	[515]	we use two publicly available datasets from real world social sites . the first one is blogcatalog which is used in to select features in linked social media data . // the second one is a heterogeneous dataset , which is referred to as nus wide . // this dataset contains both images and text .
b	109	[309]	the detailed descriptions and statistics for both datasets are provided below . // blogcatalog : it is a social blogging site where registered users are able to categorize their blogs under predefined classes . // such categorizations are used to define class labels , whereas “ following ” behaviors are used to construct linkages between users .
b	109	[309]	link content : we combine features from the previous two . // lufs : unsupervised feature selection framework for linked social media data considering both content and links . // lcmf : a matrix co factorization method that utilizes both linkage structure and content features .
b	109	[10]	lufs : unsupervised feature selection framework for linked social media data considering both content and links . // lcmf : a matrix co factorization method that utilizes both linkage structure and content features . // dt : a transfer learning method is used to bridge semantic distances between image and text by latent embeddings .
b	109	[538]	‚ cca : the canonical correlation analysis embeds two types of input sources into a common latent space by optimizing with respect to their correlations . // dt : a transfer learning method is used to bridge semantic distances between image and text by latent embeddings . // lhne : the linear version of hne solves the optimization function in equation ( 7 ) .
b	387	[70]	as a result , the network representation is inevitable in many data mining applications . // on the one hand , many data mining applications are designed for networks , such as community detection and link prediction . // on the other hand , other data mining applications can benefit from the analysis of networks , such as collective classification and dimension reduction . all of above applications rely on the analysis of network interactions or edges .
b	387	[68]	on the one hand , many data mining applications are designed for networks , such as community detection and link prediction . // on the other hand , other data mining applications can benefit from the analysis of networks , such as collective classification and dimension reduction . // all of above applications rely on the analysis of network interactions or edges .
b	387	[540, 541, 405]	on the one hand , many data mining applications are designed for networks , such as community detection and link prediction . // on the other hand , other data mining applications can benefit from the analysis of networks , such as collective classification and dimension reduction . // all of above applications rely on the analysis of network interactions or edges .
h-	387	[6, 1]	the latent features are beneficial as they are more expressive than edges and can be directly employed by off the shelf machine learning techniques . // although various network embedding methods have been proposed before , they are designed only for learning representations on a single network . // furthermore , in the era of big data , different types of related information are often available and can be fused together to form a coupled heterogeneous network , where each type of information can be represented as a separate homogeneous network .
b	387	[55]	and we compare the proposed eoe against the following three latent feature learning methods . // spectral clustering ( sc ) : this method proposes to use spectral clustering to learn latent features . // specifically , the top d eigenvectors of the normalized laplacian matrix are used as the feature vectors .
b	387	[6]	specifically , the top d eigenvectors of the normalized laplacian matrix are used as the feature vectors . // deepwalk : deepwalk learns latent features for vertices by modeling random walks as sentences of a natural language , and then re formulating language modeling as its learning objective . // since deepwalk is only applicable to unweighted edges , all weights are set to 1 as inputs to deepwalk .
b	387	[70]	link prediction . // the link prediction problem refers to inferring new interactions between network vertices by measuring the similarity between them . // we deploy two scenarios of link prediction for evaluation of the latent features , which are future link prediction and missing link prediction .
b	387	[4, 3, 355, 5]	the proposed eoe model is related to general graph embedding or network embedding methods to learn latent representations for graph or network vertices . // a couple of graph or network embedding methods have been proposed previously , but they are originally designed for dimension reduction of existing features . // specifically , their objectives are to learn low dimensional latent representations of existing features so that learning complexity brought by feature dimension would be significantly reduced .
b	387	[154]	in our scenarios , there exist no features of network vertices but the network edge information . // another graph embedding method called graph factorization learns latent features by utilizing network edges . // it presents graphs as matrices where matrix elements correspond to edges between vertices , and then learns latent features by matrix factorization .
b	387	[1]	moreover , deepwalk can only handle unweighted networks , while the proposed eoe model is applicable to both weighted and unweighted networks . // the state of the art related model is line for largescale information network embedding . // line preserves both interaction information and non interaction information , which is similar to the proposed eoe .
ho	387	[542]	the gradient ∂l ( u ) ∂ua i is not presented before , but can be easily obtained by removing polynomials including m from the gradient ∂l ( u , m , v ) ∂ua i . // for the learning rate , we employ the backtracking line search to learn an appropriate one for each of the iteration . // the condition to determine whether all minimizations converge is that the relative loss between current iteration and last iteration is smaller than a considerably small value , such as 0 .001 .
b	387	[1]	embedding visualization . // visualization of embeddings on a two dimensional space is an important application of network embedding . // if the learned embeddings preserve the network structure well , visualization provides an easy way to generate the layout of a network .
b	8	[544, 543]	to tackle the label sparsity , two branches of research studies have been proposed in recent literature . // the first is semi supervised collective classification , which leverages unlabeled nodes to enhance classification performance . // these methods typically first use a bootstrap step to predict labels for all unlabeled nodes , through a classifier trained on labeled nodes using only content features .
b	8	[142, 545, 546]	after that , predicted labels , together with known labels , are used to construct relational features and perform collective inference . // the second line of research focuses on increasing network connectivity between unlabeled and labeled nodes . // this is achieved by either adding extra edges based on node attribute similarity , or constructing a latent graph from different information sources .
h-	8	[544, 543]	our main idea of tackling node label sparsity is twofold : ( 1 ) leverage random walks to find path dependencies between unlabeled nodes and sparsely labeled nodes in the network ; and ( 2 ) learn a latent network representation that fully embeds nodes ’ content information and augmented network structure to minimize classification loss . // intuitively , the proposed dmf is motivated by observations that , for sparsely labeled networks , an unlabeled node in the network may not have labeled neighbors or have very low structure similarity to labeled nodes , so existing semi supervised and connectivity based methods would not have satisfactory performance . // alternatively , we can use network walks to examine unlabeled nodes ’ correlations to labeled nodes .
ho	8	[7]	because a given network has an exponential number of walk combinations , this essentially increases the relationship density for sparsely labeled networks . // in order to utilize network walks to guide node classification with minimum loss , we leverage an existing textassociated deepwalk ( tadw ) approach , which embeds nodes ’ linkage information together with their content features into a low dimensional space . // to enable supervised information to be propagated from labeled nodes to unlabeled nodes , we formulate a new matrix factorization objective function that integrates network representation learning with an empirical loss minimization of a linear classifier on labeled nodes .
b	8	[547, 68]	collective classification . // for networked data classification , collective classification ( cc ) has received considerable attention in recent years . // its key idea is to construct a new set of features that summarize label dependencies to improve classification accuracy .
h+	8	[544]	the variants of semi supervised algorithms mainly differ in how they train a best classifier from all available features . // a recent study by mcdowell and aha provided a comprehensive comparison of different semi supervised cc algorithms and found that learning a hybrid classifier from content features and relational features can best boost the performance of semi supervised cc algorithms . // in a follow up work , mcdowell and aha pointed out that training discriminative classifiers additionally with neighbors ’ attributes can yield further accuracy gains for semi supervised collective classification .
b	8	[142, 545, 546]	in a follow up work , mcdowell and aha pointed out that training discriminative classifiers additionally with neighbors ’ attributes can yield further accuracy gains for semi supervised collective classification . // another line of research has focused more on increasing network connectivity between unlabeled nodes and labeled nodes . // macskassy enriched the linkage of the original network by adding extra links based on node attribute similarity , and a weighted vote relational neighbor with relational labeling ( wwrn+rl ) classifier was used to classify unlabeled nodes .
b	8	[545]	another line of research has focused more on increasing network connectivity between unlabeled nodes and labeled nodes . // macskassy enriched the linkage of the original network by adding extra links based on node attribute similarity , and a weighted vote relational neighbor with relational labeling ( wwrn+rl ) classifier was used to classify unlabeled nodes . // gallagher et al . proposed to add “ ghost edges ” created via random walk with restart , to a network , which enables information propagation from labeled nodes to unlabeled ones .
b	8	[142]	macskassy enriched the linkage of the original network by adding extra links based on node attribute similarity , and a weighted vote relational neighbor with relational labeling ( wwrn+rl ) classifier was used to classify unlabeled nodes . // gallagher et al . proposed to add “ ghost edges ” created via random walk with restart , to a network , which enables information propagation from labeled nodes to unlabeled ones . // in , a latent network propagation model ( lnp ) was proposed that constructs a latent graph from different information sources and uses label propagation to predict labels of unlabeled nodes .
b	8	[256]	our work is also related to network representation learning that learns a latent representation of network nodes such that nodes close to each other in the original network space are also similar to each other in the learned representation space . // chen et al . firstly proposed a method for embeddingvertices on a directed graph into a vector space by considering graph link structure . // the proposed method preserves the locality property of vertices on a directed graph in the embedded space , by using random walks .
b	8	[33]	the proposed method preserves the locality property of vertices on a directed graph in the embedded space , by using random walks . // tang and liu proposed to extract latent social dimensions based on network connectivity , with each dimension representing a likely affiliation among social actors . // perozzi et al . proposed deepwalk that learns latent representations of network nodes , which encode structural regularities in a continuous , low dimensional vector space immediately available for statistical machine learning algorithms .
b	8	[6]	tang and liu proposed to extract latent social dimensions based on network connectivity , with each dimension representing a likely affiliation among social actors . // perozzi et al . proposed deepwalk that learns latent representations of network nodes , which encode structural regularities in a continuous , low dimensional vector space immediately available for statistical machine learning algorithms . // deepwalk uses random walks to expand the neighborhood of a node , thus providing a more global view of the network .
h-	8	[1]	despite of its empirical effectiveness , deepwalk does not provide a clear interpretation about what network properties are preserved , as pointed out in . // instead , tang et al . proposed a new algorithm called line that learns node representations by explicitly modeling two local pairwise node proximities , i .e . , the first order proximity between pairs of nodes directly linked to each other , and the second order proximity between pairs of nodes sharing common neighbors . // all of these methods have solely considered the link structure for network representation learning , but ignored the interplay between nodes ’ content features and structural features .
b	8	[7]	all of these methods have solely considered the link structure for network representation learning , but ignored the interplay between nodes ’ content features and structural features . // the most recent work related to ours is text associated deepwalk ( tadw ) , which incorporates text features of nodes into network representation learning . // in , deepwalk was proved to be equivalent to matrix factorization . based on this view , text features of nodes are naturally incorporated into the matrix factorization framework . by capturing interactions between text features and network structures , tadw can obtain better network representations for network node classification .
h+	8	[7]	the most recent work related to ours is text associated deepwalk ( tadw ) , which incorporates text features of nodes into network representation learning . // in , deepwalk was proved to be equivalent to matrix factorization . based on this view , text features of nodes are naturally incorporated into the matrix factorization framework . // by capturing interactions between text features and network structures , tadw can obtain better network representations for network node classification .
b	8	[7]	text associated deepwalk . // motivated by deepwalk , text associated deepwalk ( tadw ) incorporates node text features into network representation learning . // deepwalk originally generalizes the skip gram model from word representation learning to node representation learning . in deepwalk , nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes .
b	8	[36]	motivated by deepwalk , text associated deepwalk ( tadw ) incorporates node text features into network representation learning . // deepwalk originally generalizes the skip gram model from word representation learning to node representation learning . in deepwalk , nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes . // in deepwalk , nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes .
b	8	[127]	we compare the proposed dmf algorithm with the following baseline methods . // ica : the ica algorithm is one of the most popularly used baselines for collective classification . // we train a logistic regression classifier on labeled nodes using their node content features and relational features calculated as the number of positive neighbors and the number of negative neighbors .
b	8	[543]	after the latent graph is constructed , label propagation is performed to classify unlabeled nodes . // semi supervised rci ( semi rci ) : semi rci is the state of the art semi supervised collective classification algorithm that exploits node content features together with neighbor features and neighbor labels to improve the learning performance of the ica algorithm . // semi rci first bootstraps the labels of unlabeled nodes via the classifier trained on labeled nodes using node features and neighbor features .
b	110	[26, 6, 1]	however , for very large graphs , common information retrieval and mining tasks such as link prediction , node classification , clustering , and recommendation are time consuming . // this motivated a lot of interest in approaches that embed the network into a low dimensional space , such that the original vertices of the graph are represented as vectors . // a good embedding preserves the proximity ( i .e . , similarity ) between vertices in the original graph .
b	110	[548]	search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces . // heterogeneous information networks ( hins ) , such as dblp , yago , dbpedia and freebase , are networks with nodes and edges that may belong to multiple types . // these graph data sources contain a vast number of interrelated facts , and they can facilitate the discovery of interesting knowledge .
b	110	[550]	search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces . // heterogeneous information networks ( hins ) , such as dblp , yago , dbpedia and freebase , are networks with nodes and edges that may belong to multiple types . // these graph data sources contain a vast number of interrelated facts , and they can facilitate the discovery of interesting knowledge .
b	110	[400]	search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces . // heterogeneous information networks ( hins ) , such as dblp , yago , dbpedia and freebase , are networks with nodes and edges that may belong to multiple types . // these graph data sources contain a vast number of interrelated facts , and they can facilitate the discovery of interesting knowledge .
b	110	[551, 552, 553, 554]	heterogeneous information networks ( hins ) , such as dblp , yago , dbpedia and freebase , are networks with nodes and edges that may belong to multiple types . // these graph data sources contain a vast number of interrelated facts , and they can facilitate the discovery of interesting knowledge . // figure 1 illustrates a hin , which describes the relationships between objects ( graph vertices ) of different types ( e .g . , author , paper , venue and topic ) .
h+	110	[224]	based on meta paths , several proximity measures have been proposed . // for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarxiv:1701 .05291v1 19 jan 2017 mance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering .
h+	110	[552]	based on meta paths , several proximity measures have been proposed . // for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarxiv:1701 .05291v1 19 jan 2017 mance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering .
h+	110	[224]	for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarmance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering . // although there are a few works on embedding hins , none of them is designed for meta path based proximity in general hins .
h+	110	[151]	for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarmance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering . // although there are a few works on embedding hins , none of them is designed for meta path based proximity in general hins .
h+	110	[557, 558]	for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarmance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering . // although there are a few works on embedding hins , none of them is designed for meta path based proximity in general hins .
h+	110	[225]	for example , pathcount counts the number of meta path instances connecting the two objects , while path constrained random walk ( pcrw ) measures the probability that a random walk starting from one object would reach the other via a meta path instance . // these measures have been shown to have better perforarmance compared to proximity measures not based on meta paths , in various important tasks , such as k nn search , link prediction , recommendation , classification and clustering . // although there are a few works on embedding hins , none of them is designed for meta path based proximity in general hins .
ho	110	[224]	more specifically , we define an appropriate objective function that preserves the relative proximity based rankings the vertices in the original and the embedded space . // as shown in , meta paths with too large lengths are not very informative ; therefore , we only consider meta paths up to a given length threshold l . // we also investigate the use of negative sampling in order to accelerate the optimization process .
ho	110	[36]	as shown in , meta paths with too large lengths are not very informative ; therefore , we only consider meta paths up to a given length threshold l . // we also investigate the use of negative sampling in order to accelerate the optimization process . // we conduct extensive experiments on four real hin datasets to compare our proposed hine method with state of the art network embedding methods ( i .e . , line and deepwalk ) , which do not consider meta path based proximity . our experimental results show that our hine method with pcrw as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used .
h-	110	[1]	we also investigate the use of negative sampling in order to accelerate the optimization process . // we conduct extensive experiments on four real hin datasets to compare our proposed hine method with state of the art network embedding methods ( i .e . , line and deepwalk ) , which do not consider meta path based proximity . // our experimental results show that our hine method with pcrw as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used .
b	110	[557]	lately , there has been an increasing interest in both academia and industry in the effective search and analysis of information from hins . // the problem of classifying objects in a hin by authority propagation is studied in . // follow up work investigates a collective classification problem in hins using meta path based dependencies .
b	110	[558]	the problem of classifying objects in a hin by authority propagation is studied in . // follow up work investigates a collective classification problem in hins using meta path based dependencies . // pathselclus is a link based clustering algorithm for hins , in which a user can specify her clustering preference by providing some examples as seeds .
b	110	[225]	follow up work investigates a collective classification problem in hins using meta path based dependencies . // pathselclus is a link based clustering algorithm for hins , in which a user can specify her clustering preference by providing some examples as seeds . // the problem of link prediction on hins has been extensively studied , due to its important applications ( e .g . , in recommender systems ) .
b	110	[151]	the problem of link prediction on hins has been extensively studied , due to its important applications ( e .g . , in recommender systems ) . // a related problem is entity recommendation in hins , which takes advantage of the different types of relationships in hins to provide better recommendations . // meta path and proximity measures .
b	110	[224]	meta path and proximity measures . // meta path is a general model for the proximity between objects in a hin . // several measures have been proposed for the proximity between objects w .r .t . a given meta path p .
b	110	[224]	several measures have been proposed for the proximity between objects w .r .t . a given meta path p . // pathcount measures the number of meta path instances connecting the two objects , and pathsim is a normalized version of it . // path constrained random walk ( pcrw ) was firstly proposed for the task of relationship retrieval over bibliographic networks .
b	110	[553]	path constrained random walk ( pcrw ) was firstly proposed for the task of relationship retrieval over bibliographic networks . // later , proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on pcrw . // finally , hetesim is recently proposed as an extension of meta path based simrank .
b	110	[559]	later , proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on pcrw . // finally , hetesim is recently proposed as an extension of meta path based simrank . // in this paper , we focus on the two most popular proximity measures , i .e . , pathcount and pcrw .
b	110	[5, 355, 4, 3]	network embedding . // traditional dimensionality reduction techniques typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph . // graph factorization finds a low dimensional representation of a graph through matrix factorization , after representing the graph as an adjacency matrix .
ho	110	[1]	graph factorization finds a low dimensional representation of a graph through matrix factorization , after representing the graph as an adjacency matrix . // however , since these general techniques are not designed for networks , they do not necessarily preserve the global network structure , as pointed out in . // recently , deepwalk is proposed as a method for learning the latent representations of the nodes of a social network , from truncated random walks in the network .
b	110	[6]	however , since these general techniques are not designed for networks , they do not necessarily preserve the global network structure , as pointed out in . // recently , deepwalk is proposed as a method for learning the latent representations of the nodes of a social network , from truncated random walks in the network . // deepwalk combines random walk proximity with the skipgram model , a language model that maximizes the co occurrence probability among the words that appear within a window in a sentence .
b	110	[35]	recently , deepwalk is proposed as a method for learning the latent representations of the nodes of a social network , from truncated random walks in the network . // deepwalk combines random walk proximity with the skipgram model , a language model that maximizes the co occurrence probability among the words that appear within a window in a sentence . // however , deepwalk has certain weaknesses when applied to our problem settings . first , the random walk proximity it adopts does not consider the heterogeneity of a hin .
h-	110	[1]	second , as pointed out in , deepwalk can only preserve secondorder proximity , leading to poor performance in some tasks , such as link recover and classification , which require first order proximity to be well preserved . // line is a recently proposed embedding approach for largescale networks . // although it uses an explicit objective function to preserve the network structure , its performance suffers from the way it learns the vector representations .
h-	110	[19]	grarep further extends deepwalk to utilize high order proximities . grarep does not scale well in large networks due to the expensive computation of the power of a matrix and the involvement of svd in the learning process . // sdne is a semi supervised deep model that captures the non linear structural information over the network . // the source code of sdne is not available , so this approach can not be reproduced and compared to ours .
b	110	[81]	the source code of sdne is not available , so this approach can not be reproduced and compared to ours . // similarly , embeds entities in knowledge bases using an innovative neural network architecture and tridnr extends this embedding model to consider features from three aspects of the network : 1 ) network structure , 2 ) node content , and 3 ) label information . // our current work focuses on meta path based proximities , so it does not consider any other information in the hin besides the network structure and the types of nodes and edges .
ho	110	[224]	generally speaking , the number of possible meta paths grows exponentially with their length and can be infinite for certain hin schema ( in this case , the computation of s ( oi , oj ) is infeasible ) . // as pointed out in , shorter meta paths are more informative than longer ones , because longer meta paths link more remote objects ( which are less related semantically ) . // therefore , we use a truncated estimation of proximity , which only considers meta paths up to a length threshold l .
b	110	[6]	therefore , we use a truncated estimation of proximity , which only considers meta paths up to a length threshold l . // this is also consistent with previous works on network embedding , which aim at reserving loworder proximity ( e .g . , reserves only second order proximity , and first order and second order proximities ) . // therefore , we define as the truncated proximity between two objects oi and oj .
b	110	[1]	therefore , we use a truncated estimation of proximity , which only considers meta paths up to a length threshold l . // this is also consistent with previous works on network embedding , which aim at reserving loworder proximity ( e .g . , reserves only second order proximity , and first order and second order proximities ) . // therefore , we define as the truncated proximity between two objects oi and oj .
b	110	[1]	in our experiment settings , we ignore the heterogeneity and directly feed the hins for embedding . // line is a method that preserves first order and secondorder proximities between vertices ( see section 2 .3 for details ) . // for each object , it computes two vectors ; one for the first order and one for the second order proximities separately and then concatenates them .
ho	110	[400]	game . // we extracted from freebase a hin , which is related to video games . // the hin consists of 4 ,095 games ( g ) , 1 ,578 publishers ( p ) , 2 ,043 developers ( d ) and 197 designers ( s ) .
ho	110	[224]	dblp . the schema of dblp network is shown in figure 2 ( a ) . // we use a subset of dblp , i .e . , dblp 4 area taken of , which contains 5 ,237 papers ( p ) , 5 ,915 authors ( a ) , 18 venues ( v ) , 4 ,479 topics ( t ) . // the authors are from 4 areas : database , data mining , machine learning and information retrieval .
b	371	[561, 562, 563, 564, 565, 566]	while many believe that cascades are inherently unpredictable , recent work has shown that some key properties of information cascades , such as size , growth , and shape , can be predicted through a mixture of signals . // indeed , cascades of microblogs/tweets , photos , videos and academic papers are proved to be predictable to some extent . // in most of these studies , cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features .
b	371	[560]	while many believe that cascades are inherently unpredictable , recent work has shown that some key properties of information cascades , such as size , growth , and shape , can be predicted through a mixture of signals . // indeed , cascades of microblogs/tweets , photos , videos and academic papers are proved to be predictable to some extent . // in most of these studies , cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features .
b	371	[567]	while many believe that cascades are inherently unpredictable , recent work has shown that some key properties of information cascades , such as size , growth , and shape , can be predicted through a mixture of signals . // indeed , cascades of microblogs/tweets , photos , videos and academic papers are proved to be predictable to some extent . // in most of these studies , cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features .
b	371	[562, 560, 565, 564]	indeed , cascades of microblogs/tweets , photos , videos and academic papers are proved to be predictable to some extent . // in most of these studies , cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features . // on one hand , many of these features are specific to the particular platform or the particular type of information being diffused .
h-	371	[560]	on one hand , many of these features are specific to the particular platform or the particular type of information being diffused . // for example , whether a photo was posted with a caption is shown to be predictive of how widely it spread on facebook ; specific wording on tweets is shown to help them gain more retweets . // these features are indicative but can not be generalized to other platforms or to other types of cascades .
h-	371	[569]	on one hand , many of these features are specific to the particular platform or the particular type of information being diffused . // for example , whether a photo was posted with a caption is shown to be predictive of how widely it spread on facebook ; specific wording on tweets is shown to help them gain more retweets . // these features are indicative but can not be generalized to other platforms or to other types of cascades .
h-	371	[560]	there are also ad hoc features that appear very predictive , but their success is intriguing and sometimes magical . // for example , cheng et al . found that one of the most indicative feature to the growth of a cascade is whether any of the first a few reshares are not directly connected to the root of the diffusion . // we consider this as a major deficiency of these machine learning approaches : their performance heavily depends on the feature representations , yet there is no common principle of how to design and measure the features .
ho	371	[570]	related work . // in a networked environment , people tend to be influenced by their neighbors ’ behavior and decisions . // opinions , product advertisements , or political propaganda could spread over the network through a chain reaction of such influence , a process known as the information cascade .
b	371	[571, 572, 573]	in a networked environment , people tend to be influenced by their neighbors ’ behavior and decisions . // opinions , product advertisements , or political propaganda could spread over the network through a chain reaction of such influence , a process known as the information cascade . // we present the first deep learning method to predict the future size of information cascades
b	371	[560]	cascade prediction . // cascades of particular types of information are empirically proved to be predictable to some extent , including tweets/microblogs , photos , videos and academic papers . // one treats cascade prediction as a classification problem , which predicts whether or not a piece of information will become popular and wide spread ( above a certain threshold ) .
b	371	[567]	cascade prediction . // cascades of particular types of information are empirically proved to be predictable to some extent , including tweets/microblogs , photos , videos and academic papers . // one treats cascade prediction as a classification problem , which predicts whether or not a piece of information will become popular and wide spread ( above a certain threshold ) .
b	371	[568]	cascade prediction . // cascades of particular types of information are empirically proved to be predictable to some extent , including tweets/microblogs , photos , videos and academic papers . // one treats cascade prediction as a classification problem , which predicts whether or not a piece of information will become popular and wide spread ( above a certain threshold ) .
b	371	[562, 300]	one treats cascade prediction as a classification problem , which predicts whether or not a piece of information will become popular and wide spread ( above a certain threshold ) . // the other formulates cascade prediction as a regression problem , which predicts the numerical properties ( e .g . , size ) of a cascade in the future . // this line of work can be further categorized by whether it outputs the final size of a cascade or the size as a function of time ( i .e . , the growth of the cascade ) .
h-	371	[563]	the other formulates cascade prediction as a regression problem , which predicts the numerical properties ( e .g . , size ) of a cascade in the future . // this line of work can be further categorized by whether it outputs the final size of a cascade or the size as a function of time ( i .e . , the growth of the cascade ) . // either way , most of the methods identified temporal properties , topological structure of the cascade at the early stage , root and early adopters of the information , and the content being spread as the most predictive factors .
h-	371	[561]	the other formulates cascade prediction as a regression problem , which predicts the numerical properties ( e .g . , size ) of a cascade in the future . // this line of work can be further categorized by whether it outputs the final size of a cascade or the size as a function of time ( i .e . , the growth of the cascade ) . // either way , most of the methods identified temporal properties , topological structure of the cascade at the early stage , root and early adopters of the information , and the content being spread as the most predictive factors .
b	371	[561]	these factors are utilized for cascade prediction in two fashions . // the first mainly designs generative models of the cascade process based on temporal or structural features , which can be as simple as certain macroscopic distributions ( e .g . , of cascade size over time ) , or stochastic processes that explain the microscopic actions of passing along the information . // these generative models make various strong assumptions and oversimplify the reality .
b	371	[562, 560, 564, 565]	as a result , they generally underperform in real prediction tasks . // alternatively , these factors may be represented through handcrafted features , which are extracted from the data , combined , and weighted by discriminative machine learning algorithms to perform the classification or the regression tasks . // most work in this fashion uses standard supervised learning models ( e .g . logistic regression , svm , or random forests ) , the performance of which heavily rely on the quality of the features .
h-	371	[560, 562, 562]	some of the most predictive features are tied to particular platforms or particular cascades and are hard to be generalized , such as the ones mentioned in the section 1 . // some features are closely related to the structural properties of the social network , such as degree , density , and community structures . // these features could generalize over domains and platforms , but many may still involve arbitrary and hard decisions in computation , such as what to choose from hundreds of community detection algorithms available and how to detect structural holes .
h-	371	[562, 562]	some of the most predictive features are tied to particular platforms or particular cascades and are hard to be generalized , such as the ones mentioned in the section 1 . // some features are closely related to the structural properties of the social network , such as degree , density , and community structures . // these features could generalize over domains and platforms , but many may still involve arbitrary and hard decisions in computation , such as what to choose from hundreds of community detection algorithms available and how to detect structural holes .
b	371	[71]	some features are closely related to the structural properties of the social network , such as degree , density , and community structures . // these features could generalize over domains and platforms , but many may still involve arbitrary and hard decisions in computation , such as what to choose from hundreds of community detection algorithms available and how to detect structural holes . // besides , there are also heuristic features that perform very well in particular scenarios but it is hard to explain why they are designed as is .
b	371	[576]	some features are closely related to the structural properties of the social network , such as degree , density , and community structures . // these features could generalize over domains and platforms , but many may still involve arbitrary and hard decisions in computation , such as what to choose from hundreds of community detection algorithms available and how to detect structural holes . // besides , there are also heuristic features that perform very well in particular scenarios but it is hard to explain why they are designed as is .
b	371	[381, 219, 446, 577]	modern representation learning methods attempt to represent nodes as high dimensional vectors in a continuous space ( a .k .a . , node embeddings ) so that nodes with similar embedding vectors share similar structural properties ( e .g . , ) . // rather than learning the representation of each node , recent work also attempts to learn the representation of subgraph structures . // much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text and image .
b	371	[147]	rather than learning the representation of each node , recent work also attempts to learn the representation of subgraph structures . // much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text and image . // for example , deepwalk makes an analogy between the nodes in networks and the words in natural language and uses fixed length random walk paths to stimulate the “ context ” of a node so that node representations can be learned using the same method of learning word representations .
b	371	[149]	rather than learning the representation of each node , recent work also attempts to learn the representation of subgraph structures . // much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text and image . // for example , deepwalk makes an analogy between the nodes in networks and the words in natural language and uses fixed length random walk paths to stimulate the “ context ” of a node so that node representations can be learned using the same method of learning word representations .
b	371	[35]	much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text and image . // for example , deepwalk makes an analogy between the nodes in networks and the words in natural language and uses fixed length random walk paths to stimulate the “ context ” of a node so that node representations can be learned using the same method of learning word representations . // the representation of a graph can then be calculated by averaging the embeddings of all nodes .
b	371	[578, 579, 220]	the representation of a graph can thenbe calculated by averaging the embeddings of all nodes . // another line of related work comes from the domain of graph kernels , which computes pairwise similarities between graphs . // for example , the weisfeiler lehman subtree kernel ( wl ) computes the graph similarity based on the sub trees in each graph .
b	371	[220]	another line of related work comes from the domain of graph kernels , which computes pairwise similarities between graphs . // for example , the weisfeiler lehman subtree kernel ( wl ) computes the graph similarity based on the sub trees in each graph . // some studies have applied deep learning techniques to improve graph kernels .
b	371	[565]	we feed the feature vectors to mlp ( denoted as features deep ) . // oslor selects important nodes as sensors , and predict the outbreaks based on the cascading behaviors of these sensors . // node2vec is selected as a representative of node embedding methods .
h+	371	[37]	oslor selects important nodes as sensors , and predict the outbreaks based on the cascading behaviors of these sensors . // node2vec is selected as a representative of node embedding methods . // node2vec is a generalization of deepwalk , which is reported to be outperforming alternative methods such as deepwalk and line .
h-	371	[6]	node2vec is selected as a representative of node embedding methods . // node2vec is a generalization of deepwalk , which is reported to be outperforming alternative methods such as deepwalk and line . // we generate walks from two sources : ( 1 ) the set of cascade graphs { g } ( 2 ) the global network g .
b	371	[582]	the average of embeddings of all nodes in a cascade graph is fed through mlp to make the prediction . // embedded ic represents nodes by two types of embeddings : as a sender or as a receiver . // for prediction , the original paper used monte carlo simulations to estimate infections probabilities of each individual user .
b	371	[381]	we therefore report the performance of the latter . // pscn applies convolutional neural networks ( cnn ) to locally connected regions from graphs . // we apply pscn to both the diffusion graphs and the frontier graphs . the last hidden layer of the cascade graph and that of the frontier graph are concatenated to make the final prediction .
b	371	[381]	graph kernels . // there are a set of state of the art graph kernels : the shortest path kernel ( sp ) , the random walk kernel ( rw ) , and the weisfeiler lehman subtree kernel ( wl ) . // the rw kernel and the sp kernel are too computationally inefficient , which did not complete after 10 days for a single data set in our experiment .
h-	371	[579]	graph kernels . // there are a set of state of the art graph kernels : the shortest path kernel ( sp ) , the random walk kernel ( rw ) , and the weisfeiler lehman subtree kernel ( wl ) . // the rw kernel and the sp kernel are too computationally inefficient , which did not complete after 10 days for a single data set in our experiment .
h-	371	[220]	graph kernels . // there are a set of state of the art graph kernels : the shortest path kernel ( sp ) , the random walk kernel ( rw ) , and the weisfeiler lehman subtree kernel ( wl ) . // the rw kernel and the sp kernel are too computationally inefficient , which did not complete after 10 days for a single data set in our experiment .
ho	371	[381, 577]	the rw kernel and the sp kernel are too computationally inefficient , which did not complete after 10 days for a single data set in our experiment . // we therefore exclude them from the comparison , a decision also made by in . // for the wl kernel , we experiment with two settings : wl degree , where node degree is used as the node attribute to build subgraphs for each cascade and frontier graph ; wl id , where node id is used as the attribute . the second setting is to test whether node identity information could be incorporated into graph kernel methods .
b	680	[774]	which are estimated by maximum likelihood estimation . // perplexity ( ppl ) , an information theoretic metric that measures the quality of a probabilistic model , is a way to evaluate lms . // lower ppl indicates a better model .
b	680	[147]	therefore , nns are expected to be applied to lms , even other nlp tasks , to cover the discreteness , combination , and sparsity of natural language . // the first ffnn language model ( ffnnlm ) presented by fights the curse of dimensionality by learning a distributed representation for words , which represents a word as a low dimensional vector , called embedding . // ffnnlm performs better than n gram lm .
b	680	[390]	ffnnlm performs better than n gram lm . // then , rnn language model ( rnnlm ) also was proposed . // since then , the nnlm has gradually become the mainstream lm and has rapidly developed .
b	680	[598]	finally , conclusions are given , and new research directions of nnlms are discussed . // classic neural network language models , ffnn language models tried to introduce nns into lms . // although their model performs better than the baseline ngram lm , their model with poor generalization ability can not capture context dependent features due to no hidden layer .
b	680	[147]	inspired by the ngram lms , ffnnlms consider the previous words as the context for predicting the next word . // proposed the architecture of the original ffnnlm , as shown in figure 1 . // this ffnnlm can be expressed as : y = b + w x + u tanh ( d + hx ) , ( 4 ) w ( t ) s ( t ) y ( t ) s ( t 1 ) x ( t ) u v .
b	680	[390, 653, 596]	bengio et al . , proposed the architecture of the original ffnnlm , as shown in figure 1 . // this ffnnlm can be expressed as : y = b + w x + u tanh ( d + hx ) , ( 4 ) w ( t ) s ( t ) y ( t ) s ( t 1 ) x ( t ) u v delayed figure 2 : the rnnlm proposed by . // the rnn has an internal state that changes with the input on each time step , taking into account all previous contexts .
b	680	[147]	moreover , fully connected nn needs to learn many trainable parameters , even though these parameters are less than n gram lm , which still is expensive and inefficient . // rnn language models proposed the idea of using rnn for lms . // they claimed that introducing more structure and parameter sharing into nns could capture longer contextual information .
b	680	[390, 653]	they claimed that introducing more structure and parameter sharing into nns could capture longer contextual information . // the first rnnlm was proposed by . // as shown in figure 2 , at time step t , the rnnlm can be described as : xt = t , st = f ( uxt + b ) , yt = g ( v st + d ) , where u , w , v are weight matrixes ; b , d are the biases of the state layer and the output layer respectively .
b	680	[390, 653]	the first rnnlm was proposed by . // as shown in figure 2 , at time step t , the rnnlm can be described as : xt = t , st = f ( uxt + b ) , yt = g ( v st + d ) , ( 5 ) where u , w , v are weight matrixes ; b , d are the biases of the state layer and the output layer respectively ; in , f is the sigmoid function , and g is the softmax function . // rnnlms could be trained by the backpropagation through time ( bptt ) or the truncated bptt algorithm .
b	680	[664]	for example , man in superman has the same meaning as the one in policeman . // explored rnnlm and ffnnlm at the character level . // character level nnlm can be used for solving out of vocabulary ( oov ) word problem , improving the modeling of uncommon and unknown words because character features reveal structural similarities between words .
b	680	[684]	one approach is to organize character level features word by word and then use them for word level lms . // proposed convolutional neural network ( cnn ) for extracting character level feature and lstm for receiving these character level features of the word in a time step . // hwang and sung solved the problem of character level nnlms using a hierarchical rnn architecture consisting of multiple modules with different time scales .
b	680	[747]	kim et al . , proposed convolutional neural network ( cnn ) for extracting character level feature and lstm for receiving these character level features of the word in a time step . // solved the problem of character level nnlms using a hierarchical rnn architecture consisting of multiple modules with different time scales . // another solution is to input character and word level features into nnlm simultaneously .
b	680	[765]	however , similarity can also be derived from word shape features ( affixes , uppercase , hyphens , etc . ) or other annotations ( such as pos ) . // inspired by factored lms , proposed a factored nnlms , a novel neural probabilistic lm that can learn the mapping from words and specific features of the words to continuous spaces . // many studies have explored the selection of factors .
b	680	[736]	different linguistic features are considered first . // introduced morphological , grammatical , and semantic features to extend rnnlms . // adel et al . , also ex plored the effects of other factors , such as part of speech tags , brown word clusters , open class words , and clusters of open class word embeddings .
b	680	[638]	wu et al . , introduced morphological , grammatical , and semantic features to extend rnnlms . // also explored the effects of other factors , such as part of speech tags , brown word clusters , open class words , and clusters of open class word embeddings . // experimental results showed that brown word clusters , part of speech tags , and open words are most effective for a mandarin english code switching task .
b	680	[715]	for example , mikolov and zweig used the distribution of topics calculated from fixed length blocks of previous words . // proposed a new approach to incorporating corpus bag of words ( bow ) context into language modeling . // besides , some methods based on text independent factors were proposed .
b	680	[666]	besides , some methods based on text independent factors were proposed . // proposed a neural knowledge language model that applied the notation knowledge provided by knowledge graph for rnnlms . // the factored model allows the model to summarize word classes with same characteristics .
b	680	[696, 734]	a bidirectional nn can be established , which is conditional on future data . // introduced bidirectional rnn and lstm neural networks ( birnn and bilstm ) into speech recognition or other nlp tasks . // the birnns utilize past and future contexts by processing the input data in both directions .
b	680	[661]	cache mechanism was originally proposed for reducing ppl of nnlms . // attempted to combine ffnnlm with the cache mechanism and proposed a structure of the cache based nnlms , which leads to discrete probability change . // to solve this problem , et al . , proposed a continuous cache model , where the change depends on the inner product of the hidden representations .
b	680	[616]	soutner et al . , attempted to combine ffnnlm with the cache mechanism and proposed a structure of the cache based nnlms , which leads to discrete probability change . // to solve this problem , proposed a continuous cache model , where the change depends on the inner product of the hidden representations . // another type of cache mechanism is that cache is used as a speed up technique for nnlms .
b	680	[771]	the main idea of this method is to store the outputs and states of lms in a hash table for future prediction given the same contextual history . // for example , proposed the use of four caches to accelerate model reasoning . // the caches are respectively query to language model probability cache , history to hidden state vector cache , history to class normalization factor cache , and history and class id to sub vocabulary normalization factor cache .
b	680	[642, 597]	bahdanau et al . , first proposed the application of the attention mechanism to nlp tasks ( machine translation in this paper ) . // proved that the attention mechanism could improve the performance of rnnlms . // the attention mechanism obtains the target areas that need to be focused on by a set of attention coefficients for each input .
b	680	[707]	on the basis of the above improved attention mechanisms , some com [ petitive lms or methods of word representation are proposed . // transformer proposed by is the basis for the development of the subsequent models . // transformer is a novel structure based entirely on the attention mechanism , which consists of an encoder and a decoder .
b	680	[606]	transformer is a novel structure based entirely on the attention mechanism , which consists of an encoder and a decoder . // since then , gpt and bert have been proposed . // the main difference is that gpt uses transformer ’ s decoder , and bert uses transformer ’ s encoder .
b	680	[90]	hierarchical softmax some methods based on hierarchical softmax that decompose target conditional probability into multiples of some conditional probabilities are proposed . // used a hierarchical binary tree ( by the similarity from wordnet ) of an output layer , in which the v words in vocabulary are regarded as its leaves . // this technique allows exponentially fast calculations of word probabilities and their gradients .
b	680	[89]	however , it performs much worse than non hierarchical one despite using expert knowledge . // improved it by a simple feature based algorithm for automatically building word trees from data . // the performance of the above two models is mostly dependent on the tree , which is usually heuristically constructed .
b	680	[653]	since then , many scholars have improved this model . // hierarchical models based on word frequency classification and brown clustering were proposed . // it was proved that the model with brown clustering performed better .
b	680	[738]	experimental results showed that adopting importance sampling leads to ten times faster the training of nnlms without significantly increasing ppl . // proposed an adaptive importance sampling method using an adaptive n gram model instead of the simple unigram model . // other improvements have been proposed , such as parallel training of small models to estimate loss for importance sampling , multiple importance sampling and likelihood weighting scheme , two stage sampling , and so on .
b	746	[636, 721, 599, 662, 620]	// introduction unsupervised representation learning has been highly successful in the domain of natural language processing . // typically , these methods first pretrain neural networks on large scale unlabeled text corpora , and then finetune the models or representations on downstream tasks .
b	746	[636, 599, 662]	among them , autoregressive ( ar ) language modeling and autoencoding ( ae ) have been the two most successful pretraining objectives . // ar language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model . // specifically , given a text sequence x = ( x1 , · · · , xt ) , ar language modeling factorizes the likelihood into a forward product p ( x ) = qt t=1 p ( xt | x < t ) or a backward one p ( x ) = q1 t=t p ( xt | x > t ) .
h-	746	[615]	moreover , since the predicted tokens are masked in the input , bert is not able to model the joint probability using the product rule as in ar language modeling . // in other words , bert assumes the predicted tokens are independent of each other given the unmasked tokens , which is oversimplified as high order , long range dependency is prevalent in natural language . // faced with the pros and cons of existing language pretraining objectives , in this work , we propose xlnet , a generalized autoregressive method that leverages the best of both ar language modeling and ae while avoiding their limitations .
ho	746	[615]	in addition to a novel pretraining objective , xlnet improves architectural designs for pretraining . // inspired by the latest advancements in ar language modeling , xlnet integrates the segment recurrence mechanism and relative encoding scheme of transformer xl into pretraining , which empirically improves the performance especially for tasks involving a longer text sequence . // naively applying a transformer ( xl ) architecture to permutation based language modeling does not work because the factorization order is arbitrary and the target is ambiguous .
h-	746	[620]	empirically , xlnet achieves state of the art results on 18 tasks , i .e . , 7 glue language understanding tasks , 3 reading comprehension tasks including squad and race , 7 text classification tasks including yelp and imdb , and the clueweb09 b document ranking task . // under a set of fair comparison experiments , xlnet consistently outperforms bert on multiple benchmarks . // related work the idea of permutation based ar modeling has been explored in , but there are several key differences .
h-	746	[620]	input noise : the input to bert contains artificial symbols like that never occur in downstream tasks , which creates a pretrain finetune discrepancy . // replacing with original tokens as in does not solve the problem because original tokens can be only used with a small probability — otherwise eq . ( 2 ) will be trivial to optimize . //
ho	746	[602]	a natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses . // borrowing ideas from orderless nade , we propose the permutation language modeling objective that not only retains the benefits of ar models but also allows models to capture bidirectional contexts . // specifically , for a sequence x of length t , there are t ! different orders to perform a valid autoregressive factorization .
ho	746	[615]	for unselected tokens , their query representations need not be computed , which saves speed and memory . // incorporating ideas from transformer xl since our objective function fits in the ar framework , we incorporate the state of the art ar language model , transformer xl , into our pretraining framework , and name our method after it . // to avoid clutter , we omit the implementation details including multi head attention , residual connection , layer normalization and position wise feed forward as used in transformer ( xl ) .
h+	746	[615]	there are two benefits of using relative segment encodings . // first , the inductive bias of relative encodings improves generalization . // second , it opens the possibility of finetuning on tasks that have more than two input segments , which is not possible using absolute segment encodings .
ho	746	[640]	to prove a general point beyond one example , we now turn to more formal expressions . // inspired by previous work , given a sequence x = , we define a set of target context pairs of interest , i = { ( x , u ) } , where u is a set of tokens in x that form a context of x . // intuitively , we want the model to learn the dependency of x on u through a pretraining loss term log p ( x | u ) .
h-	746	[662]	in other words , the xlnet objective contains more effective training signals , which empirically leads to better performance in section 3 . // comparison with language modeling borrowing examples and notations from section 2 .6 .1 , a standard ar language model like gpt is only able to cover the dependency ( x = york , u = { new } ) but not ( x = new , u = { york } ) . // xlnet , on the other hand , is able to cover both in expectation over all factorization orders .
b	746	[691, 602, 710]	our single model outperforms the best ensemble by 7 .6 points in accuracy . // bridging the gap between language modeling and pretraining with a deep root in density estimation , language modeling has been a rapidly developing research area . // however , there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling , as analyzed in section 2 .6 .2 .
h-	746	[615, 659, 756]	our single model outperforms the best ensemble by 7 .6 points in accuracy . // 2 .6 .3 bridging the gap between language modeling and pretraining with a deep root in density estimation3 , language modeling has been a rapidly developing research area . // however , there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling , as analyzed in section 2 .6 .2 .
ho	746	[629]	as an example , we integrate transformer xl into xlnet to demonstrate the usefulness of the latest language modeling progress . // experiments 3 .1 pretraining and implementation following bert , we use the bookscorpus and english wikipedia as part of our pretraining data , which have 13gb plain text combined . // in addition , we include giga5 ( 16gb text ) , clueweb 2012 b ( extended from ) , and common crawl for pretraining .
ho	746	[612]	we use heuristics to aggressively filter out short or low quality articles for clueweb 2012 b and common crawl , which results in 19gb and 78gb text respectively . // after tokenization with sentencepiece , we obtain 2 .78b , 1 .09b , 4 .75b , 4 .30b , and 19 .97b subword pieces for wikipedia , bookscorpus , giga5 , clueweb , and common crawl respectively , which are 32 .89b in total . // our largest model xlnet large has the same architecture hyperparameters as bert large , which results in a similar model size .
b	746	[769]	all bert and xlnet results are obtained with a 24 layer architecture with similar model sizes ( aka bert large ) . // the race dataset contains near 100k questions taken from the english exams for middle and high school chinese students in the age range between 12 to 18 , with the answers generated by human experts . // this is one of the most difficult reading comprehension datasets that involve challenging reasoning questions .
b	746	[683]	this is one of the most difficult reading comprehension datasets that involve challenging reasoning questions . // moreover , the average length of the passages in race are longer than 300 , which is significantly longer than other popular reading comprehension datasets such as squad . // as a result , this dataset serves as a challenging benchmark for long text understanding .
b	746	[683]	squad is a large scale reading comprehension dataset with two tasks . // squad 1 .1 contains questions that always have a corresponding answer in the given passages , while squad 2 .0 introduces unanswerable questions . // to finetune an xlnet on squad 2 .0 , we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering .
b	746	[620]	squad 1 .1 contains questions that always have a corresponding answer in the given passages , while squad 2 .0 introduces unanswerable questions . // to finetune an xlnet on squad 2 .0 , we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering . // since v1 .1 and v2 .0 share the same answerable questions in the training set , we simply remove the answerability prediction part from the model finetuned on v2 .0 for evaluation on v1 .1 .
b	746	[694]	since v1 .1 and v2 .0 share the same answerable questions in the training set , we simply remove the answerability prediction part from the model finetuned on v2 .0 for evaluation on v1 .1 . // as the top leaderboard entries all employ some form of data augmentation , we jointly train an xlnet on squad 2 .0 and newsqa for our leaderboard submission . // as shown in table 2 , xlnet obtains the state of the art single model results on the leaderboard , outperforming a series of bert based methods .
ho	746	[702]	only single task training is employed for the four large datasets . // for qnli , we employed a pairwise relevance ranking scheme as in for our test set submission . // however , for fair comparison with bert , our result on the qnli dev set is based on a standard classification paradigm .
ho	746	[688]	however , for fair comparison with bert , our result on the qnli dev set is based on a standard classification paradigm . // for wnli , we use the loss described in . // a multi task ensemble xlnet achieves the state of the art results on 7 out of 9 tasks on the public leaderboard .
ho	746	[639]	all models are pretrained on the same data . // clueweb09 b dataset following the setting in previous work , we use the clueweb09 b dataset to evaluate the performance on document ranking . // the queries were created by the trec 2009 2012 web tracks based on 50m documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method .
h-	746	[650]	this illustrates that xlnet learns better low level word embeddings than bert . // note that for fair comparison we exclude the results in as it uses additional entity related data . // 3 .7 ablation study we perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics .
h-	606	[599, 618]	we introduce a new language representation model called bert , which stands for bidirectional encoder representations from transformers . // unlike recent language representation models , bert is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers . // as a result , the pre trained bert model can be finetuned with just one additional output layer to create state of the art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .
b	606	[636, 599, 618, 732]	it obtains new state of the art results on eleven natural language processing tasks , including pushing the glue score to 80 .5 % , multinli accuracy to 86 .7 % , squad v1 .1 question answering test f1 to 93 .2 and squad v2 .0 test f1 to 83 .1 . // language model pre training has been shown to be effective for improving many natural language processing tasks . // these include sentence level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token level tasks such as named entity recognition and question answering , where models are required to produce fine grained output at the token level .
b	606	[593, 591]	language model pre training has been shown to be effective for improving many natural language processing tasks . // these include sentence level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token level tasks such as named entity recognition and question answering , where models are required to produce fine grained output at the token level . // there are two existing strategies for applying pre trained language representations to downstream tasks : feature based and fine tuning .
b	606	[599]	there are two existing strategies for applying pre trained language representations to downstream tasks : feature based and fine tuning . // the feature based approach , such as elmo , uses task specific architectures that include the pre trained representations as additional features . // the fine tuning approach , such as the generative pre trained transformer ( openai gpt ) , introduces minimal task specific parameters , and is trained on the downstream tasks by simply fine tuning all pretrained parameters .
b	606	[618]	the feature based approach , such as elmo , uses task specific architectures that include the pre trained representations as additional features . // the fine tuning approach , such as the generative pre trained transformer ( openai gpt ) , introduces minimal task specific parameters , and is trained on the downstream tasks by simply fine tuning all pretrained parameters . // the two approaches share the same objective function during pre training , where they use unidirectional language models to learn general language representations .
b	606	[729, 706, 697]	there is a long history of pre training general language representations , and we briefly review the most widely used approaches . // unsupervised feature based approaches learning widely applicable representations of words has been an active area of research for decades , including non neural and neural methods . // pre trained word embeddings are an integral part of modern nlp systems , offering significant improvements over embeddings learned from scratch .
b	606	[717, 36, 167]	there is a long history of pre training general language representations , and we briefly review the most widely used approaches . // unsupervised feature based approaches learning widely applicable representations of words has been an active area of research for decades , including non neural and neural methods . // pre trained word embeddings are an integral part of modern nlp systems , offering significant improvements over embeddings learned from scratch .
b	606	[759]	unsupervised feature based approaches learning widely applicable representations of words has been an active area of research for decades , including non neural and neural methods . // pre trained word embeddings are an integral part of modern nlp systems , offering significant improvements over embeddings learned from scratch . // to pretrain word embedding vectors , left to right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context .
b	606	[735, 595, 663]	to pretrain word embedding vectors , left to right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context . // these approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings . // to train sentence representations , prior work has used objectives to rank candidate next sentences , left to right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .
b	606	[735, 595]	these approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings . // to train sentence representations , prior work has used objectives to rank candidate next sentences , left to right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives . // elmo and its predecessor generalize traditional word embedding research along a different dimension .
b	606	[692]	these approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings . // to train sentence representations , prior work has used objectives to rank candidate next sentences , left to right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives . // elmo and its predecessor generalize traditional word embedding research along a different dimension .
h+	606	[636, 732, 618]	unsupervised fine tuning approaches as with the feature based approaches , the first works in this direction only pre trained word embedding parameters from unlabeled text . // more recently , sentence or document encoders which produce contextual token representations have been pre trained from unlabeled text and fine tuned for a supervised downstream task . // the advantage of these approaches is that few parameters need to be learned from scratch .
h+	606	[618]	the advantage of these approaches is that few parameters need to be learned from scratch . // at least partly due to this advantage , openai gpt achieved previously state of the art results on many sentencelevel tasks from the glue benchmark . //
b	606	[636, 656]	transfer learning from supervised data there has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation . // computer vision research has also demonstrated the importance of transfer learning from large pre trained models , where an effective recipe is to fine tune models pre trained with imagenet . // we introduce bert and its detailed implementation in this section .
h-	606	[337]	in all of our experiments , we mask 15 % of all wordpiece tokens in each sequence at random . // in contrast to denoising auto encoders , we only predict the masked words rather than reconstructing the entire input . // although this allows us to obtain a bidirectional pre trained model , a downside is that we are creating a mismatch between pre training and fine tuning , since the token does not appear during fine tuning .
ho	606	[735, 595]	pre training data the pre training procedure largely follows the existing literature on language model pre training . // for the pre training corpus we use the bookscorpus ( 800m words ) and english wikipedia ( 2 ,500m words ) . // for wikipedia we extract only the text passages and ignore lists , tables , and headers .
b	606	[701]	experiments in this section , we present bert fine tuning results on 11 nlp tasks . // glue the general language understanding evaluation ( glue ) benchmark is a collection of diverse natural language understanding tasks . // detailed descriptions of glue datasets are included in appendix b .1 .
b	606	[719, 605, 627, 599, 599, 732]	we fine tune for 3 epochs with a learning rate of 5e 5 and a batch size of 32 . // table 2 shows top leaderboard entries as well as results from top published systems . // the top results from the squad leaderboard do not have up to date public system descriptions available ,11 and are allowed to use any public data when training their systems .
ho	606	[705]	the top results from the squad leaderboard do not have up to date public system descriptions available ,11 and are allowed to use any public data when training their systems . // we therefore use modest data augmentation in our system by first fine tuning on triviaqa befor fine tuning on squad . // our best performing system outperforms the top leaderboard system by +1 .5 f1 in ensembling and +1 .3 f1 as a single system .
b	606	[749, 755]	we fine tuned for 2 epochs with a learning rate of 5e 5 and a batch size of 48 . // the results compared to prior leaderboard entries and top published work are shown in table 3 , excluding systems that use bert as one of their components . // we observe a +5 .1 f1 improvement over the previous best system .
b	684	[147, 617]	the count based models are simple to train , but probabilities of rare n grams can be poorly estimated due to data sparsity ( despite smoothing techniques ) . // neural language models ( nlm ) address the n gram data sparsity issue through parameterization of words as vectors ( word embeddings ) and using them as inputs to a neural network . // the parameters are learned as part of the training process .
h+	684	[643]	character level convolutional neural network in our model , the input at time t is an output from a character level convolutional neural network ( charcnn ) , which we describe in this section . // cnns have achieved state of the art results on computer vision and have also been shown to be effective for various nlp tasks . //
h+	684	[306]	// cnns have achieved state of the art results on computer vision and have also been shown to be effective for various nlp tasks . // architectures employed for nlp applications differ in that they typically involve temporal rather than spatial convolutions .
b	684	[617]	related work neural language models ( nlm ) encompass a rich family of neural network architectures for language modeling . // some example architectures include feed forward , recurrent , sum product , log bilinear , and convolutional networks . // in order to address the rare word problem , alexandrescu and kirchhoff ( 2006 ) —building on analogous work on count based n gram language models by bilmes and kirchhoff ( 2003 ) —represent a word as a set of shared factor embeddings .
b	684	[646]	related work neural language models ( nlm ) encompass a rich family of neural network architectures for language modeling . // some example architectures include feed forward , recurrent , sum product , log bilinear , and convolutional networks . // in order to address the rare word problem , alexandrescu and kirchhoff ( 2006 ) —building on analogous work on count based n gram language models by bilmes and kirchhoff ( 2003 ) —represent a word as a set of shared factor embeddings .
b	684	[610]	related work neural language models ( nlm ) encompass a rich family of neural network architectures for language modeling . // some example architectures include feed forward , recurrent , sum product , log bilinear , and convolutional networks . // in order to address the rare word problem , alexandrescu and kirchhoff ( 2006 ) —building on analogous work on count based n gram language models by bilmes and kirchhoff ( 2003 ) —represent a word as a set of shared factor embeddings .
b	670	[623]	the ability to learn effectively from raw text is crucial to alleviating the dependence on supervised learning in natural language processing ( nlp ) . // most deep learning methods require substantial amounts of manually labeled data , which restricts their applicability in many domains that suffer from a dearth of annotated resources . // in these situations , models that can leverage linguistic information from unlabeled data provide a valuable alternative to gathering more annotation , which can be time consuming and expensive .
b	670	[125, 36, 167]	further , even in cases where considerable supervision is available , learning good representations in an unsupervised fashion can provide a significant performance boost . // the most compelling evidence for this so far has been the extensive use of pretrained word embeddings to improve performance on a range of nlp tasks . // leveraging more than word level information from unlabeled text , however , is challenging for two main reasons .
b	670	[679, 306, 307, 727]	further , even in cases where considerable supervision is available , learning good representations in an unsupervised fashion can provide a significant performance boost . // the most compelling evidence for this so far has been the extensive use of pretrained word embeddings to improve performance on a range of nlp tasks . // leveraging more than word level information from unlabeled text , however , is challenging for two main reasons .
b	670	[721]	first , it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer . // recent research has looked at various objectives such as language modeling , machine translation , and discourse coherence , with each method outperforming the others on different tasks . // second , there is no consensus on the most effective way to transfer these learned representations to the target task . existing techniques involve a combination of making task specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives .
b	670	[728]	first , it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer . // recent research has looked at various objectives such as language modeling , machine translation , and discourse coherence , with each method outperforming the others on different tasks . // second , there is no consensus on the most effective way to transfer these learned representations to the target task . existing techniques involve a combination of making task specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives .
h-	670	[741, 599]	second , there is no consensus on the most effective way to transfer these learned representations to the target task . // existing techniques involve a combination of making task specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives . // these uncertainties have made it difficult to develop effective semi supervised learning approaches for language processing .
h-	670	[625]	second , there is no consensus on the most effective way to transfer these learned representations to the target task . // existing techniques involve a combination of making task specific changes to the model architecture , using intricate learning schemes and adding auxiliary learning objectives . // these uncertainties have made it difficult to develop effective semi supervised learning approaches for language processing .
h+	670	[707]	subsequently , we adapt these parameters to a target task using the corresponding supervised objective . // for our model architecture , we use the transformer , document generation , and syntactic parsing . // this model choice provides us with a more structured memory for handling long term dependencies in text , compared to alternatives like recurrent networks , resulting in robust transfer performance across diverse tasks .
h+	670	[707]	subsequently , we adapt these parameters to a target task using the corresponding supervised objective . // for our model architecture , we use the transformer , document generation , and syntactic parsing . // this model choice provides us with a more structured memory for handling long term dependencies in text , compared to alternatives like recurrent networks , resulting in robust transfer performance across diverse tasks .
h+	670	[654]	subsequently , we adapt these parameters to a target task using the corresponding supervised objective . // for our model architecture , we use the transformer , which has been shown to perform strongly on various tasks such as machine translation , document generation , and syntactic parsing . // this model choice provides us with a more structured memory for handling long term dependencies in text , compared to alternatives like recurrent networks , resulting in robust transfer performance across diverse tasks .
ho	670	[611]	this model choice provides us with a more structured memory for handling long term dependencies in text , compared to alternatives like recurrent networks , resulting in robust transfer performance across diverse tasks . // during transfer , we utilize task specific input adaptations derived from traversal style approaches , which process structured text input as a single contiguous sequence of tokens . // as we demonstrate in our experiments , these adaptations enable us to fine tune effectively with minimal changes to the architecture of the pre trained model .
b	670	[730]	our general task agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task , significantly improving upon the state of the art in 9 out of the 12 tasks studied . // for instance , we achieve absolute improvements of 8 .9 % on commonsense reasoning ( stories cloze test ) , 5 .7 % on question answering ( race ) , 1 .5 % on textual entailment ( multinli ) and 5 .5 % on the recently introduced glue multi task benchmark . // we also analyzed zero shot behaviors of the pre trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks .
b	670	[601]	our general task agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task , significantly improving upon the state of the art in 9 out of the 12 tasks studied . // for instance , we achieve absolute improvements of 8 .9 % on commonsense reasoning ( stories cloze test ) , 5 .7 % on question answering ( race ) , 1 .5 % on textual entailment ( multinli ) and 5 .5 % on the recently introduced glue multi task benchmark . // we also analyzed zero shot behaviors of the pre trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks .
b	670	[701]	our general task agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task , significantly improving upon the state of the art in 9 out of the 12 tasks studied . // for instance , we achieve absolute improvements of 8 .9 % on commonsense reasoning ( stories cloze test ) , 5 .7 % on question answering ( race ) , 1 .5 % on textual entailment ( multinli ) and 5 .5 % on the recently introduced glue multi task benchmark . // we also analyzed zero shot behaviors of the pre trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks .
b	670	[634, 767, 780]	semi supervised learning for nlp our work broadly falls under the category of semi supervised learning for natural language . // this paradigm has attracted significant interest , with applications to tasks like sequence labeling or text classification . // the earliest approaches used unlabeled data to compute word level or phrase level statistics , which were then used as features in a supervised model .
b	670	[767]	this paradigm has attracted significant interest , with applications to tasks like sequence labeling or text classification . // the earliest approaches used unlabeled data to compute word level or phrase level statistics , which were then used as features in a supervised model . // over the last few years , researchers have demonstrated the benefits of using word embeddings , which are trained on unlabeled corpora , to improve performance on a variety of tasks .
b	670	[306, 36, 167]	the earliest approaches used unlabeled data to compute word level or phrase level statistics , which were then used as features in a supervised model . // over the last few years , researchers have demonstrated the benefits of using word embeddings , which are trained on unlabeled corpora , to improve performance on a variety of tasks . // these approaches , however , mainly transfer word level information , whereas we aim to capture higher level semantics .
b	670	[679, 306, 307, 727]	the earliest approaches used unlabeled data to compute word level or phrase level statistics , which were then used as features in a supervised model . // over the last few years , researchers have demonstrated the benefits of using word embeddings , which are trained on unlabeled corpora , to improve performance on a variety of tasks . // these approaches , however , mainly transfer word level information , whereas we aim to capture higher level semantics .
b	670	[478, 689, 337]	unsupervised pre training unsupervised pre training is a special case of semi supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective . // early works explored the use of the technique in image classification and regression tasks . // subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks .
b	670	[347]	unsupervised pre training unsupervised pre training is a special case of semi supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective . // early works explored the use of the technique in image classification and regression tasks . // subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks .
h+	670	[714]	early works explored the use of the technique in image classification and regression tasks . // subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks . // in recent work , the method has been used to help train deep neural networks on various tasks like image classification , speech recognition , entity disambiguation and machine translation .
h+	670	[752]	subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks . // in recent work , the method has been used to help train deep neural networks on various tasks like image classification , speech recognition , entity disambiguation and machine translation . // the closest line of work to ours involves pre training a neural network using a language modeling objective and then fine tuning it on a target task with supervision .
h+	670	[607]	subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks . // in recent work , the method has been used to help train deep neural networks on various tasks like image classification , speech recognition , entity disambiguation and machine translation . // the closest line of work to ours involves pre training a neural network using a language modeling objective and then fine tuning it on a target task with supervision .
h+	670	[708]	subsequent research demonstrated that pre training acts as a regularization scheme , enabling better generalization in deep neural networks . // in recent work , the method has been used to help train deep neural networks on various tasks like image classification , speech recognition , entity disambiguation and machine translation . // the closest line of work to ours involves pre training a neural network using a language modeling objective and then fine tuning it on a target task with supervision .
h-	670	[732]	// dai et al . and howard and ruder follow this method to improve text classification . // however , although the pre training phase helps capture some linguistic information , their usage of lstm models restricts their prediction ability to a short range .
ho	670	[625]	early work by collobert and weston used a wide variety of auxiliary nlp tasks such as pos tagging , chunking , named entity recognition , and language modeling to improve semantic role labeling . // more recently , rei added an auxiliary language modeling objective to their target task objective and demonstrated performance gains on sequence labeling tasks . // our experiments also use an auxiliary objective , but as we show , unsupervised pre training already learns several linguistic aspects relevant to target tasks .
ho	670	[773]	these parameters are trained using stochastic gradient descent . // in our experiments , we use a multi layer transformer decoder for the language model , which is a variant of the transformer . // this model applies a multi headed self attention operation over the input context tokens followed by position wise feedforward layers to produce an output distribution over target tokens : h0 = uwe + wp hl = transformer_block ( hl−1 ) ∀i ∈ p ( u ) = softmax ( hnwt e ) ( 2 ) where u = ( u−k , .
h-	670	[599]	since our pre trained model was trained on contiguous sequences of text , we require some modifications to apply it to these tasks . // previous work proposed learning task specific architectures on top of transferred representations . // such an approach re introduces a significant amount of task specific customization and does not use transfer learning for these additional architectural components .
ho	670	[611]	such an approach re introduces a significant amount of task specific customization and does not use transfer learning for these additional architectural components . // instead , we use a traversal style approach , where we convert structured inputs into an ordered sequence that our pre trained model can process . // these input transformations allow us to avoid making extensive changes to the architecture across tasks .
ho	670	[595]	each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers . // unsupervised pre training we use the bookscorpus dataset for training the language model . // it contains over 7 ,000 unique unpublished books from a variety of genres including adventure , fantasy , and romance .
b	670	[630]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[601]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[701]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[753]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[739]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[730]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[588]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[693]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
b	670	[471]	crucially , it contains long stretches of contiguous text , which allows the generative model to learn to condition on long range information . // an alternative dataset , the 1b word benchmark , which is used by a similar approach , elmo , is approximately the same size with natural language inference snli , multinli , question nli , rte , scitail question answering race , story cloze sentence similarity msr paraphrase corpus , quora question pairs , sts benchmark classification stanford sentiment treebank 2 , cola but is shuffled at a sentence level destroying long range structure . // our language model achieves a very low token level perplexity of 18 .4 on this corpus .
ho	670	[707]	our language model achieves a very low token level perplexity of 18 .4 on this corpus . // our model largely follows the original transformer work . // we trained a 12 layer decoder only transformer with masked self attention heads ( 768 dimensional states and 12 attention heads ) .
ho	670	[413]	for the position wise feed forward networks , we used 3072 dimensional inner states . // we used the adam optimization scheme with a max learning rate of 2 .5e 4 . // the learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule .
ho	670	[604]	since layernorm is used extensively throughout the model , a simple weight initialization of n ( 0 , 0 .02 ) was sufficient . // we used a bytepair encoding ( bpe ) vocabulary with 40 ,000 merges and residual , embedding , and attention dropouts with a rate of 0 .1 for regularization . // we also employed a modified version of l2 regularization proposed in , with w = 0 .01 on all non bias or gain weights .
ho	670	[701]	4 .2 supervised fine tuning we perform experiments on a variety of supervised tasks including natural language inference , question answering , semantic similarity , and text classification . // some of these tasks are available as part of the recently released glue multi task benchmark , which we make use of . // figure 1 provides an overview of all the tasks and datasets .
b	670	[609]	we use the recently released race dataset , consisting of english passages with associated questions from middle and high school exams . // this corpus has been shown to contain more reasoning type questions that other datasets like cnn or squad , providing the perfect evaluation for our model which is trained to handle long range contexts . // in addition , we evaluate on the story cloze test , which involves selecting the correct ending to multi sentence stories from two options .
b	670	[591]	we use the recently released race dataset , consisting of english passages with associated questions from middle and high school exams . // this corpus has been shown to contain more reasoning type questions that other datasets like cnn or squad , providing the perfect evaluation for our model which is trained to handle long range contexts . // in addition , we evaluate on the story cloze test , which involves selecting the correct ending to multi sentence stories from two options .
b	670	[471]	the corpus of linguistic acceptability ( cola ) contains expert judgements on whether a sentence is grammatical or not , and tests the innate linguistic bias of trained models . // the stanford sentiment treebank ( sst 2 ) , on the other hand , is a standard binary classification task . // our model obtains an score of 45 .4 on cola , which is an especially big jump over the previous best result of 35 .0 , showcasing the innate linguistic bias learned by our model .
b	599	[36, 167]	we also present an analysis showing that exposing the deep internals of the pre trained network is crucial , allowing downstream models to mix different types of semi supervision signals . // pre trained word representations are a key component in many neural language understanding models . // however , learning high quality representations can be challenging .
h-	599	[763, 721]	for this reason , we call them elmo ( embeddings from language models ) representations . // unlike previous approaches for learning contextualized word vectors , elmo representations are deep , in the sense that they are a function of all of the internal layers of the bilm . // more specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top lstm layer .
h-	599	[600, 36, 167]	our trained models and code are publicly available , and we expect that elmo will provide similar gains for many other nlp problems . // related work due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state ofthe art nlp architectures , including for question answering , textual entailment and semantic role labeling . // however , these approaches for learning word vectors only allow a single contextindependent representation for each word .
h-	599	[649, 626]	finally , an analysis of both elmo and cove reveals that deep representations outperform those derived from just the top layer of an lstm . our trained models and code are publicly available , and we expect that elmo will provide similar gains for many other nlp problems . // related work due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state ofthe art nlp architectures , including for question answering , textual entailment and semantic role labeling . // however , these approaches for learning word vectors only allow a single contextindependent representation for each word .
h-	599	[647]	finally , an analysis of both elmo and cove reveals that deep representations outperform those derived from just the top layer of an lstm . our trained models and code are publicly available , and we expect that elmo will provide similar gains for many other nlp problems . // related work due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state ofthe art nlp architectures , including for question answering , textual entailment and semantic role labeling . // however , these approaches for learning word vectors only allow a single contextindependent representation for each word .
h+	599	[651]	however , these approaches for learning word vectors only allow a single contextindependent representation for each word . // previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information or learning separate vectors for each word sense . // our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi sense information into downstream tasks without explicitly training to predict predefined sense classes .
b	599	[678]	other recent work has also focused on learning context dependent representations . // context2vec uses a bidirectional long short term memory ( lstm ) to encode the context around a pivot word . // other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( mt ) system or an unsupervised language model ( peters et al . , 2017 ) .
b	599	[721]	context2vec uses a bidirectional long short term memory ( lstm ) to encode the context around a pivot word . // other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( mt ) system or an unsupervised language model . // both of these approaches benefit from large datasets , although the mt approach is limited by the size of parallel corpora .
b	599	[677]	previous work has also shown that different layers of deep birnns encode different types of information . // for example , introducing multi task syntactic supervision ( e .g . , part of speech tags ) at the lower levels of a deep lstm can improve overall performance of higher level tasks such as dependency parsing or ccg super tagging . // in an rnn based encoder decoder machine translation system , belinkov et al .
b	599	[678]	it showed that the representations learned at the first layer in a 2 layer lstm encoder are better at predicting pos tags then second layer . // finally , the top layer of an lstm for encoding word context has been shown to learn representations of word sense . // we show that similar signals are also induced by the modified language model objective of our elmo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi supervision .
h-	599	[167]	in contrast , after pretraining the bilm with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal bilm representations for cases where downstream training data size dictates a smaller supervised model . // elmo embeddings from language models unlike most widely used word embeddings , elmo word representations are functions of the entire input sentence , as described in this section . // they are computed on top of two layer bilms with character convolutions ( sec .
ho	599	[763]	for inclusion in a downstream model , elmo collapses all layers in r into a single vector , elmo . // in the simplest case , elmo just selects the top layer as in taglm and cove . // more generally , we compute a task specific weighting of all bilm layers .
b	599	[768, 413, 723, 704, 594]	γ is of practical importance to aid the optimization process ( see supplemental material for details ) . // considering that the activations of each bilm layer have a different distribution , in some cases it also helped to apply layer normalization to each bilm layer before weighting . // using bilm for supervised nlp tasks given a pre trained bilm and a supervised architecture for a target nlp task , it is a simple process to use the bilm to improve the task model .
b	599	[681]	where a bi attention layer follows the bilstms , or the coreference resolution experiments where a clustering model is layered on top of the bilstms . // finally , we found it beneficial to add a moderate amount of dropout to elmo and in some cases to regularize the elmo weights by adding λkwk 2 2 to the loss . // this imposes an inductive bias on the elmo weights to stay close to an average of all bilm layers .
b	599	[591]	in the remainder of this section we provide high level sketches of the individual task results ; see the supplemental material for full experimental details . // the stanford question answering dataset ( squad ) contains 100k+ crowd sourced questionanswer pairs where the answer is a span in a given wikipedia paragraph . // our baseline model is an improved version of the bidirectional attention flow model in seo et al .
h-	599	[775]	overall , adding elmo to the esim model improves accuracy by an average of 0 .7 % across five random seeds . // a five member ensemble pushes the overall accuracy to 89 .3 % , exceeding the previous ensemble best of 88 .9 % . // semantic role labeling a semantic role labeling ( srl ) system models the predicate argument structure of a sentence , and is often described as answering “ who did what to whom ” .
b	599	[718]	named entity extraction the conll 2003 ner task consists of newswire from the reuters rcv1 corpus tagged with four different entity types ( per , loc , org , misc ) . // following recent state of the art systems , the baseline model uses pre trained word embeddings , a character based cnn representation , two bilstm layers and a conditional random field ( crf ) loss . //
b	167	[669]	semantic vector space models of language represent each word with a real valued vector . // these vectors can be used as features in a variety of applications , such as information retrieval , document classification , question answering , named entity recognition , and parsing . // most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations .
b	167	[759]	semantic vector space models of language represent each word with a real valued vector . // these vectors can be used as features in a variety of applications , such as information retrieval , document classification , question answering , named entity recognition , and parsing . // most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations .
b	167	[725, 712]	semantic vector space models of language represent each word with a real valued vector . // these vectors can be used as features in a variety of applications , such as information retrieval , document classification , question answering , named entity recognition , and parsing . // most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations .
h+	167	[711]	a main problem with hal and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure : the number of times two words co occur with the or and , for example , will have a large effect on their similarity despite conveying relatively little about their semantic relatedness . // a number of techniques exist that addresses this shortcoming of hal , such as the coals method , in which the co occurrence matrix is first transformed by an entropy or correlation based normalization . // an advantage of this type of transformation is that the raw co occurrence counts , which for a reasonably sized corpus might span 8 or 9 orders of magnitude , are compressed so as to be distributed more evenly in a smaller interval .
h+	167	[35]	we found that α = 3/4 gives a modest improvement over a linear version with α = 1 . // although we offer only empirical motivation for choosing the value 3/4 , it is interesting that a similar fractional power scaling was found to give the best performance in . // relationship to other models because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus , there should be commonalities between the models .
ho	167	[754]	this is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words relationship to one another . // for all our experiments , we set xmax = 100 , α = 3/4 , and train the model using adagrad , stochastically sampling nonzero elements from x , with initial learning rate of 0 .05 . // we run 50 iterations for vectors smaller than 300 dimensions , and 100 iterations otherwise ( see section 4 .6 for more details about the convergence rate ) .
h+	36	[778]	distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words . // one of the earliest use of word representations dates back to 1986 due to rumelhart , hinton , and williams . // this idea has since been applied to statistical language modeling with considerable success .
h+	36	[147]	one of the earliest use of word representations dates back to 1986 due to rumelhart , hinton , and williams . // this idea has since been applied to statistical language modeling with considerable success . // the follow up work includes applications to automatic speech recognition and machine translation , and a wide range of nlp tasks .
b	36	[657, 665]	this idea has since been applied to statistical language modeling with considerable success . // the follow up work includes applications to automatic speech recognition and machine translation , and a wide range of nlp tasks . // recently , mikolov et al . introduced the skip gram model , an efficient method for learning highquality vector representations of words from large amounts of unstructured text data .
b	36	[35]	the follow up work includes applications to automatic speech recognition and machine translation , and a wide range of nlp tasks . // recently , mikolov et al . introduced the skip gram model , an efficient method for learning highquality vector representations of words from large amounts of unstructured text data . // unlike most of the previously used neural network architectures for learning word vectors , training of the skipgram model ( see figure 1 ) does not involve dense matrix multiplications .
ho	36	[91]	we show that subsampling of frequent words during training results in a significant speedup ( around 2x 10x ) , and improves accuracy of the representations of less frequent words . // in addition , we present a simplified variant of noise contrastive estimation ( nce ) for training the skip gram model that results in faster training and better vector representations for frequent words , compared to more complex hierarchical softmax that was used in the prior work . // word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words .
ho	36	[35]	we show that subsampling of frequent words during training results in a significant speedup ( around 2x 10x ) , and improves accuracy of the representations of less frequent words . // in addition , we present a simplified variant of noise contrastive estimation ( nce ) for training the skip gram model that results in faster training and better vector representations for frequent words , compared to more complex hierarchical softmax that was used in the prior work . // word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words .
h+	36	[90]	hierarchical softmax a computationally efficient approximation of the full softmax is the hierarchical softmax . // in the context of neural network language models , it was first introduced by morin and bengio . // the main advantage is that instead of evaluating w output nodes in the neural network to obtain the probability distribution , it is needed to evaluate only about log2 ( w ) nodes .
h+	36	[89]	the structure of the tree used by the hierarchical softmax has a considerable effect on the performance . // mnih and hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy . // in our work we use a binary huffman tree , as it assigns short codes to the frequent words which results in fast training .
h+	36	[91]	it has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models . // negative sampling an alternative to the hierarchical softmax is noise contrastive estimation ( nce ) , which was introduced by gutmann and hyvarinen and applied to language modeling by mnih and teh . // nce posits that a good model should be able to differentiate data from noise by means of logistic regression .
h+	36	[125]	nce posits that a good model should be able to differentiate data from noise by means of logistic regression . // this is similar to hinge loss used by collobert and weston who trained the models by ranking the data above noise . // while nce can be shown to approximately maximize the log probability of the softmax , the skipgram model is only concerned with learning high quality vector representations , so we are free to simplify nce as long as the vector representations retain their quality .
ho	36	[35]	// it can be argued that the linearity of the skip gram model makes its vectors more suitable for such linear analogical reasoning , but the results of mikolov et al . also show that the vectors learned by the standard sigmoidal recurrent neural networks ( which are highly non linear ) improve on this task significantly as the amount of the training data increases , suggesting that non linear models also have a preference for a linear structure of the word representations . // learning phrases as discussed earlier , many phrases have a meaning that is not a simple composition of the meanings of its individual words .
ho	36	[35]	we show how to train distributed representations of words and phrases with the skip gram model and demonstrate that these representations exhibit linear structure that makes precise analogical reasoning possible . // the techniques introduced in this paper can be used also for training the continuous bag of words model introduced in . // we successfully trained models on several orders of magnitude more data than the previously published models , thanks to the computationally efficient model architecture .
b	733	[722]	language models ( lms ) play a crucial role in many speech and language processing tasks , among others speech recognition , machine translation and optical character recognition . // the current state of the art are recurrent neural network ( rnn ) based lms , and more specifically long short term memory models ( lstm ) lms and their variants ( e .g . gated recurrent units ( gru ) . // lstms and grus are usually very similar in performance , with gru models often even outperforming lstm models despite the fact that they have less parameters to train .
h-	733	[624]	// kim et al . achieve state of the art results in language modeling for several languages by combining a character level cnn with highway and lstm layers . // however , the major improvement is achieved by adding the highway layers : for a small model size , the purely character level model without highway layers does not perform better than the word level model , even though the character model has two hidden layers of 300 lstm units each and is compared to a word model of two hidden layers of only 200 units ( in order to keep the number of parameters similar ) .
h-	733	[684]	we can conclude that in various nlp tasks , characters have recently been introduced in several different manners . // however , the models investigated in related work are either not tested on a competitive baseline or do not perform better than our models . // in this paper , we introduce a new and straightforward manner to incorporate characters in a lm that ( as far as we know ) has not been investigated before .
ho	733	[390]	enabling the comparison between different models . // we adopt the same pre processing as used by previous work to facilitate comparison , which implies that the dataset contains only lowercase characters ( the size of the character vocabulary is 48 ) . // unknown words are mapped to hunki , but since we do not have the original text , we can not use the characters of the unknown words for ptb .
h-	777	[590, 720]	although the traditional approach for language modeling is good at capturing statistical co occurrences of entities when they are observed frequently in a corpus ( e .g . , words like verbs , pronouns , and prepositions ) , it is in general limited in its ability to encode or decode knowledge , which are often represented by named entities such as person names , place names , years , etc . // in fact , it has been shown that the traditional approaches trained with a very large corpus have some ability to encode/decode knowledge to some extent . // however , we claim that simply feeding a larger corpus into a bigger model hardly results in a good knowledge language model .
ho	777	[652]	however , we claim that simply feeding a larger corpus into a bigger model hardly results in a good knowledge language model . // the primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge related words . // in particular , for applications such as question answering and dialogue modeling , these words are of our main interest .
b	777	[590, 720, 592]	the primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge related words . // in particular , for applications such as question answering and dialogue modeling , these words are of our main interest . // specifically , in the recurrent neural network language model ( rnnlm ) the computational complexity is linearly dependent on the number of vocabulary words .
b	777	[674]	in particular , for applications such as question answering and dialogue modeling , these words are of our main interest . // specifically , in the recurrent neural network language model ( rnnlm ) the computational complexity is linearly dependent on the number of vocabulary words . // thus , including all words of a language is computationally prohibitive .
ho	777	[750]	in this paper , we propose a neural knowledge language model ( nklm ) as a step towards addressing the limitations of traditional language modeling when it comes to exploiting factual knowledge . // in particular , we incorporate symbolic knowledge information from knowledge graphs into the rnnlm . // a knowledge graph ( kg ) is a collection of facts which have a form of ( subject , relationship , object ) .
b	777	[671]	kgs are managed and updated in a similar way that wikipedia pages are managed to date . // the kg embedding methods , which is an extension of word embedding techniques from neural language models , provide distributed representations for the entities in the kg . // in addition , the graph can be traversed for reasoning .
b	777	[761, 685]	one option is to generate a “ vocabulary word ” from the vocabulary softmax as is in the rnnlm . // the other option is to generate a “ knowledge word ” by copying a word contained in the description of the predicted fact . // considering that the fact description is short and often consists of out of vocabulary words , we predict the position of the chosen word within the fact description .
ho	777	[660]	to this end , we introduce a new dataset , called wikifacts . // for each topic in the dataset , a set of facts from the freebase kg and a wikipedia description of the same topic is provided along with the alignment information . // this alignment is done automatically by performing string matching between the fact description and the wikipedia description .
b	777	[637]	related work language modeling is among the most important challenges in natural language processing and understanding . // beyond its usage as a standalone application , it has been an indispensable component in many language/speech tasks such as speech recognition , machine translation , and dialogue systems . // incorporating a language model into such downstream tasks leads to remarkable performance improvements , e .g . , by filtering out many grammatically correct but statistically unlikely outcomes .
b	777	[590, 720]	related work language modeling is among the most important challenges in natural language processing and understanding . // beyond its usage as a standalone application , it has been an indispensable component in many language/speech tasks such as speech recognition , machine translation , and dialogue systems . // incorporating a language model into such downstream tasks leads to remarkable performance improvements , e .g . , by filtering out many grammatically correct but statistically unlikely outcomes .
b	777	[671, 674]	incorporating a language model into such downstream tasks leads to remarkable performance improvements , e .g . , by filtering out many grammatically correct but statistically unlikely outcomes . // there have been remarkable advances in language modeling research based on neural networks . // in particular , the rnnlms are interesting for their ability to take advantage of longer term temporal dependencies without a strong conditional independence assumption .
b	777	[389]	it is especially 1we do not investigate the reasoning ability in this paper but highlight this example because explicit representation of facts would help handling such examples . // note worthy that the rnnlm using the long short term memory ( lstm ) has recently advanced to the level of outperforming carefully tuned large scale traditional n gram based language models . // there have been many efforts to speedup the language models so that it can cover a larger vocabulary .
h-	777	[90, 89]	there have been many efforts to speedup the language models so that it can cover a larger vocabulary . // these methods approximate the softmax output using hierarchical softmax , importance sampling , noise contrastive estimation , etc . // however , although helpful to mitigate the computational problem , this approaches still suffer from the statistical problem due to rare or unknown words .
h-	777	[764, 673]	there have been many efforts to speedup the language models so that it can cover a larger vocabulary . // these methods approximate the softmax output using hierarchical softmax , importance sampling , noise contrastive estimation , etc . // however , although helpful to mitigate the computational problem , this approaches still suffer from the statistical problem due to rare or unknown words .
h+	777	[761, 682]	having the unk word as the output of a generative language model is also inconvenient when the generated sentence is part of a user interface ( e .g , dialogue system ) . // to help deal with the rare/unknown word problem , the pointer networks have been adopted to implement the copy mechanism and applied to machine translation and text summarization . // with this approach , the ( unknown ) word to copy from the context is inferred from other ( known ) context words .
b	777	[672]	context dependent ( or topic based ) language models have been studied to better capture long term dependencies , by learning some context representation from the history . // modeled the topic as a latent variable and proposed an em based approach . // in , the topic features are learned by latent dirichlet allocation ( lda ) .
b	777	[698]	modeled the topic as a latent variable and proposed an em based approach . // in , the topic features are learned by latent dirichlet allocation ( lda ) . // our knowledge memory is also related to the recent literature on neural networks with external memory which is applied to question answering and reading comprehension .
b	777	[726, 737]	in , the topic features are learned by latent dirichlet allocation ( lda ) . // our knowledge memory is also related to the recent literature on neural networks with external memory which is applied to question answering and reading comprehension . // in , given simple sentences as facts which are stored in the external memory , the question answering task is studied .
b	777	[655, 757]	in , the topic features are learned by latent dirichlet allocation ( lda ) . // our knowledge memory is also related to the recent literature on neural networks with external memory which is applied to question answering and reading comprehension . // in , given simple sentences as facts which are stored in the external memory , the question answering task is studied .
b	777	[609, 758]	in , the topic features are learned by latent dirichlet allocation ( lda ) . // our knowledge memory is also related to the recent literature on neural networks with external memory which is applied to question answering and reading comprehension . // in , given simple sentences as facts which are stored in the external memory , the question answering task is studied .
