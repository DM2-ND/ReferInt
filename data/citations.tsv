Index	GLobal_Citing-ID	Global_Cited-ID	Citation_Content(GC)	Intention_Label	Citing_Paper_Title	Cited_Num_list	Cited_References_Title
1	587	[1]	Information networks are becoming ubiquitous across a large spectrum of real-world applications in forms of social networks, citation networks, telecommunication networks and biological networks, etc. // The scale of these networks ranges from hundreds to millions or even billions of vertices ==[1]==. // Analyzing information networks plays a crucial role in a variety of emerging applications across many disciplines.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
2	587	[2]	As a result, such a traditional routine often makes many network analytic tasks computationally expensive and intractable over largescale networks. // Taking community detection as an example, most existing algorithms involve calculating the spectral decomposition of a matrix ==[2]== with at least quadratic time complexity with respect to the number of vertices. // This computational overhead makes algorithms hard to scale to large-scale networks with millions of vertices.	h-	Network Representation Learning- A Survey	[2]	['[2]  F. D. Malliaros and M. Vazirgiannis, “Clustering and community detection in directed networks: A survey,” Physics Reports, vol. 533, no. 4, pp. 95–142, 2013. ']
3	587	[3]	The idea is to find a low-dimensional manifold structure hidden in the high-dimensional data geometry reflected by the constructed graph, so that connected nodes are kept closer to each other in the new embedding space. // Isomap ==[3]==, Locally Linear Embedding (LLE) [4] and Laplacian Eigenmap [5] are examples of algorithms based on this rationale. // However, graph embedding algorithms are designed on i.i.d. data mainly for dimensionality reduction purpose.	h-	Network Representation Learning- A Survey	[3]	['[3]  J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” Science, vol. 290, no. 5500, pp. 2319–2323, 2000. ']
4	587	[4]	The idea is to find a low-dimensional manifold structure hidden in the high-dimensional data geometry reflected by the constructed graph, so that connected nodes are kept closer to each other in the new embedding space. // Isomap [3], Locally Linear Embedding (LLE) ==[4]== and Laplacian Eigenmap [5] are examples of algorithms based on this rationale. // However, graph embedding algorithms are designed on i.i.d. data mainly for dimensionality reduction purpose.	h-	Network Representation Learning- A Survey	[4]	['[4]  S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by locally linear embedding,” Science, vol. 290, no. 5500, pp. 2323– 2326, 2000. ']
5	587	[5]	The idea is to find a low-dimensional manifold structure hidden in the high-dimensional data geometry reflected by the constructed graph, so that connected nodes are kept closer to each other in the new embedding space. // Isomap [3], Locally Linear Embedding (LLE) [4] and Laplacian Eigenmap ==[5]== are examples of algorithms based on this rationale. // However, graph embedding algorithms are designed on i.i.d. data mainly for dimensionality reduction purpose.	h-	Network Representation Learning- A Survey	[5]	['[5]  M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in Advances in Neural Information Processing Systems, 2002, pp. 585–591. ']
6	587	[6]	Since 2008, significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks. // Many NRL algorithms, e.g., ==[6]==, [7], [8], [9], have been proposed to embed existing networks, showing promising performance for various applications. // These algorithms embed a network into a latent, lowdimensional space that preserves structure proximity and attribute affinity, such that the original vertices of the network can be represented as low-dimensional vectors.	h+	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
7	587	[7]	Since 2008, significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks. // Many NRL algorithms, e.g., [6], ==[7]==, [8], [9], have been proposed to embed existing networks, showing promising performance for various applications. // These algorithms embed a network into a latent, lowdimensional space that preserves structure proximity and attribute affinity, such that the original vertices of the network can be represented as low-dimensional vectors.	h+	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
8	587	[8]	Since 2008, significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks. // Many NRL algorithms, e.g., [6], [7], ==[8]==, [9], have been proposed to embed existing networks, showing promising performance for various applications. // These algorithms embed a network into a latent, lowdimensional space that preserves structure proximity and attribute affinity, such that the original vertices of the network can be represented as low-dimensional vectors.	h+	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
9	587	[9]	Since 2008, significant research efforts have shifted tothe development of effective and scalable representation learning techniques that are directly designed for complex information networks. // Many NRL algorithms, e.g., [6], [7], [8], ==[9]==, have been proposed to embed existing networks, showing promising performance for various applications. // These algorithms embed a network into a latent, lowdimensional space that preserves structure proximity and attribute affinity, such that the original vertices of the network can be represented as low-dimensional vectors.	h+	Network Representation Learning- A Survey	[9]	['[9]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016, pp. 1145–1152. ']
10	587	[10]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification ==[10]==, [11], link prediction [12], [13], clustering [2], recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[10]	['[10]  S. Zhu, K. Yu, Y. Chi, and Y. Gong, “Combining content and link for classification using matrix factorization,” in Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2007, pp. 487–494. ']
11	587	[11]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], ==[11]==, link prediction [12], [13], clustering [2], recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[11]	['[11]  S. Bhagat, G. Cormode, and S. Muthukrishnan, “Node classification in social networks,” Social Network Data Analytics, pp. 115– 148, 2011. ']
12	587	[12]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction ==[12]==, [13], clustering [2], recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[12]	['[12]  L. Lu and T. Zhou, “Link prediction in complex networks: A ¨ survey,” Physica A: Statistical Mechanics and its Applications, vol. 390, no. 6, pp. 1150–1170, 2011. ']
13	587	[13]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], ==[13]==, clustering [2], recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[13]	['[13]  S. Gao, L. Denoyer, and P. Gallinari, “Temporal link prediction by integrating content and structure information,” in Proceedings of the 20th ACM International Conference on Information and Knowledge Management, 2011, pp. 1169–1174. ']
14	587	[2]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering ==[2]==, recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[2]	['[2]  F. D. Malliaros and M. Vazirgiannis, “Clustering and community detection in directed networks: A survey,” Physics Reports, vol. 533, no. 4, pp. 95–142, 2013. ']
15	587	[14]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering [2], recommendation ==[14]==, [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[14]	['[14]  C. Zhang, K. Zhang, Q. Yuan, H. Peng, Y. Zheng, T. Hanratty, S. Wang, and J. Han, “Regions, periods, activities: Uncovering urban dynamics via cross-modal representation learning,” in Proceedings of the 26th International Conference on World Wide Web, 2017, pp. 361–370. ']
16	587	[15]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering [2], recommendation [14], ==[15]==, similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[15]	['[15]  M. Xie, H. Yin, H. Wang, F. Xu, W. Chen, and S. Wang, “Learning graph-based POI embedding for location-based recommendation,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 15–24. ']
17	587	[16]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering [2], recommendation [14], [15], similarity search ==[16]==, and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[16]	['[16]  Z. Liu, V. W. Zheng, Z. Zhao, F. Zhu, K. C.-C. Chang, M. Wu, and J. Ying, “Distance-aware DAG embedding for proximity search on heterogeneous graphs,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2355–2362. ']
18	587	[17]	The resulting compact, low-dimensional vector representations can be then taken as features to any vector-based machine learning algorithms. // This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering [2], recommendation [14], [15], similarity search [16], and visualization ==[17]==. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search [18] in knowledge engineering and database systems.	b	Network Representation Learning- A Survey	[17]	['[17]  J. Tang, J. Liu, and Q. Mei, “Visualizing large-scale and highdimensional data,” in Proceedings of the 25th International Conference on World Wide Web, 2016, pp. 287–297. ']
19	587	[15]	This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering [2], recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing ==[15]==, and knowledge graph search [18] in knowledge engineering and database systems. // Challenges.	b	Network Representation Learning- A Survey	[15]	['[15]  M. Xie, H. Yin, H. Wang, F. Xu, W. Chen, and S. Wang, “Learning graph-based POI embedding for location-based recommendation,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 15–24. ']
20	587	[18]	This paves the way for a wide range of network analytic tasks to be easily and efficiently tackled in the new vector space, such as node classification [10], [11], link prediction [12], [13], clustering [2], recommendation [14], [15], similarity search [16], and visualization [17]. // Using vector representation to represent complex networks has now been gradually advanced to many other domains, such as point-of-interest recommendation in urban computing [15], and knowledge graph search ==[18]== in knowledge engineering and database systems. // Challenges.	b	Network Representation Learning- A Survey	[18]	['[18]  Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu, “Learning entity and relation embeddings for knowledge graph completion,” in Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2015, pp. 2181–2187. ']
21	587	[19]	To learn informative vertex representations, network representation learning should preserve network structure, such that vertices similar/close to each other in the original structure space should also be represented similarly in the learned vector space. // However, as stated in ==[19]==, [20], the structure-level similarity between vertices is reflected not only at the local neighborhood structure but also at the more global community structure. // Therefore, the local and global structure should be simultaneously preserved in network representation learning.	b	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
22	587	[20]	To learn informative vertex representations, network representation learning should preserve network structure, such that vertices similar/close to each other in the original structure space should also be represented similarly in the learned vector space. // However, as stated in [19], ==[20]==, the structure-level similarity between vertices is reflected not only at the local neighborhood structure but also at the more global community structure. // Therefore, the local and global structure should be simultaneously preserved in network representation learning.	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
23	587	[21]	A few graph embedding and representation learning related surveys exist in the recent literature. // The first is ==[21]==, which reviews a few representative methods for network representation learning and visits some key concepts aroundthe idea of representation learning and its connections to other related field such as dimensionality reduction, deep learning, and network science. // The other two surveys [22], [23] focus on reviewing graph embedding techniques aiming to preserve only network structure.	b	Network Representation Learning- A Survey	[21]	['[21]  L. G. Moyano, “Learning network representations,” The European Physical Journal Special Topics, vol. 226, no. 3, pp. 499–518, 2017. ']
24	587	[22]	The first is [21], which reviews a few representative methods for network representation learning and visits some key concepts aroundthe idea of representation learning and its connections to other related field such as dimensionality reduction, deep learning, and network science. // The other two surveys ==[22]==, [23] focus on reviewing graph embedding techniques aiming to preserve only network structure. // Recently, [24], [25] extend to cover work leveraging other side information, such as vertex attributes and/or vertex labels, to harness representation learning.	b	Network Representation Learning- A Survey	[22]	['[22]  P. Goyal and E. Ferrara, “Graph embedding techniques, applications, and performance: A survey,” Knowledge Based Systems, vol. 151, pp. 78–94, 2018. ']
25	587	[23]	The first is [21], which reviews a few representative methods for network representation learning and visits some key concepts aroundthe idea of representation learning and its connections to other related field such as dimensionality reduction, deep learning, and network science. // The other two surveys [22], ==[23]== focus on reviewing graph embedding techniques aiming to preserve only network structure. // Recently, [24], [25] extend to cover work leveraging other side information, such as vertex attributes and/or vertex labels, to harness representation learning.	b	Network Representation Learning- A Survey	[23]	['[23]  W. L. Hamilton, R. Ying, and J. Leskovec, “Representation learning on graphs: Methods and applications,” IEEE Data Engineering Bulletin, vol. 40, no. 3, pp. 52–74, 2017. ']
26	587	[24]	The other two surveys [22], [23] focus on reviewing graph embedding techniques aiming to preserve only network structure. // Recently, ==[24]==, [25] extend to cover work leveraging other side information, such as vertex attributes and/or vertex labels, to harness representation learning. // In summary, existing surveys have the following limitations.	b	Network Representation Learning- A Survey	[24]	['[24]  P. Cui, X. Wang, J. Pei, and W. Zhu, “A survey on network embedding,” IEEE Transactions on Knowledge and Data Engineering, 2018. ']
27	587	[25]	The other two surveys [22], [23] focus on reviewing graph embedding techniques aiming to preserve only network structure. // Recently, [24], ==[25]== extend to cover work leveraging other side information, such as vertex attributes and/or vertex labels, to harness representation learning. // In summary, existing surveys have the following limitations.	b	Network Representation Learning- A Survey	[25]	['[25]  H. Cai, V. W. Zheng, and K. C.-C. Chang, “A comprehensive survey of graph embedding: Problems, techniques and applications,” IEEE Transactions on Knowledge and Data Engineering, 2018. ']
28	587	[1]	Definition 2 (First-order Proximity). // The first-order proximity is the local pairwise proximity between two connected vertices ==[1]==. // For each vertex pair (vi , vj ), if (vi , vj ) ∈ E, the first-order proximity between vi and vj is wij ; otherwise, the first-order proximity between vi and vj is 0.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
29	587	[1]	Definition 3 (Second-order Proximity and High-order Proximity). // The second-order proximity captures the 2-step relations between each pair of vertices ==[1]==. // For each vertex pair (vi , vj ), the second order proximity is determined by the number of common neighbors shared by the two vertices, which can also be measured by the 2-step transition probability from vi to vj equivalently	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
30	587	[26]	For each vertex pair (vi , vj ), the second order proximity is determined by the number of common neighbors shared by the two vertices, which can also be measured by the 2-step transition probability from vi to vj equivalently. // Compared with the second-order proximity, the highorder proximity ==[26]== captures more global structure, which explores k-step (k ≥ 3) relations between each pair of vertices. // For each vertex pair (vi , vj ), the higherorder proximity is measured by the k-step (k ≥ 3) transition probability from vertex vi to vertex vj , which can also be reflected by the number of k-step (k ≥ 3) paths from vi to vj .	b	Network Representation Learning- A Survey	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “GraRep: Learning graph representations with global structural information,” in Proceedings of the 24th ACM International Conference on Information and Knowledge Management, 2015, pp. 891–900. ']
31	587	[27]	The intracommunity proximity is the pairwise proximity between vertices in a same community. // Many networks have community structure, where vertex-vertex connections within the same community are dense, but connections to vertices outside the community are sparse ==[27]==. // As cluster structure, a community preserves certain kinds of common properties of vertices within it.	b	Network Representation Learning- A Survey	[27]	['[27]  M. Girvan and M. E. Newman, “Community structure in social and biological networks,” Proceedings of the National Academy of Sciences, vol. 99, no. 12, pp. 7821–7826, 2002. ']
32	587	[27]	The intracommunity proximity is the pairwise proximity between vertices in a same community. // Many networks have community structure, where vertex-vertex connections within the same community are dense, but connections to vertices outside the community are sparse ==[27]==. // As cluster structure, a community preserves certain kinds of common properties of vertices within it.	b	Network Representation Learning- A Survey	[27]	['[27]  M. Girvan and M. E. Newman, “Community structure in social and biological networks,” Proceedings of the National Academy of Sciences, vol. 99, no. 12, pp. 7821–7826, 2002. ']
33	587	[7]	In addition to network structure, vertex attributes can provide direct evidence to measure contentlevel similarity between vertices. // As shown in ==[7]==, [20], [29], vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations. // vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations.	ho	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
34	587	[20]	In addition to network structure, vertex attributes can provide direct evidence to measure contentlevel similarity between vertices. // As shown in [7], ==[20]==, [29], vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations. // vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations.	ho	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
35	587	[28]	In addition to network structure, vertex attributes can provide direct evidence to measure contentlevel similarity between vertices. // As shown in [7], [20], ==[29]==, vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations. // vertex attributes and network structure can help each other filter out noisy information and compensate each other to jointly learn informative vertex representations.	ho	Network Representation Learning- A Survey	[29]	['[29]  S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted Boltzmann machine for linked data,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1753–1762. ']
36	587	[29]	Vertex labels provide direct information about the semantic categorization of each network vertex to certain classes or groups. // Vertex labels are strongly influenced by and inherently correlated to both network structure and vertex attributes ==[30]==. // Though vertex labels are usually partially observed, when coupled with network structure and vertex attributes, they encourage a network structure and vertex attribute consistent labeling, and help learn informative and discriminative vertex representations.	ho	Network Representation Learning- A Survey	[30]	['[30]  X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2017, pp. 731–739. ']
37	587	[7]	In general, there are three main types of information sources: network structure, vertex attributes, and vertex labels. // Most of the unsupervised NRL algorithms focus on preserving network structure for learning vertex representations, and only a few algorithms (e.g., TADW ==[7]==, HSCA [8]) attempt to leverage vertex attributes. // By contrast, under the semi-supervised learning setting, half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations.	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
38	587	[8]	In general, there are three main types of information sources: network structure, vertex attributes, and vertex labels. // Most of the unsupervised NRL algorithms focus on preserving network structure for learning vertex representations, and only a few algorithms (e.g., TADW [7], HSCA ==[8]==) attempt to leverage vertex attributes. // By contrast, under the semi-supervised learning setting, half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations.	b	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
39	587	[30]	Most of the unsupervised NRL algorithms focus on preserving network structure for learning vertex representations, and only a few algorithms (e.g., TADW [7], HSCA [8]) attempt to leverage vertex attributes. // By contrast, under the semi-supervised learning setting, half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations. On both settings, most of the algorithms focus on preserving microscopic structure, while very few algorithms (e.g., MNMF ==[28]==, DP [41], HARP [42]) attempt to take advantage of the mesoscopic and macroscopic structure. // Approaches to network representation learning in the above two different settings can be summarized into five categories from algorithmic perspectives.	b	Network Representation Learning- A Survey	[28]	['[28]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 203–209. ']
40	587	[31]	Most of the unsupervised NRL algorithms focus on preserving network structure for learning vertex representations, and only a few algorithms (e.g., TADW [7], HSCA [8]) attempt to leverage vertex attributes. // By contrast, under the semi-supervised learning setting, half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations. On both settings, most of the algorithms focus on preserving microscopic structure, while very few algorithms (e.g., MNMF [28], DP ==[41]==, HARP [42]) attempt to take advantage of the mesoscopic and macroscopic structure. // Approaches to network representation learning in the above two different settings can be summarized into five categories from algorithmic perspectives.	b	Network Representation Learning- A Survey	[41]	['[41]  R. Feng, Y. Yang, W. Hu, F. Wu, and Y. Zhuang, “Representation learning for scale-free networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 282–289. ']
41	587	[32]	Most of the unsupervised NRL algorithms focus on preserving network structure for learning vertex representations, and only a few algorithms (e.g., TADW [7], HSCA [8]) attempt to leverage vertex attributes. // By contrast, under the semi-supervised learning setting, half of the algorithms intend to couple vertex attributes with network structure and vertex labels to learn vertex representations. On both settings, most of the algorithms focus on preserving microscopic structure, while very few algorithms (e.g., MNMF [28], DP [41], HARP ==[42]==) attempt to take advantage of the mesoscopic and macroscopic structure. // Approaches to network representation learning in the above two different settings can be summarized into five categories from algorithmic perspectives.	b	Network Representation Learning- A Survey	[42]	['[42]  H. Chen, B. Perozzi, Y. Hu, and S. Skiena, “HARP: Hierarchical representation learning for networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2127–2134. ']
42	587	[7]	Matrix factorization based methods represent the connections between network vertices in the form of a matrix and use matrix factorization to obtain the embeddings. // Different types of matrices are constructed to preserve network structure, such as the k-step transition probability matrix, the modularity matrix, or the vertex-context matrix ==[7]==. // By assuming that such high-dimensional vertex representations are only affected by a small quantity of latent factors, matrix factorization is used to embed the highdimensional vertex representations into a latent, low-dimensional structure preserving space.	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
43	587	[33]	Factorization strategies vary across different algorithms according to their objectives. // For example, in the Modularity Maximization method ==[31]==, eigen decomposition is performed on the modularity matrix to learn community indicative vertex representations; in the TADW algorithm [7], inductive matrix factorization [54] is carried out on the vertexcontext matrix to simultaneously preserve vertex textual features and network structure in the learning of vertex representations. // Although matrix factorization based methods have been proved effective in learning informative vertex representations, the scalability is a major bottleneck because carrying out factorization on a matrix with millions of rows and columns is memory intensive and computationally expensive or, sometime, even infeasible.	h-	Network Representation Learning- A Survey	[31]	['[31]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009, pp. 817– 826. ']
44	587	[7]	Factorization strategies vary across different algorithms according to their objectives. // For example, in the Modularity Maximization method [31], eigen decomposition is performed on the modularity matrix to learn community indicative vertex representations; in the TADW algorithm ==[7]==, inductive matrix factorization [54] is carried out on the vertexcontext matrix to simultaneously preserve vertex textual features and network structure in the learning of vertex representations. // Although matrix factorization based methods have been proved effective in learning informative vertex representations, the scalability is a major bottleneck because carrying out factorization on a matrix with millions of rows and columns is memory intensive and computationally expensive or, sometime, even infeasible.	h-	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
45	587	[34]	Factorization strategies vary across different algorithms according to their objectives. // For example, in the Modularity Maximization method [31], eigen decomposition is performed on the modularity matrix to learn community indicative vertex representations; in the TADW algorithm [7], inductive matrix factorization ==[54]== is carried out on the vertexcontext matrix to simultaneously preserve vertex textual features and network structure in the learning of vertex representations. // Although matrix factorization based methods have been proved effective in learning informative vertex representations, the scalability is a major bottleneck because carrying out factorization on a matrix with millions of rows and columns is memory intensive and computationally expensive or, sometime, even infeasible.	h-	Network Representation Learning- A Survey	[54]	['[54]  N. Natarajan and I. S. Dhillon, “Inductive matrix completion for predicting gene–disease associations,” Bioinformatics, vol. 30, no. 12, pp. i60–i68, 2014. ']
46	587	[35]	By performing truncated random walks, an information network is transformed into a collection of vertex sequences, in which, the occurrence frequency of a vertex-context pair measures the structural distance between them. // Borrowing the idea of word representation learning ==[55]==, [56], vertex representations are then learned by using each vertex to predict its contexts. // DeepWalk [6] is the pioneer work in using random walks to learn vertex representations.	b	Network Representation Learning- A Survey	[55]	['[55]  T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013. ']
47	587	[36]	By performing truncated random walks, an information network is transformed into a collection of vertex sequences, in which, the occurrence frequency of a vertex-context pair measures the structural distance between them. // Borrowing the idea of word representation learning [55], ==[56]==, vertex representations are then learned by using each vertex to predict its contexts. // DeepWalk [6] is the pioneer work in using random walks to learn vertex representations.	b	Network Representation Learning- A Survey	[56]	['[56]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in Neural Information Processing Systems, 2013, pp. 3111–3119. ']
48	587	[6]	Borrowing the idea of word representation learning [55], [56], vertex representations are then learned by using each vertex to predict its contexts. // DeepWalk ==[6]== is the pioneer work in using random walks to learn vertex representations. // Node2vec [34] further exploits a biased random walk strategy to capture more global structure.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
49	587	[37]	DeepWalk [6] is the pioneer work in using random walks to learn vertex representations. // Node2vec ==[34]== further exploits a biased random walk strategy to capture more global structure. // As the extensions of the structure preserving only version, algorithms like DDRW [45], GENE [48] and SemNE [49] incorporate vertex labels with network structure to harness representation learning, PPNE [44] imports vertex attributes, and Tri-DNR [50] enforces the model with both vertex labels and attributes.	b	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
50	587	[38]	Node2vec [34] further exploits a biased random walk strategy to capture more global structure. // As the extensions of the structure preserving only version, algorithms like DDRW ==[45]==, GENE [48] and SemNE [49] incorporate vertex labels with network structure to harness representation learning, PPNE [44] imports vertex attributes, and Tri-DNR [50] enforces the model with both vertex labels and attributes. // As these models can be trained in an online manner, they have great potential to scale up.	b	Network Representation Learning- A Survey	[45]	['[45]  J. Li, J. Zhu, and B. Zhang, “Discriminative deep random walk for network classification,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, vol. 1, 2016, pp. 1004–1013. ']
51	587	[39]	Node2vec [34] further exploits a biased random walk strategy to capture more global structure. // As the extensions of the structure preserving only version, algorithms like DDRW [45], GENE ==[48]== and SemNE [49] incorporate vertex labels with network structure to harness representation learning, PPNE [44] imports vertex attributes, and Tri-DNR [50] enforces the model with both vertex labels and attributes. // As these models can be trained in an online manner, they have great potential to scale up.	b	Network Representation Learning- A Survey	[48]	['[48]  J. Chen, Q. Zhang, and X. Huang, “Incorporate group information to enhance network embedding,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1901–1904. ']
52	587	[40]	Node2vec [34] further exploits a biased random walk strategy to capture more global structure. // As the extensions of the structure preserving only version, algorithms like DDRW [45], GENE [48] and SemNE ==[49]== incorporate vertex labels with network structure to harness representation learning, PPNE [44] imports vertex attributes, and Tri-DNR [50] enforces the model with both vertex labels and attributes. // As these models can be trained in an online manner, they have great potential to scale up.	b	Network Representation Learning- A Survey	[49]	['[49]  C. Li, Z. Li, S. Wang, Y. Yang, X. Zhang, and J. Zhou, “Semisupervised network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 131–147. ']
53	587	[30]	Node2vec [34] further exploits a biased random walk strategy to capture more global structure. // As the extensions of the structure preserving only version, algorithms like DDRW [45], GENE [48] and SemNE [49] incorporate vertex labels with network structure to harness representation learning, PPNE ==[44]== imports vertex attributes, and Tri-DNR [50] enforces the model with both vertex labels and attributes. // As these models can be trained in an online manner, they have great potential to scale up.	b	Network Representation Learning- A Survey	[44]	['[44]  C. Li, S. Wang, D. Yang, Z. Li, Y. Yang, X. Zhang, and J. Zhou, “PPNE: Property preserving network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 163–179. ']
54	587	[41]	Node2vec [34] further exploits a biased random walk strategy to capture more global structure. // As the extensions of the structure preserving only version, algorithms like DDRW [45], GENE [48] and SemNE [49] incorporate vertex labels with network structure to harness representation learning, PPNE [44] imports vertex attributes, and Tri-DNR ==[50]== enforces the model with both vertex labels and attributes. // As these models can be trained in an online manner, they have great potential to scale up.	b	Network Representation Learning- A Survey	[50]	['[50]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 1895–1901. ']
55	587	[1]	Different from approaches that use matrix or random walk to capture network structure, the edge modeling based methods directly learn vertex representations from vertex-vertex connections. // For capturing the firstorder and second-order proximity, LINE ==[1]== models a joint probability distribution and a conditional probability distribution, respectively, on connected vertices. // To learn the representations of linked documents, LDE [51] models the document-document relationships by maximizing the conditional probability between connected documents.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
56	587	[42]	For capturing the firstorder and second-order proximity, LINE [1] models a joint probability distribution and a conditional probability distribution, respectively, on connected vertices. // To learn the representations of linked documents, LDE ==[51]== models the document-document relationships by maximizing the conditional probability between connected documents. // PRBM [29] adapts the RBM [57] model to linked data by making the hidden RBM representations of connected vertices similar to each other.	b	Network Representation Learning- A Survey	[51]	['[51]  S. Wang, J. Tang, C. Aggarwal, and H. Liu, “Linked document embedding for classification,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 115–124. ']
57	587	[28]	To learn the representations of linked documents, LDE [51] models the document-document relationships by maximizing the conditional probability between connected documents. // PRBM ==[29]== adapts the RBM [57] model to linked data by making the hidden RBM representations of connected vertices similar to each other. // GraphGAN [37] adopts Generative Adversarial Nets (GAN) [58] to accurately model the vertex connectivity probability.	b	Network Representation Learning- A Survey	[29]	['[29]  S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted Boltzmann machine for linked data,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1753–1762. ']
58	587	[43]	To learn the representations of linked documents, LDE [51] models the document-document relationships by maximizing the conditional probability between connected documents. // PRBM [29] adapts the RBM ==[57]== model to linked data by making the hidden RBM representations of connected vertices similar to each other. // GraphGAN [37] adopts Generative Adversarial Nets (GAN) [58] to accurately model the vertex connectivity probability.	b	Network Representation Learning- A Survey	[57]	['[57]  G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507, 2006. ']
59	587	[44]	PRBM [29] adapts the RBM [57] model to linked data by making the hidden RBM representations of connected vertices similar to each other. // GraphGAN ==[37]== adopts Generative Adversarial Nets (GAN) [58] to accurately model the vertex connectivity probability. // Edge modeling based methods are more efficient compared to matrix factorization and random walk based methods.	h+	Network Representation Learning- A Survey	[37]	['[37]  H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo, “GraphGAN: Graph representation learning with generative adversarial nets,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2508–2515. ']
60	587	[45]	PRBM [29] adapts the RBM [57] model to linked data by making the hidden RBM representations of connected vertices similar to each other. // GraphGAN [37] adopts Generative Adversarial Nets (GAN) ==[58]== to accurately model the vertex connectivity probability. // Edge modeling based methods are more efficient compared to matrix factorization and random walk based methods.	h+	Network Representation Learning- A Survey	[58]	['[58]  I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems, 2014, pp. 2672–2680. ']
61	587	[46]	Deep learning based methods. // To extract complex structure features and learn deep, highly nonlinear vertex representations, the deep learning techniques ==[59]==, [60] are also applied to network representation learning. // For example, DNGR [9] applies the stacked denoising autoencoders (SDAE) [60] on the high-dimensional matrix representations to learn deep low-dimensional vertex representations.	b	Network Representation Learning- A Survey	[59]	['[59]  R. Salakhutdinov and G. Hinton, “Semantic hashing,” International Journal of Approximate Reasoning, vol. 50, no. 7, pp. 969–978, 2009. ']
62	587	[47]	Deep learning based methods. // To extract complex structure features and learn deep, highly nonlinear vertex representations, the deep learning techniques [59], ==[60]== are also applied to network representation learning. // For example, DNGR [9] applies the stacked denoising autoencoders (SDAE) [60] on the high-dimensional matrix representations to learn deep low-dimensional vertex representations.	b	Network Representation Learning- A Survey	[60]	['[60]  P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,” Journal of Machine Learning Research, vol. 11, no. Dec, pp. 3371–3408, 2010. ']
63	587	[9]	To extract complex structure features and learn deep, highly nonlinear vertex representations, the deep learning techniques [59], [60] are also applied to network representation learning. // For example, DNGR ==[9]== applies the stacked denoising autoencoders (SDAE) [60] on the high-dimensional matrix representations to learn deep low-dimensional vertex representations. // SDNE [19] uses a semi-supervised deep autoencoder model [59] to model non-linearity in network structure.	b	Network Representation Learning- A Survey	[9]	['[9]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016, pp. 1145–1152. ']
64	587	[47]	To extract complex structure features and learn deep, highly nonlinear vertex representations, the deep learning techniques [59], [60] are also applied to network representation learning. // For example, DNGR [9] applies the stacked denoising autoencoders (SDAE) ==[60]== on the high-dimensional matrix representations to learn deep low-dimensional vertex representations. // SDNE [19] uses a semi-supervised deep autoencoder model [59] to model non-linearity in network structure.	b	Network Representation Learning- A Survey	[60]	['[60]  P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,” Journal of Machine Learning Research, vol. 11, no. Dec, pp. 3371–3408, 2010. ']
65	587	[19]	For example, DNGR [9] applies the stacked denoising autoencoders (SDAE) [60] on the high-dimensional matrix representations to learn deep low-dimensional vertex representations. // SDNE ==[19]== uses a semi-supervised deep autoencoder model [59] to model non-linearity in network structure. //  Deep learning based methods have theability to capture non-linearity in networks, but their computational time cost is usually high.	h-	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
66	587	[46]	For example, DNGR [9] applies the stacked denoising autoencoders (SDAE) [60] on the high-dimensional matrix representations to learn deep low-dimensional vertex representations. // SDNE [19] uses a semi-supervised deep autoencoder model ==[59]== to model non-linearity in network structure. //  Deep learning based methods have theability to capture non-linearity in networks, but their computational time cost is usually high.	h-	Network Representation Learning- A Survey	[59]	['[59]  R. Salakhutdinov and G. Hinton, “Semantic hashing,” International Journal of Approximate Reasoning, vol. 50, no. 7, pp. 969–978, 2009. ']
67	587	[31]	Hybrid methods. // Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP ==[41]== enhances spectral embedding [5] and DeepWalk [6] with the degree penalty principle to preserve the macroscopic scale-free property. // HARP [42] takes advantage of random walk based methods (DeepWalk [6] and node2vec [34]) and edge modeling based method (LINE [1]) to learn vertex representations from small sampled networks to the original network.	b	Network Representation Learning- A Survey	[41]	['[41]  R. Feng, Y. Yang, W. Hu, F. Wu, and Y. Zhuang, “Representation learning for scale-free networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 282–289. ']
68	587	[5]	Hybrid methods. // Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP [41] enhances spectral embedding ==[5]== and DeepWalk [6] with the degree penalty principle to preserve the macroscopic scale-free property. // HARP [42] takes advantage of random walk based methods (DeepWalk [6] and node2vec [34]) and edge modeling based method (LINE [1]) to learn vertex representations from small sampled networks to the original network.	b	Network Representation Learning- A Survey	[5]	['[5]  M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in Advances in Neural Information Processing Systems, 2002, pp. 585–591. ']
69	587	[6]	Hybrid methods. // Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP [41] enhances spectral embedding [5] and DeepWalk ==[6]== with the degree penalty principle to preserve the macroscopic scale-free property. // HARP [42] takes advantage of random walk based methods (DeepWalk [6] and node2vec [34]) and edge modeling based method (LINE [1]) to learn vertex representations from small sampled networks to the original network.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
70	587	[32]	Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP [41] enhances spectral embedding [5] and DeepWalk [6] with the degree penalty principle to preserve the macroscopic scale-free property. // HARP ==[42]== takes advantage of random walk based methods (DeepWalk [6] and node2vec [34]) and edge modeling based method (LINE [1]) to learn vertex representations from small sampled networks to the original network. // We summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in Table 3.	b	Network Representation Learning- A Survey	[42]	['[42]  H. Chen, B. Perozzi, Y. Hu, and S. Skiena, “HARP: Hierarchical representation learning for networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2127–2134. ']
71	587	[6]	Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP [41] enhances spectral embedding [5] and DeepWalk [6] with the degree penalty principle to preserve the macroscopic scale-free property. // HARP [42] takes advantage of random walk based methods (DeepWalk ==[6]== and node2vec [34]) and edge modeling based method (LINE [1]) to learn vertex representations from small sampled networks to the original network. // We summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in Table 3.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
72	587	[37]	Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP [41] enhances spectral embedding [5] and DeepWalk [6] with the degree penalty principle to preserve the macroscopic scale-free property. // HARP [42] takes advantage of random walk based methods (DeepWalk [6] and node2vec ==[34]==) and edge modeling based method (LINE [1]) to learn vertex representations from small sampled networks to the original network. // We summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in Table 3.	b	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
73	587	[1]	Some other methods make use of a mixture of above methods to learn vertex representations. For example, DP [41] enhances spectral embedding [5] and DeepWalk [6] with the degree penalty principle to preserve the macroscopic scale-free property. // HARP [42] takes advantage of random walk based methods (DeepWalk [6] and node2vec [34]) and edge modeling based method (LINE ==[1]==) to learn vertex representations from small sampled networks to the original network. // We summarize all five categories of network representation learning techniques and compare their advantages anddisadvantages in Table 3.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
74	587	[6]	DeepWalk. // DeepWalk ==[6]== generalizes the idea of the Skip-Gram model [55], [56] that utilizes word context in sentences to learn latent representations of words, to the learning of latent vertex representations in networks, by making an analogy between natural language sentence and short random walk sequence. // Given a random walk sequence with length L, {v1, v2, · · · , vL}, following SkipGram, DeepWalk learns the representation of vertex vi by using it to predict its context vertices, which is achieved by the optimization problem: min f − log Pr({vi−t, · · · , vi+t} \ vi|f(vi)), (1) where {vi−t, · · · , vi+t} \ vi are the context vertices of vertex vi within t window size	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
75	587	[35]	DeepWalk. // DeepWalk [6] generalizes the idea of the Skip-Gram model ==[55]==, [56] that utilizes word context in sentences to learn latent representations of words, to the learning of latent vertex representations in networks, by making an analogy between natural language sentence and short random walk sequence. // Given a random walk sequence with length L, {v1, v2, · · · , vL}, following SkipGram, DeepWalk learns the representation of vertex vi by using it to predict its context vertices, which is achieved by the optimization problem: min f − log Pr({vi−t, · · · , vi+t} \ vi|f(vi)), (1) where {vi−t, · · · , vi+t} \ vi are the context vertices of vertex vi within t window size	b	Network Representation Learning- A Survey	[55]	['[55]  T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013. ']
76	587	[36]	DeepWalk. // DeepWalk [6] generalizes the idea of the Skip-Gram model [55], ==[56]== that utilizes word context in sentences to learn latent representations of words, to the learning of latent vertex representations in networks, by making an analogy between natural language sentence and short random walk sequence. // Given a random walk sequence with length L, {v1, v2, · · · , vL}, following SkipGram, DeepWalk learns the representation of vertex vi by using it to predict its context vertices, which is achieved by the optimization problem: min f − log Pr({vi−t, · · · , vi+t} \ vi|f(vi)), (1) where {vi−t, · · · , vi+t} \ vi are the context vertices of vertex vi within t window size	b	Network Representation Learning- A Survey	[56]	['[56]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Advances in Neural Information Processing Systems, 2013, pp. 3111–3119. ']
77	587	[1]	Large-scale Information Network Embedding (LINE). // Instead of exploiting random walks to capture network structure, LINE ==[1]== learns vertex representations by explicitly modeling the first-order and second-order proximity. // To preserve the first-order proximity, LINE minimizes the following objective: O1 = d(ˆp1(·, ·), p1(·, ·)). (3) For each vertex pair vi and vj with (vi , vj ) ∈ E, p1(·, ·) is the joint distribution modeled by their latent embeddings rvi and rvj . pˆ1(vi , vj ) is the empirical distribution between them. d(·, ·) is the distance between two distributions.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
78	587	[6]	GraRep. // Following the idea of DeepWalk ==[6]==, GraRep [26] extends the skip-gram model to capture the high-order proximity, i.e., vertices sharing common k-step neighbors (k ≥ 1) should have similar latent representations. // Specifically, for each vertex, GraRep defines its kstep neighbors (k ≥ 1) as context vertices, and for each 1 ≤ k ≤ K, to learn k-step vertex representations, GraRep employs the matrix factorization version of skip-gram: h U k , Σ k , V k i = SV D(X k ). (5) where Xk is the log k-step transition probability matrix. The k-step representation for vertex vi is constructed as the ith row of matrix U k d (Σk d ) 1 2 , where U k d is the first-d columns of U k and Σ k d is the diagonal matrix composed of the top d singular values.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
79	587	[26]	GraRep. // Following the idea of DeepWalk [6], GraRep ==[26]== extends the skip-gram model to capture the high-order proximity, i.e., vertices sharing common k-step neighbors (k ≥ 1) should have similar latent representations. // Specifically, for each vertex, GraRep defines its kstep neighbors (k ≥ 1) as context vertices, and for each 1 ≤ k ≤ K, to learn k-step vertex representations, GraRep employs the matrix factorization version of skip-gram: h U k , Σ k , V k i = SV D(X k ). (5) where Xk is the log k-step transition probability matrix. The k-step representation for vertex vi is constructed as the ith row of matrix U k d (Σk d ) 1 2 , where U k d is the first-d columns of U k and Σ k d is the diagonal matrix composed of the top d singular values.	b	Network Representation Learning- A Survey	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “GraRep: Learning graph representations with global structural information,” in Proceedings of the 24th ACM International Conference on Information and Knowledge Management, 2015, pp. 891–900. ']
80	587	[9]	Deep Neural Networks for Graph Representations (DNGR). // To overcome the weakness of truncated random walks in exploiting vertex contextual information, i.e., the difficulty in capturing correct contextual information for vertices at the boundary of sequences and the difficulty in determining the walk length and the number of walks, DNGR ==[9]== utilizes the random surfing model to capture contextual relatedness between each pair of vertices and preserves them into |V |-dimensional vertex representations X. // To extract complex features and model non-linearities, DNGR applies the stacked denoising autoencoders (SDAE) [60] to the high-dimensional vertex representations X to learn deep low-dimensional vertex representations.	b	Network Representation Learning- A Survey	[9]	['[9]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016, pp. 1145–1152. ']
81	587	[47]	To overcome the weakness of truncated random walks in exploiting vertex contextual information, i.e., the difficulty in capturing correct contextual information for vertices at the boundary of sequences and the difficulty in determining the walk length and the number of walks, DNGR [9] utilizes the random surfing model to capture contextual relatedness between each pair of vertices and preserves them into |V |-dimensional vertex representations X. // To extract complex features and model non-linearities, DNGR applies the stacked denoising autoencoders (SDAE) ==[60]== to the high-dimensional vertex representations X to learn deep low-dimensional vertex representations. // Structural Deep Network Embedding (SDNE).	b	Network Representation Learning- A Survey	[60]	['[60]  P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,” Journal of Machine Learning Research, vol. 11, no. Dec, pp. 3371–3408, 2010. ']
82	587	[19]	Structural Deep Network Embedding (SDNE). // SDNE ==[19]== is a deep learning based approach that uses a semi-supervised deep autoencoder model to capture non-linearity in network structure. // In the unsupervised component, SDNE learns the second-order proximity preserving vertex representations via reconstructing the |V |- dimensional vertex adjacent matrix representations, which tries to minimize L2nd = X |V | i=1 k(r (0) vi − rˆ (0) vi )  bik, (6) where r (0) vi = Si: is the input representation and rˆ (0) vi is the reconstructed representation	b	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
83	587	[37]	node2vec. // In contrast to the rigid strategy of defining neighborhood (context) for each vertex, node2vec ==[34]== designs a flexible neighborhood sampling strategy, i.e., biased random walk, which smoothly interpolates between two extreme sampling strategies, i.e., Breadth-first Sampling (BFS) and Depth-first Sampling (DFS). // The biased random walk exploited in node2vec can better preserve both the secondorder and high-order proximity	h+	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
84	587	[48]	High-order Proximity Preserved Embedding (HOPE). // HOPE ==[35]== learns vertex representations that capture the asymmetric high-order proximity in directed networks. // In undirected networks, the transitivity is symmetric, but it is asymmetric in directed networks.	b	Network Representation Learning- A Survey	[35]	['[35]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1105–1114. ']
85	587	[49]	To preserve the asymmetric transitivity, HOPE learns two vertex embedding vectors U s , Ut ∈ R |V |×d , which is called source and target embedding vectors, respectively. // After constructing the high-order proximity matrix S from four proximity measures, i.e., Katz Index ==[61]==, Rooted PageRank [62], Common Neighbors and AdamicAdar. HOPE learns vertex embeddings by solving the following matrix factorization problem: min Us,Ut kS − U s · U tT k 2 F . // Asymmetric Proximity Preserving graph embedding (APP).	b	Network Representation Learning- A Survey	[61]	['[61]  L. Katz, “A new status index derived from sociometric analysis,” Psychometrika, vol. 18, no. 1, pp. 39–43, 1953. ']
86	587	[50]	To preserve the asymmetric transitivity, HOPE learns two vertex embedding vectors U s , Ut ∈ R |V |×d , which is called source and target embedding vectors, respectively. // After constructing the high-order proximity matrix S from four proximity measures, i.e., Katz Index [61], Rooted PageRank ==[62]==, Common Neighbors and AdamicAdar. HOPE learns vertex embeddings by solving the following matrix factorization problem: min Us,Ut kS − U s · U tT k 2 F . // Asymmetric Proximity Preserving graph embedding (APP).	b	Network Representation Learning- A Survey	[62]	['[62]  H. H. Song, T. W. Cho, V. Dave, Y. Zhang, and L. Qiu, “Scalable proximity estimation and link prediction in online social networks,” in Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement Conference, 2009, pp. 322–335. ']
87	587	[51]	After constructing the high-order proximity matrix S from four proximity measures, i.e., Katz Index [61], Rooted PageRank [62], Common Neighbors and AdamicAdar. HOPE learns vertex embeddings by solving the following matrix factorization problem: min Us,Ut kS − U s · U tT k 2 F . // Asymmetric Proximity Preserving graph embedding (APP). APP ==[36]== is another NRL algorithm designed to capture asymmetric proximity, by using a Monte Carlo approach to approximate the asymmetric Rooted PageRank proximity [62]. // Similar to HOPE, APP has two representations for each vertex vi, the one as a source role r s vi and the other as a target role rtvi.	b	Network Representation Learning- A Survey	[36]	['[36]  C. Zhou, Y. Liu, X. Liu, Z. Liu, and J. Gao, “Scalable graph embedding for asymmetric proximity,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 2942–2948. ']
88	587	[50]	After constructing the high-order proximity matrix S from four proximity measures, i.e., Katz Index [61], Rooted PageRank [62], Common Neighbors and AdamicAdar. HOPE learns vertex embeddings by solving the following matrix factorization problem: min Us,Ut kS − U s · U tT k 2 F . // Asymmetric Proximity Preserving graph embedding (APP). APP [36] is another NRL algorithm designed to capture asymmetric proximity, by using a Monte Carlo approach to approximate the asymmetric Rooted PageRank proximity ==[62]==. // Similar to HOPE, APP has two representations for each vertex vi, the one as a source role r s vi and the other as a target role rtvi.	b	Network Representation Learning- A Survey	[62]	['[62]  H. H. Song, T. W. Cho, V. Dave, Y. Zhang, and L. Qiu, “Scalable proximity estimation and link prediction in online social networks,” in Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement Conference, 2009, pp. 322–335. ']
89	587	[44]	GraphGAN. // GraphGAN ==[37]== learns vertex representations by modeling the connectivity behavior through an adversarial learning framework. // Inspired by GAN (Generative Adversarial Nets) [58], GraphGAN works through two components: (i) Generator G(v|vc), which fits the distribution of the vertices connected to vc across V and generates the likely connected vertices, and (ii) Discriminator D(v, vc), which outputs a connecting probability for the vertex pair (v, vc), to differentiate the vertex pairs generated by G(v|vc) from the ground truth.	b	Network Representation Learning- A Survey	[37]	['[37]  H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo, “GraphGAN: Graph representation learning with generative adversarial nets,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2508–2515. ']
90	587	[45]	GraphGAN [37] learns vertex representations by modeling the connectivity behavior through an adversarial learning framework. // Inspired by GAN (Generative Adversarial Nets) ==[58]==, GraphGAN works through two components: (i) Generator G(v|vc), which fits the distribution of the vertices connected to vc across V and generates the likely connected vertices, and (ii) Discriminator D(v, vc), which outputs a connecting probability for the vertex pair (v, vc), to differentiate the vertex pairs generated by G(v|vc) from the ground truth. // G(v|vc) and D(v, vc) competes in a way that G(v|vc) tries to fit the true connecting distribution as much as possible and generates fake connected vertex pairs to fool D(v, vc), while D(v, vc) tries to increase its discriminative power to distinguish the vertex pairs generated by G(v|vc) from the ground truth.	b	Network Representation Learning- A Survey	[58]	['[58]  I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems, 2014, pp. 2672–2680. ']
91	587	[1]	The proximity preserved by microscopic structure preserving NRL algorithms is summarized in Table 4. // Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE ==[1]==, SDNE [19] and GraphGAN [37] consider the first-order proximity. // From the methodology perspective, DeepWalk [6], node2vec [34] and APP [36] employ random walks to capture vertex neighborhood structure.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
92	587	[19]	The proximity preserved by microscopic structure preserving NRL algorithms is summarized in Table 4. // Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE [1], SDNE ==[19]== and GraphGAN [37] consider the first-order proximity. // From the methodology perspective, DeepWalk [6], node2vec [34] and APP [36] employ random walks to capture vertex neighborhood structure.	b	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
93	587	[44]	The proximity preserved by microscopic structure preserving NRL algorithms is summarized in Table 4. // Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE [1], SDNE [19] and GraphGAN ==[37]== consider the first-order proximity. // From the methodology perspective, DeepWalk [6], node2vec [34] and APP [36] employ random walks to capture vertex neighborhood structure.	b	Network Representation Learning- A Survey	[37]	['[37]  H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo, “GraphGAN: Graph representation learning with generative adversarial nets,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2508–2515. ']
94	587	[6]	Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE [1], SDNE [19] and GraphGAN [37] consider the first-order proximity. // From the methodology perspective, DeepWalk ==[6]==, node2vec [34] and APP [36] employ random walks to capture vertex neighborhood structure. // GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
95	587	[37]	Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE [1], SDNE [19] and GraphGAN [37] consider the first-order proximity. // From the methodology perspective, DeepWalk [6], node2vec ==[34]== and APP [36] employ random walks to capture vertex neighborhood structure. // GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up.	b	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
96	587	[51]	Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE [1], SDNE [19] and GraphGAN [37] consider the first-order proximity. // From the methodology perspective, DeepWalk [6], node2vec [34] and APP ==[36]== employ random walks to capture vertex neighborhood structure. // GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up.	b	Network Representation Learning- A Survey	[36]	['[36]  C. Zhou, Y. Liu, X. Liu, Z. Liu, and J. Gao, “Scalable graph embedding for asymmetric proximity,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 2942–2948. ']
97	587	[26]	From the methodology perspective, DeepWalk [6], node2vec [34] and APP [36] employ random walks to capture vertex neighborhood structure. // GraRep ==[26]== and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up. // LINE [1] and GraphGAN [37] directly model the connectivity behavior, while deep learning based methods (DNGR [9] and SDNE [19]) learn non-linear vertex representations.	h-	Network Representation Learning- A Survey	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “GraRep: Learning graph representations with global structural information,” in Proceedings of the 24th ACM International Conference on Information and Knowledge Management, 2015, pp. 891–900. ']
98	587	[48]	From the methodology perspective, DeepWalk [6], node2vec [34] and APP [36] employ random walks to capture vertex neighborhood structure. // GraRep [26] and HOPE ==[35]== are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up. // LINE [1] and GraphGAN [37] directly model the connectivity behavior, while deep learning based methods (DNGR [9] and SDNE [19]) learn non-linear vertex representations.	h-	Network Representation Learning- A Survey	[35]	['[35]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1105–1114. ']
99	587	[1]	GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up. // LINE ==[1]== and GraphGAN [37] directly model the connectivity behavior, while deep learning based methods (DNGR [9] and SDNE [19]) learn non-linear vertex representations. //  Structural Role Proximity Preserving NRL.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
100	587	[44]	GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up. // LINE [1] and GraphGAN ==[37]== directly model the connectivity behavior, while deep learning based methods (DNGR [9] and SDNE [19]) learn non-linear vertex representations. //  Structural Role Proximity Preserving NRL.	b	Network Representation Learning- A Survey	[37]	['[37]  H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, and M. Guo, “GraphGAN: Graph representation learning with generative adversarial nets,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2508–2515. ']
101	587	[9]	GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up. // LINE [1] and GraphGAN [37] directly model the connectivity behavior, while deep learning based methods (DNGR ==[9]== and SDNE [19]) learn non-linear vertex representations. //  Structural Role Proximity Preserving NRL.	b	Network Representation Learning- A Survey	[9]	['[9]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016, pp. 1145–1152. ']
102	587	[19]	GraRep [26] and HOPE [35] are realized by performing factorization on a |V | × |V | scale matrix, making them hard to scale up. // LINE [1] and GraphGAN [37] directly model the connectivity behavior, while deep learning based methods (DNGR [9] and SDNE ==[19]==) learn non-linear vertex representations. //  Structural Role Proximity Preserving NRL.	b	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
103	587	[52]	struct2vec. // struct2vec ==[38]== first encodes the vertex structural role similarity into a multilayer graph, where the weights of edges at each layer are determined by the structural role difference at the corresponding scale. // DeepWalk [6] is then performed on the multilayer graph to learn vertex representations, such that vertices close to each other in the multilayer graph (with high structural role similarity) are embedded closely in the new representation space.	b	Network Representation Learning- A Survey	[38]	['[38]  L. F. Ribeiro, P. H. Saverese, and D. R. Figueiredo, “struc2vec: Learning node representations from structural identity,” in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2017, pp. 385–394. ']
104	587	[6]	struct2vec [38] first encodes the vertex structural role similarity into a multilayer graph, where the weights of edges at each layer are determined by the structural role difference at the corresponding scale. // DeepWalk ==[6]== is then performed on the multilayer graph to learn vertex representations, such that vertices close to each other in the multilayer graph (with high structural role similarity) are embedded closely in the new representation space. // For each vertex pair (vi, vj ), considering their k-hop neighborhood formed by their neighbors within k steps, their structural distance at scale k, Dk(vi , vj ), is defined as Dk(vi, vj ) = Dk−1(vi, vj ) + g(s(Rk(vi)), s(Rk(vj ))), (14) where Rk(vi) is the set of vertices in its k-hop neighborhood, s(Rk(vi)) is the ordered degree sequence of the vertices in Rk(vi), and g(s(Rk(vi)), s(Rk(vj ))) is the distance between the ordered degree sequences s(Rk(vi)) and s(Rk(vj )).	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
105	587	[53]	GraphWave. // By making use of the spectral graph wavelet diffusion patterns, GraphWave ==[39]== embeds vertex neighborhood structure into a low-dimensional space and preserves the structural role proximity. // The assumption is that, if two vertices residing distantly in the network share similar structural roles, the graph wavelets starting at them will diffuse similarly across their neighbors.	b	Network Representation Learning- A Survey	[39]	['[39]  C. Donnat, M. Zitnik, D. Hallac, and J. Leskovec, “Spectral graph wavelets for structural role similarity in networks,” arXiv preprint arXiv:1710.10321, 2017. ']
106	587	[54]	Structural and Neighborhood Similarity preserving network embedding (SNS). // SNS ==[40]== enhances the random walk based method with structural role proximity. // To preserve vertex structural roles, SNS represents each vertex as a Graphlet Degree Vector with each element being the number of times the given vertex is touched by the corresponding orbit of graphlets.	b	Network Representation Learning- A Survey	[40]	['[40]  T. Lyu, Y. Zhang, and Y. Zhang, “Enhancing the network embedding quality with structural similarity,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 147–156. 24 ']
107	587	[33]	Thus, the problem boils down to one classical network analytic task—community detection—that aims to discover a set of communities with denser within-group connections than between-group connections. // Three clustering techniques, including modularity maximization ==[31]==, spectral clustering [32] and edge clustering [33] are employed to discover latent social dimensions. Each social dimension describes the likelihood of a vertex belonging to a plausible affiliation. //	b	Network Representation Learning- A Survey	[31]	['[31]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009, pp. 817– 826. ']
108	587	[55]	Thus, the problem boils down to one classical network analytic task—community detection—that aims to discover a set of communities with denser within-group connections than between-group connections. // Three clustering techniques, including modularity maximization [31], spectral clustering ==[32]== and edge clustering [33] are employed to discover latent social dimensions. Each social dimension describes the likelihood of a vertex belonging to a plausible affiliation. //	b	Network Representation Learning- A Survey	[32]	['[32]  ——, “Leveraging social media networks for classification,” Data Mining and Knowledge Discovery, vol. 23, no. 3, pp. 447–478, 2011. ']
109	587	[56]	Thus, the problem boils down to one classical network analytic task—community detection—that aims to discover a set of communities with denser within-group connections than between-group connections. // Three clustering techniques, including modularity maximization [31], spectral clustering [32] and edge clustering ==[33]== are employed to discover latent social dimensions. Each social dimension describes the likelihood of a vertex belonging to a plausible affiliation. //	b	Network Representation Learning- A Survey	[33]	['[33]  ——, “Scalable learning of collective behavior based on sparse social dimensions,” in Proceedings of the 18th ACM International Conference on Information and Knowledge Management, 2009, pp. 1107–1116. ']
110	587	[30]	Modularized Nonnegative Matrix Factorization (MNMF). // M-NMF ==[28]== augments the second-order and highorder proximity with broader community structure to learn more informative vertex embeddings U ∈ R |V |×d using the following objective: min M,U,H,C kS − MU T k 2 F + αkH − UCT k 2 F − βtr(H TBH) s.t., M ≥ 0, U ≥ 0, H ≥ 0, C ≥ 0, tr(H TH) = |V |, where vertex embedding U is learned by minimizing kS − MU Tk 2 F , with S ∈ R |V |×|V | being the vertex pairwise proximity matrix, which captures the second-order and the high-order proximity when taken as representations. //	b	Network Representation Learning- A Survey	[28]	['[28]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 203–209. ']
111	587	[33]	Summary. // The algorithms of learning latent social dimensions ==[31]==, [32], [33] only consider the community structure to learn vertex representation, while M-NMF [28] integrates microscopic structure preserving (the secondorder and high-order proximity) with the intra-community proximity. // These methods primarily rely on matrix factorization to detect community structure, which makes them hard to scale to large-scale networks.	h-	Network Representation Learning- A Survey	[31]	['[31]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009, pp. 817– 826. ']
112	587	[55]	Summary. // The algorithms of learning latent social dimensions [31], ==[32]==, [33] only consider the community structure to learn vertex representation, while M-NMF [28] integrates microscopic structure preserving (the secondorder and high-order proximity) with the intra-community proximity. // These methods primarily rely on matrix factorization to detect community structure, which makes them hard to scale to large-scale networks.	h-	Network Representation Learning- A Survey	[32]	['[32]  ——, “Leveraging social media networks for classification,” Data Mining and Knowledge Discovery, vol. 23, no. 3, pp. 447–478, 2011. ']
113	587	[56]	Summary. // The algorithms of learning latent social dimensions [31], [32], ==[33]== only consider the community structure to learn vertex representation, while M-NMF [28] integrates microscopic structure preserving (the secondorder and high-order proximity) with the intra-community proximity. // These methods primarily rely on matrix factorization to detect community structure, which makes them hard to scale to large-scale networks.	h-	Network Representation Learning- A Survey	[33]	['[33]  ——, “Scalable learning of collective behavior based on sparse social dimensions,” in Proceedings of the 18th ACM International Conference on Information and Knowledge Management, 2009, pp. 1107–1116. ']
114	587	[30]	Summary. // The algorithms of learning latent social dimensions [31], [32], [33] only consider the community structure to learn vertex representation, while M-NMF ==[28]== integrates microscopic structure preserving (the secondorder and high-order proximity) with the intra-community proximity. // These methods primarily rely on matrix factorization to detect community structure, which makes them hard to scale to large-scale networks.	h-	Network Representation Learning- A Survey	[28]	['[28]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 203–209. ']
115	587	[31]	Many real-world networks present the macroscopic scale-free property, which depicts the phenomenon that vertex degree follows a longtailed distribution, i.e., most vertices are sparsely connected and only few vertices have dense edges. // To capture the scale-free property, ==[41]== proposes the degree penalty principle (DP): penalizing the proximity between high-degree vertices. This principle is then coupled with two NRL algorithms (i.e., spectral embedding [5] and DeepWalk [6]) to learn scale-free property preserving vertex representations. // Hierarchical Representation Learning for Networks (HARP).	b	Network Representation Learning- A Survey	[41]	['[41]  R. Feng, Y. Yang, W. Hu, F. Wu, and Y. Zhuang, “Representation learning for scale-free networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 282–289. ']
116	587	[5]	Many real-world networks present the macroscopic scale-free property, which depicts the phenomenon that vertex degree follows a longtailed distribution, i.e., most vertices are sparsely connected and only few vertices have dense edges. // To capture the scale-free property, [41] proposes the degree penalty principle (DP): penalizing the proximity between high-degree vertices. This principle is then coupled with two NRL algorithms (i.e., spectral embedding ==[5]== and DeepWalk [6]) to learn scale-free property preserving vertex representations. // Hierarchical Representation Learning for Networks (HARP).	b	Network Representation Learning- A Survey	[5]	['[5]  M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in Advances in Neural Information Processing Systems, 2002, pp. 585–591. ']
117	587	[6]	Many real-world networks present the macroscopic scale-free property, which depicts the phenomenon that vertex degree follows a longtailed distribution, i.e., most vertices are sparsely connected and only few vertices have dense edges. // To capture the scale-free property, [41] proposes the degree penalty principle (DP): penalizing the proximity between high-degree vertices. This principle is then coupled with two NRL algorithms (i.e., spectral embedding [5] and DeepWalk ==[6]==) to learn scale-free property preserving vertex representations. // Hierarchical Representation Learning for Networks (HARP).	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
118	587	[32]	Hierarchical Representation Learning for Networks (HARP). // To capture the global patterns in networks, HARP ==[42]== samples small networks to approximate the global structure. // The vertex representations learned from sampled networks are taken as the initialization for inferring the vertex representations of the original network.	b	Network Representation Learning- A Survey	[42]	['[42]  H. Chen, B. Perozzi, Y. Hu, and S. Skiena, “HARP: Hierarchical representation learning for networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2127–2134. ']
119	587	[6]	To obtain smooth solutions, a series of smaller networks are successively sampled from the original network by coalescing edges and vertices, and the vertex representations are hierarchically inferred back from the smallest network to the original network. // In HARP, DeepWalk ==[6]== and LINE [1] are used to learn vertex representations. // Summary: DP [41] and HARP [42] are both realized by adapting the existing NRL algorithms to capture the macroscopic structure.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
120	587	[1]	To obtain smooth solutions, a series of smaller networks are successively sampled from the original network by coalescing edges and vertices, and the vertex representations are hierarchically inferred back from the smallest network to the original network. // In HARP, DeepWalk [6] and LINE ==[1]== are used to learn vertex representations. // Summary: DP [41] and HARP [42] are both realized by adapting the existing NRL algorithms to capture the macroscopic structure.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
121	587	[31]	In HARP, DeepWalk [6] and LINE [1] are used to learn vertex representations. // Summary: DP ==[41]== and HARP [42] are both realized by adapting the existing NRL algorithms to capture the macroscopic structure. // The former tries to preserve the scale-free property, while the latter makes the learned vertex representations respect the global network structure.	b	Network Representation Learning- A Survey	[41]	['[41]  R. Feng, Y. Yang, W. Hu, F. Wu, and Y. Zhuang, “Representation learning for scale-free networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 282–289. ']
122	587	[32]	In HARP, DeepWalk [6] and LINE [1] are used to learn vertex representations. // Summary: DP [41] and HARP ==[42]== are both realized by adapting the existing NRL algorithms to capture the macroscopic structure. // The former tries to preserve the scale-free property, while the latter makes the learned vertex representations respect the global network structure.	b	Network Representation Learning- A Survey	[42]	['[42]  H. Chen, B. Perozzi, Y. Hu, and S. Skiena, “HARP: Hierarchical representation learning for networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2127–2134. ']
123	587	[7]	Text-Associated DeepWalk (TADW). // TADW ==[7]== firstly proves the equivalence between DeepWalk [6] and the following matrix factorization: min W,H kM − WTHk 2 F + λ 2 (kWk 2 F + kHk 2 F ), (20) where W and H are learned latent embeddings and M is the vertex-context matrix carrying transition probability between each vertex pair within k steps. //	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
124	587	[6]	Text-Associated DeepWalk (TADW). // TADW [7] firstly proves the equivalence between DeepWalk ==[6]== and the following matrix factorization: min W,H kM − WTHk 2 F + λ 2 (kWk 2 F + kHk 2 F ), (20) where W and H are learned latent embeddings and M is the vertex-context matrix carrying transition probability between each vertex pair within k steps. //	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
125	587	[7]	Homophily, Structure, and Content Augmented Network Representation Learning (HSCA). // Despite its ability to incorporate textural features, TADW ==[7]== only considers structural context of network vertices, i.e., the second-order and high-order proximity, but ignores the important homophily property (the first-order proximity) in its learning framework. // HSCA [20] is proposed to simultaneously integrates homophily, structural context, and vertex content to learn effective network representations. For TADW, the learned representation for the i-th vertex vi is WT i ,(HTi:) T T , where Wi: and Ti: is the i-th row of W and T, respectively.	h-	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
126	587	[20]	Despite its ability to incorporate textural features, TADW [7] only considers structural context of network vertices, i.e., the second-order and high-order proximity, but ignores the important homophily property (the first-order proximity) in its learning framework. // HSCA ==[20]== is proposed to simultaneously integrates homophily, structural context, and vertex content to learn effective network representations. For TADW, the learned representation for the i-th vertex vi is WT i ,(HTi:) T T , where Wi: and Ti: is the i-th row of W and T, respectively. //	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
127	587	[43]	Paired Restricted Boltzmann Machine (pRBM). // By leveraging the strength of Restricted Boltzmann Machine (RBM) ==[57]==, [29] designs a novel model called Paired RBM (pRBM) to learn vertex representations by combining vertex attributes and link information. // The pRBM considers the networks with vertices associated with binary attributes. For each edge (vi , vj ) ∈ E, the attributes for vi and vj are v (i) and v (j) ∈ {0, 1} m, and their hidden representations are h (i) and h (j) ∈ {0, 1} d .	b	Network Representation Learning- A Survey	[57]	['[57]  G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507, 2006. ']
128	587	[28]	Paired Restricted Boltzmann Machine (pRBM). // By leveraging the strength of Restricted Boltzmann Machine (RBM) [57], ==[29]== designs a novel model called Paired RBM (pRBM) to learn vertex representations by combining vertex attributes and link information. // The pRBM considers the networks with vertices associated with binary attributes. For each edge (vi , vj ) ∈ E, the attributes for vi and vj are v (i) and v (j) ∈ {0, 1} m, and their hidden representations are h (i) and h (j) ∈ {0, 1} d .	b	Network Representation Learning- A Survey	[29]	['[29]  S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted Boltzmann machine for linked data,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1753–1762. ']
129	587	[57]	User Profile Preserving Social Network Embedding (UPP-SNE). // UPP-SNE ==[43]== leverages user profile features to enhance the embedding learning of users in social networks. Compared with textural content features, user profiles have two unique properties: (1) user profiles are noise, sparse and incomplete and (2) different dimensions of user profile features are topic-inconsistent. // To filter out noise and extract useful information from user profiles, UPP-SNE constructs user representations by performing a non-linear mapping on user profile features, which is guided by network structure.	b	Network Representation Learning- A Survey	[43]	['[43]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “User profile preserving social network embedding,” in Proceedings of the 26th International Joint Conference on Artificial Intelligence, 2017, pp. 3378–3384. ']
130	587	[58]	Compared with textural content features, user profiles have two unique properties: (1) user profiles are noise, sparse and incomplete and (2) different dimensions of user profile features are topic-inconsistent. // To filter out noise and extract useful information from user profiles, UPP-SNE constructs user representations by performing a non-linear mapping on user profile features, which is guided by network structure. The approximated kernel mapping ==[63]== is used in UPPSNE to construct user embedding from user profile features: f(vi) = ϕ(xi) = 1 √ d h cos(µ T 1 xi), · · · , cos(µ T d xi), sin(µ T 1 xi), · · · ,sin(µ T d xi) iT , (26) where xi is the user profile feature vector of vertex vi and µi is the corresponding coefficient vector. //	b	Network Representation Learning- A Survey	[63]	['[63]  A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” in Advances in neural information processing systems, 2008, pp. 1177–1184. ']
131	587	[6]	// To supervise the learning of the non-linear mapping and make user profiles and network structure complement each other, the objective of DeepWalk ==[6]== is used: min f − log Pr({vi−t, · · · , vi+t} \ vi|f(vi)), (27) where {vi−t, · · · , vi+t}\vi is the context vertices of vertex vi within t-window size in the given random walk sequence. // Property Preserving Network Embedding (PPNE)	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
132	587	[30]	Property Preserving Network Embedding (PPNE). // To learn content augmented vertex representations, PPNE ==[44]== jointly optimizes two objectives: (i) the structuredriven objective and (ii) the attribute-driven objective. // Following DeepWalk, the structure-driven objective aims to make vertices sharing similar context vertices represented closely.	b	Network Representation Learning- A Survey	[44]	['[44]  C. Li, S. Wang, D. Yang, Z. Li, Y. Yang, X. Zhang, and J. Zhou, “PPNE: Property preserving network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 163–179. ']
133	587	[7]	The above unsupervised content augmented NRL algorithms incorporate vertex content features in three ways. // The first, used by TADW ==[7]== and HSCA [20], is to couple the network structure with vertex content features via inductive matrix factorization [54]. //  This process can be considered as a linear transformation on vertex attributes constrained by network structure.	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
134	587	[20]	The above unsupervised content augmented NRL algorithms incorporate vertex content features in three ways. // The first, used by TADW [7] and HSCA ==[20]==, is to couple the network structure with vertex content features via inductive matrix factorization [54]. //  This process can be considered as a linear transformation on vertex attributes constrained by network structure.	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
135	587	[34]	The above unsupervised content augmented NRL algorithms incorporate vertex content features in three ways. // The first, used by TADW [7] and HSCA [20], is to couple the network structure with vertex content features via inductive matrix factorization ==[54]==. //  This process can be considered as a linear transformation on vertex attributes constrained by network structure.	b	Network Representation Learning- A Survey	[54]	['[54]  N. Natarajan and I. S. Dhillon, “Inductive matrix completion for predicting gene–disease associations,” Bioinformatics, vol. 30, no. 12, pp. i60–i68, 2014. ']
136	587	[43]	The second is to perform a non-linear mapping to construct new vertex embeddings that respect network structure. // For example, RBM ==[57]== and the approximated kernel mapping [63] is used by pRBM [29] and UPP-SNE [43], respectively, to achieve this goal. // The third used by PPNE [44] is to add an attribute preserving constraint to the structure preserving optimization objective.	b	Network Representation Learning- A Survey	[57]	['[57]  G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507, 2006. ']
137	587	[58]	The second is to perform a non-linear mapping to construct new vertex embeddings that respect network structure. // For example, RBM [57] and the approximated kernel mapping ==[63]== is used by pRBM [29] and UPP-SNE [43], respectively, to achieve this goal. // The third used by PPNE [44] is to add an attribute preserving constraint to the structure preserving optimization objective.	b	Network Representation Learning- A Survey	[63]	['[63]  A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” in Advances in neural information processing systems, 2008, pp. 1177–1184. ']
138	587	[28]	The second is to perform a non-linear mapping to construct new vertex embeddings that respect network structure. // For example, RBM [57] and the approximated kernel mapping [63] is used by pRBM ==[29]== and UPP-SNE [43], respectively, to achieve this goal. // The third used by PPNE [44] is to add an attribute preserving constraint to the structure preserving optimization objective.	b	Network Representation Learning- A Survey	[29]	['[29]  S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted Boltzmann machine for linked data,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1753–1762. ']
139	587	[57]	The second is to perform a non-linear mapping to construct new vertex embeddings that respect network structure. // For example, RBM [57] and the approximated kernel mapping [63] is used by pRBM [29] and UPP-SNE ==[43]==, respectively, to achieve this goal. // The third used by PPNE [44] is to add an attribute preserving constraint to the structure preserving optimization objective.	b	Network Representation Learning- A Survey	[43]	['[43]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “User profile preserving social network embedding,” in Proceedings of the 26th International Joint Conference on Artificial Intelligence, 2017, pp. 3378–3384. ']
140	587	[30]	For example, RBM [57] and the approximated kernel mapping [63] is used by pRBM [29] and UPP-SNE [43], respectively, to achieve this goal. // The third used by PPNE ==[44]== is to add an attribute preserving constraint to the structure preserving optimization objective. //  SEMI-SUPERVISED NETWORK REPRESENTATION LEARNING	b	Network Representation Learning- A Survey	[44]	['[44]  C. Li, S. Wang, D. Yang, Z. Li, Y. Yang, X. Zhang, and J. Zhou, “PPNE: Property preserving network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 163–179. ']
141	587	[59]	Discriminative Deep Random Walk (DDRW). // Inspired by the discriminative representation learning ==[64]==, [65], DDRW [45] proposes to learn discriminative network representations through jointly optimizing the objective of DeepWalk [6] together with the following L2-loss Support Vector Classification classification objective: Lc = C X |V | i=1 (σ(1 − Yikβ T rvi ))2 + 1 2 β T β, (30) where σ(x) = x, if x > 0 and otherwise σ(x) = 0. //	b	Network Representation Learning- A Survey	[64]	['[64]  J. Zhu, A. Ahmed, and E. P. Xing, “MedLDA: Maximum margin supervised topic models,” Journal of Machine Learning Research, vol. 13, no. Aug, pp. 2237–2278, 2012. ']
142	587	[60]	Discriminative Deep Random Walk (DDRW). // Inspired by the discriminative representation learning [64], ==[65]==, DDRW [45] proposes to learn discriminative network representations through jointly optimizing the objective of DeepWalk [6] together with the following L2-loss Support Vector Classification classification objective: Lc = C X |V | i=1 (σ(1 − Yikβ T rvi ))2 + 1 2 β T β, (30) where σ(x) = x, if x > 0 and otherwise σ(x) = 0. //	b	Network Representation Learning- A Survey	[65]	['[65]  J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. R. Bach, “Supervised dictionary learning,” in Advances in Neural Information Processing Systems, 2009, pp. 1033–1040. ']
143	587	[38]	Discriminative Deep Random Walk (DDRW). // Inspired by the discriminative representation learning [64], [65], DDRW ==[45]== proposes to learn discriminative network representations through jointly optimizing the objective of DeepWalk [6] together with the following L2-loss Support Vector Classification classification objective: Lc = C X |V | i=1 (σ(1 − Yikβ T rvi ))2 + 1 2 β T β, (30) where σ(x) = x, if x > 0 and otherwise σ(x) = 0. //	b	Network Representation Learning- A Survey	[45]	['[45]  J. Li, J. Zhu, and B. Zhang, “Discriminative deep random walk for network classification,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, vol. 1, 2016, pp. 1004–1013. ']
144	587	[6]	Discriminative Deep Random Walk (DDRW). // Inspired by the discriminative representation learning [64], [65], DDRW [45] proposes to learn discriminative network representations through jointly optimizing the objective of DeepWalk ==[6]== together with the following L2-loss Support Vector Classification classification objective: Lc = C X |V | i=1 (σ(1 − Yikβ T rvi ))2 + 1 2 β T β, (30) where σ(x) = x, if x > 0 and otherwise σ(x) = 0. //	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
145	587	[61]	Discriminative Deep Random Walk (DDRW). // DDRW is generalized to handle multi-class classification by using the one-against-rest strategy ==[66]==. // DeepWalk [6] together with the following L2-loss Support Vector Classification classification objective.	b	Network Representation Learning- A Survey	[66]	['[66]  R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “LIBLINEAR: A library for large linear classification,” Journal of Machine Learning Research, vol. 9, no. Aug, pp. 1871–1874, 2008. ']
146	587	[62]	Max-Margin DeepWalk (MMDW). // Similarly, MMDW ==[46]== couples the objective of the matrix factorization version DeepWalk [7] with the following multi-class Support Vector Machine objective with {(rv1 , Y1:), · · · ,(rvT , YT:)} training set: min W,ξ LSV M = min W,ξ 1 2 kWk 2 2 + C X T i=1 ξi , s.t. w T li rvi − w T j rvi ≥ e j i − ξi , ∀i, j, (32) where e j i = 1, if Yij = −1. Otherwise, e j i = 0, if Yij = 1. //	b	Network Representation Learning- A Survey	[46]	['[46]  C. Tu, W. Zhang, Z. Liu, and M. Sun, “Max-Margin DeepWalk: discriminative learning of network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 3889–3895. ']
147	587	[7]	Max-Margin DeepWalk (MMDW). // Similarly, MMDW [46] couples the objective of the matrix factorization version DeepWalk ==[7]== with the following multi-class Support Vector Machine objective with {(rv1 , Y1:), · · · ,(rvT , YT:)} training set: min W,ξ LSV M = min W,ξ 1 2 kWk 2 2 + C X T i=1 ξi , s.t. w T li rvi − w T j rvi ≥ e j i − ξi , ∀i, j, (32) where e j i = 1, if Yij = −1. Otherwise, e j i = 0, if Yij = 1. //	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
148	587	[63]	Transductive LINE (TLINE). // Along similar lines, TLINE ==[47]== is proposed as a semisupervised extension of LINE [1] that simultaneously learns LINE’s vertex representations and an SVM classifier. Given a set of labeled vertices {v1, v2, · · · , vL} and {vL+1, · · · , v|V |}, TLINE trains a multi-class SVM classifier on {v1, v2, · · · , vL} by optimizing the objective: Osvm = XL i=1 XK k=1 max(0, 1 − Yikwk T rvi ) + λkwkk 2 2. // Based on LINE’s formulations that preserve the firstorder and second-order proximity, TLINE optimizes two objective functions.	b	Network Representation Learning- A Survey	[47]	['[47]  X. Zhang, W. Chen, and H. Yan, “TLINE: scalable transductive network embedding,” in Information Retrieval Technology, 2016, pp. 98–110. ']
149	587	[1]	Transductive LINE (TLINE). // Along similar lines, TLINE [47] is proposed as a semisupervised extension of LINE ==[1]== that simultaneously learns LINE’s vertex representations and an SVM classifier. Given a set of labeled vertices {v1, v2, · · · , vL} and {vL+1, · · · , v|V |}, TLINE trains a multi-class SVM classifier on {v1, v2, · · · , vL} by optimizing the objective: Osvm = XL i=1 XK k=1 max(0, 1 − Yikwk T rvi ) + λkwkk 2 2. // Based on LINE’s formulations that preserve the firstorder and second-order proximity, TLINE optimizes two objective functions.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
150	587	[39]	Group Enhanced Network Embedding (GENE). // GENE ==[48]== integrates group (label) information with network structure in a probabilistic manner. // GENE assumes that vertices should be embedded closely in lowdimensional space, if they share similar neighbors or join similar groups.	b	Network Representation Learning- A Survey	[48]	['[48]  J. Chen, Q. Zhang, and X. Huang, “Incorporate group information to enhance network embedding,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1901–1904. ']
151	587	[6]	GENE assumes that vertices should be embedded closely in lowdimensional space, if they share similar neighbors or join similar groups. // Inspired by DeepWalk ==[6]== and document modeling [67], [68], the mechanism of GENE for learning group label informed vertex representations is achieved by maximizing the following log probability: L = X gi∈Y  α X W∈Wgi X vj∈W log Pr(vj |vj−t, · · · , vj+t, gi)+ β X vˆj∈Wˆ gj log Pr(ˆvj |gi)   , (37) where Y is the set of different groups, Wgi is the set of random walk sequences labeled with gi , Wˆ gi is the set of vertices randomly sampled from group gi. //	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
152	587	[64]	GENE assumes that vertices should be embedded closely in lowdimensional space, if they share similar neighbors or join similar groups. // Inspired by DeepWalk [6] and document modeling ==[67]==, [68], the mechanism of GENE for learning group label informed vertex representations is achieved by maximizing the following log probability: L = X gi∈Y  α X W∈Wgi X vj∈W log Pr(vj |vj−t, · · · , vj+t, gi)+ β X vˆj∈Wˆ gj log Pr(ˆvj |gi)   , (37) where Y is the set of different groups, Wgi is the set of random walk sequences labeled with gi , Wˆ gi is the set of vertices randomly sampled from group gi. //	b	Network Representation Learning- A Survey	[67]	['[67]  Q. Le and T. Mikolov, “Distributed representations of sentences and documents,” in Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1188–1196. ']
153	587	[65]	GENE assumes that vertices should be embedded closely in lowdimensional space, if they share similar neighbors or join similar groups. // Inspired by DeepWalk [6] and document modeling [67], ==[68]==, the mechanism of GENE for learning group label informed vertex representations is achieved by maximizing the following log probability: L = X gi∈Y  α X W∈Wgi X vj∈W log Pr(vj |vj−t, · · · , vj+t, gi)+ β X vˆj∈Wˆ gj log Pr(ˆvj |gi)   , (37) where Y is the set of different groups, Wgi is the set of random walk sequences labeled with gi , Wˆ gi is the set of vertices randomly sampled from group gi. //	b	Network Representation Learning- A Survey	[68]	['[68]  N. Djuric, H. Wu, V. Radosavljevic, M. Grbovic, and N. Bhamidipati, “Hierarchical neural language models for joint representation of streaming documents and their content,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 248–255. ']
154	587	[40]	Semi-supervised Network Embedding (SemiNE). // SemiNE ==[49]== learns semi-supervised vertex representations in two stages. // In the first stage, SemiNE exploits the DeepWalk [6] framework to learn vertex representations in an unsupervised manner	b	Network Representation Learning- A Survey	[49]	['[49]  C. Li, Z. Li, S. Wang, Y. Yang, X. Zhang, and J. Zhou, “Semisupervised network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 131–147. ']
155	587	[41]	Tri-Party Deep Network Representation (TriDNR). // Using a coupled neural network framework, TriDNR ==[50]== learns vertex representations from three information sources: network structure, vertex content and vertex labels. // To capture the vertex content and label information, TriDNR adapts the Paragraph Vector model [67] to describe the vertex-word correlation and the label-word correspondence by maximizing the following objective: LP V = X i∈L log Pr(w−b : wb|ci) +X |V | i=1 log Pr(w−b : wb|vi), (39) where {w−b : wb} is a sequence of words inside a contextual window of length 2b, ci is the class label of vertex vi , and L is the set of indices of labeled vertices.	b	Network Representation Learning- A Survey	[50]	['[50]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 1895–1901. ']
156	587	[64]	Using a coupled neural network framework, TriDNR [50] learns vertex representations from three information sources: network structure, vertex content and vertex labels. // To capture the vertex content and label information, TriDNR adapts the Paragraph Vector model ==[67]== to describe the vertex-word correlation and the label-word correspondence by maximizing the following objective: LP V = X i∈L log Pr(w−b : wb|ci) +X |V | i=1 log Pr(w−b : wb|vi). //(39) where {w−b : wb} is a sequence of words inside a contextual window of length 2b, ci is the class label of vertex vi , and L is the set of indices of labeled vertices.	b	Network Representation Learning- A Survey	[67]	['[67]  Q. Le and T. Mikolov, “Distributed representations of sentences and documents,” in Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1188–1196. ']
157	587	[42]	Linked Document Embedding (LDE). // LDE ==[51]== is proposed to learn representations for linked documents, which are actually the vertices of citation or webpage networks. // Similar to TriDNR [50], LDE learns vertex representations by modeling three kinds of relations, i.e., word-word-document relations, document-document relations, and document-label relations.	b	Network Representation Learning- A Survey	[51]	['[51]  S. Wang, J. Tang, C. Aggarwal, and H. Liu, “Linked document embedding for classification,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 115–124. ']
158	587	[41]	LDE [51] is proposed to learn representations for linked documents, which are actually the vertices of citation or webpage networks. // Similar to TriDNR ==[50]==, LDE learns vertex representations by modeling three kinds of relations, i.e., word-word-document relations, document-document relations, and document-label relations. // LDE is realized by solving the following optimization problem:	b	Network Representation Learning- A Survey	[50]	['[50]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 1895–1901. ']
159	587	[8]	Discriminative Matrix Factorization (DMF). // To empower vertex representations with discriminative ability, DMF ==[8]== enforces the objective of TADW (21) with an empirical loss minimization for a linear classifier trained on labeled vertices: min W,H,η 1 2 X |V | i,j=1 (Mij − w T i Htj ) 2 + µ 2 X n∈L (Yn1 − η Txn) 2 + λ1 2 (kHk 2 F + kηk 2 2) + λ2 2 kWk 2 F , (42) where wi is the i-th column of vertex representation matrix W and tj is j-th column of vertex textual feature matrix T, and L is the set of indices of labeled vertices. //	b	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
160	587	[66]	Predictive Labels And Neighbors with Embeddings Transductively Or Inductively from Data (Planetoid). // Planetoid ==[52]== leverages network embedding together with vertex attributes to carry out semi-supervised learning. // Planetoid learns vertex embeddings by minimizing the loss for predicting structural context, which is formulated as	b	Network Representation Learning- A Survey	[52]	['[52]  Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semisupervised learning with graph embeddings,” in Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML), 2016, pp. 40–48. ']
161	587	[38]	We now summarize and compare the discriminative learning strategies used by semi-supervised NRL algorithms in Table 5 in terms of their advantages and disadvantages. // Three strategies are used to achieve discriminative learning. The first strategy (i.e., DDRW ==[45]==, MMDW [46], TLINE [47], DMF [8], SemiNE [49]) is to enforce classification loss minimization on vertex representations, i.e., fitting the vertex representations to a classifier. // This provides a direct way to separate vertices of different categories from each other in the new embedding space.	b	Network Representation Learning- A Survey	[45]	['[45]  J. Li, J. Zhu, and B. Zhang, “Discriminative deep random walk for network classification,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, vol. 1, 2016, pp. 1004–1013. ']
162	587	[62]	We now summarize and compare the discriminative learning strategies used by semi-supervised NRL algorithms in Table 5 in terms of their advantages and disadvantages. // Three strategies are used to achieve discriminative learning. The first strategy (i.e., DDRW [45], MMDW ==[46]==, TLINE [47], DMF [8], SemiNE [49]) is to enforce classification loss minimization on vertex representations, i.e., fitting the vertex representations to a classifier. // This provides a direct way to separate vertices of different categories from each other in the new embedding space.	b	Network Representation Learning- A Survey	[46]	['[46]  C. Tu, W. Zhang, Z. Liu, and M. Sun, “Max-Margin DeepWalk: discriminative learning of network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 3889–3895. ']
163	587	[63]	We now summarize and compare the discriminative learning strategies used by semi-supervised NRL algorithms in Table 5 in terms of their advantages and disadvantages. // Three strategies are used to achieve discriminative learning. The first strategy (i.e., DDRW [45], MMDW [46], TLINE ==[47]==, DMF [8], SemiNE [49]) is to enforce classification loss minimization on vertex representations, i.e., fitting the vertex representations to a classifier. // This provides a direct way to separate vertices of different categories from each other in the new embedding space.	b	Network Representation Learning- A Survey	[47]	['[47]  X. Zhang, W. Chen, and H. Yan, “TLINE: scalable transductive network embedding,” in Information Retrieval Technology, 2016, pp. 98–110. ']
164	587	[8]	We now summarize and compare the discriminative learning strategies used by semi-supervised NRL algorithms in Table 5 in terms of their advantages and disadvantages. // Three strategies are used to achieve discriminative learning. The first strategy (i.e., DDRW [45], MMDW [46], TLINE [47], DMF ==[8]==, SemiNE [49]) is to enforce classification loss minimization on vertex representations, i.e., fitting the vertex representations to a classifier. // This provides a direct way to separate vertices of different categories from each other in the new embedding space.	b	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
165	587	[40]	We now summarize and compare the discriminative learning strategies used by semi-supervised NRL algorithms in Table 5 in terms of their advantages and disadvantages. // Three strategies are used to achieve discriminative learning. The first strategy (i.e., DDRW [45], MMDW [46], TLINE [47], DMF [8], SemiNE ==[49]==) is to enforce classification loss minimization on vertex representations, i.e., fitting the vertex representations to a classifier. // This provides a direct way to separate vertices of different categories from each other in the new embedding space.	b	Network Representation Learning- A Survey	[49]	['[49]  C. Li, Z. Li, S. Wang, Y. Yang, X. Zhang, and J. Zhou, “Semisupervised network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 131–147. ']
166	587	[39]	This provides a direct way to separate vertices of different categories from each other in the new embedding space. // The second strategy (used by GENE ==[48]==, TriDNR [50], LDE [51] and Planetoid [52]) is achieved by modeling vertex label relation, such that vertices with same labels have similar vector representations. // The third strategy used by LANE [30] is to jointly embed vertices and labels into a common space.	b	Network Representation Learning- A Survey	[48]	['[48]  J. Chen, Q. Zhang, and X. Huang, “Incorporate group information to enhance network embedding,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1901–1904. ']
167	587	[41]	This provides a direct way to separate vertices of different categories from each other in the new embedding space. // The second strategy (used by GENE [48], TriDNR ==[50]==, LDE [51] and Planetoid [52]) is achieved by modeling vertex label relation, such that vertices with same labels have similar vector representations. // The third strategy used by LANE [30] is to jointly embed vertices and labels into a common space.	b	Network Representation Learning- A Survey	[50]	['[50]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 1895–1901. ']
168	587	[42]	This provides a direct way to separate vertices of different categories from each other in the new embedding space. // The second strategy (used by GENE [48], TriDNR [50], LDE ==[51]== and Planetoid [52]) is achieved by modeling vertex label relation, such that vertices with same labels have similar vector representations. // The third strategy used by LANE [30] is to jointly embed vertices and labels into a common space.	b	Network Representation Learning- A Survey	[51]	['[51]  S. Wang, J. Tang, C. Aggarwal, and H. Liu, “Linked document embedding for classification,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 115–124. ']
169	587	[66]	This provides a direct way to separate vertices of different categories from each other in the new embedding space. // The second strategy (used by GENE [48], TriDNR [50], LDE [51] and Planetoid ==[52]==) is achieved by modeling vertex label relation, such that vertices with same labels have similar vector representations. // The third strategy used by LANE [30] is to jointly embed vertices and labels into a common space.	b	Network Representation Learning- A Survey	[52]	['[52]  Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semisupervised learning with graph embeddings,” in Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML), 2016, pp. 40–48. ']
170	587	[29]	The second strategy (used by GENE [48], TriDNR [50], LDE [51] and Planetoid [52]) is achieved by modeling vertex label relation, such that vertices with same labels have similar vector representations. // The third strategy used by LANE ==[30]== is to jointly embed vertices and labels into a common space. // Fitting vertex representations to a classifier can take advantage of the discriminative power in vertex labels. Algorithms using this strategy only require a small number of labeled vertices (e.g., 10%) to achieve significant performance gain over their unsupervised counterparts. They are thus more effective for discriminative learning in sparsely labeled scenarios.	b	Network Representation Learning- A Survey	[30]	['[30]  X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2017, pp. 731–739. ']
171	587	[67]	They are thus more effective for discriminative learning in sparsely labeled scenarios. However, fitting vertex representations to a classifier is more prone to overfitting. // Regularization and DropOut ==[69]== are often introduced to overcome this problem. // By contrast, modeling vertex label relation and joint vertex embedding requires more vertex labels to make vertex representations more discriminative, but they can better capture intra-class proximity, i.e., vertices belonging to the same class are kept closer to each other in the new embedding space.	b	Network Representation Learning- A Survey	[69]	['[69]  G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving neural networks by preventing coadaptation of feature detectors,” arXiv preprint arXiv:1207.0580, 2012. ']
172	587	[10]	Often, because network vertices are partially or sparsely labeled due to high labeling costs, a large portion of vertices in networks have unknown labels. // The problem of vertex classification aims to predict the labels of unlabeled vertices given a partially labeled network ==[10]==, [11]. // Since vertices are not independent but connected to each other in the form of a network via links, vertex classification should exploit these dependencies for jointly classifying the labels of vertices.	b	Network Representation Learning- A Survey	[10]	['[10]  S. Zhu, K. Yu, Y. Chi, and Y. Gong, “Combining content and link for classification using matrix factorization,” in Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2007, pp. 487–494. ']
173	587	[11]	Often, because network vertices are partially or sparsely labeled due to high labeling costs, a large portion of vertices in networks have unknown labels. // The problem of vertex classification aims to predict the labels of unlabeled vertices given a partially labeled network [10], ==[11]==. // Since vertices are not independent but connected to each other in the form of a network via links, vertex classification should exploit these dependencies for jointly classifying the labels of vertices.	b	Network Representation Learning- A Survey	[11]	['[11]  S. Bhagat, G. Cormode, and S. Muthukrishnan, “Node classification in social networks,” Social Network Data Analytics, pp. 115– 148, 2011. ']
174	587	[68]	Since vertices are not independent but connected to each other in the form of a network via links, vertex classification should exploit these dependencies for jointly classifying the labels of vertices. // Among others, collective classification proposes to construct a new set of vertex features that summarize label dependencies in the neighborhood, which has been shown to be most effective in classifying many real-world networks ==[70]==, [71]. // Network representation learning follows the same principle that automatically learns vertex features based on network structure.	h+	Network Representation Learning- A Survey	[70]	['[70]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. EliassiRad, “Collective classification in network data,” AI Magazine, vol. 29, no. 3, p. 93, 2008. ']
175	587	[69]	Since vertices are not independent but connected to each other in the form of a network via links, vertex classification should exploit these dependencies for jointly classifying the labels of vertices. // Among others, collective classification proposes to construct a new set of vertex features that summarize label dependencies in the neighborhood, which has been shown to be most effective in classifying many real-world networks [70], ==[71]==. // Network representation learning follows the same principle that automatically learns vertex features based on network structure.	h+	Network Representation Learning- A Survey	[71]	['[71]  P. Kazienko and T. Kajdanowicz, “Label-dependent node classification in the network,” Neurocomputing, vol. 75, no. 1, pp. 199– 209, 2012. ']
176	587	[1]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., ==[1]==, [6], [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
177	587	[6]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], ==[6]==, [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
178	587	[7]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], ==[7]==, [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
179	587	[20]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], ==[20]==, [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
180	587	[37]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], [20], ==[34]==), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
181	587	[8]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., ==[8]==, [30], [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
182	587	[29]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], ==[30]==, [45], [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[30]	['[30]  X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2017, pp. 731–739. ']
183	587	[38]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], ==[45]==, [46], [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[45]	['[45]  J. Li, J. Zhu, and B. Zhang, “Discriminative deep random walk for network classification,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, vol. 1, 2016, pp. 1004–1013. ']
184	587	[62]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], ==[46]==, [47]), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[46]	['[46]  C. Tu, W. Zhang, Z. Liu, and M. Sun, “Max-Margin DeepWalk: discriminative learning of network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 3889–3895. ']
185	587	[63]	Network representation learning follows the same principle that automatically learns vertex features based on network structure. // Existing studies have evaluated the discriminative power of the learned vertex representations under two settings: unsupervised settings (e.g., [1], [6], [7], [20], [34]), where vertex representations are learned separately, followed by applying discriminative classifiers like SVM or logistic regression on the new embeddings, and semisupervised settings (e.g., [8], [30], [45], [46], ==[47]==), where representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred from labeled vertices can directly benefit the learning of informative vertex representations. // These studies have proved that better vertex representations can contribute to high classification accuracy.	b	Network Representation Learning- A Survey	[47]	['[47]  X. Zhang, W. Chen, and H. Yan, “TLINE: scalable transductive network embedding,” in Information Retrieval Technology, 2016, pp. 98–110. ']
186	587	[13]	Link Prediction. // Another important application of network representation learning is link prediction ==[13]==, [72], which aims to infer the existence of new relationships or emerging interactions between pairs of entities based on the currently observed links and their properties. // The approaches developed to solve this problem can enable the discovery of implicit or missing interactions in the network, the identification of spurious links, as well as understanding the network evolution mechanism.	b	Network Representation Learning- A Survey	[13]	['[13]  S. Gao, L. Denoyer, and P. Gallinari, “Temporal link prediction by integrating content and structure information,” in Proceedings of the 20th ACM International Conference on Information and Knowledge Management, 2011, pp. 1169–1174. ']
187	587	[70]	Link Prediction. // Another important application of network representation learning is link prediction [13], ==[72]==, which aims to infer the existence of new relationships or emerging interactions between pairs of entities based on the currently observed links and their properties. // The approaches developed to solve this problem can enable the discovery of implicit or missing interactions in the network, the identification of spurious links, as well as understanding the network evolution mechanism.	b	Network Representation Learning- A Survey	[72]	['[72]  D. Liben-Nowell and J. Kleinberg, “The link-prediction problem for social networks,” Journal of the American Society for Information Science and Technology, vol. 58, no. 7, pp. 1019–1031, 2007. ']
188	587	[19]	Good network representations should be able to capture explicit and implicit connections between network vertices thus enabling application to link prediction. // ==[19]== and [35] predict missing links based on the learned vertex representations on social networks. // [34] also applies network representation learning to collaboration networks and protein-protein interaction networks. They demonstrate that on these networks links predicted using the learned representations achieve better performance than traditional similarity-based link prediction approaches.	b	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
189	587	[48]	Good network representations should be able to capture explicit and implicit connections between network vertices thus enabling application to link prediction. // [19] and ==[35]== predict missing links based on the learned vertex representations on social networks. // [34] also applies network representation learning to collaboration networks and protein-protein interaction networks. They demonstrate that on these networks links predicted using the learned representations achieve better performance than traditional similarity-based link prediction approaches.	b	Network Representation Learning- A Survey	[35]	['[35]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1105–1114. ']
190	587	[37]	[19] and [35] predict missing links based on the learned vertex representations on social networks. // ==[34]== also applies network representation learning to collaboration networks and protein-protein interaction networks. // They demonstrate that on these networks links predicted using the learned representations achieve better performance than traditional similarity-based link prediction approaches.	h+	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
191	587	[71]	Clustering. // Network clustering refers to the task of partitioning network nodes into a set of clusters, such that vertices are densely connected to each other within the same cluster, but connected to few vertices from other clusters ==[74]==. // Such cluster structures, or communities widely occur in a wide spectrum of networked systems from bioinformatics, computer science, physics, sociology, etc., and have strong implications.	b	Network Representation Learning- A Survey	[74]	['[74]  S. Fortunato, “Community detection in graphs,” Physics Reports, vol. 486, no. 3–5, pp. 75–174, 2010. ']
192	587	[72]	Researchers have proposed a large body of network clustering algorithms based on various metrics of similarity or strength of connection between vertices. // Min-max cut and normalized cut methods ==[75]==, [76] seek to recursively partition a graph into two clusters that maximize the number of intra-cluster connections and minimize the number of intercluster connections. // Modularity-based methods (e.g., [77], [78]) aim to maximize the modularity of a clustering, which is the fraction of intra-cluster edges minus the expected fraction assuming the edges were randomly distributed. A network partitioning with high modularity would have dense intra-cluster connections but sparse inter-cluster connections.	b	Network Representation Learning- A Survey	[75]	['[75]  C. H. Q. Ding, X. He, H. Zha, M. Gu, and H. D. Simon, “A minmax cut algorithm for graph partitioning and data clustering,” in Proceedings of the 2001 IEEE International Conference on Data Mining, 2001, pp. 107–114. ']
193	587	[73]	Researchers have proposed a large body of network clustering algorithms based on various metrics of similarity or strength of connection between vertices. // Min-max cut and normalized cut methods [75], ==[76]== seek to recursively partition a graph into two clusters that maximize the number of intra-cluster connections and minimize the number of intercluster connections. // Modularity-based methods (e.g., [77], [78]) aim to maximize the modularity of a clustering, which is the fraction of intra-cluster edges minus the expected fraction assuming the edges were randomly distributed. A network partitioning with high modularity would have dense intra-cluster connections but sparse inter-cluster connections.	b	Network Representation Learning- A Survey	[76]	['[76]  J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888–905, 2000. ']
194	587	[74]	Min-max cut and normalized cut methods [75], [76] seek to recursively partition a graph into two clusters that maximize the number of intra-cluster connections and minimize the number of intercluster connections. // Modularity-based methods (e.g., ==[77]==, [78]) aim to maximize the modularity of a clustering, which is the fraction of intra-cluster edges minus the expected fraction assuming the edges were randomly distributed. // A network partitioning with high modularity would have dense intra-cluster connections but sparse inter-cluster connections.	b	Network Representation Learning- A Survey	[77]	['[77]  N. M. E. J. and M. Girvan, “Finding and evaluating community structure in networks,” Physics Review, vol. 69, no. 026113, 2004. ']
195	587	[75]	Min-max cut and normalized cut methods [75], [76] seek to recursively partition a graph into two clusters that maximize the number of intra-cluster connections and minimize the number of intercluster connections. // Modularity-based methods (e.g., [77], ==[78]==) aim to maximize the modularity of a clustering, which is the fraction of intra-cluster edges minus the expected fraction assuming the edges were randomly distributed. // A network partitioning with high modularity would have dense intra-cluster connections but sparse inter-cluster connections.	b	Network Representation Learning- A Survey	[78]	['[78]  M. E. Newman, “Modularity and community structure in networks,” Proceedings of the National Academy of Sciences, vol. 103, no. 23, pp. 8577–8582, 2006. ']
196	587	[76]	A network partitioning with high modularity would have dense intra-cluster connections but sparse inter-cluster connections. // Some other methods (e.g., ==[79]==) try to identify nodes with similar structural roles like bridges and outliers. // Recent NRL methods (e.g., GraRep [26], DNGR [9], MNMF [28], and pRBM [29]) used the clustering performance to evaluate the quality of the learned network representations on different networks. Intuitively, better representations would lead to better clustering performance.	b	Network Representation Learning- A Survey	[79]	['[79]  X. Xu, N. Yuruk, Z. Feng, and T. A. J. Schweiger, “SCAN: A structural clustering algorithm for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2007, pp. 824–833. ']
197	587	[26]	Some other methods (e.g., [79]) try to identify nodes with similar structural roles like bridges and outliers. // Recent NRL methods (e.g., GraRep ==[26]==, DNGR [9], MNMF [28], and pRBM [29]) used the clustering performance to evaluate the quality of the learned network representations on different networks. Intuitively, better representations would lead to better clustering performance. //  Intuitively, better representations would lead to better clustering performance.	b	Network Representation Learning- A Survey	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “GraRep: Learning graph representations with global structural information,” in Proceedings of the 24th ACM International Conference on Information and Knowledge Management, 2015, pp. 891–900. ']
198	587	[9]	Some other methods (e.g., [79]) try to identify nodes with similar structural roles like bridges and outliers. // Recent NRL methods (e.g., GraRep [26], DNGR ==[9]==, MNMF [28], and pRBM [29]) used the clustering performance to evaluate the quality of the learned network representations on different networks. Intuitively, better representations would lead to better clustering performance. //  Intuitively, better representations would lead to better clustering performance.	b	Network Representation Learning- A Survey	[9]	['[9]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proceedings of the 30th AAAI Conference on Artificial Intelligence, 2016, pp. 1145–1152. ']
199	587	[30]	Some other methods (e.g., [79]) try to identify nodes with similar structural roles like bridges and outliers. // Recent NRL methods (e.g., GraRep [26], DNGR [9], MNMF ==[28]==, and pRBM [29]) used the clustering performance to evaluate the quality of the learned network representations on different networks. Intuitively, better representations would lead to better clustering performance. //  Intuitively, better representations would lead to better clustering performance.	b	Network Representation Learning- A Survey	[28]	['[28]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 203–209. ']
200	587	[28]	Some other methods (e.g., [79]) try to identify nodes with similar structural roles like bridges and outliers. // Recent NRL methods (e.g., GraRep [26], DNGR [9], MNMF [28], and pRBM ==[29]==) used the clustering performance to evaluate the quality of the learned network representations on different networks. Intuitively, better representations would lead to better clustering performance. //  Intuitively, better representations would lead to better clustering performance.	b	Network Representation Learning- A Survey	[29]	['[29]  S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted Boltzmann machine for linked data,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1753–1762. ']
201	587	[28]	These works followed the common approach that first applies an unsupervised NRL algorithm to learn vertex representations, and then performs k-means clustering on the learned representations to cluster the vertices. // In particular, pRBM ==[29]== showed that NRL methods outperforms the baseline that uses original features for clustering without learning representations. // This suggests that effective representation learning can improve the clustering performance.	h+	Network Representation Learning- A Survey	[29]	['[29]  S. Wang, J. Tang, F. Morstatter, and H. Liu, “Paired restricted Boltzmann machine for linked data,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1753–1762. ']
202	587	[77]	Visualization techniques play critical roles in managing, exploring, and analyzing complex networked data. // ==[80]== surveys a range of methods used to visualize graphs from an information visualization perspective. // This work compares various traditional layouts used to visualize graphs, such as tree-, 3D-, and hyperbolic-based methods, and shows that classical visualization techniques are proved effective for small or intermediate sized networks; they however confront a big challenge when applied to large-scale networks.	ho	Network Representation Learning- A Survey	[80]	['[80]  I. Herman, G. Melanc¸on, and M. S. Marshall, “Graph visualization and navigation in information visualization: A survey,” IEEE Transactions on Visualization and Computer Graphics, vol. 6, no. 1, pp. 24–43, 2000. ']
203	587	[17]	Few systems can claim to deal effectively with thousands of vertices, although networks with this order of magnitude often occur in a wide variety of applications. // Consequently, a first step in the visualization process is often to reduce the size of the network to display. One common approach is essentially to find an extremely low-dimensional representation of a network that preserves the intrinsic structure, i.e., keeping similar vertices close and dissimilar vertices far apart, in the low-dimensional space ==[17]==. // Network representation learning has the same objective that embeds a large network into a new latent space of low dimensionality	b	Network Representation Learning- A Survey	[17]	['[17]  J. Tang, J. Liu, and Q. Mei, “Visualizing large-scale and highdimensional data,” in Proceedings of the 25th International Conference on World Wide Web, 2016, pp. 287–297. ']
204	587	[78]	Network representation learning has the same objective that embeds a large network into a new latent space of low dimensionality. // After new embeddings are obtained in the vector space, popular methods such as t-distributed stochastic neighbor embedding (t-SNE) ==[81]== can be applied to visualize the network in a 2-D or 3-D space. // By taking the learned vertex representations as input, LINE [1] used the t-SNE package to visualize the DBLP co-author network after the authors are mapped into a 2-D space, and showed that LINE is able to cluster authors in the same field to the same community.	b	Network Representation Learning- A Survey	[81]	['[81]  L. v. d. Maaten and G. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579– 2605, 2008. ']
205	587	[20]	By taking the learned vertex representations as input, LINE [1] used the t-SNE package to visualize the DBLP co-author network after the authors are mapped into a 2-D space, and showed that LINE is able to cluster authors in the same field to the same community. // HSCA ==[20]== illustrated the advantages of the content-augmented NRL algorithm by visualizing the citations networks. // Semi-supervised algorithms (e.g., TLINE [47], TriDNR [50], and DMF [8]) demonstrated that the visualization results have better clustering structures with vertex labels properly imported.	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
206	587	[63]	HSCA [20] illustrated the advantages of the content-augmented NRL algorithm by visualizing the citations networks. // Semi-supervised algorithms (e.g., TLINE ==[47]==, TriDNR [50], and DMF [8]) demonstrated that the visualization results have better clustering structures with vertex labels properly imported. // Recommendation	b	Network Representation Learning- A Survey	[47]	['[47]  X. Zhang, W. Chen, and H. Yan, “TLINE: scalable transductive network embedding,” in Information Retrieval Technology, 2016, pp. 98–110. ']
207	587	[41]	HSCA [20] illustrated the advantages of the content-augmented NRL algorithm by visualizing the citations networks. // Semi-supervised algorithms (e.g., TLINE [47], TriDNR ==[50]==, and DMF [8]) demonstrated that the visualization results have better clustering structures with vertex labels properly imported. // Recommendation	b	Network Representation Learning- A Survey	[50]	['[50]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” in Proceedings of the 25th International Joint Conference on Artificial Intelligence, 2016, pp. 1895–1901. ']
208	587	[8]	HSCA [20] illustrated the advantages of the content-augmented NRL algorithm by visualizing the citations networks. // Semi-supervised algorithms (e.g., TLINE [47], TriDNR [50], and DMF ==[8]==) demonstrated that the visualization results have better clustering structures with vertex labels properly imported. // Recommendation	b	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
209	587	[79]	For these types of social networks, POI recommendation intends to recommend user interested objects, depending on their own context, such as the geographic location of the users and their interests. // Traditionally, this is solved by using approaches, such as collaborative filtering, to leverage spatial and temporal correlation between user activities and geographical distance ==[82]==. // However, because each user’s check-in records are very sparse, finding similar users or calculating transition probability between users and locations is a significant challenge.	h-	Network Representation Learning- A Survey	[82]	['[82]  Y. Zheng, L. Zhang, X. Xie, and W.-Y. Ma, “Mining interesting locations and travel sequences from GPS trajectories,” in Proceedings of the World Wide Web International Conference, 2009, pp. 791– 800. ']
210	587	[14]	However, because each user’s check-in records are very sparse, finding similar users or calculating transition probability between users and locations is a significant challenge. // Recently, spatial-temporal embedding ==[14]==, [15], [83] has emerged to learn low-dimensional dense vectors to represent users, locations, and point-of-interests etc. // As a result, each user, location, and POI can be represented as a lowdimensional vector, respectively, for similarity search and many other analysis.	b	Network Representation Learning- A Survey	[14]	['[14]  C. Zhang, K. Zhang, Q. Yuan, H. Peng, Y. Zheng, T. Hanratty, S. Wang, and J. Han, “Regions, periods, activities: Uncovering urban dynamics via cross-modal representation learning,” in Proceedings of the 26th International Conference on World Wide Web, 2017, pp. 361–370. ']
211	587	[15]	However, because each user’s check-in records are very sparse, finding similar users or calculating transition probability between users and locations is a significant challenge. // Recently, spatial-temporal embedding [14], ==[15]==, [83] has emerged to learn low-dimensional dense vectors to represent users, locations, and point-of-interests etc. // As a result, each user, location, and POI can be represented as a lowdimensional vector, respectively, for similarity search and many other analysis.	b	Network Representation Learning- A Survey	[15]	['[15]  M. Xie, H. Yin, H. Wang, F. Xu, W. Chen, and S. Wang, “Learning graph-based POI embedding for location-based recommendation,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 15–24. ']
212	587	[80]	However, because each user’s check-in records are very sparse, finding similar users or calculating transition probability between users and locations is a significant challenge. // Recently, spatial-temporal embedding [14], [15], ==[83]== has emerged to learn low-dimensional dense vectors to represent users, locations, and point-of-interests etc. // As a result, each user, location, and POI can be represented as a lowdimensional vector, respectively, for similarity search and many other analysis.	b	Network Representation Learning- A Survey	[83]	['[83]  P. Wang, J. Zhang, G. Liu, Y. Fu, and C. Aggarwal, “Ensemblespotting: Prioritizing vibrant communities via POI embedding with multi-view spatial graphs,” in Proceedings of the 2018 SIAM International Conference on Data Mining, 2018, pp. 351–359. ']
213	587	[81]	Traditionally, knowledge graph search is carried out through database driven approaches to explore schema mapping between entities, including entity relationships. // Recent advancement in network representation learning has inspired structured embeddings of knowledge bases ==[84]==. // Such embedding methods intend to learn a low-dimensional vector representation for knowledge graph entities, such that generic database queries, such as top-k search, can be carried out by comparing vector representation of the query object and objects in the database.	b	Network Representation Learning- A Survey	[84]	['[84]  A. Bordes, J. Weston, R. Collobert, Y. Bengio et al., “Learning structured embeddings of knowledge bases.” in Proceedings of the 25th AAAI Conference on Artificial Intelligence, 2011, pp. 301–306. ']
214	587	[82]	In addition to using vector representation to represent knowledge graph entities, research has also proposed to use such representation to further enhance and complete the knowledge graph itself. // For example, knowledge graph completion intends to discover complete relationships between entities, and a recent work ==[85]== has proposed to use graph context to find missing links between entities. // This is similar to link prediction in social networks, but the entities are typically heterogeneous and a pair of entities may also have different types of relationships.	b	Network Representation Learning- A Survey	[85]	['[85]  J. Feng, M. Huang, Y. Yang et al., “GAKE: Graph aware knowledge embedding,” in Proceedings of the 26th International Conference on Computational Linguistics (COLING), 2016, pp. 641–651. 25 ']
215	587	[83]	The Facebook network is a combination of 10 Facebook ego-networks, where each vertex contains user profile attributes. // The Amherst, Hamilton, Mich and Rochester ==[86]== datasets are the Facebook networks formed by users from the corresponding US universities, where each user has six user profile features. // Often, user profile features are noisy, incomplete, and long-tail distributed.	b	Network Representation Learning- A Survey	[86]	['[86]  A. L. Traud, P. J. Mucha, and M. A. Porter, “Social structure of facebook networks,” Physica A: Statistical Mechanics and its Applications, vol. 391, no. 16, pp. 4165–4180, 2012. ']
216	587	[1]	Language Network. // The language network Wikipedia ==[1]== is a word co-occurrence network constructed from the entire set of English Wikipedia pages. There is no class label on this network. // The word embeddings learned from this network is evaluated by word analogy and document classification.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
217	587	[1]	The citation networks are directed information networks formed by author-author citation relationships or paper-paper citation relationships. They are collected from different databases of academic papers, such as DBLP and Citeseer. // Among the commonly used citation networks, DBLP (AuthorCitation) ==[1]== is a weighted citation network between authors with the edge weight defined by the number of papers written by one author and cited by the other author, while DBLP (PaperCitation) ==[1]==, Cora, Citeseer, PubMed and Citeseer-M10 are the binary paper citation networks, which are also attached with vertex text attributes as the content of papers. // Compared with user profile features in social network, the vertex text features here are more topic-centric, informative and can better complement network structure to learn effective vertex representations.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
218	587	[1]	The citation networks are directed information networks formed by author-author citation relationships or paper-paper citation relationships. They are collected from different databases of academic papers, such as DBLP and Citeseer. // Among the commonly used citation networks, DBLP (AuthorCitation) ==[1]== is a weighted citation network between authors with the edge weight defined by the number of papers written by one author and cited by the other author, while DBLP (PaperCitation) ==[1]==, Cora, Citeseer, PubMed and Citeseer-M10 are the binary paper citation networks, which are also attached with vertex text attributes as the content of papers. // Compared with user profile features in social network, the vertex text features here are more topic-centric, informative and can better complement network structure to learn effective vertex representations.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
219	587	[84]	Collaboration Network. // The collaboration network Arxiv GR-QC ==[88]== describes the co-author relationships for papers in the research field of General Relativity and Quantum Cosmology. // In this network, vertices represent authors and edges indicate co-author relationships between authors. Because there is no category information for vertices, this network is used for the link prediction task to evaluate the quality of learned vertex representations.	b	Network Representation Learning- A Survey	[88]	['[88]  J. Leskovec, J. Kleinberg, and C. Faloutsos, “Graph evolution: densification and shrinking diameters,” ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 1, no. 1, p. 2, 2007. ']
220	587	[85]	Webpage Network. // Webpage Networks (Wikipedia, WebKB and Political Blog ==[89]==) are composed of real-world webpages and hyperlinks between them, where the vertex represents a webpage and the edge indicates that there is a hyperlink from one webpage to another. // Webpage text content is often collected as vertex features.	b	Network Representation Learning- A Survey	[89]	['[89]  L. A. Adamic and N. Glance, “The political blogosphere and the 2004 US election: divided they blog,” in Proceedings of the 3rd International Workshop on Link Discovery, 2005, pp. 36–43. ']
221	587	[86]	Biological Network. // As a typical biological network, the Protein-Protein Interaction network ==[90]== is a subgraph of the PPI network for Homo Sapiens. The vertex here represents a protein and the edge indicates that there is an interaction between proteins. The labels of vertices are obtained from the hallmark gene sets [91] and represent biological states. // Communication Network.	b	Network Representation Learning- A Survey	[90]	['[90]  B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. Bahler, V. Wood ¨ et al., “The BioGRID interaction database: 2008 update,” Nucleic Acids Research, vol. 36, pp. D637–D640, 2008. ']
222	587	[87]	Biological Network. // As a typical biological network, the Protein-Protein Interaction network [90] is a subgraph of the PPI network for Homo Sapiens. The vertex here represents a protein and the edge indicates that there is an interaction between proteins. The labels of vertices are obtained from the hallmark gene sets ==[91]== and represent biological states. // Communication Network.	b	Network Representation Learning- A Survey	[91]	['[91]  A. Liberzon, A. Subramanian, R. Pinchback, H. Thorvaldsdottir, ´ P. Tamayo, and J. P. Mesirov, “Molecular signatures database (MSigDB) 3.0,” Bioinformatics, vol. 27, no. 12, pp. 1739–1740, 2011. ']
223	587	[53]	Traffic Network. // European Airline Networks used in ==[39]== are constructed from 6 airlines operating flights between European airports: 4 commercial airlines (Air France, Easyjet, Lufthansa, and RyanAir) and 2 cargo airlines (TAP Portugal, and European Airline Transport). // For each airline network, vertices are airports and edges represent the direct flights between airports. In all, 45 airports are labeled as hub airports, regional hubs, commercial hubs, and focus cities, according to their structural roles.	b	Network Representation Learning- A Survey	[39]	['[39]  C. Donnat, M. Zitnik, D. Hallac, and J. Leskovec, “Spectral graph wavelets for structural role similarity in networks,” arXiv preprint arXiv:1710.10321, 2017. ']
224	587	[19]	The known links in the original network are served as the ground-truth for evaluating reconstruction performance. // Precision@k and MAP ==[19]== are often used as evaluation metrics. // This evaluation method can check whether the learned vertex representations well preserve network structure and support network formation.	b	Network Representation Learning- A Survey	[19]	['[19]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1225–1234. ']
225	587	[88]	To validate the effectiveness of NRL algorithms, vertex clustering is also carried out by applying k-means clustering algorithm to the learned vertex representations. // Communities in networks are served as the ground truth to assess the quality of clustering results, which is measured by Accuracy and NMI (normalized mutual information) ==[92]==. // The hypothesis is that, if the learned vertex representations are indeed informative, vertex clustering on learned vertex representations should be able to discover community structures. That is, good vertex representations are expected to generate good clustering results.	b	Network Representation Learning- A Survey	[92]	['[92]  A. Strehl and J. Ghosh, “Cluster ensembles – a knowledge reuse framework for combining multiple partitions,” Journal of Machine Learning Research, vol. 3, pp. 583–617, 2003. ']
226	587	[78]	Visualization provides a straightforward way to visually evaluate the quality of the learned vertex representations. // Often, t-distributed stochastic neighbor embedding (t-SNE) ==[81]== is applied to project the learned vertex representation vectors into a 2-D space, where the distribution of vertex 2-D mappings can be easily visualized. // If vertex representations are of good quality, in the 2-D space, vertices within a same class or community should be embedded closely, and the 2-D mappings of vertices in different classes or communities should be far apart from each other.	b	Network Representation Learning- A Survey	[81]	['[81]  L. v. d. Maaten and G. Hinton, “Visualizing data using t-SNE,” Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579– 2605, 2008. ']
227	587	[6]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk ==[6]==, LINE [1], node2vec [34], MNMF [28], TADW [7], HSCA [20], UPP-SNE [43]) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
228	587	[1]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk [6], LINE ==[1]==, node2vec [34], MNMF [28], TADW [7], HSCA [20], UPP-SNE [43]) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
229	587	[37]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk [6], LINE [1], node2vec ==[34]==, MNMF [28], TADW [7], HSCA [20], UPP-SNE [43]) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
230	587	[30]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk [6], LINE [1], node2vec [34], MNMF ==[28]==, TADW [7], HSCA [20], UPP-SNE [43]) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[28]	['[28]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proceedings of the 31st AAAI Conference on Artificial Intelligence, 2017, pp. 203–209. ']
231	587	[7]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk [6], LINE [1], node2vec [34], MNMF [28], TADW ==[7]==, HSCA [20], UPP-SNE [43]) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
232	587	[20]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk [6], LINE [1], node2vec [34], MNMF [28], TADW [7], HSCA ==[20]==, UPP-SNE [43]) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
233	587	[57]	Note that, because semi-supervised NRL algorithms are task-dependent: the target task may be binary or multi-class, or multi-label classification, or because they use different classification strategies, it would be difficult to assess the effectiveness of network embedding under the same settings. // Therefore, our empirical study focuses on comparing seven unsupervised NRL algorithms (DeepWalk [6], LINE [1], node2vec [34], MNMF [28], TADW [7], HSCA [20], UPP-SNE ==[43]==) on vertex classification and vertex clustering, which are the two most commonly used evaluation methods in the literature. // Our empirical studies are based on seven benchmark datasets: Amherst, Hamilton, Mich, Rochester, Citeseer, Cora and Facebook.	b	Network Representation Learning- A Survey	[43]	['[43]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “User profile preserving social network embedding,” in Proceedings of the 26th International Joint Conference on Artificial Intelligence, 2017, pp. 3378–3384. ']
234	587	[89]	On Citeseer and Cora, node2vec performs much better than other only structure preserving NRL algorithms, including its equivalent version DeepWalk. // For each vertex context pair (vi , vj ), DeepWalk and node2vec use two different strategies to approximate the probability Pr(vj |vi): hierarchical softmax ==[93]==, [94] and negative sampling [95]. // The better clustering performance of node2vec over DeepWalk proves the advantage of negative sampling over hierarchical softmax, which is consistent with the word embedding results as reported in [67].	h-	Network Representation Learning- A Survey	[93]	['[93]  A. Mnih and G. E. Hinton, “A scalable hierarchical distributed language model,” in Advances in Neural Information Processing Systems, 2009, pp. 1081–1088. ']
235	587	[90]	On Citeseer and Cora, node2vec performs much better than other only structure preserving NRL algorithms, including its equivalent version DeepWalk. // For each vertex context pair (vi , vj ), DeepWalk and node2vec use two different strategies to approximate the probability Pr(vj |vi): hierarchical softmax [93], ==[94]== and negative sampling [95]. // The better clustering performance of node2vec over DeepWalk proves the advantage of negative sampling over hierarchical softmax, which is consistent with the word embedding results as reported in [67].	h-	Network Representation Learning- A Survey	[94]	['[94]  F. Morin and Y. Bengio, “Hierarchical probabilistic neural network language model,” in Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics, vol. 5, 2005, pp. 246–252. ']
236	587	[91]	On Citeseer and Cora, node2vec performs much better than other only structure preserving NRL algorithms, including its equivalent version DeepWalk. // For each vertex context pair (vi , vj ), DeepWalk and node2vec use two different strategies to approximate the probability Pr(vj |vi): hierarchical softmax [93], [94] and negative sampling ==[95]==. // The better clustering performance of node2vec over DeepWalk proves the advantage of negative sampling over hierarchical softmax, which is consistent with the word embedding results as reported in [67].	h+	Network Representation Learning- A Survey	[95]	['[95]  M. U. Gutmann and A. Hyvarinen, “Noise-contrastive estimation ¨ of unnormalized statistical models, with applications to natural image statistics,” Journal of Machine Learning Research, vol. 13, no. Feb, pp. 307–361, 2012. ']
237	587	[64]	For each vertex context pair (vi , vj ), DeepWalk and node2vec use two different strategies to approximate the probability Pr(vj |vi): hierarchical softmax [93], [94] and negative sampling [95]. // The better clustering performance of node2vec over DeepWalk proves the advantage of negative sampling over hierarchical softmax, which is consistent with the word embedding results as reported in ==[67]==. // Complexity Analysis.	ho	Network Representation Learning- A Survey	[67]	['[67]  Q. Le and T. Mikolov, “Distributed representations of sentences and documents,” in Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1188–1196. ']
238	587	[7]	The corresponding NRL algorithms usually perform factorization on |V |×|V | structure preserving matrix, which is quite time-consuming. // Efforts have been made to reduce the complexity of matrix factorization. For example, TADW ==[7]==, DMF [8] and HSCA [20] take advantage of the sparsity of the original vertex-context matrix. // HOPE [35] and GraphWave [39] adopt advanced techniques [96] [97] to perform matrix eigen decomposition.	b	Network Representation Learning- A Survey	[7]	['[7]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2015, pp. 2111–2117. ']
239	587	[8]	The corresponding NRL algorithms usually perform factorization on |V |×|V | structure preserving matrix, which is quite time-consuming. // Efforts have been made to reduce the complexity of matrix factorization. For example, TADW [7], DMF ==[8]== and HSCA [20] take advantage of the sparsity of the original vertex-context matrix. // HOPE [35] and GraphWave [39] adopt advanced techniques [96] [97] to perform matrix eigen decomposition.	b	Network Representation Learning- A Survey	[8]	['[8]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Collective classification via discriminative matrix factorization on sparsely labeled networks,” in Proceedings of the 25th ACM International Conference on Information and Knowledge Management, 2016, pp. 1563–1572. ']
240	587	[20]	The corresponding NRL algorithms usually perform factorization on |V |×|V | structure preserving matrix, which is quite time-consuming. // Efforts have been made to reduce the complexity of matrix factorization. For example, TADW [7], DMF [8] and HSCA ==[20]== take advantage of the sparsity of the original vertex-context matrix. // HOPE [35] and GraphWave [39] adopt advanced techniques [96] [97] to perform matrix eigen decomposition.	b	Network Representation Learning- A Survey	[20]	['[20]  D. Zhang, J. Yin, X. Zhu, and C. Zhang, “Homophily, structure, and content augmented network representation learning,” in Proceedings of the 16th IEEE International Conference on Data Mining, 2016, pp. 609–618. ']
241	587	[48]	Efforts have been made to reduce the complexity of matrix factorization. For example, TADW [7], DMF [8] and HSCA [20] take advantage of the sparsity of the original vertex-context matrix. // HOPE ==[35]== and GraphWave [39] adopt advanced techniques [96] [97] to perform matrix eigen decomposition. //  FUTURE RESEARCH DIRECTIONS.	b	Network Representation Learning- A Survey	[35]	['[35]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1105–1114. ']
242	587	[53]	Efforts have been made to reduce the complexity of matrix factorization. For example, TADW [7], DMF [8] and HSCA [20] take advantage of the sparsity of the original vertex-context matrix. // HOPE [35] and GraphWave ==[39]== adopt advanced techniques [96] [97] to perform matrix eigen decomposition. //  FUTURE RESEARCH DIRECTIONS.	b	Network Representation Learning- A Survey	[39]	['[39]  C. Donnat, M. Zitnik, D. Hallac, and J. Leskovec, “Spectral graph wavelets for structural role similarity in networks,” arXiv preprint arXiv:1710.10321, 2017. ']
243	587	[92]	Efforts have been made to reduce the complexity of matrix factorization. For example, TADW [7], DMF [8] and HSCA [20] take advantage of the sparsity of the original vertex-context matrix. // HOPE [35] and GraphWave [39] adopt advanced techniques ==[96]== [97] to perform matrix eigen decomposition. //  FUTURE RESEARCH DIRECTIONS.	b	Network Representation Learning- A Survey	[96]	['[96]  M. Hochstenbach, “A Jacobi–Davidson type method for the generalized singular value problem,” Linear Algebra and its Applications, vol. 431, no. 3-4, pp. 471–487, 2009. ']
244	587	[93]	Efforts have been made to reduce the complexity of matrix factorization. For example, TADW [7], DMF [8] and HSCA [20] take advantage of the sparsity of the original vertex-context matrix. // HOPE [35] and GraphWave [39] adopt advanced techniques [96] ==[97]== to perform matrix eigen decomposition. //  FUTURE RESEARCH DIRECTIONS.	b	Network Representation Learning- A Survey	[97]	['[97]  D. I. Shuman, P. Vandergheynst, and P. Frossard, “Chebyshev polynomial approximation for distributed signal processing,” in Distributed Computing in Sensor Systems and Workshops (DCOSS), 2011 International Conference on, 2011, pp. 1–8. ']
245	587	[48]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction ==[35]==, community detection [98], [99], [100], [101], class imbalance learning [102], active learning [103], and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[35]	['[35]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1105–1114. ']
246	587	[94]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection ==[98]==, [99], [100], [101], class imbalance learning [102], active learning [103], and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[98]	['[98]  S. Cavallari, V. W. Zheng, H. Cai, K. C.-C. Chang, and E. Cambria, “Learning community embedding with community detection and node embedding on graphs,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 377–386. ']
247	587	[95]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection [98], ==[99]==, [100], [101], class imbalance learning [102], active learning [103], and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[99]	['[99]  L. Yang, X. Cao, and Y. Guo, “Multi-facet network embedding: Beyond the general solution of detection and representation,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 499–506. ']
248	587	[96]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection [98], [99], ==[100]==, [101], class imbalance learning [102], active learning [103], and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[100]	['[100]  Y. Zhang, T. Lyu, and Y. Zhang, “COSINE: Communitypreserving social network embedding from information diffusion cascades,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2620–2627. ']
249	587	[97]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection [98], [99], [100], ==[101]==, class imbalance learning [102], active learning [103], and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[101]	['[101]  C. Wang, S. Pan, G. Long, X. Zhu, and J. Jiang, “MGAE: Marginalized hraph autoencoder for graph clustering,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 889–898. ']
250	587	[98]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection [98], [99], [100], [101], class imbalance learning ==[102]==, active learning [103], and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[102]	['[102]  Z. Wang, X. Ye, C. Wang, Y. Wu, C. Wang, and K. Liang, “RSDNE: Exploring relaxed similarity and dissimilarity from completelyimbalanced labels for network embedding,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 475–482. ']
251	587	[99]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection [98], [99], [100], [101], class imbalance learning [102], active learning ==[103]==, and information retrieval [104]. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[103]	['[103]  X. Huang, Q. Song, J. Li, and X. Hu, “Exploring expert cognition for attributed network embedding,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2018. ']
252	587	[100]	To date, most existing NRL algorithms are task-independent, and task-specific NRL algorithms have primarily focused on vertex classification under the semi-supervised setting. // Only very recently, a few studies have started to design task-specific NRL algorithms for link prediction [35], community detection [98], [99], [100], [101], class imbalance learning [102], active learning [103], and information retrieval ==[104]==. // The advantage of using network representation learning as an intermediate layer to solve the target task is that the best possible information preserved in the new representation can further benefit the subsequent task.	b	Network Representation Learning- A Survey	[104]	['[104]  V. Misra and S. Bhatia, “Bernoulli embeddings for graphs,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 3812–3819. ']
253	587	[6]	There is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results. // To better understand DeepWalk ==[6]==, LINE [1], and node2vec [34], [105] discovers the theoretical connections between them and graph Laplacians. // However, in-depth theoretical analysis about network representation learning is necessary, as it provides a deep understanding of algorithms and helps interpret empirical results.	ho	Network Representation Learning- A Survey	[6]	['[6]  B. Perozzi, R. Al-Rfou, and S. Skiena, “DeepWalk: Online learning of social representations,” in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 701–710. ']
254	587	[1]	There is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results. // To better understand DeepWalk [6], LINE ==[1]==, and node2vec [34], [105] discovers the theoretical connections between them and graph Laplacians. // However, in-depth theoretical analysis about network representation learning is necessary, as it provides a deep understanding of algorithms and helps interpret empirical results.	ho	Network Representation Learning- A Survey	[1]	['[1]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “LINE: Large-scale information network embedding,” in Proceedings of the 24th International Conference on World Wide Web, 2015, pp. 1067–1077. ']
255	587	[37]	There is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results. // To better understand DeepWalk [6], LINE [1], and node2vec ==[34]==, [105] discovers the theoretical connections between them and graph Laplacians. // However, in-depth theoretical analysis about network representation learning is necessary, as it provides a deep understanding of algorithms and helps interpret empirical results.	ho	Network Representation Learning- A Survey	[34]	['[34]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 855–864. ']
256	587	[101]	There is a lack of theoretical analysis with regard to properties of algorithms and what contributes to good empirical results. // To better understand DeepWalk [6], LINE [1], and node2vec [34], ==[105]== discovers the theoretical connections between them and graph Laplacians. // However, in-depth theoretical analysis about network representation learning is necessary, as it provides a deep understanding of algorithms and helps interpret empirical results.	h-	Network Representation Learning- A Survey	[105]	['[105]  J. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang, “Network embedding as matrix factorization: unifying DeepWalk, LINE, PTE, and node2vec,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2018. ']
257	587	[102]	The vertices/edges may also be described by some time-varying information. // Dynamic networks have unique characteristics that make static network embedding fail to work: (i) vertex content features may drift over time; (ii) the addition of new vertices/edges requires learning or updating vertex representations to be efficient; and (iii) network size is not fixed. The work on dynamic network embedding is rather limited; the majority of existing approaches (e.g., ==[106]==, [107], [108]) assume that the node set is fixed and deal with the dynamics caused by the deletion/addition of edges only. // However, a more challenging problem is to predict the representations of new added vertices, which is referred to as “out-of-sample” problem.	b	Network Representation Learning- A Survey	[106]	['[106]  D. Yang, S. Wang, C. Li, X. Zhang, and Z. Li, “From properties to links: Deep network embedding on incomplete graphs,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 367–376. ']
258	587	[103]	The vertices/edges may also be described by some time-varying information. // Dynamic networks have unique characteristics that make static network embedding fail to work: (i) vertex content features may drift over time; (ii) the addition of new vertices/edges requires learning or updating vertex representations to be efficient; and (iii) network size is not fixed. The work on dynamic network embedding is rather limited; the majority of existing approaches (e.g., [106], ==[107]==, [108]) assume that the node set is fixed and deal with the dynamics caused by the deletion/addition of edges only. // However, a more challenging problem is to predict the representations of new added vertices, which is referred to as “out-of-sample” problem.	b	Network Representation Learning- A Survey	[107]	['[107]  J. Li, H. Dani, X. Hu, J. Tang, Y. Chang, and H. Liu, “Attributed network embedding for learning in a dynamic environment,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 387–396. ']
259	587	[104]	The vertices/edges may also be described by some time-varying information. // Dynamic networks have unique characteristics that make static network embedding fail to work: (i) vertex content features may drift over time; (ii) the addition of new vertices/edges requires learning or updating vertex representations to be efficient; and (iii) network size is not fixed. The work on dynamic network embedding is rather limited; the majority of existing approaches (e.g., [106], [107], ==[108]==) assume that the node set is fixed and deal with the dynamics caused by the deletion/addition of edges only. // However, a more challenging problem is to predict the representations of new added vertices, which is referred to as “out-of-sample” problem.	b	Network Representation Learning- A Survey	[108]	['[108]  L. Zhou, Y. Yang, X. Ren, F. Wu, and Y. Zhuang, “Dynamic network embedding by modeling triadic closure process,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 571–578. ']
260	587	[66]	However, a more challenging problem is to predict the representations of new added vertices, which is referred to as “out-of-sample” problem. // A few attempts such as ==[52]==, [109], [110] are made to exploit inductive learning to address this issue. // They learn an explicit mapping function from a network at a snapshot, and use this function to infer the representations of out-ofsample vertices, based on their available information such as attributes or neighborhood structure.	b	Network Representation Learning- A Survey	[52]	['[52]  Z. Yang, W. W. Cohen, and R. Salakhutdinov, “Revisiting semisupervised learning with graph embeddings,” in Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML), 2016, pp. 40–48. ']
261	587	[105]	However, a more challenging problem is to predict the representations of new added vertices, which is referred to as “out-of-sample” problem. // A few attempts such as [52], ==[109]==, [110] are made to exploit inductive learning to address this issue. // They learn an explicit mapping function from a network at a snapshot, and use this function to infer the representations of out-ofsample vertices, based on their available information such as attributes or neighborhood structure.	b	Network Representation Learning- A Survey	[109]	['[109]  W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” pp. 1025–1035, 2017. ']
262	587	[106]	However, a more challenging problem is to predict the representations of new added vertices, which is referred to as “out-of-sample” problem. // A few attempts such as [52], [109], ==[110]== are made to exploit inductive learning to address this issue. // They learn an explicit mapping function from a network at a snapshot, and use this function to infer the representations of out-ofsample vertices, based on their available information such as attributes or neighborhood structure.	b	Network Representation Learning- A Survey	[110]	['[110]  J. Ma, P. Cui, and W. Zhu, “DepthLGP: Learning embeddings of out-of-sample nodes in dynamic networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp. 370–377. ']
263	587	[107]	Deep learning based methods can capture non-linearity in networks, but their computational cost is usually high. // Traditional deep learning architectures take advantage of GPU to speed up training on Euclidean structured data ==[111]==. // However, networks do not have such a structure, and therefore require new solutions to improve the scalability [112].	h-	Network Representation Learning- A Survey	[111]	['[111]  R. A. Rossi, R. Zhou, and N. K. Ahmed, “Deep feature learning for graphs,” arXiv preprint arXiv:1704.08829, 2017. ']
264	587	[108]	Traditional deep learning architectures take advantage of GPU to speed up training on Euclidean structured data [111]. // However, networks do not have such a structure, and therefore require new solutions to improve the scalability ==[112]==. // Afew attempts such as [52], [109], [110] are made to exploit inductive learning to address this issue.	ho	Network Representation Learning- A Survey	[112]	['[112]  M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, “Geometric deep learning: going beyond euclidean data,” IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 18–42, 2017. ']
265	587	[16]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by ==[16]==, [113], [114], [115], [116], [117], [118], [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[16]	['[16]  Z. Liu, V. W. Zheng, Z. Zhao, F. Zhu, K. C.-C. Chang, M. Wu, and J. Ying, “Distance-aware DAG embedding for proximity search on heterogeneous graphs,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 2355–2362. ']
266	587	[109]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], ==[113]==, [114], [115], [116], [117], [118], [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[113]	['[113]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang, “Heterogeneous network embedding via deep architectures,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 119– 128. ']
267	587	[110]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], ==[114]==, [115], [116], [117], [118], [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[114]	['[114]  Z. Huang and N. Mamoulis, “Heterogeneous information network embedding for meta path based proximity,” in arXiv:1701.05291, 2017. ']
268	587	[111]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], ==[115]==, [116], [117], [118], [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[115]	['[115]  Y. Dong, N. V. Chawla, and A. Swami, “metapath2vec: Scalable representation learning for heterogeneous networks,” in Proceedings of the 23th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2017, pp. 135–144. ']
269	587	[112]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], [115], ==[116]==, [117], [118], [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[116]	['[116]  K. Tu, P. Cui, X. Wang, F. Wang, and W. Zhu, “Structural deep embedding for hyper-networks,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, 2018, pp. 426–433. ']
270	587	[113]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], [115], [116], ==[117]==, [118], [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[117]	['[117]  Y. Ma, Z. Ren, Z. Jiang, J. Tang, and D. Yin, “Multi-dimensional network embedding with hierarchical structure,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2018. ']
271	587	[114]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], [115], [116], [117], ==[118]==, [119], [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[118]	['[118]  Y. Zhang, Y. Xiong, X. Kong, and Y. Zhu, “Learning node embeddings in interaction graphs,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 397–406. ']
272	587	[115]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], [115], [116], [117], [118], ==[119]==, [120], [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[119]	['[119]  M. Qu, J. Tang, J. Shang, X. Ren, M. Zhang, and J. Han, “An attention-based collaboration framework for multi-view network representation learning,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 1767– 1776. ']
273	587	[116]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], [115], [116], [117], [118], [119], ==[120]==, [121] have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[120]	['[120]  T.-y. Fu, W.-C. Lee, and Z. Lei, “HIN2Vec: Explore meta-paths in heterogeneous information networks for representation learning,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 1797–1806. ']
274	587	[117]	An HIN is composed of different types of entities, such as text, images, or videos, and the interdependencies between entities are very complex. // This makes it very difficult to measure rich semantics and proximity between vertices and seek a common and coherent embedding space. Recent studies by [16], [113], [114], [115], [116], [117], [118], [119], [120], ==[121]== have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic proximity between distant HIN vertices for representation learning. // However, the research along this line is still at early stage.	b	Network Representation Learning- A Survey	[121]	['[121]  Y. Chen and C. Wang, “HINE: Heterogeneous information network embedding,” in International Conference on Database Systems for Advanced Applications, 2017, pp. 180–195. ']
275	587	[118]	The existence of negative links makes the traditional homophily based network representation learning algorithms unable to be directly applied. // Some studies ==[122]==, [123], [124] tackles signed network representation learning through directly modeling the polar of links. // How to fully encode network structure and vertex attributes for signed network embedding remains an open question.	b	Network Representation Learning- A Survey	[122]	['[122]  S. Wang, J. Tang, C. Aggarwal, Y. Chang, and H. Liu, “Signed network embedding in social media,” in Proceedings of the 2017 SIAM International Conference on Data Mining, 2017, pp. 327–335. ']
276	587	[70]	The existence of negative links makes the traditional homophily based network representation learning algorithms unable to be directly applied. // Some studies [122], ==[123]==, [124] tackles signed network representation learning through directly modeling the polar of links. // How to fully encode network structure and vertex attributes for signed network embedding remains an open question.	b	Network Representation Learning- A Survey	[123]	['[123]  S. Wang, C. Aggarwal, J. Tang, and H. Liu, “Attributed signed network embedding,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 137–146. ']
277	587	[117]	The existence of negative links makes the traditional homophily based network representation learning algorithms unable to be directly applied. // Some studies [122], [123], ==[124]== tackles signed network representation learning through directly modeling the polar of links. // How to fully encode network structure and vertex attributes for signed network embedding remains an open question.	b	Network Representation Learning- A Survey	[124]	['[124]  H. Wang, F. Zhang, M. Hou, X. Xie, M. Guo, and Q. Liu, “SHINE: Signed heterogeneous information network embedding for sentiment link prediction,” in Proceedings of the 10th ACM International Conference on Web Search and Data Mining, 2018. ']
278	587	[119]	Real-world networks are often noisy and uncertain, which makes traditional NRL algorithms unable to produce stable and robust representations. // ANE (Adversarial Network Embedding) ==[125]== and ARGA (Adversarially Regularized Graph Autoencoder) [126] learn robust vertex representations via enforcing an adversarial learning regularizer [58]. // To deal with the uncertainty in the existence of edges, URGE (Uncertain Graph Embedding) [127] encodes the edge existence probability into the vertex representation learning process.	b	Network Representation Learning- A Survey	[125]	['[125]  Q. Dai, Q. Li, J. Tang, and D. Wang, “Adversarial network embedding,” in Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp. 2167–2174. ']
279	587	[120]	Real-world networks are often noisy and uncertain, which makes traditional NRL algorithms unable to produce stable and robust representations. // ANE (Adversarial Network Embedding) [125] and ARGA (Adversarially Regularized Graph Autoencoder) ==[126]== learn robust vertex representations via enforcing an adversarial learning regularizer [58]. // To deal with the uncertainty in the existence of edges, URGE (Uncertain Graph Embedding) [127] encodes the edge existence probability into the vertex representation learning process.	b	Network Representation Learning- A Survey	[126]	['[126]  S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially regularized graph autoencoder,” Proceedings of the 26th International Joint Conference on Artificial Intelligence, 2018. ']
280	587	[45]	Real-world networks are often noisy and uncertain, which makes traditional NRL algorithms unable to produce stable and robust representations. // ANE (Adversarial Network Embedding) [125] and ARGA (Adversarially Regularized Graph Autoencoder) [126] learn robust vertex representations via enforcing an adversarial learning regularizer ==[58]==. // To deal with the uncertainty in the existence of edges, URGE (Uncertain Graph Embedding) [127] encodes the edge existence probability into the vertex representation learning process.	b	Network Representation Learning- A Survey	[58]	['[58]  I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems, 2014, pp. 2672–2680. ']
281	587	[103]	ANE (Adversarial Network Embedding) [125] and ARGA (Adversarially Regularized Graph Autoencoder) [126] learn robust vertex representations via enforcing an adversarial learning regularizer [58]. // To deal with the uncertainty in the existence of edges, URGE (Uncertain Graph Embedding) ==[127]== encodes the edge existence probability into the vertex representation learning process. // It is of great importance to have more research efforts on enhancing the robustness of network representation learning.	b	Network Representation Learning- A Survey	[127]	['[127]  J. Hu, R. Cheng, Z. Huang, Y. Fang, and S. Luo, “On embedding uncertain graphs,” in Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp. 157–166.']
282	6	[121, 68]	Sparsity enables the design of efficient discrete algorithms, but can make it harder to generalize in statistical learning. // Machine learning applications in networks (such as network classification ==[15, 37]==, content recommendation [11], anomaly detection [5], and missing link prediction [22]) must be able to deal with this sparsity in order to survive. // In this paper we introduce deep learning (unsupervised feature learning) [2] techniques, which have proven successful in natural language processing, into network analysis for the first time. We develop an algorithm (DeepWalk) that learns social representations of a graph’s vertices, by modeling a stream of short random walks.	ho	DeepWalk- Online Learning of Social Representations	[15, 37]	['[15]  L. Getoor and B. Taskar. Introduction to statistical relational learning. MIT press, 2007. ', '[37]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
283	6	[122]	Sparsity enables the design of efficient discrete algorithms, but can make it harder to generalize in statistical learning. // Machine learning applications in networks (such as network classification [15, 37], content recommendation ==[11]==, anomaly detection [5], and missing link prediction [22]) must be able to deal with this sparsity in order to survive. // In this paper we introduce deep learning (unsupervised feature learning) [2] techniques, which have proven successful in natural language processing, into network analysis for the first time. We develop an algorithm (DeepWalk) that learns social representations of a graph’s vertices, by modeling a stream of short random walks.	ho	DeepWalk- Online Learning of Social Representations	[11]	['[11]  F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. Knowledge and Data Engineering, IEEE Transactions on, 19(3):355–369, 2007. ']
284	6	[123]	Sparsity enables the design of efficient discrete algorithms, but can make it harder to generalize in statistical learning. // Machine learning applications in networks (such as network classification [15, 37], content recommendation [11], anomaly detection ==[5]==, and missing link prediction [22]) must be able to deal with this sparsity in order to survive. // In this paper we introduce deep learning (unsupervised feature learning) [2] techniques, which have proven successful in natural language processing, into network analysis for the first time. We develop an algorithm (DeepWalk) that learns social representations of a graph’s vertices, by modeling a stream of short random walks.	ho	DeepWalk- Online Learning of Social Representations	[5]	['[5]  V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Computing Surveys (CSUR), 41(3):15, 2009. ']
285	6	[70]	Sparsity enables the design of efficient discrete algorithms, but can make it harder to generalize in statistical learning. // Machine learning applications in networks (such as network classification [15, 37], content recommendation [11], anomaly detection [5], and missing link prediction ==[22]==) must be able to deal with this sparsity in order to survive. // In this paper we introduce deep learning (unsupervised feature learning) [2] techniques, which have proven successful in natural language processing, into network analysis for the first time. We develop an algorithm (DeepWalk) that learns social representations of a graph’s vertices, by modeling a stream of short random walks.	ho	DeepWalk- Online Learning of Social Representations	[22]	['[22]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
286	6	[124]	Machine learning applications in networks (such as network classification [15, 37], content recommendation [11], anomaly detection [5], and missing link prediction [22]) must be able to deal with this sparsity in order to survive. // In this paper we introduce deep learning (unsupervised feature learning) ==[2]== techniques, which have proven successful in natural language processing, into network analysis for the first time. // We develop an algorithm (DeepWalk) that learns social representations of a graph’s vertices, by modeling a stream of short random walks.	h+	DeepWalk- Online Learning of Social Representations	[2]	['[2]  Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. 2013. ']
287	6	[125]	DeepWalk generalizes neural language models to process a special language composed of a set of randomly-generated walks. // These neural language models have been used to capture the semantic and syntactic structure of human language ==[6]==, and even logical analogies [28]. // DeepWalk takes a graph as input and produces a latent representation as an output.	b	DeepWalk- Online Learning of Social Representations	[6]	['[6]  R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2008. ']
288	6	[126]	DeepWalk generalizes neural language models to process a special language composed of a set of randomly-generated walks. // These neural language models have been used to capture the semantic and syntactic structure of human language [6], and even logical analogies ==[28]==. // DeepWalk takes a graph as input and produces a latent representation as an output.	b	DeepWalk- Online Learning of Social Representations	[28]	['[28]  T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representations. In Proceedings of NAACL-HLT, pages 746–751, 2013. ']
289	6	[127, 128]	In the relational classification problem, the links between feature vectors violate the traditional i.i.d. assumption. // Techniques to address this problem typically use approximate inference techniques ==[31, 35]== to leverage the dependency information to improve classification results. // We distance ourselves from these approaches by learning label independent representations of the graph.	ro	DeepWalk- Online Learning of Social Representations	[31, 35]	['[31]  J. Neville and D. Jensen. Iterative classification in relational data. In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 13–20, 2000. ', '[35]  J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988. ']
290	6	[33, 55]	Our representation quality is not influenced by the choice of labeled vertices, so they can be shared among tasks. // DeepWalk outperforms other latent representation methods for creating social dimensions ==[39, 41]==, especially when labeled nodes are scarce. // Strong performance with our representations is possible with very simple linear classifiers (e.g. logistic regression).	h-e	DeepWalk- Online Learning of Social Representations	[39, 41]	['[39]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 817–826, New York, NY, USA, 2009. ACM. ', '[41]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ']
291	6	[68]	In our case, we can utilize the significant information about the dependence of the examples embedded in the structure of G to achieve superior performance. // In the literature, this is known as the relational classification (or the collective classification problem ==[37]==). // Traditional approaches to relational classification pose the problem as an inference in an undirected Markov network, and then use iterative approximate inference algorithms (such as the iterative classification algorithm [31], Gibbs Sampling [14], or label relaxation [18]) to compute the posterior distribution of labels given the network structure. We propose a different approach to capture the network topology information.	ho	DeepWalk- Online Learning of Social Representations	[37]	['[37]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
292	6	[127]	In the literature, this is known as the relational classification (or the collective classification problem [37]). // Traditional approaches to relational classification pose the problem as an inference in an undirected Markov network, and then use iterative approximate inference algorithms (such as the iterative classification algorithm ==[31]==, Gibbs Sampling [14], or label relaxation [18]) to compute the posterior distribution of labels given the network structure. // We propose a different approach to capture the network topology information.	ro	DeepWalk- Online Learning of Social Representations	[31]	['[31]  J. Neville and D. Jensen. Iterative classification in relational data. In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 13–20, 2000. ']
293	6	[129]	In the literature, this is known as the relational classification (or the collective classification problem [37]). // Traditional approaches to relational classification pose the problem as an inference in an undirected Markov network, and then use iterative approximate inference algorithms (such as the iterative classification algorithm [31], Gibbs Sampling ==[14]==, or label relaxation [18]) to compute the posterior distribution of labels given the network structure. // We propose a different approach to capture the network topology information.	ro	DeepWalk- Online Learning of Social Representations	[14]	['[14]  S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721–741, 1984. ']
294	6	[130]	In the literature, this is known as the relational classification (or the collective classification problem [37]). // Traditional approaches to relational classification pose the problem as an inference in an undirected Markov network, and then use iterative approximate inference algorithms (such as the iterative classification algorithm [31], Gibbs Sampling [14], or label relaxation ==[18]==) to compute the posterior distribution of labels given the network structure. // We propose a different approach to capture the network topology information.	ro	DeepWalk- Online Learning of Social Representations	[18]	['[18]  R. A. Hummel and S. W. Zucker. On the foundations of relaxation labeling processes. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (3):267–287, 1983. ']
295	6	[131]	Instead of mixing the label space as part of the feature space, we propose an unsupervised method which learns features that capture the graph structure independent of the labels’ distribution. // This separation between the structural representation and the labeling task avoids cascading errors, which can occur in iterative methods ==[33]==. // Moreover, the same representation can be used for multiple classification problems concerning that network.	h-	DeepWalk- Online Learning of Social Representations	[33]	['[33]  J. Neville and D. Jensen. A bias/variance decomposition for models using collective inference. Machine Learning, 73(1):87–106, 2008. ']
296	6	[122]	It is a stochastic process with random variables W1 vi , W2 vi , . . . , Wk vi such that Wk+1 vi is a vertex chosen at random from the neighbors of vertex vk. // Random walks have been used as a similarity measure for a variety of problems in content recommendation ==[11]== and community detection [1]. // They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph [38].	b	DeepWalk- Online Learning of Social Representations	[11]	['[11]  F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. Knowledge and Data Engineering, IEEE Transactions on, 19(3):355–369, 2007. ']
297	6	[132]	It is a stochastic process with random variables W1 vi , W2 vi , . . . , Wk vi such that Wk+1 vi is a vertex chosen at random from the neighbors of vertex vk. // Random walks have been used as a similarity measure for a variety of problems in content recommendation [11] and community detection ==[1]==. // They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph [38].	b	DeepWalk- Online Learning of Social Representations	[1]	['[1]  R. Andersen, F. Chung, and K. Lang. Local graph partitioning using pagerank vectors. In Foundations of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages 475–486. IEEE, 2006. ']
298	6	[133]	Random walks have been used as a similarity measure for a variety of problems in content recommendation [11] and community detection [1]. // They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph ==[38]==. // It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network.	ho	DeepWalk- Online Learning of Social Representations	[38]	['[38]  D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 81–90. ACM, 2004. ']
299	6	[35, 36]	However as the walk length grows, computing this objective function becomes unfeasible. // A recent relaxation in language modeling ==[26, 27]== turns the prediction problem on its head. // First, instead of using the context to predict a missing word, it uses one word to predict the context.	b	DeepWalk- Online Learning of Social Representations	[26, 27]	['[26]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013. ', '[27]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119. 2013. ']
300	6	[35]	For each vertex vi we generate a random walk |Wvi | = t, and then use it to update our representations (Line 7). // We use the SkipGram algorithm ==[26]== to update these representations in accordance with our objective function in Eq. 2. // SkipGram.	hoe	DeepWalk- Online Learning of Social Representations	[26]	['[26]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013. ']
301	6	[35]	SkipGram. // SkipGram is a language model that maximizes the cooccurrence probability among the words that appear within a window, w, in a sentence ==[26]==. // Algorithm 2 iterates over all possible collocations in random walk that appear within the window w (lines 1-2).	b	DeepWalk- Online Learning of Social Representations	[26]	['[26]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013. ']
302	6	[134]	The model parameter set is {Φ, T} where the size of each is O(d|V |). // Stochastic gradient descent (SGD) ==[4]== is used to optimize these parameters (Line 4, Algorithm 2). // The derivatives are estimated using the back-propagation algorithm.	b	DeepWalk- Online Learning of Social Representations	[4]	['[4]  L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nˆımes 91, Nimes, France, 1991. EC2. ']
303	6	[135]	This allows us to use asynchronous version of stochastic gradient descent (ASGD), in the multi-worker case. // Given that our updates are sparse and we do not acquire a lock to access the model shared parameters, ASGD will achieve an optimal rate of convergence ==[36]==. // While we run experiments on one machine using multiple threads, it has been demonstrated that this technique is highly scalable, and can be used in very large scale machine learning [8]. Figure 4 presents the effects of parallelizing DeepWalk.	h+e	DeepWalk- Online Learning of Social Representations	[36]	['[36]  B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, pages 693–701. 2011. ']
304	6	[136]	Given that our updates are sparse and we do not acquire a lock to access the model shared parameters, ASGD will achieve an optimal rate of convergence [36]. // While we run experiments on one machine using multiple threads, it has been demonstrated that this technique is highly scalable, and can be used in very large scale machine learning ==[8]==. // Figure 4 presents the effects of parallelizing DeepWalk.	h+e	DeepWalk- Online Learning of Social Representations	[8]	['[8]  J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1232–1240. 2012. ']
305	6	[33]	An overview of the graphs we consider in our experiments is given in Figure 1. // BlogCatalog ==[39]== is a network of social relationships provided by blogger authors. // The labels represent the topic categories provided by the authors.	b	DeepWalk- Online Learning of Social Representations	[39]	['[39]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 817–826, New York, NY, USA, 2009. ACM. ']
306	6	[33]	The labels represent the topic categories provided by the authors. // Flickr ==[39]== is a network of the contacts between users of the photo sharing website. // The labels represent the interest groups of the users such as ‘black and white photos’.	b	DeepWalk- Online Learning of Social Representations	[39]	['[39]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 817–826, New York, NY, USA, 2009. ACM. ']
307	6	[56]	The labels represent the interest groups of the users such as ‘black and white photos’. // YouTube ==[40]== is a social network between users of the popular video sharing website. // The labels here represent groups of viewers that enjoy common video genres (e.g. anime and wrestling).	b	DeepWalk- Online Learning of Social Representations	[40]	['[40]  L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107–1116. ACM, 2009. ']
308	6	[55]	To validate the performance of our approach we compare it against a number of baselines. // SpectralClustering ==[41]==: This method generates a representation in R d from the d-smallest eigenvectors of Le, the normalized graph Laplacian of G. // Utilizing the eigenvectors of Le implicitly assumes that graph cuts will be useful for classification.	b	DeepWalk- Online Learning of Social Representations	[41]	['[41]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ']
309	6	[33]	Utilizing the eigenvectors of Le implicitly assumes that graph cuts will be useful for classification. // Modularity ==[39]==: This method generates a representation in R d from the top-d eigenvectors of B, the Modularity matrix of G. The eigenvectors of B encode information about modular graph partitions of G [34]. // Using them as features assumes that modular graph partitions will be useful for classification.	b	DeepWalk- Online Learning of Social Representations	[39]	['[39]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 817–826, New York, NY, USA, 2009. ACM. ']
310	6	[75]	Utilizing the eigenvectors of Le implicitly assumes that graph cuts will be useful for classification. // Modularity [39]: This method generates a representation in R d from the top-d eigenvectors of B, the Modularity matrix of G. The eigenvectors of B encode information about modular graph partitions of G ==[34]==. // Using them as features assumes that modular graph partitions will be useful for classification.	b	DeepWalk- Online Learning of Social Representations	[34]	['[34]  M. E. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):8577–8582, 2006. ']
311	6	[56]	Using them as features assumes that modular graph partitions will be useful for classification. // EdgeCluster ==[40]==: This method uses k-means clustering to cluster the adjacency matrix of G. // Its has been shown to perform comparably to the Modularity method, with the added advantage of scaling to graphs which are too large for spectral decomposition.	b	DeepWalk- Online Learning of Social Representations	[40]	['[40]  L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107–1116. ACM, 2009. ']
312	6	[137]	Its has been shown to perform comparably to the Modularity method, with the added advantage of scaling to graphs which are too large for spectral decomposition. // wvRN ==[24]==: The weighted-vote Relational Neighbor is a relational classifier. // Given the neighborhood Ni of vertex vi, wvRN estimates Pr(yi|Ni) with the (appropriately normalized) weighted mean of its neighbors (i.e Pr(yi|Ni) = 1 ZPvj∈Ni wij Pr(yj | Nj )).	b	DeepWalk- Online Learning of Social Representations	[24]	['[24]  S. A. Macskassy and F. Provost. A simple relational classifier. In Proceedings of the Second Workshop on Multi-Relational Data Mining (MRDM-2003) at KDD-2003, pages 64–76, 2003. ']
313	6	[138]	Given the neighborhood Ni of vertex vi, wvRN estimates Pr(yi|Ni) with the (appropriately normalized) weighted mean of its neighbors (i.e Pr(yi|Ni) = 1 ZPvj∈Ni wij Pr(yj | Nj )). // It has shown surprisingly good performance in real networks, and has been advocated as a sensible relational classification baseline ==[25]==. // Majority: This na¨ıve method simply chooses the most frequent labels in the training set.	h+e	DeepWalk- Online Learning of Social Representations	[25]	['[25]  S. A. Macskassy and F. Provost. Classification in networked data: A toolkit and a univariate case study. The Journal of Machine Learning Research, 8:935–983, 2007. ']
314	6	[33, 56]	Multi-Label Classification. // To facilitate the comparison between our method and the relevant baselines, we use the exact same datasets and experimental procedure as in ==[39, 40]==. // Specifically, we randomly sample a portion (TR) of the labeled nodes, and use them as training data.	hoe	DeepWalk- Online Learning of Social Representations	[39, 40]	['[39]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 817–826, New York, NY, USA, 2009. ACM. ', '[40]  L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107–1116. ACM, 2009. ']
315	6	[33, 56]	We repeat this process 10 times, and report the average performance in terms of both Macro-F1 and Micro-F1. // When possible we report the original results ==[39, 40]== here directly. // For all models we use a one-vs-rest logistic regression implemented by LibLinear [10] for classification.	hoe	DeepWalk- Online Learning of Social Representations	[39, 40]	['[39]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 817–826, New York, NY, USA, 2009. ACM. ', '[40]  L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107–1116. ACM, 2009. ']
316	6	[61]	When possible we report the original results [39, 40] here directly. // For all models we use a one-vs-rest logistic regression implemented by LibLinear ==[10]== for classification. // We present results for DeepWalk with (γ = 80, w = 10, d = 128). The results for (SpectralClustering, Modularity, EdgeCluster) use Tang and Liu’s preferred dimensionality, d = 500.	hoe	DeepWalk- Online Learning of Social Representations	[10]	['[10]  R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, 2008. ']
317	6	[139]	The main differences between our proposed method and previous work can be summarized as follows. // We learn our latent social representations, instead of computing statistics related to centrality ==[12]== or partitioning [41]. // We do not attempt to extend the classification procedure itself (through collective inference [37] or graph kernels [20])	ro	DeepWalk- Online Learning of Social Representations	[12]	['[12]  B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. In Advances in Social Network Mining and Analysis, pages 1–19. Springer, 2010. ']
318	6	[55]	The main differences between our proposed method and previous work can be summarized as follows. // We learn our latent social representations, instead of computing statistics related to centrality [12] or partitioning ==[41]==. // We do not attempt to extend the classification procedure itself (through collective inference [37] or graph kernels [20])	ro	DeepWalk- Online Learning of Social Representations	[41]	['[41]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ']
319	6	[68]	We learn our latent social representations, instead of computing statistics related to centrality [12] or partitioning [41]. // We do not attempt to extend the classification procedure itself (through collective inference ==[37]== or graph kernels [20]). // We propose a scalable online method which uses only local information. Most methods require global information and are offline [16, 39–41].	ro	DeepWalk- Online Learning of Social Representations	[37]	['[37]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
320	6	[140]	We learn our latent social representations, instead of computing statistics related to centrality [12] or partitioning [41]. // We do not attempt to extend the classification procedure itself (through collective inference [37] or graph kernels ==[20]==). // We propose a scalable online method which uses only local information. Most methods require global information and are offline [16, 39–41].	ro	DeepWalk- Online Learning of Social Representations	[20]	['[20]  R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In ICML, volume 2, pages 315–322, 2002. ']
321	6	[129, 137, 127, 128]	Relational Learning. // Relational classification (or collective classification) methods ==[14,24,31,35]== use links between data items as part of the classification process. // Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge [37].	b	DeepWalk- Online Learning of Social Representations	[14, 24, 31, 35]	['[14]  S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721–741, 1984. ', '[24]  S. A. Macskassy and F. Provost. A simple relational classifier. In Proceedings of the Second Workshop on Multi-Relational Data Mining (MRDM-2003) at KDD-2003, pages 64–76, 2003. ', '[31]  J. Neville and D. Jensen. Iterative classification in relational data. In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 13–20, 2000. ', '[35]  J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988. ']
322	6	[68]	Relational classification (or collective classification) methods [14,24,31,35] use links between data items as part of the classification process. // Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge ==[37]==. // The most relevant relational classification algorithms to our work incorporate community information by learning clusters [32], by adding edges between nearby nodes [13], by using PageRank [23], or by extending relational classification to take additional features into account [43].	h-	DeepWalk- Online Learning of Social Representations	[37]	['[37]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
323	6	[141]	Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge [37]. // The most relevant relational classification algorithms to our work incorporate community information by learning clusters ==[32]==, by adding edges between nearby nodes [13], by using PageRank [23], or by extending relational classification to take additional features into account [43]. // Our work takes a substantially different approach.	ro	DeepWalk- Online Learning of Social Representations	[32]	['[32]  J. Neville and D. Jensen. Leveraging relational autocorrelation with latent group models. In Proceedings of the 4th International Workshop on Multi-relational Mining, MRDM ’05, pages 49–55, New York, NY, USA, 2005. ACM. ']
324	6	[142]	Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge [37]. // The most relevant relational classification algorithms to our work incorporate community information by learning clusters [32], by adding edges between nearby nodes ==[13]==, by using PageRank [23], or by extending relational classification to take additional features into account [43]. // Our work takes a substantially different approach.	ro	DeepWalk- Online Learning of Social Representations	[13]	['[13]  B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Faloutsos. Using ghost edges for classification in sparsely labeled networks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’08, pages 256–264, New York, NY, USA, 2008. ACM. ']
325	6	[143]	Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge [37]. // The most relevant relational classification algorithms to our work incorporate community information by learning clusters [32], by adding edges between nearby nodes [13], by using PageRank ==[23]==, or by extending relational classification to take additional features into account [43]. // Our work takes a substantially different approach.	ro	DeepWalk- Online Learning of Social Representations	[23]	['[23]  F. Lin and W. Cohen. Semi-supervised classification of network data using very few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192–199, Aug 2010. ']
326	6	[144]	Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge [37]. // The most relevant relational classification algorithms to our work incorporate community information by learning clusters [32], by adding edges between nearby nodes [13], by using PageRank [23], or by extending relational classification to take additional features into account ==[43]==. // Our work takes a substantially different approach.	ro	DeepWalk- Online Learning of Social Representations	[43]	['[43]  X. Wang and G. Sukthankar. Multi-label relational neighbor classification using social context features. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 464–472. ACM, 2013. ']
327	6	[6]	In contrast to these methods, we frame the feature creation procedure as a representation learning problem. // Graph Kernels ==[42]== have been proposed as a way to use relational data as part of the classification process, but are quite slow unless approximated [19]. // Our approach is complementary; instead of encoding the structure as part of a kernel function, we learn a representation which allows them to be used directly as features for any classification method.	h-	DeepWalk- Online Learning of Social Representations	[42]	['[42]  S. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. The Journal of Machine Learning Research, 99:1201–1242, 2010. ']
328	6	[145]	In contrast to these methods, we frame the feature creation procedure as a representation learning problem. // Graph Kernels [42] have been proposed as a way to use relational data as part of the classification process, but are quite slow unless approximated ==[19]==. // Our approach is complementary; instead of encoding the structure as part of a kernel function, we learn a representation which allows them to be used directly as features for any classification method.	h+	DeepWalk- Online Learning of Social Representations	[19]	['[19]  U. Kang, H. Tong, and J. Sun. Fast random walk graph kernel. In SDM, pages 828–838, 2012. ']
329	6	[146]	Unsupervised Feature Learning. // Distributed representations have been proposed to model structural relationship between concepts ==[17]==. // These representations are trained by the back-propagation and gradient descent.	b	DeepWalk- Online Learning of Social Representations	[17]	['[17]  G. E. Hinton. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1–12. Amherst, MA, 1986. ']
330	6	[147]	Computational costs and numerical instability led to these techniques to be abandoned for almost a decade. // Recently, distributed computing allowed for larger models to be trained ==[3]==, and the growth of data for unsupervised learning algorithms to emerge [9]. // Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision [21], speech recognition [7], and natural language processing [6].	ho	DeepWalk- Online Learning of Social Representations	[3]	['[3]  Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003. ']
331	6	[148]	Computational costs and numerical instability led to these techniques to be abandoned for almost a decade. // Recently, distributed computing allowed for larger models to be trained [3], and the growth of data for unsupervised learning algorithms to emerge ==[9]==. // Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision [21], speech recognition [7], and natural language processing [6].	ho	DeepWalk- Online Learning of Social Representations	[9]	['[9]  D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010. ']
332	6	[149]	Recently, distributed computing allowed for larger models to be trained [3], and the growth of data for unsupervised learning algorithms to emerge [9]. // Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision ==[21]==, speech recognition [7], and natural language processing [6]. // CONCLUSIONS	ho	DeepWalk- Online Learning of Social Representations	[21]	['[21]  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, volume 1, page 4, 2012. ']
333	6	[150]	Recently, distributed computing allowed for larger models to be trained [3], and the growth of data for unsupervised learning algorithms to emerge [9]. // Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision [21], speech recognition ==[7]==, and natural language processing [6]. // CONCLUSIONS	ho	DeepWalk- Online Learning of Social Representations	[7]	['[7]  G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012. ']
334	6	[125]	Recently, distributed computing allowed for larger models to be trained [3], and the growth of data for unsupervised learning algorithms to emerge [9]. // Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision [21], speech recognition [7], and natural language processing ==[6]==. // CONCLUSIONS	ho	DeepWalk- Online Learning of Social Representations	[6]	['[6]  R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2008. ']
335	1	[78]	This paper studies the problem of embedding information networks into lowdimensional spaces, in which every vertex is represented as a low-dimensional vector. // Such a low-dimensional embedding is very useful in a variety of applications such as visualization ==[21]==, node classification [3], link prediction [10], and recommendation [23]. // Various methods of graph embedding have been proposed in the machine learning literature (e.g., [4, 20, 2]).	h+	LINE- Large-scale Information Network Embedding	[21]	['[21]  L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008. ']
336	1	[11]	This paper studies the problem of embedding information networks into lowdimensional spaces, in which every vertex is represented as a low-dimensional vector. // Such a low-dimensional embedding is very useful in a variety of applications such as visualization [21], node classification ==[3]==, link prediction [10], and recommendation [23]. // Various methods of graph embedding have been proposed in the machine learning literature (e.g., [4, 20, 2]).	h+	LINE- Large-scale Information Network Embedding	[3]	['[3]  S. Bhagat, G. Cormode, and S. Muthukrishnan. Node classification in social networks. In Social Network Data Analytics, pages 115–148. Springer, 2011. ']
337	1	[70]	This paper studies the problem of embedding information networks into lowdimensional spaces, in which every vertex is represented as a low-dimensional vector. // Such a low-dimensional embedding is very useful in a variety of applications such as visualization [21], node classification [3], link prediction ==[10]==, and recommendation [23]. // Various methods of graph embedding have been proposed in the machine learning literature (e.g., [4, 20, 2]).	h+	LINE- Large-scale Information Network Embedding	[10]	['[10]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
338	1	[151]	This paper studies the problem of embedding information networks into lowdimensional spaces, in which every vertex is represented as a low-dimensional vector. // Such a low-dimensional embedding is very useful in a variety of applications such as visualization [21], node classification [3], link prediction [10], and recommendation ==[23]==. // Various methods of graph embedding have been proposed in the machine learning literature (e.g., [4, 20, 2]).	h+	LINE- Large-scale Information Network Embedding	[23]	['[23]  X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 283–292. ACM, 2014.']
339	1	[152, 3, 5]	Such a low-dimensional embedding is very useful in a variety of applications such as visualization [21], node classification [3], link prediction [10], and recommendation [23]. // Various methods of graph embedding have been proposed in the machine learning literature (e.g., ==[4, 20, 2]==). // They generally perform well on smaller networks, but problem becomes much more challenging when a real world information network is concerned, which typically contains millions of nodes and billions of edges.	h-	LINE- Large-scale Information Network Embedding	[4, 20, 2]	['[4]  T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000. ', '[20]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ', '[2]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
340	1	[153]	The problem becomes much more challenging when a real world information network is concerned, which typically contains millions of nodes and billions of edges. // For example, the Twitter followee-follower network contains 175 million active users and around twenty billion edges in 2012 ==[14]==. // Most existing graph embedding algorithms do not scale for networks of this size.	ho	LINE- Large-scale Information Network Embedding	[14]	['[14]  S. A. Myers, A. Sharma, P. Gupta, and J. Lin. Information network or social network?: the structure of the twitter follow graph. In Proceedings of the companion publication of the 23rd international conference on World wide web companion, pages 493–498. International World Wide Web Conferences Steering Committee, 2014. ']
341	1	[152]	Most existing graph embedding algorithms do not scale for networks of this size. // For example, the time complexity of classical graph embedding algorithms such as MDS ==[4]==, IsoMap [20], Laplacian eigenmap [2] are at least quadratic to the number of vertices, which is too expensive for networks with millions of nodes. // Although a few very recent studies approach the embedding of large-scale networks, these methods either use an indirect approach that is not designed for networks (e.g., [1]) or lack a clear objective function tailored for network embedding (e.g., [16]).	h-	LINE- Large-scale Information Network Embedding	[4]	['[4]  T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000. ']
342	1	[3]	Most existing graph embedding algorithms do not scale for networks of this size. // For example, the time complexity of classical graph embedding algorithms such as MDS [4], IsoMap ==[20]==, Laplacian eigenmap [2] are at least quadratic to the number of vertices, which is too expensive for networks with millions of nodes. // Although a few very recent studies approach the embedding of large-scale networks, these methods either use an indirect approach that is not designed for networks (e.g., [1]) or lack a clear objective function tailored for network embedding (e.g., [16]).	h-	LINE- Large-scale Information Network Embedding	[20]	['[20]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
343	1	[5]	Most existing graph embedding algorithms do not scale for networks of this size. // For example, the time complexity of classical graph embedding algorithms such as MDS [4], IsoMap [20], Laplacian eigenmap ==[2]== are at least quadratic to the number of vertices, which is too expensive for networks with millions of nodes. // Although a few very recent studies approach the embedding of large-scale networks, these methods either use an indirect approach that is not designed for networks (e.g., [1]) or lack a clear objective function tailored for network embedding (e.g., [16]).	h-	LINE- Large-scale Information Network Embedding	[2]	['[2]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
344	1	[154]	For example, the time complexity of classical graph embedding algorithms such as MDS [4], IsoMap [20], Laplacian eigenmap [2] are at least quadratic to the number of vertices, which is too expensive for networks with millions of nodes. // Although a few very recent studies approach the embedding of large-scale networks, these methods either use an indirect approach that is not designed for networks (e.g., ==[1]==) or lack a clear objective function tailored for network embedding (e.g., [16]). // We anticipate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the embedding of millions of nodes.	h-	LINE- Large-scale Information Network Embedding	[1]	['[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37–48. International World Wide Web Conferences Steering Committee, 2013. ']
345	1	[6]	For example, the time complexity of classical graph embedding algorithms such as MDS [4], IsoMap [20], Laplacian eigenmap [2] are at least quadratic to the number of vertices, which is too expensive for networks with millions of nodes. // Although a few very recent studies approach the embedding of large-scale networks, these methods either use an indirect approach that is not designed for networks (e.g., [1]) or lack a clear objective function tailored for network embedding (e.g., ==[16]==). // We anticipate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the embedding of millions of nodes.	h-	LINE- Large-scale Information Network Embedding	[16]	['[16]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
346	1	[3]	Naturally, the local structures are represented by the observed links in the networks, which capture the first-order proximity between the vertices. // Most existing graph embedding algorithms are designed to preserve this first-order proximity, e.g., IsoMap ==[20]== and Laplacian eigenmap [2], even if they do not scale. // We observe that in a real-world network many (if not the majority of) legitimate links are actually not observed.	h-	LINE- Large-scale Information Network Embedding	[20]	['[20]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
347	1	[5]	Naturally, the local structures are represented by the observed links in the networks, which capture the first-order proximity between the vertices. // Most existing graph embedding algorithms are designed to preserve this first-order proximity, e.g., IsoMap [20] and Laplacian eigenmap ==[2]==, even if they do not scale. // We observe that in a real-world network many (if not the majority of) legitimate links are actually not observed.	h-	LINE- Large-scale Information Network Embedding	[2]	['[2]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
348	1	[155]	The general notion of the second-order proximity can be interpreted as nodes with shared neighbors being likely to be similar. Such an intuition can be found in the theories of sociology and linguistics. // For example, “the degree of overlap of two people’s friendship networks correlates with the strength of ties between them,” in a social network ==[6]==; and “You shall know a word by the company it keeps” (Firth, J. R. 1957:11) in text corpora [5]. // Indeed, people who share many common friends are likely to share the same interest and become friends, and words that are used together with many similar words are likely to have similar meanings.	b	LINE- Large-scale Information Network Embedding	[6]	['[6]  M. S. Granovetter. The strength of weak ties. American journal of sociology, pages 1360–1380, 1973. ']
349	1	[156]	The general notion of the second-order proximity can be interpreted as nodes with shared neighbors being likely to be similar. Such an intuition can be found in the theories of sociology and linguistics. // For example, “the degree of overlap of two people’s friendship networks correlates with the strength of ties between them,” in a social network [6]; and “You shall know a word by the company it keeps” (Firth, J. R. 1957:11) in text corpora ==[5]==. // Indeed, people who share many common friends are likely to share the same interest and become friends, and words that are used together with many similar words are likely to have similar meanings.	b	LINE- Large-scale Information Network Embedding	[5]	['[5]  J. R. Firth. A synopsis of linguistic theory, 1930–1955. In J. R. Firth (Ed.), Studies in linguistic analysis, pages 1–32. ']
350	1	[152]	RELATED WORK. // Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) ==[4]==, IsoMap [20], LLE [18] and Laplacian Eigenmap [2]. // These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph [22] into a low dimensional space.	b	LINE- Large-scale Information Network Embedding	[4]	['[4]  T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000. ']
351	1	[3]	RELATED WORK. // Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) [4], IsoMap ==[20]==, LLE [18] and Laplacian Eigenmap [2]. // These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph [22] into a low dimensional space.	b	LINE- Large-scale Information Network Embedding	[20]	['[20]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
352	1	[4]	RELATED WORK. // Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) [4], IsoMap [20], LLE ==[18]== and Laplacian Eigenmap [2]. // These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph [22] into a low dimensional space.	b	LINE- Large-scale Information Network Embedding	[18]	['[18]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ']
353	1	[5]	RELATED WORK. // Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) [4], IsoMap [20], LLE [18] and Laplacian Eigenmap ==[2]==. // These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph [22] into a low dimensional space.	b	LINE- Large-scale Information Network Embedding	[2]	['[2]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
354	1	[157]	Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) [4], IsoMap [20], LLE [18] and Laplacian Eigenmap [2]. // These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph ==[22]== into a low dimensional space. // However, these algorithms usually rely on solving the leading eigenvectors of the affinity matrices, the complexity of which is at least quadratic to the number of nodes, making them inefficient to handle large-scale networks.	b	LINE- Large-scale Information Network Embedding	[22]	['[22]  S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1):40–51, 2007. ']
355	1	[154]	However, these algorithms usually rely on solving the leading eigenvectors of the affinity matrices, the complexity of which is at least quadratic to the number of nodes, making them inefficient to handle large-scale networks. // Among the most recent literature is a technique called graph factorization ==[1]==. // It finds the low-dimensional embedding of a large graph through matrix factorization, which is optimized using stochastic gradient descent.	b	LINE- Large-scale Information Network Embedding	[1]	['[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37–48. International World Wide Web Conferences Steering Committee, 2013. ']
356	1	[6]	Practically, the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs. // The most recent work related with ours is DeepWalk ==[16]==, which deploys a truncated random walk for social network embedding. // Although empirically effective, the DeepWalk does not provide a clear objective that articulates what network properties are preserved.	h-	LINE- Large-scale Information Network Embedding	[16]	['[16]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
357	1	[70]	Because of this importance, many existing graph embedding algorithms such as IsoMap, LLE, Laplacian eigenmap, and graph factorization have the objective to preserve the first-order proximity // However, in a real world information network, the links observed are only a small proportion, with many others missing ==[10]==. //  A pair of nodes on a missing link has a zero first-order proximity, even though they are intrinsically very similar to each other.	ho	LINE- Large-scale Information Network Embedding	[10]	['[10]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
358	1	[158]	Therefore, we minimize the following objective function: O2 = X i∈V λid(ˆp2(·|vi), p2(·|vi)), (5) where d(·, ·) is the distance between two distributions. // As the importance of the vertices in the network may be different, we introduce λi in the objective function to represent the prestige of vertex i in the network, which can be measured by the degree or estimated through algorithms such as PageRank ==[15]==. // The empirical distribution ˆp2(·|vi) is defined as ˆp2(vj |vi) = wij di , where wij is the weight of the edge (i, j) and di is the out-degree of vertex i, i.e. di = P k∈N(i) wik, where N(i) is the set of out-neighbors of vi.	hoe	LINE- Large-scale Information Network Embedding	[15]	['[15]  L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. 1999. ']
359	1	[36]	Optimizing objective (6) is computationally expensive, which requires the summation over the entire set of vertices when calculating the conditional probability p2(·|vi). // To address this problem, we adopt the approach of negative sampling proposed in ==[13]==, which samples multiple negative edges according to some noisy distribution for each edge (i, j). // More specifically, it specifies the following objective function for each edge (i, j): log σ(~u0 j T · ~ui) +XK i=1 Evn∼Pn(v)[log σ(−~u0 n T · ~ui)], (7) where σ(x) = 1/(1 + exp(−x)) is the sigmoid function.	hoe	LINE- Large-scale Information Network Embedding	[13]	['[13]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119, 2013. ']
360	1	[135]	To avoid the trivial solution, we can still utilize the negative sampling approach (7) by just changing ~u0T j to ~uT j. // We adopt the asynchronous stochastic gradient algorithm (ASGD) ==[17]== for optimizing Eqn (7). // In each step, the ASGD algorithm samples a mini-batch of edges and then updates the model parameters.	hoe	LINE- Large-scale Information Network Embedding	[17]	['[17]  B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693–701, 2011. ']
361	1	[6]	Flickr and Youtube2. // The Flickr network is denser than the Youtube network (the same network as used in DeepWalk ==[16]==). //  (3) Citation Networks.	b	LINE- Large-scale Information Network Embedding	[16]	['[16]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
362	1	[159]	Two types of citation networks are used: an author citation network and a paper citation network. // We use the DBLP data set ==[19]== to construct the citation networks between authors and between papers. // The author citation network records the number of papers written by one author and cited by another author.	hoe	LINE- Large-scale Information Network Embedding	[19]	['[19]  J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990–998. ACM, 2008. ']
363	1	[154]	We do not compare with some classical graph embedding algorithms such as MDS, IsoMap, and Laplacian eigenmap, as they cannot handle networks of this scale. // Graph factorization (GF) ==[1]==: We compare with the matrix factorization techniques for graph factorization. // An information network can be represented as an affinity matrix, and is able to represent each vertex with a low-dimensional vector through matrix factorization. Graph factorization is optimized through stochastic gradient descent and is able to handle large networks. It only applies to undirected networks.	b	LINE- Large-scale Information Network Embedding	[1]	['[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37–48. International World Wide Web Conferences Steering Committee, 2013. ']
364	1	[6]	Graph factorization is optimized through stochastic gradient descent and is able to handle large networks. It only applies to undirected networks. // DeepWalk ==[16]==. DeepWalk is an approach recently proposed for social network embedding, which is only applicable for networks with binary edges. // For each vertex, truncated random walks starting from the vertex are used to obtain the contextual information, and therefore only second-order proximity is utilized.	b	LINE- Large-scale Information Network Embedding	[16]	['[16]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
365	1	[36]	The mini-batch size of the stochastic gradient descent is set as 1 for all the methods. // Similar to ==[13]==, the learning rate is set with the starting value ρ0 = 0.025 and ρt = ρ0(1−t/T), where T is the total number of mini-batches or edge samples. // For fair comparisons, the dimensionality of the embeddings of the language network is set to 200, as used in word embedding [13].	hoe	LINE- Large-scale Information Network Embedding	[13]	['[13]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119, 2013. ']
366	1	[6]	For fair comparisons, the dimensionality of the embeddings of the language network is set to 200, as used in word embedding [13]. // For other networks, the dimension is set as 128 by default, as used in ==[16]==. // Other default settings include: the number of negative samples K = 5 for LINE and LINE-SGD; the total number of samples T = 10 billion for LINE(1st) and LINE(2nd), T = 20 billion for GF; window size win = 10, walk length t = 40, walks per vertex γ = 40 for DeepWalk. All the embedding vectors are finally normalized by setting || ~w||2 = 1.	hoe	LINE- Large-scale Information Network Embedding	[16]	['[16]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
367	1	[35]	We start with the results on the language network, which contains two million nodes and a billion edges. // Two applications are used to evaluate the effectiveness of the learned embeddings: word analogy ==[12]== and document classification. // Word Analogy.	hoe	LINE- Large-scale Information Network Embedding	[12]	['[12]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
368	1	[35]	Word Analogy. // This task is introduced by Mikolov et al. ==[12]==. // Given a word pair (a, b) and a word c, the task aims to find a word d, such that the relation between c and d is similar to the relation between a and b, or denoted as: a : b → c :?.	b	LINE- Large-scale Information Network Embedding	[12]	['[12]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
369	1	[35]	For DeepWalk, dierent cuto thresholds are tried to convert the language network into a binary network, and the best performance is achieved when all the edges are kept in the network. // We also compare with the state-of-the-art word embedding model SkipGram ==[12]==, which learns the word embeddings directly from the original Wikipedia pages and is also implicitly a matrix factorization approach [8]. // The window size is set as 5, the same as used for constructing the language network.	b	LINE- Large-scale Information Network Embedding	[12]	['[12]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
370	1	[160]	For DeepWalk, dierent cuto thresholds are tried to convert the language network into a binary network, and the best performance is achieved when all the edges are kept in the network. // We also compare with the state-of-the-art word embedding model SkipGram [12], which learns the word embeddings directly from the original Wikipedia pages and is also implicitly a matrix factorization approach ==[8]==. // The window size is set as 5, the same as used for constructing the language network.	b	LINE- Large-scale Information Network Embedding	[8]	['[8]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185, 2014. ']
371	1	[161]	All document vectors are used to train a one-vs-rest logistic regression classier using the LibLinear package4. // We report the classification metrics Micro-F1 and Macro-F1 ==[11]==. // The results are averaged over 10 different runs by sampling different training data.	hoe	LINE- Large-scale Information Network Embedding	[11]	['[11]  C. D. Manning, P. Raghavan, and H. Schutze. ¨ Introduction to information retrieval, volume 1. Cambridge university press Cambridge, 2008. ']
372	1	[78]	Laying out this co-author network is very challenging as the three research elds are very close to each other. // We first map the co-author network into a low-dimensional space with different embedding approaches and then further map the lowdimensional vectors of the vertices to a 2-D space with the t-SNE package ==[21]==. // Fig. 2 compares the visualization results with different embedding approaches.	hoe	LINE- Large-scale Information Network Embedding	[21]	['[21]  L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008. ']
373	37	[162]	Many important tasks in network analysis involve predictions over nodes and edges. // In a typical node classification task, we are interested in predicting the most probable labels of nodes in a network ==[33]==. // For example, in a social network, we might be interested in predicting interests of users, or in a protein-protein interaction network we might be interested in predicting functional labels of proteins [25, 37].	b	node2vec- Scalable Feature Learning for Networks	[33]	['[33]  G. Tsoumakas and I. Katakis. Multi-label classification: An overview. Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006. ']
374	37	[163, 164]	In a typical node classification task, we are interested in predicting the most probable labels of nodes in a network [33]. // For example, in a social network, we might be interested in predicting interests of users, or in a protein-protein interaction network we might be interested in predicting functional labels of proteins ==[25, 37]==. //	b	node2vec- Scalable Feature Learning for Networks	[25, 37]	['[25]  P. Radivojac, W. T. Clark, T. R. Oron, A. M. Schnoes, T. Wittkop, A. Sokolov, K. Graim, C. Funk, Verspoor, et al. A large-scale evaluation of computational protein function prediction. Nature methods, 10(3):221–227, 2013. ', '[37]  S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. Like like alike: joint friendship and interest propagation in social networks. In WWW, 2011. ']
375	37	[70]	For example, in a social network, we might be interested in predicting interests of users, or in a protein-protein interaction network we might be interested in predicting functional labels of proteins [25, 37]. // Similarly, in link prediction, we wish to predict whether a pair of nodes in a network should have an edge connecting them ==[18]==. // Link prediction is useful in a wide variety of domains; for instance, in genomics, it helps us discover novel interactions between genes, and in social networks, it can identify real-world friends [2, 34].	b	node2vec- Scalable Feature Learning for Networks	[18]	['[18]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. J. of the American society for information science and technology, 58(7):1019–1031, 2007. ']
376	37	[165, 166]	Similarly, in link prediction, we wish to predict whether a pair of nodes in a network should have an edge connecting them [18]. // Link prediction is useful in a wide variety of domains; for instance, in genomics, it helps us discover novel interactions between genes, and in social networks, it can identify real-world friends ==[2, 34]==. // Any supervised machine learning algorithm requires a set of informative, discriminating, and independent features.	b	node2vec- Scalable Feature Learning for Networks	[2, 34]	['[2]  L. Backstrom and J. Leskovec. Supervised random walks: predicting and recommending links in social networks. In WSDM, 2011. ', '[34]  A. Vazquez, A. Flammini, A. Maritan, and A. Vespignani. Global protein function prediction from protein-protein interaction networks. Nature biotechnology, 21(6):697–700, 2003. ']
377	37	[124]	Even if one discounts the tedious effort required for feature engineering, such features are usually designed for specific tasks and do not generalize across different prediction tasks. // An alternative approach is to learn feature representations by solving an optimization problem ==[4]==. // The challenge in feature learning is defining an objective function, which involves a trade-off in balancing computational efficiency and predictive accuracy.	b	node2vec- Scalable Feature Learning for Networks	[4]	['[4]  Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE TPAMI, 35(8):1798–1828, 2013. ']
378	37	[35, 167]	At the other extreme, the objective function can be defined to be independent of the downstream prediction task and the representations can be learned in a purely unsupervised way. // This makes the optimization computationally efficient and with a carefully designed objective, it results in task-independent features that closely match task-specific approaches in predictive accuracy ==[21, 23]==. // However, current techniques fail to satisfactorily define and optimize a reasonable objective required for scalable unsupervised feature learning in networks.	h-	node2vec- Scalable Feature Learning for Networks	[21, 23]	['[21]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013. ', '[23]  J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word representation. In EMNLP, 2014. ']
379	37	[168, 4, 3, 157]	However, current techniques fail to satisfactorily define and optimize a reasonable objective required for scalable unsupervised feature learning in networks. // Classic approaches based on linear and non-linear dimensionality reduction techniques such as Principal Component Analysis, Multi-Dimensional Scaling and their extensions ==[3, 27, 30, 35]== optimize an objective that transforms a representative data matrix of the network such that it maximizes the variance of the data representation. // Consequently, these approaches invariably involve eigende composition of the appropriate data matrix which is expensive for large real-world networks.	h-	node2vec- Scalable Feature Learning for Networks	[3, 27, 30, 35]	['[3]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, 2001. ', '[27]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ', '[30]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ', '[35]  S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. IEEE TPAMI, 29(1):40–51, 2007. ']
380	37	[6, 1]	The objective can be efficiently optimized using stochastic gradient descent (SGD) akin to backpropogation on just single hidden-layer feedforward neural networks. // Recent attempts in this direction ==[24, 28]== propose efficient algorithms but rely on a rigid notion of a network neighborhood, which results in these approaches being largely insensitive to connectivity patterns unique to networks. // Specifically, nodes in networks could be organized based on communities they belong to (i.e., homophily).	h-	node2vec- Scalable Feature Learning for Networks	[24, 28]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ', '[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
381	37	[71, 169, 170]	Recent attempts in this direction [24, 28] propose efficient algorithms but rely on a rigid notion of a network neighborhood, which results in these approaches being largely insensitive to connectivity patterns unique to networks. // Specifically, nodes in networks could be organized based on communities they belong to (i.e., homophily); in other cases, the organization could be based on the structural roles of nodes in the network (i.e., structural equivalence) ==[7, 10, 36]==. // For instance, in Figure 1, we observe nodes u and s1 belonging to the same tightly knit community of nodes, while the nodes u and s6 in the two distinct communities share the same structural role of a hub node.	b	node2vec- Scalable Feature Learning for Networks	[7, 10, 36]	['[7]  S. Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75 – 174, 2010. ', '[10]  K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu, L. Akoglu, D. Koutra, C. Faloutsos, and L. Li. RolX: structural role extraction & mining in large graphs. In KDD, 2012. ', '[36]  J. Yang and J. Leskovec. Overlapping communities explain core-periphery organization of networks. Proceedings of the IEEE, 102(12):1892–1902, 2014. ']
382	37	[35]	We propose node2vec, a semi-supervised algorithm for scalable feature learning in networks. // We optimize a custom graph-based objective function using SGD motivated by prior work on natural language processing ==[21]==. // Intuitively, our approach returns feature representations that maximize the likelihood of preserving network neighborhoods of nodes in a d-dimensional feature space.	ho	node2vec- Scalable Feature Learning for Networks	[21]	['[21]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013. ']
383	37	[6, 1]	We achieve this by developing a family of biased random walks, which efficiently explore diverse neighborhoods of a given node. // The resulting algorithm is flexible, giving us control over the search space through tunable parameters, in contrast to rigid search procedures in prior work ==[24, 28]==. // Consequently, our method generalizes prior work and can model the full spectrum of equivalences observed in networks.	h-	node2vec- Scalable Feature Learning for Networks	[24, 28]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ', '[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
384	37	[6, 1]	Our experiments focus on two common prediction tasks in networks: a multi-label classification task, where every node is assigned one or more class labels and a link prediction task, where we predict the existence of an edge given a pair of nodes. // We contrast the performance of node2vec with state-of-the-art feature learning algorithms ==[24, 28]==. // We experiment with several real-world networks from diverse domains, such as social networks, information networks, as well as networks from systems biology.	b	node2vec- Scalable Feature Learning for Networks	[24, 28]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ', '[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
385	37	[139, 171]	Feature engineering has been extensively studied by the machine learning community under various headings. // In networks, the conventional paradigm for generating features for nodes is based on feature extraction techniques which typically involve some seed hand-crafted features based on network properties ==[8, 11]==. // In contrast, our goal is to automate the whole process by casting feature extraction as a representation learning problem in which case we do not require any hand-engineered features.	h-	node2vec- Scalable Feature Learning for Networks	[8, 11]	['[8]  B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. In Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis. Springer, 2009. ', '[11]  K. Henderson, B. Gallagher, L. Li, L. Akoglu, T. Eliassi-Rad, H. Tong, and C. Faloutsos. It’s who you know: graph mining using recursive structural features. In KDD, 2011. ']
386	37	[168, 4, 3, 157]	Under this linear algebra perspective, these methods can be viewed as dimensionality reduction techniques.Under this linear algebra perspective, these methods can be viewed as dimensionality reduction techniques. // Several linear (e.g., PCA) and non-linear (e.g., IsoMap) dimensionality reduction techniques have been proposed ==[3, 27, 30, 35]==. // These methods suffer from both computational and statistical performance drawbacks. In terms of computational efficiency, eigendecomposition of a data matrix is expensive unless the solution quality is significantly compromised with approximations, and hence, these methods are hard to scale to large networks.	h-	node2vec- Scalable Feature Learning for Networks	[3, 27, 30, 35]	['[3]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, 2001. ', '[27]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ', '[30]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ', '[35]  S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. IEEE TPAMI, 29(1):40–51, 2007. ']
387	37	[55]	Secondly, these methods optimize for objectives that are not robust to the diverse patterns observed in networks (such as homophily and structural equivalence) and make assumptions about the relationship between the underlying network structure and the prediction task. // For instance, spectral clustering makes a strong homophily assumption that graph cuts will be useful for classification ==[29]==. // Such assumptions are reasonable in many scenarios, but unsatisfactory in effectively generalizing across diverse networks.	h-	node2vec- Scalable Feature Learning for Networks	[29]	['[29]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ']
388	37	[35]	Recent advancements in representational learning for natural language processing opened new ways for feature learning of discrete objects such as words. // In particular, the Skip-gram model ==[21]== aims to learn continuous feature representations for words by optimizing a neighborhood preserving likelihood objective. // The algorithm proceeds as follows: It scans over the words of a document, and for every word it aims to embed it such that the word’s features can predict nearby words (i.e., words inside some context window).	b	node2vec- Scalable Feature Learning for Networks	[21]	['[21]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013. ']
389	37	[36]	It scans over the words of a document, and for every word it aims to embed it such that the word’s features can predict nearby words (i.e., words inside some context window). // The word feature representations are learned by optmizing the likelihood objective using SGD with negative sampling ==[22]==. // The Skip-gram objective is based on the distributional hypothesis which states that words in similar contexts tend to have similar meanings [9].	b	node2vec- Scalable Feature Learning for Networks	[22]	['[22]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. ']
390	37	[172]	The word feature representations are learned by optmizing the likelihood objective using SGD with negative sampling [22]. // The Skip-gram objective is based on the distributional hypothesis which states that words in similar contexts tend to have similar meanings ==[9]==. // That is, similar words tend to appear in similar word neighborhoods	b	node2vec- Scalable Feature Learning for Networks	[9]	['[9]  Z. S. Harris. Word. Distributional Structure, 10(23):146–162, 1954. ']
391	37	[6, 1]	That is, similar words tend to appear in similar word neighborhoods. // Inspired by the Skip-gram model, recent research established an analogy for networks by representing a network as a “document” ==[24, 28]==. // The same way as a document is an ordered sequence of words, one could sample sequences of nodes from the underlying network and turn a network into a ordered sequence of nodes.	b	node2vec- Scalable Feature Learning for Networks	[24, 28]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ', '[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
392	37	[6, 1]	In fact, as we shall show, there is no clear winning sampling strategy that works across all networks and all prediction tasks. // This is a major shortcoming of prior work which fail to offer any flexibility in sampling of nodes from a network ==[24, 28]==. // Our algorithm node2vec overcomes this limitation by designing a flexible objective that is not tied to a particular sampling strategy and provides parameters to tune the explored search space (see Section 3).	h-	node2vec- Scalable Feature Learning for Networks	[24, 28]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ', '[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
393	37	[173, 174, 175, 176, 177]	Our algorithm node2vec overcomes this limitation by designing a flexible objective that is not tied to a particular sampling strategy and provides parameters to tune the explored search space (see Section 3). // Finally, for both node and edge based prediction tasks, there is a body of recent work for supervised feature learning based on existing and novel graph-specific deep network architectures ==[15, 16, 17, 31, 39]==. // These architectures directly minimize the loss function for a downstream prediction task using several layers of non-linear transformations which results in high accuracy, but at the cost of scalability due to high training time requirements.	h-	node2vec- Scalable Feature Learning for Networks	[15, 16, 17, 31, 39]	['[15]  K. Li, J. Gao, S. Guo, N. Du, X. Li, and A. Zhang. LRBM: A restricted boltzmann machine based approach for representation learning on linked data. In ICDM, 2014. ', '[16]  X. Li, N. Du, H. Li, K. Li, J. Gao, and A. Zhang. A deep learning approach to link prediction in dynamic networks. In ICDM, 2014. ', '[17]  Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In ICLR, 2016. ', '[31]  F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In AAAI, 2014. ', '[39]  S. Zhai and Z. Zhang. Dropout training of matrix factorization and autoencoder for link prediction in sparse graphs. In SDM, 2015.']
394	37	[35, 6]	For every source node u ∈ V , we define NS(u) ⊂ V as a network neighborhood of node u generated through a neighborhood sampling strategy S. // We proceed by extending the Skip-gram architecture to networks ==[21, 24]==. // We seek to optimize the following objective function, which maximizes the log-probability of observing a network neighborhood NS(u) for a node u conditioned on its feature representation, given by f: max f X u∈V log P r(NS(u)|f(u)).	ho	node2vec- Scalable Feature Learning for Networks	[21, 24]	['[21]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013. ', '[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ']
395	37	[36]	With the above assumptions, the objective in Eq. 1 simplifies to. // The per-node partition function, Zu = P v∈V exp(f(u) · f(v)), is expensive to compute for large networks and we approximate it using negative sampling ==[22]==. // We optimize Eq. 2 using stochastic gradient ascent over the model parameters defining the features f.	h+	node2vec- Scalable Feature Learning for Networks	[22]	['[22]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. ']
396	37	[178]	The breadth-first and depth-first sampling represent extreme scenarios in terms of the search space they explore leading to interesting implications on the learned representations. // In particular, prediction tasks on nodes in networks often shuttle between two kinds of similarities: homophily and structural equivalence ==[12]==. // Under the homophily hypothesis [7, 36] nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together (e.g., nodes s1 and u in Figure 1 belong to the same network community).	b	node2vec- Scalable Feature Learning for Networks	[12]	['[12]  P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis. J. of the American Statistical Association, 2002. ']
397	37	[71, 170]	In particular, prediction tasks on nodes in networks often shuttle between two kinds of similarities: homophily and structural equivalence [12]. // Under the homophily hypothesis ==[7, 36]== nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together (e.g., nodes s1 and u in Figure 1 belong to the same network community). //  In contrast, under the structural equivalence hypothesis [10]nodes that have similar structural roles in networks should be embedded closely together (e.g., nodes u and s6 in Figure 1 act as hubs of their corresponding communities).	b	node2vec- Scalable Feature Learning for Networks	[7, 36]	['[7]  S. Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75 – 174, 2010. ', '[36]  J. Yang and J. Leskovec. Overlapping communities explain core-periphery organization of networks. Proceedings of the IEEE, 102(12):1892–1902, 2014. ']
398	37	[169]	Under the homophily hypothesis [7, 36] nodes that are highly interconnected and belong to similar network clusters or communities should be embedded closely together (e.g., nodes s1 and u in Figure 1 belong to the same network community). // In contrast, under the structural equivalence hypothesis ==[10]== nodes that have similar structural roles in networks should be embedded closely together (e.g., nodes u and s6 in Figure 1 act as hubs of their corresponding communities). // Importantly, unlike homophily, structural equivalence does not emphasize connectivity; nodes could be far apart in the network and still have the same structural role.	b	node2vec- Scalable Feature Learning for Networks	[10]	['[10]  K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu, L. Akoglu, D. Koutra, C. Faloutsos, and L. Li. RolX: structural role extraction & mining in large graphs. In KDD, 2012. ']
399	37	[179]	We now aim to empirically demonstrate this fact and show that node2vec in fact can discover embeddings that obey both principles. // We use a network where nodes correspond to characters in the novel Les Misérables ==[13]== and edges connect coappearing characters. // The network has 77 nodes and 254 edg	ho	node2vec- Scalable Feature Learning for Networks	[13]	['[13]  D. E. Knuth. The Stanford GraphBase: a platform for combinatorial computing, volume 37. Addison-Wesley Reading, 1993. ']
400	37	[55]	// Spectral clustering ==[29]==: This is a matrix factorization approach in which we take the top d eigenvectors of the normalized Laplacian matrix of graph G as the feature vector representations for nodes. // DeepWalk [24]: This approach learns d-dimensional feature representations by simulating uniform random walks. The sampling strategy in DeepWalk can be seen as a special case of node2vec with p = 1 and q = 1.	b	node2vec- Scalable Feature Learning for Networks	[29]	['[29]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ']
401	37	[6]	Spectral clustering [29]: This is a matrix factorization approach in which we take the top d eigenvectors of the normalized Laplacian matrix of graph G as the feature vector representations for nodes. // DeepWalk ==[24]==: This approach learns d-dimensional feature representations by simulating uniform random walks. The sampling strategy in DeepWalk can be seen as a special case of node2vec with p = 1 and q = 1. // LINE [28]: This approach learns d-dimensional feature representations in two separate phases.	b	node2vec- Scalable Feature Learning for Networks	[24]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ']
402	37	[1]	DeepWalk [24]: This approach learns d-dimensional feature representations by simulating uniform random walks. The sampling strategy in DeepWalk can be seen as a special case of node2vec with p = 1 and q = 1. // LINE ==[28]==: This approach learns d-dimensional feature representations in two separate phases. // In the first phase, it learns d/2 dimensions by BFS-style simulations over immediate neighbors of nodes.	b	node2vec- Scalable Feature Learning for Networks	[28]	['[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
403	37	[6]	In the second phase, it learns the next d/2 dimensions by sampling nodes strictly at a 2-hop distance from the source nodes. // We exclude other matrix factorization approaches which have already been shown to be inferior to DeepWalk ==[24]==. // We also exclude a recent approach, GraRep [6], that generalizes LINE to incorporate information from network neighborhoods beyond 2-hops, but is unable to efficiently scale to large networks.	ho	node2vec- Scalable Feature Learning for Networks	[24]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ']
404	37	[26]	We exclude other matrix factorization approaches which have already been shown to be inferior to DeepWalk [24]. // We also exclude a recent approach, GraRep ==[6]==, that generalizes LINE to incorporate information from network neighborhoods beyond 2-hops, but is unable to efficiently scale to large networks. // In contrast to the setup used in prior work for evaluating samplingbased feature learning algorithms, we generate an equal number of samples for each method and then evaluate the quality of the obtained features on the prediction task.	h-	node2vec- Scalable Feature Learning for Networks	[6]	['[6]  S. Cao, W. Lu, and Q. Xu. GraRep: Learning Graph Representations with global structural information. In CIKM, 2015. ']
405	37	[36]	DeepWalk uses hierarchical sampling to approximate the softmax probabilities with an objective similar to the one use by node2vec. // However, hierarchical softmax is inefficient when compared with negative sampling ==[22]==. // Hence, keeping everything else the same, we switch to negative sampling in DeepWalk which is also the de facto approximation in node2vec and LINE.	ho	node2vec- Scalable Feature Learning for Networks	[22]	['[22]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. ']
406	37	[180]	This is a challenging task especially if L is large. // BlogCatalog ==[38]==: This is a network of social relationships of the bloggers listed on the BlogCatalog website. // The labels represent blogger interests inferred through the metadata provided by the bloggers, the network has 10,312 nodes, 333,983 edges, and 39 different labels.	b	node2vec- Scalable Feature Learning for Networks	[38]	['[38]  R. Zafarani and H. Liu. Social computing data repository at ASU, 2009. ']
407	37	[181]	The network has 10,312 nodes, 333,983 edges, and 39 different labels. // Protein-Protein Interactions (PPI) ==[5]==: We use a subgraph of the PPI network for Homo Sapiens. // The subgraph corresponds to the graph induced by nodes for which we could obtain labels from the hallmark gene sets [19] and represent biological states.	ho	node2vec- Scalable Feature Learning for Networks	[5]	['[5]  B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. Bähler, V. Wood, et al. The BioGRID interaction database. Nucleic acids research, 36:D637–D640, 2008. ']
408	37	[87]	Protein-Protein Interactions (PPI) [5]: We use a subgraph of the PPI network for Homo Sapiens. // The subgraph corresponds to the graph induced by nodes for which we could obtain labels from the hallmark gene sets ==[19]== and represent biological states. // The network has 3,890 nodes, 76,584 edges, and 50 different labels.	ho	node2vec- Scalable Feature Learning for Networks	[19]	['[19]  A. Liberzon, A. Subramanian, R. Pinchback, H. Thorvaldsdóttir, P. Tamayo, and J. P. Mesirov. Molecular signatures database (MSigDB) 3.0. Bioinformatics, 27(12):1739–1740, 2011. ']
409	37	[182]	The network has 3,890 nodes, 76,584 edges, and 50 different labels. // Wikipedia ==[20]==: This is a cooccurrence network of words appearing in the first million bytes of the Wikipedia dump. // The labels represent the Part-of-Speech (POS) tags inferred using the Stanford POS-Tagger [32].	b	node2vec- Scalable Feature Learning for Networks	[20]	['[20]  M. Mahoney. Large text compression benchmark. www.mattmahoney.net/dc/textdata, 2011. ']
410	37	[183]	Wikipedia [20]: This is a cooccurrence network of words appearing in the first million bytes of the Wikipedia dump. // The labels represent the Part-of-Speech (POS) tags inferred using the Stanford POS-Tagger ==[32]==. //  The network has 4,777 nodes, 184,812 edges, and 40 different labels.	b	node2vec- Scalable Feature Learning for Networks	[32]	['[32]  K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL, 2003. ']
411	37	[184]	We test our benchmarks on the following datasets. // Facebook ==[14]==: In the Facebook network, nodes represent users, and edges represent a friendship relation between any two users. // The network has 4,039 nodes and 88,234 edges.	b	node2vec- Scalable Feature Learning for Networks	[14]	['[14]  J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. ']
412	37	[181]	The network has 4,039 nodes and 88,234 edges. // Protein-Protein Interactions (PPI) ==[5]==: In the PPI network for Homo Sapiens, nodes represent proteins, and an edge indicates a biological interaction between a pair of proteins. // The network has 19,706 nodes and 390,633 edges.	b	node2vec- Scalable Feature Learning for Networks	[5]	['[5]  B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. Bähler, V. Wood, et al. The BioGRID interaction database. Nucleic acids research, 36:D637–D640, 2008. ']
413	37	[184]	The network has 19,706 nodes and 390,633 edges. // arXiv ASTRO-PH ==[14]==: This is a collaboration network generated from papers submitted to the e-print arXiv where nodes represent scientists, and an edge is present between two scientists if they have collaborated in a paper. // The network has 18,722 nodes and 198,110 edges.	b	node2vec- Scalable Feature Learning for Networks	[14]	['[14]  J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data, June 2014. ']
414	37	[185]	The best p and q parameter settings for each node2vec entry are omitted for ease of presentation. // A general observation we can draw from the results is that the learned feature representations for node pairs significantly outperform the heuristic benchmark scores with node2vec achieving the best AUC improvement on 12.6% on the arXiv dataset over the best performing baseline (Adamic-Adar ==[1]==). // Amongst the feature learning algorithms, node2vec outperforms both DeepWalk and LINE in all networks with gain up to 3.8% and 6.5% respectively in the AUC scores for the best possible choices of the binary operator for each algorithm.	h-e	node2vec- Scalable Feature Learning for Networks	[1]	['[1]  L. A. Adamic and E. Adar. Friends and neighbors on the web. Social networks, 25(3):211–230, 2003. ']
415	37	[6]	Both DeepWalk and LINE can be seen as rigid search strategies over networks. // DeepWalk ==[24]== proposes search using uniform random walks. // The obvious limitation with such a strategy is that it gives us no control over the explored neighborhoods.	h-	node2vec- Scalable Feature Learning for Networks	[24]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online learning of social representations. In KDD, 2014. ']
416	37	[1]	The obvious limitation with such a strategy is that it gives us no control over the explored neighborhoods. // LINE ==[28]== proposes primarily a breadth-first strategy, sampling nodes and optimizing the likelihood independently over only 1-hop and 2-hop neighbors. // The effect of such an exploration is easier to characterize, but it is restrictive and provides no flexibility in exploring nodes at further depths.	h-	node2vec- Scalable Feature Learning for Networks	[28]	['[28]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale Information Network Embedding. In WWW, 2015. ']
417	417	[186]	// Behaviors are the products of the interaction between multiple types of contexts ==[25]==. // The multi-type contexts often include operators, goals, resources, spatiotemporal and social dimensions. Consider paper-publishing behavior as an example: it has authors, target conference/journal, datasets, problems, methods, references, and so on.	ho	Multi-Type Itemset Embedding for Learning Behavior Success	[25]	['[25]  David Matsumoto. 2007. Culture, context, and behavior. Journal of personality 75, 6 (2007), 1285–1320. ']
418	417	[187, 188]	Contextual behavior modeling has been broadly used for pattern discovery and predictive analysis. // Tensor methods decompose multidimensional counts (e.g., numbers of co-occurrence among one author, one conference, and one reference) into lowdimensional latent vectors ==[20, 24]==, however, real behaviors do not guarantee one context item per dimension/type [22] – a paper can have multiple authors and multiple references. // Jamali et al. proposed a multicontextual factor model to learn latent factors of users and items for product recommendation [19], but neither tensor methods nor factor models can scale for massive behavior data.	h-	Multi-Type Itemset Embedding for Learning Behavior Success	[20, 24]	['[20]  Meng Jiang, Peng Cui, Fei Wang, Xinran Xu, Wenwu Zhu, and Shiqiang Yang. 2014. Fema:  exible evolutionary multi-faceted analysis for dynamic behavioral pattern discovery. In KDD. 1186–1195. ', '[24]  Defu Lian, Zhenyu Zhang, Yong Ge, Fuzheng Zhang, Nicholas Jing Yuan, and Xing Xie. 2016. Regularized Content-Aware Tensor Factorization Meets TemporalAware Location Recommendation. In ICDM. 1029–1034. ']
419	417	[189]	Contextual behavior modeling has been broadly used for pattern discovery and predictive analysis. // Tensor methods decompose multidimensional counts (e.g., numbers of co-occurrence among one author, one conference, and one reference) into lowdimensional latent vectors [20, 24], however, real behaviors do not guarantee one context item per dimension/type ==[22]== – a paper can have multiple authors and multiple references. // Jamali et al. proposed a multicontextual factor model to learn latent factors of users and items for product recommendation [19], but neither tensor methods nor factor models can scale for massive behavior data.	ho	Multi-Type Itemset Embedding for Learning Behavior Success	[22]	['[22]  Meng Jiang, Christos Faloutsos, and Jiawei Han. 2016. Catchtartan: Representing and summarizing dynamic multicontextual behaviors. In KDD. 945–954. ']
420	417	[190]	Tensor methods decompose multidimensional counts (e.g., numbers of co-occurrence among one author, one conference, and one reference) into lowdimensional latent vectors [20, 24], however, real behaviors do not guarantee one context item per dimension/type [22] – a paper can have multiple authors and multiple references. // Jamali et al. proposed a multicontextual factor model to learn latent factors of users and items for product recommendation ==[19]==, but neither tensor methods nor factor models can scale for massive behavior data. // Recently embedding methods have been proposed to eciently learn low-dimensional vectors of nodes from large heterogeneous networks.	h-	Multi-Type Itemset Embedding for Learning Behavior Success	[19]	['[19]  Mohsen Jamali and Laks Lakshmanan. 2013. HeteroMF: recommendation in heterogeneous information networks using context dependent factor models. In WWW. 643–654. ']
421	417	[191, 192, 193]	Contextual behavior modeling. // There has been a wide line of research on learning latent representations of context items in behavior data towards various applications ==[3, 21, 34]==. //  Agarwal et al. proposed localized factor models combining multi-context information to improve predictive accuracy in recommender systems [2].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[3, 21, 34]	['[3]  Alex Beutel, Kenton Murray, Christos Faloutsos, and Alexander J Smola. 2014. Coba : collaborative bayesian  ltering. In WWW. 97–108. ', '[21]  Meng Jiang, Peng Cui, Nicholas Jing Yuan, Xing Xie, and Shiqiang Yang. 2016. Little Is Much: Bridging Cross-Platform Behaviors through Overlapped Crowds.. In AAAI. 13–19. ', '[34]  Alan Said, Shlomo Berkovsky, and Ernesto W De Luca. 2010. Putting things in context: Challenge on context-aware movie recommendation. In Workshop on Context-Aware Movie Recommendation. 2–6. ']
422	417	[194]	There has been a wide line of research on learning latent representations of context items in behavior data towards various applications [3, 21, 34]. // Agarwal et al. proposed localized factor models combining multi-context information to improve predictive accuracy in recommender systems ==[2]==. // Jamali et al. proposed context-dependent factor models to learn latent factors of users and items for recommendation [19].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[2]	['[2]  Deepak Agarwal, Bee-Chung Chen, and Bo Long. 2011. Localized factor models for multi-context recommendation. In KDD. 609–617. ']
423	417	[190]	Agarwal et al. proposed localized factor models combining multi-context information to improve predictive accuracy in recommender systems [2]. // Jamali et al. proposed context-dependent factor models to learn latent factors of users and items for recommendation ==[19]==. // Besides factor models, tensor decompositions have been widely used for modeling multi-contextual data [33].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[19]	['[19]  Mohsen Jamali and Laks Lakshmanan. 2013. HeteroMF: recommendation in heterogeneous information networks using context dependent factor models. In WWW. 643–654. ']
424	417	[195]	Jamali et al. proposed context-dependent factor models to learn latent factors of users and items for recommendation [19]. // Besides factor models, tensor decompositions have been widely used for modeling multi-contextual data ==[33]==. // Jiang et al. proposed a tensor-sequence decomposition approach for discovering multifaceted behavioral patterns [20].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[33]	['[33]  Ste en Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag recommendation. In WSDM. 81–90. ']
425	417	[187]	Besides factor models, tensor decompositions have been widely used for modeling multi-contextual data [33]. // Jiang et al. proposed a tensor-sequence decomposition approach for discovering multifaceted behavioral patterns ==[20]==. // Ermiş et al. studied various alternative tensor models for link prediction in heterogeneous data [12].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[20]	['[20]  Meng Jiang, Peng Cui, Fei Wang, Xinran Xu, Wenwu Zhu, and Shiqiang Yang. 2014. Fema:  exible evolutionary multi-faceted analysis for dynamic behavioral pattern discovery. In KDD. 1186–1195. ']
426	417	[196]	Jiang et al. proposed a tensor-sequence decomposition approach for discovering multifaceted behavioral patterns [20]. // Ermiş et al. studied various alternative tensor models for link prediction in heterogeneous data ==[12]==. // Lian et al. proposed regularized tensor factorization for spatiotemporal recommendation [24].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[12]	['[12]  Beyza Ermiş, Evrim Acar, and A Taylan Cemgil. 2015. Link prediction in heterogeneous data via generalized coupled tensor factorization. DMKD 29, 1 (2015), 203–236. ']
427	417	[188]	Ermiş et al. studied various alternative tensor models for link prediction in heterogeneous data [12]. // Lian et al. proposed regularized tensor factorization for spatiotemporal recommendation ==[24]==. // Yang et al. developed a predictive task guided tensor decomposition model for representation learning from Electronic Health Records [41].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[24]	['[24]  Defu Lian, Zhenyu Zhang, Yong Ge, Fuzheng Zhang, Nicholas Jing Yuan, and Xing Xie. 2016. Regularized Content-Aware Tensor Factorization Meets TemporalAware Location Recommendation. In ICDM. 1029–1034. ']
428	417	[197]	Lian et al. proposed regularized tensor factorization for spatiotemporal recommendation [24]. // Yang et al. developed a predictive task guided tensor decomposition model for representation learning from Electronic Health Records ==[41]==. // Perros et al. designed a scalable PARAFAC2 tensor model for large and sparse datasets [31].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[41]	['[41]  Kai Yang, Xiang Li, Haifeng Liu, Jing Mei, Guo Tong Xie, Junfeng Zhao, Bing Xie, and Fei Wang. 2017. TaGiTeD: Predictive Task Guided Tensor Decomposition for Representation Learning from Electronic Health Records.. In AAAI. 2824–2830.']
429	417	[198]	Yang et al. developed a predictive task guided tensor decomposition model for representation learning from Electronic Health Records [41]. // Perros et al. designed a scalable PARAFAC2 tensor model for large and sparse datasets ==[31]==. // However, the computational cost of factorizing a large-scale matrix or tensor is usually very expensive, and none of the existing behavior modeling methods can eciently learn item representations to optimize success rate on massive behavior data.	h-	Multi-Type Itemset Embedding for Learning Behavior Success	[31]	['[31]  Ioakeim Perros, Evangelos E Papalexakis, Fei Wang, Richard Vuduc, Elizabeth Searles, Michael Thompson, and Jimeng Sun. 2017. SPARTan: Scalable PARAFAC2 for Large & Sparse Data. In KDD. ']
430	417	[26, 24, 199, 7]	Network data representation learning. // Network embedding methods learn node representations that preserve node proximities (e.g., one-hop or two-hop connections) in network data ==[4, 9, 10, 40]==. // DeepWalk [30] used random walks to expand the neighborhood of a node and expected nodes with higher proximity yield similar representations.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[4, 9, 10, 40]	['[4]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In CIKM. ACM, 891–900. ', '[9]  Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. 2017. A Survey on Network Embedding. arXiv preprint arXiv:1711.08752 (2017). ', '[10]  Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic, and Narayan Bhamidipati. 2015. Hate speech detection with comment embeddings. In WWW. 29–30. ', '[40]  Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. 2015. Network Representation Learning with Rich Text Information.. In IJCAI. 2111– 2117. ']
431	417	[6]	Network embedding methods learn node representations that preserve node proximities (e.g., one-hop or two-hop connections) in network data [4, 9, 10, 40]. // DeepWalk ==[30]== used random walks to expand the neighborhood of a node and expected nodes with higher proximity yield similar representations. // node2vec [14] presented biased random walkers to diversify the neighborhood.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[30]	['[30]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In KDD. 701–710. ']
432	417	[37]	DeepWalk [30] used random walks to expand the neighborhood of a node and expected nodes with higher proximity yield similar representations. // node2vec ==[14]== presented biased random walkers to diversify the neighborhood. // LINE [37] provided clear objectives for homogeneous network embedding that articulates what network properties are preserved.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[14]	['[14]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD. 855–864. ']
433	417	[1]	node2vec [14] presented biased random walkers to diversify the neighborhood. // LINE ==[37]== provided clear objectives for homogeneous network embedding that articulates what network properties are preserved. // We have spotted a series of heterogeneous network embedding work [6, 11, 15, 18, 39] that capture heterogeneous structural properties in network data.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[37]	['[37]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW. 1067–1077. ']
434	417	[109, 111, 200, 110, 19]	LINE [37] provided clear objectives for homogeneous network embedding that articulates what network properties are preserved. // We have spotted a series of heterogeneous network embedding work ==[6, 11, 15, 18, 39]== that capture heterogeneous structural properties in network data. // If we explicitly represent behavior entries as nodes and thus behavior datasets are represented as behavior-item heterogeneous bipartite networks, existing network embedding methods can be applied to learn representations of both items and behaviors.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[6, 11, 15, 18, 39]	['[6]  Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang. 2015. Heterogeneous network embedding via deep architectures. In KDD. 119–128. ', '[11]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks. In KDD. 135–144. ', '[15]  Huan Gui, Jialu Liu, Fangbo Tao, Meng Jiang, Brandon Norick, Lance Kaplan, and Jiawei Han. 2017. Embedding Learning with Events in Heterogeneous Information Networks. TKDE (2017). ', '[18]  Zhipeng Huang and Nikos Mamoulis. 2017. Heterogeneous Information Network Embedding for Meta Path based Proximity. arXiv:1701.05291 (2017). ', '[39]  Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In KDD. 1225–1234. ']
435	417	[201]	Therefore, it finds complementary items that will maximize the success of a target behavior. // There are other network embedding methods designed to learn task-specific node representations by utilizing label information in supervised fashion such as PTE ==[36]==. // Chen et al. [7] proposed a task-guided and path-augmented heterogeneous network embedding model to identify author names given anonymized paper’s title.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[36]	['[36]  Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. 1165–1174. ']
436	417	[202]	There are other network embedding methods designed to learn task-specific node representations by utilizing label information in supervised fashion such as PTE [36]. // Chen et al. ==[7]== proposed a task-guided and path-augmented heterogeneous network embedding model to identify author names given anonymized paper’s title. // The most recent work PNE [8] decomposes a partially labeled network into two bipartite networks and encodes the node label information by learning label and context vectors from the label-context network. In our case, we only have the success rate for behaviors, which can be binary or real values; and, we aims at learning the item representations preserving behavior success.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[7]	['[7]  Ting Chen and Yizhou Sun. 2017. Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identi cation. In WSDM. 295–304. ']
437	417	[203]	Chen et al. [7] proposed a task-guided and path-augmented heterogeneous network embedding model to identify author names given anonymized paper’s title. // The most recent work PNE ==[8]== decomposes a partially labeled network into two bipartite networks and encodes the node label information by learning label and context vectors from the label-context network. // In our case, we only have the success rate for behaviors, which can be binary or real values; and, we aims at learning the item representations preserving behavior success.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[8]	['[8]  Weizheng Chen, Xianling Mao, Xiangyu Li, Yan Zhang, and Xiaoming Li. 2017. PNE: Label Embedding Enhanced Network Embedding. In Paci c-Asia Conference on Knowledge Discovery and Data Mining. Springer, 547–560. ']
438	417	[204, 35, 36]	With the success of deep learning techniques, representation learning becomes popular starting from practices on text data. // Mikolov et al. proposed the word2vec framework to learn the distributed representations of words in natural language ==[13, 26, 27]==. // Pennington et al. proposed GloVe to learn word vectors from nonzero elements in a word-word cooccurrence matrix [29].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[13, 26, 27]	['[13]  Yoav Goldberg and Omer Levy. 2014. word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method. arXiv:1402.3722 (2014). ', '[26]  Tomas Mikolov, Kai Chen, Greg Corrado, and Je rey Dean. 2013. E cient estimation of word representations in vector space. arXiv:1301.3781 (2013). ', '[27]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je  Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. 3111–3119. ']
439	417	[167]	Mikolov et al. proposed the word2vec framework to learn the distributed representations of words in natural language [13, 26, 27]. // Pennington et al. proposed GloVe to learn word vectors from nonzero elements in a word-word cooccurrence matrix ==[29]==. // Le et al. extended the embedded objects from words or phrases to paragraphs [23].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[29]	['[29]  Je rey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In EMNLP. 1532–1543. ']
440	417	[64]	Pennington et al. proposed GloVe to learn word vectors from nonzero elements in a word-word cooccurrence matrix [29]. // Le et al. extended the embedded objects from words or phrases to paragraphs ==[23]==. // Recently, Nichel et al. proposed Poincaré embedding [28] based on a non-Euclidean space to preserve hierarchical semantic structures [5].	b	Multi-Type Itemset Embedding for Learning Behavior Success	[23]	['[23]  Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In ICML. 1188–1196. ']
441	417	[205]	Le et al. extended the embedded objects from words or phrases to paragraphs [23]. // Recently, Nichel et al. proposed Poincaré embedding ==[28]== based on a non-Euclidean space to preserve hierarchical semantic structures [5]. // Our work focuses on representation learning from behavior data.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[28]	['[28]  Maximilian Nickel and Douwe Kiela. 2017. Poincaré Embeddings for Learning Hierarchical Representations. arXiv:1705.08039 (2017). ']
442	417	[206]	Le et al. extended the embedded objects from words or phrases to paragraphs [23]. // Recently, Nichel et al. proposed Poincaré embedding [28] based on a non-Euclidean space to preserve hierarchical semantic structures ==[5]==. // Our work focuses on representation learning from behavior data.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[5]	['[5]  Pablo Castells, Miriam Fernandez, and David Vallet. 2007. An adaptation of the vector-space model for ontology-based information retrieval. TKDE 19, 2 (2007). ']
443	417	[207]	For tweet-posting behaviors, if the success is measured as the existence of their context itemsets in tweet data, we have tons of positive behaviors but no real negative ones. // If the success is measured as the behavior’s popularity, the success rate is the number of views, likes, retweets, or shares ==[17]==. // In this case, positive behaviors are popular posts and negative behaviors are unpopular but still real posts.	ho	Multi-Type Itemset Embedding for Learning Behavior Success	[17]	['[17]  Yuheng Hu, Fei Wang, and Subbarao Kambhampati. 2013. Listening to the Crowd: Automated Analysis of Events via Aggregated Twitter Sentiment.. In IJCAI. 2640–2646. ']
444	417	[6]	In this way, the contributions of each context item towards behavior’s success are preserved. // Unlike network embedding models such as DeepWalk ==[30]==, LINE [37] and node2vec [14] that preserve proximities and were evaluated on clustering tasks, our behavior data embedding model, also a multi-type itemset embedding model, preserves success property. // We will evaluate it on the two tasks of behavior modeling we have introduced in Section 1 and compete with existing works in experiments.	ro	Multi-Type Itemset Embedding for Learning Behavior Success	[30]	['[30]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In KDD. 701–710. ']
445	417	[1]	In this way, the contributions of each context item towards behavior’s success are preserved. // Unlike network embedding models such as DeepWalk [30], LINE ==[37]== and node2vec [14] that preserve proximities and were evaluated on clustering tasks, our behavior data embedding model, also a multi-type itemset embedding model, preserves success property. // We will evaluate it on the two tasks of behavior modeling we have introduced in Section 1 and compete with existing works in experiments.	ro	Multi-Type Itemset Embedding for Learning Behavior Success	[37]	['[37]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW. 1067–1077. ']
446	417	[37]	In this way, the contributions of each context item towards behavior’s success are preserved. // Unlike network embedding models such as DeepWalk [30], LINE [37] and node2vec ==[14]== that preserve proximities and were evaluated on clustering tasks, our behavior data embedding model, also a multi-type itemset embedding model, preserves success property. // We will evaluate it on the two tasks of behavior modeling we have introduced in Section 1 and compete with existing works in experiments.	ro	Multi-Type Itemset Embedding for Learning Behavior Success	[14]	['[14]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD. 855–864. ']
447	417	[208]	Since there is not any itemset embedding method in the literature, we compare our LearnSuc with the dimensionality reduction methods [38] and the state-of-the-art network embedding models. // PCA ==[16]==: It learns data representations that describe as much of the variance in the data as possible. // LINE [37]: This homogeneous network embedding method preserves the 1st- and 2nd-order of node proximity.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[16]	['[16]  Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review 53, 2 (2011), 217–288. ']
448	417	[1]	PCA [16]: It learns data representations that describe as much of the variance in the data as possible. // LINE ==[37]==: This homogeneous network embedding method preserves the 1st- and 2nd-order of node proximity. // DeepWalk [30]: It uses local information obtained from truncated random walks to learn latent representations of vertices in a network.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[37]	['[37]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW. 1067–1077. ']
449	417	[6]	LINE [37]: This homogeneous network embedding method preserves the 1st- and 2nd-order of node proximity. // DeepWalk ==[30]==: It uses local information obtained from truncated random walks to learn latent representations of vertices in a network. // node2vec [14]: It learns continuous feature representations for nodes in networks by maximizing the likelihood of preserving network neighborhoods of nodes.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[30]	['[30]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In KDD. 701–710. ']
450	417	[37]	DeepWalk [30]: It uses local information obtained from truncated random walks to learn latent representations of vertices in a network. // node2vec ==[14]==: It learns continuous feature representations for nodes in networks by maximizing the likelihood of preserving network neighborhoods of nodes. // metapath2vec [11]: This method learns node representations based on meta-path-based random walks from heterogeneous networks.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[14]	['[14]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD. 855–864. ']
451	417	[111]	node2vec [14]: It learns continuous feature representations for nodes in networks by maximizing the likelihood of preserving network neighborhoods of nodes. // metapath2vec ==[11]==: This method learns node representations based on meta-path-based random walks from heterogeneous networks. // We evaluate LearnSuc’s alternatives such as LearnSuc-Pn and LearnSuc-Pt to compare the eectiveness of dierent negative sampling strategies.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[11]	['[11]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks. In KDD. 135–144. ']
452	417	[111]	Overall performance. // The best baseline method is a heterogeneous network embedding method metapath2vec ==[11]== which gives an MAE of 0.0567 and an RMSE of 0.1648 that are much better than the error by random guess (0.5), because pair-wise similarity plays an important role in generating behaviors. // For example, co-authors are often working in very similar research elds	h+e	Multi-Type Itemset Embedding for Learning Behavior Success	[11]	['[11]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks. In KDD. 135–144. ']
453	417	[208]	The best variant of our itemset embedding method LearnSuc holds (1) type-distribution-constrained negative behavior sampling strategy and (2) type weights as {3,1,1,1} (authors tend to have higher weights). It gives an MAE of 0.0432 (-23.8% relatively) and an RMSE of 0.1243 (-24.6% relatively). // The traditional dimensionality reduction technique PCA ==[16]== is able to capture 28.4% of total varianc. //  Its performance is not signicantly dierent from the performance of LINE [37]	hoe	Multi-Type Itemset Embedding for Learning Behavior Success	[16]	['[16]  Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review 53, 2 (2011), 217–288. ']
454	417	[1]	The traditional dimensionality reduction technique PCA [16] is able to capture 28.4% of total varianc. // Its performance is not significantly different from the performance of LINE ==[37]==. //  The network embedding methods show very high AUC and F1 because the pairwise similarities between the items(e.g., authors and keywords) do have impact on the chance of collaboration	hoe	Multi-Type Itemset Embedding for Learning Behavior Success	[37]	['[37]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW. 1067–1077. ']
455	417	[6]	Comparing network embedding methods. // First, DeepWalk ==[30]== and node2vec [14] perform better than LINE [37] in this task. It shows preserving random walk-based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration. //  Second, metapath2vec [11] learns the low-dimensional representations of nodes from rich meta path-based features	h+e	Multi-Type Itemset Embedding for Learning Behavior Success	[30]	['[30]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In KDD. 701–710. ']
456	417	[37]	Comparing network embedding methods. // First, DeepWalk [30] and node2vec ==[14]== perform better than LINE [37] in this task. It shows preserving random walk-based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration. //  Second, metapath2vec [11] learns the low-dimensional representations of nodes from rich meta path-based features	h+e	Multi-Type Itemset Embedding for Learning Behavior Success	[14]	['[14]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD. 855–864. ']
457	417	[1]	Comparing network embedding methods. // First, DeepWalk [30] and node2vec [14] perform better than LINE ==[37]== in this task. It shows preserving random walk-based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration. //  Second, metapath2vec [11] learns the low-dimensional representations of nodes from rich meta path-based features	h-e	Multi-Type Itemset Embedding for Learning Behavior Success	[37]	['[37]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In WWW. 1067–1077. ']
458	417	[111]	First, DeepWalk [30] and node2vec [14] perform better than LINE [37] in this task. It shows preserving random walk-based local information or network neighborhoods is more effective than preserving connections an common neighbors on predicting a collaboration. // Second, metapath2vec ==[11]== learns the low-dimensional representations of nodes from rich meta path-based features. // It models the heterogeneity of the network.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[11]	['[11]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable Representation Learning for Heterogeneous Networks. In KDD. 135–144. ']
459	417	[209]	All the papers have been successful (of a success rate above 0.6) but higher success rate indicates more likely to be impactful. // One of the good examples is the paper “Inferring Social Ties across Heterogeneous Networks” ==[35]== published in WSDM 2012. // The leading author Prof. Jie Tang (Tsinghua University) is a data mining expert on a subset of the keywords such as “factor graph”, “heterogeneous network”, “predictive model”; and the co-author Prof. Jon Kleinberg (Cornell University) has world-level reputation in computational social science of the keywords such as “social theory”, “social influence”, and “social ties”.	b	Multi-Type Itemset Embedding for Learning Behavior Success	[35]	['[35]  Jie Tang, Tiancheng Lou, and Jon Kleinberg. 2012. Inferring social ties across heterogeneous networks. In WSDM. 743–752. ']
460	52	[210, 211]	In almost all networks, nodes tend to have one or more functions that greatly determine their role in the system. // For example, individuals in a social network have a social role or social position ==[11, 19]==, while proteins in a protein-protein interaction (PPI) network exert specific functions [1, 22]. // Intuitively, different nodes in such networks may perform similar functions, such as interns in the social network of a corporation or catalysts in the PPI network of a cell.	ho	struc2vec- Learning Node Representations from Structural Identity	[11, 19]	['[11]  Francois Lorrain and Harrison C White. 1971. Structural equivalence of individuals in social networks. \x8ae Journal of mathematical sociology 1 (1971). ', '[19]  Lee Douglas Sailer. 1978. Structural equivalence: Meaning and de\x80nition, computation and application. Social Networks (1978). ']
461	52	[212, 213]	In almost all networks, nodes tend to have one or more functions that greatly determine their role in the system. // For example, individuals in a social network have a social role or social position [11, 19], while proteins in a protein-protein interaction (PPI) network exert specific functions ==[1, 22]==. // Intuitively, different nodes in such networks may perform similar functions, such as interns in the social network of a corporation or catalysts in the PPI network of a cell.	ho	struc2vec- Learning Node Representations from Structural Identity	[1, 22]	['[1]  Nir Atias and Roded Sharan. 2012. Comparative analysis of protein networks: hard problems, practical solutions. Commun. ACM 55 (2012). ', '[22]  R Singh, J Xu, and B Berger. 2008. Global alignment of multiple protein interaction networks with application to functional orthology detection. PNAS (2008). ']
462	52	[210, 214, 211]	In this context, not even the labels of the nodes matter but just their relationship to other nodes (edges). // Indeed, mathematical sociologists have worked on this problem since the 1970s, defining and computing structural identity of individuals in social networks ==[11, 17, 19]==. // Beyond sociology, the role of webpages in the webgraph is another example of identity (in this case, hubs and authorities) emerging from the network structure, as dened by the celebrated work of Kleinberg [8].	b	struc2vec- Learning Node Representations from Structural Identity	[11, 17, 19]	['[11]  Francois Lorrain and Harrison C White. 1971. Structural equivalence of individuals in social networks. \x8ae Journal of mathematical sociology 1 (1971). ', '[17]  Narciso Pizarro. 2007. Structural Identity and Equivalence of Individuals in Social Networks Beyond Duality. International Sociology 22 (2007). ', '[19]  Lee Douglas Sailer. 1978. Structural equivalence: Meaning and de\x80nition, computation and application. Social Networks (1978). ']
463	52	[215]	Indeed, mathematical sociologists have worked on this problem since the 1970s, defining and computing structural identity of individuals in social networks [11, 17, 19]. // Beyond sociology, the role of webpages in the webgraph is another example of identity (in this case, hubs and authorities) emerging from the network structure, as defined by the celebrated work of Kleinberg ==[8]==. // The most common practical approaches to determine the structural identity of nodes are based on distances or recursions.	b	struc2vec- Learning Node Representations from Structural Identity	[8]	['[8]  Jon M Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM) (1999). ']
464	52	[122, 216]	The most common practical approaches to determine the structural identity of nodes are based on distances or recursions. // In the former, a distance function that leverages the neighborhood of the nodes is used to measure the distance between all node pairs, with clustering or matching then performed to place nodes into equivalent classes ==[5, 9]==. // In the later, a recursion with respect to neighboring nodes is constructed and then iteratively unfolded until convergence, with final values used to determine the equivalent classes [3, 8, 26].	b	struc2vec- Learning Node Representations from Structural Identity	[5, 9]	['[5]  F Fouss, A Piro\x8ae, J Renders, and M Saerens. 2007. Random-Walk Computation of Similarities Between Nodes of a Graph with Application to Collaborative Recommendation. IEEE Trans. on Knowl. and Data Eng. (2007). ', '[9]  Elizabeth A Leicht, Pe\x8aer Holme, and Mark EJ Newman. 2006. Vertex similarity in networks. Physical Review E 73 (2006). ']
465	52	[217, 215, 218]	In the former, a distance function that leverages the neighborhood of the nodes is used to measure the distance between all node pairs, with clustering or matching then performed to place nodes into equivalent classes [5, 9]. // In the later, a recursion with respect to neighboring nodes is constructed and then iteratively unfolded until convergence, with final values used to determine the equivalent classes ==[3, 8, 26]==. // While such approaches have advantages and disadvantages, we provide an alternative methodology, one based on unsupervised learning of representations for the structural identity of nodes (to be presented).	b	struc2vec- Learning Node Representations from Structural Identity	[3, 8, 26]	['[3]  V Blondel, A Gajardo, M Heymans, P Senellart, and P Van Dooren. 2004. A measure of similarity between graph vertices: Applications to synonym extraction and web searching. SIAM review (2004). ', '[8]  Jon M Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM (JACM) (1999). ', '[26]  Laura A Zager and George C Verghese. 2008. Graph similarity scoring and matching. Applied mathematics le\x88ers (2008).']
466	52	[37, 219, 6, 1]	While such approaches have advantages and disadvantages, we provide an alternative methodology, one based on unsupervised learning of representations for the structural identity of nodes (to be presented). // Recent efforts in learning latent representations for nodes in networks have been quite successful in performing classification and prediction tasks ==[6, 14, 16, 23]==. // In particular, these efforts encode nodes using as context a generalized notion of their neighborhood (e.g., w steps of a random walk, or nodes with neighbors in common).	h+	struc2vec- Learning Node Representations from Structural Identity	[6, 14, 16, 23]	['[6]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In ACM SIGKDD. ', '[14]  A Narayanan, M Chandramohan, L Chen, Y Liu, and S Saminathan. 2016. subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs. In Workshop on Mining and Learning with Graphs. ', '[16]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In ACM SIGKDD. ', '[23]  Jian Tang, Meng \x8b, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. In WWW. ']
467	52	[6]	. Figure 1 illustrates the problem, where nodes u and v play similar roles (i.e., have similar local structures) but are very far apart in the network. Since their neighborhoods have no common nodes, recent approaches cannot capture their structural similarity (as we soon show). // It is worth noting why recent approaches for learning node representations such as DeepWalk ==[16]== and node2vec [6] succeed in classification tasks but tend to fail in structural equivalence tasks. // The key point is that many node features in most real networks exhibit a strong homophily (e.g., two blogs with the same political inclination are much more likely to be connected than at random).	h-	struc2vec- Learning Node Representations from Structural Identity	[16]	['[16]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In ACM SIGKDD. ']
468	52	[37]	. Figure 1 illustrates the problem, where nodes u and v play similar roles (i.e., have similar local structures) but are very far apart in the network. Since their neighborhoods have no common nodes, recent approaches cannot capture their structural similarity (as we soon show). // It is worth noting why recent approaches for learning node representations such as DeepWalk [16] and node2vec ==[6]== succeed in classification tasks but tend to fail in structural equivalence tasks. // The key point is that many node features in most real networks exhibit a strong homophily (e.g., two blogs with the same political inclination are much more likely to be connected than at random).	h-	struc2vec- Learning Node Representations from Structural Identity	[6]	['[6]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In ACM SIGKDD. ']
469	52	[6]	Such context can be leveraged by language models to learn latent representation for the nodes. // We implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks, comparing its performance with DeepWalk ==[16]== and node2vec [6] – two state-of-the-art techniques for learning latent representations for nodes, and with RolX [7] – a recent approach to identify roles of nodes. // Our results indicate that while DeepWalk and node2vec fail to capture the notion of structural identity, struc2vec excels on this task – even when the original network is subject to strong random noise (random edge removal).	h-	struc2vec- Learning Node Representations from Structural Identity	[16]	['[16]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In ACM SIGKDD. ']
470	52	[37]	Such context can be leveraged by language models to learn latent representation for the nodes. // We implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks, comparing its performance with DeepWalk [16] and node2vec ==[6]== – two state-of-the-art techniques for learning latent representations for nodes, and with RolX [7] – a recent approach to identify roles of nodes. // Our results indicate that while DeepWalk and node2vec fail to capture the notion of structural identity, struc2vec excels on this task – even when the original network is subject to strong random noise (random edge removal).	h-	struc2vec- Learning Node Representations from Structural Identity	[6]	['[6]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In ACM SIGKDD. ']
471	52	[169]	Such context can be leveraged by language models to learn latent representation for the nodes. // We implement an instance of struc2vec and show its potential through numerical experiments on toy examples and real networks, comparing its performance with DeepWalk [16] and node2vec [6] – two state-of-the-art techniques for learning latent representations for nodes, and with RolX ==[7]== – a recent approach to identify roles of nodes. // Our results indicate that while DeepWalk and node2vec fail to capture the notion of structural identity, struc2vec excels on this task – even when the original network is subject to strong random noise (random edge removal).	b	struc2vec- Learning Node Representations from Structural Identity	[7]	['[7]  K Henderson, B Gallagher, T Eliassi-Rad, H Tong, S Basu, L Akoglu, D Koutra, C Faloutsos, and L Li. 2012. Rolx: structural role extraction & mining in large graphs. In ACM SIGKDD. ']
472	52	[147]	The technique is instrumental for Machine Learning applications that leverage network data, as node embeddings can be directly used in tasks such as classification and clustering. // In Natural Language Processing ==[2]==, generating dense embeddings for sparse data has a long history. //	b	struc2vec- Learning Node Representations from Structural Identity	[2]	['[2]  Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A ´ Neural Probabilistic Language Model. JMLR (2003). ']
473	52	[35, 36]	In Natural Language Processing [2], generating dense embeddings for sparse data has a long history. // Recently, Skip-Gram ==[12, 13]== was proposed as an effcient technique to learn embeddings for text data (e.g., sentences). // Among other properties, the learned language model places semantically similar words near each other in space.	h+	struc2vec- Learning Node Representations from Structural Identity	[12, 13]	['[12]  Tomas Mikolov, Kai Chen, Greg Corrado, and Je\x82rey Dean. 2013. E\x81cient Estimation of Word Representations in Vector Space. In ICLR Workshop. ', '[13]  T Mikolov, I Sutskever, K Chen, G Corrado, and J Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS. ']
474	52	[6]	Among other properties, the learned language model places semantically similar words near each other in space. // Learning a language model from a network was first proposed by DeepWalk ==[16]==. // It uses random walks to generate sequences of nodes from the network, which are then treated as sentences by Skip-Gram.	b	struc2vec- Learning Node Representations from Structural Identity	[16]	['[16]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In ACM SIGKDD. ']
475	52	[37]	Intuitively, nodes close in the network will tend to have similar contexts (sequences) and thus have embeddings that are near one another. // This idea was later extended by node2vec ==[6]==. // By proposing a biased second order random walk model, node2vec provides more flexibility when generating the context of a vertex.	b	struc2vec- Learning Node Representations from Structural Identity	[6]	['[6]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In ACM SIGKDD. ']
476	52	[219]	However, a fundamental limitation is that structurally similar nodes will never share the same context if their distance (hop count) is larger than the Skip-Gram window. // subgraph2vec ==[14]== is another recent approach for learning embeddings for rooted subgraphs, and unlike the previous techniques it does not use random walks to generate context. // Alternatively, the context of a node is simply dened by its neighbors.	b	struc2vec- Learning Node Representations from Structural Identity	[14]	['[14]  A Narayanan, M Chandramohan, L Chen, Y Liu, and S Saminathan. 2016. subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs. In Workshop on Mining and Learning with Graphs. ']
477	52	[220]	Additionally, subgraph2vec captures structural equivalence by embedding nodes with the same local structure to the same point in space. // Nonetheless, the notion of structural equivalence is very rigid since it is defined as a binary property dictated by the Weisfeiler-Lehman isomorphism test ==[21]==. // Thus, two nodes that are structurally very similar (but fail the test) and have non-overlapping neighbors may not be close in space.	ho	struc2vec- Learning Node Representations from Structural Identity	[21]	['[21]  N Shervashidze, P Schweitzer, E van Leeuwen, K Mehlhorn, and K Borgwardt. 2011. Weisfeiler-Lehman Graph Kernels. JMLR (2011). ']
478	52	[9, 19]	Thus, two nodes that are structurally very similar (but fail the test) and have non-overlapping neighbors may not be close in space. // Similarly to subgraph2vec, considerable effort has recently been made on learning richer representations for network nodes ==[4, 24]==. // However, building representations that explicitly capture structural identity is a relative orthogonal problem that has not received much attention.	b	struc2vec- Learning Node Representations from Structural Identity	[4, 24]	['[4]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep Neural Networks for Learning Graph Representations. In AAAI. ', '[24]  Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural Deep Network Embedding. In ACM SIGKDD. ']
479	52	[169]	However, building representations that explicitly capture structural identity is a relative orthogonal problem that has not received much aention. is is the focus of struc2vec. // This is the focus of struc2vec. A recent approach to explicitly identify the role of nodes using just the network structure is RolX ==[7]==. // This unsupervised approach is based on enumerating various structural features for nodes, finding the more suited basis vector for this joint feature space, and then assigning for every node a distribution over the identified roles (basis), allowing for mixed membership across the roles.	b	struc2vec- Learning Node Representations from Structural Identity	[7]	['[7]  K Henderson, B Gallagher, T Eliassi-Rad, H Tong, S Basu, L Akoglu, D Koutra, C Faloutsos, and L Li. 2012. Rolx: structural role extraction & mining in large graphs. In ACM SIGKDD. ']
480	52	[35]	Informally, the task can be defined as learning word probabilities given a context. // In particular, Skip-Gram ==[12]== has proven to be effective at learning meaningful representations for a variety of data. // In order to apply it to networks, it suffices to use artificially generated node sequences instead of word sentence	h+	struc2vec- Learning Node Representations from Structural Identity	[12]	['[12]  Tomas Mikolov, Kai Chen, Greg Corrado, and Je\x82rey Dean. 2013. E\x81cient Estimation of Word Representations in Vector Space. In ICLR Workshop. ']
481	52	[221]	Karate network. // THe Zachary’s Karate Club ==[25]== is a network composed of 34 nodes and 78 edges, where each node represents a club member and edges denote if two members have interacted outside the club. // In this network, edges are commonly interpreted as indications of friendship between members.	b	struc2vec- Learning Node Representations from Structural Identity	[25]	['[25]  Wayne W Zachary. 1977. An information \x83ow model for con\x83ict and \x80ssion in small groups. Journal of anthropological research (1977). ']
482	111	[222]	// Neural network-based learning models can represent latent embeddings that capture the internal relations of rich, complex data across various modalities, such as image, audio, and language ==[15]==. // Social and information networks are similarly rich and complex data that encode the dynamics and types of human interactions, and are similarly amenable to representation learning using neural networks.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[15]	['[15]  Yann LeCun, Yoshua Bengio, and Geo\x82rey Hinton. 2015. Deep learning. Nature 521, 7553 (2015), 436–444. ']
483	111	[124]	Social and information networks are similarly rich and complex data that encode the dynamics and types of human interactions, and are similarly amenable to representation learning using neural networks. // In particular, by mapping the way that people choose friends and maintain connections as a “social language,” recent advances in natural language processing (NLP) ==[3]== can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec [17, 18]. // A number of recent research publications have proposed word2vec-based network representation learning frameworks, such as DeepWalk [22], LINE [30], and node2vec [8].	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[3]	['[3]  Yoshua Bengio, Aaron Courville, and Pierre Vincent. 2013. Representation learning: A review and new perspectives. IEEE TPAMI 35, 8 (2013), 1798–1828. ']
484	111	[35, 36]	Social and information networks are similarly rich and complex data that encode the dynamics and types of human interactions, and are similarly amenable to representation learning using neural networks. // In particular, by mapping the way that people choose friends and maintain connections as a “social language,” recent advances in natural language processing (NLP) [3] can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec ==[17, 18]==. // A number of recent research publications have proposed word2vec-based network representation learning frameworks, such as DeepWalk [22], LINE [30], and node2vec [8].	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[17, 18]	['[17]  Tomas Mikolov, Kai Chen, Greg Corrado, and Je\x82rey Dean. 2013. E\x81cient Estimation of Word Representations in Vector Space. CoRR abs/1301.3781 (2013). h\x8ap://arxiv.org/abs/1301.3781 ', '[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\x82 Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS ’13. 3111–3119. ']
485	111	[6]	In particular, by mapping the way that people choose friends and maintain connections as a “social language,” recent advances in natural language processing (NLP) [3] can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec [17, 18]. // A number of recent research publications have proposed word2vec-based network representation learning frameworks, such as DeepWalk ==[22]==, LINE [30], and node2vec [8]. // Instead of handcrafted network feature design, these representation learning methods enable the automatic discovery of useful and meaningful (latent) features from the “raw networks.”	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[22]	['[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ']
486	111	[1]	In particular, by mapping the way that people choose friends and maintain connections as a “social language,” recent advances in natural language processing (NLP) [3] can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec [17, 18]. // A number of recent research publications have proposed word2vec-based network representation learning frameworks, such as DeepWalk [22], LINE ==[30]==, and node2vec [8]. // Instead of handcrafted network feature design, these representation learning methods enable the automatic discovery of useful and meaningful (latent) features from the “raw networks.”	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[30]	['[30]  Jian Tang, Meng \x8b, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding.. In WWW ’15. ACM. ']
487	111	[37]	In particular, by mapping the way that people choose friends and maintain connections as a “social language,” recent advances in natural language processing (NLP) [3] can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec [17, 18]. // A number of recent research publications have proposed word2vec-based network representation learning frameworks, such as DeepWalk [22], LINE [30], and node2vec ==[8]==. // Instead of handcrafted network feature design, these representation learning methods enable the automatic discovery of useful and meaningful (latent) features from the “raw networks.”	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
488	111	[223]	However, these work has thus far focused on representation learning for homogeneous networks—representative of singular type of nodes and relationships. // Yet a large number of social and information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes ==[25]==. // These heterogeneous networks present unique challenges that cannot be handled by representation learning models that are specifically designed for homogeneous networks.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[25]	['[25]  Yizhou Sun and Jiawei Han. 2012. Mining Heterogeneous Information Networks: Principles and Methodologies. Morgan & Claypool Publishers. ']
489	111	[224]	Can we directly apply homogeneous network-oriented embedding architectures (e.g., skip-gram) to heterogeneous networks?// By solving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classification ==[13]==, clustering [27, 28], and similarity search [26, 35]. // In contrast to conventional meta-path-based methods [25], the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths.	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[13]	['[13]  Ming Ji, Jiawei Han, and Marina Danilevsky. 2011. Ranking-based classi\x80cation of heterogeneous information networks. In KDD ’11. ACM, 1298–1306. ']
490	111	[225, 226]	Can we directly apply homogeneous network-oriented embedding architectures (e.g., skip-gram) to heterogeneous networks?// By solving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classification [13], clustering ==[27, 28]==, and similarity search [26, 35]. // In contrast to conventional meta-path-based methods [25], the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths.	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[27, 28]	['[27]  Yizhou Sun, Brandon Norick, Jiawei Han, Xifeng Yan, Philip S. Yu, and Xiao Yu. 2012. Integrating Meta-path Selection with User-guided Object Clustering in Heterogeneous Information Networks. In KDD ’12. ACM, 1348–1356. ', '[28]  Yizhou Sun, Yintao Yu, and Jiawei Han. 2009. Ranking-based Clustering of Heterogeneous Information Networks with Star Network Schema. In KDD ’09. ACM, 797–806. ']
491	111	[224, 227]	Can we directly apply homogeneous network-oriented embedding architectures (e.g., skip-gram) to heterogeneous networks?// By solving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classification [13], clustering [27, 28], and similarity search ==[26, 35]==. // In contrast to conventional meta-path-based methods [25], the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths.	h+	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[26, 35]	['[26]  Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In VLDB ’11. 992–1003. ', '[35]  Jing Zhang, Jie Tang, Cong Ma, Hanghang Tong, Yu Jing, and Juanzi Li. 2015. Panther: Fast top-k similarity search on large networks. In KDD ’15. ACM, 1445–1454.']
492	111	[223]	By solving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classification [13], clustering [27, 28], and similarity search [26, 35]. // In contrast to conventional meta-path-based methods ==[25]==, the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths. // For example, if authors have never published papers in the same venue—imagine one publishes 10 papers all in NIPS and the other has 10 publications all in ICML; their “APCPA”-based PathSim similarity [26] would be zero—this will be naturally overcome by network representation learning.	h-	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[25]	['[25]  Yizhou Sun and Jiawei Han. 2012. Mining Heterogeneous Information Networks: Principles and Methodologies. Morgan & Claypool Publishers. ']
493	111	[224]	In contrast to conventional meta-path-based methods [25], the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths. //  For example, if authors have never published papersin the same venue—imagine one publishes 10 papers all in NIPS and the other has 10 publications all in ICML; their “APCPA”-based PathSim similarity ==[26]== would be zero—this will be naturally overcome by network representation learning. // Contributions.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[26]	['[26]  Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In VLDB ’11. 992–1003. ']
494	111	[223]	The goal of metapath2vec is to maximize the likelihood of preserving both the structures and semantics of a given heterogeneous network. // In metapath2vec, we first propose meta-path ==[25]== based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes. // Second, we extend the skip-gram model [18] to facilitate the modeling of geographically and semantically close nodes.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[25]	['[25]  Yizhou Sun and Jiawei Han. 2012. Mining Heterogeneous Information Networks: Principles and Methodologies. Morgan & Claypool Publishers. ']
495	111	[36]	In metapath2vec, we first propose meta-path [25] based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes. // Second, we extend the skip-gram model ==[18]== to facilitate the modeling of geographically and semantically close nodes. // Finally, we develop a heterogeneous negative sampling-based method, referred to as metapath2vec++, that enables the accurate and effecient prediction of a node’s heterogeneous neighborhood.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\x82 Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS ’13. 3111–3119. ']
496	111	[37, 6, 1]	Finally, we develop a heterogeneous negative sampling-based method, referred to as metapath2vec++, that enables the accurate and effecient prediction of a node’s heterogeneous neighborhood. // The proposed metapath2vec and metapath2vec++ models are different from conventional network embedding models, which focus on homogeneous networks ==[8, 22, 30]==. // Specifically, conventional models suffer from the identical treatment of different types of nodes and relations, leading to the production of indistinguishable representations for heterogeneous nodes—as evident through our evaluation.	h-	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8, 22, 30]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ', '[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ', '[30]  Jian Tang, Meng \x8b, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding.. In WWW ’15. ACM. ']
497	111	[201]	Specically, conventional models suer from the identical treatment of dierent types of nodes and relations, leading to the production of indistinguishable representations for heterogeneous nodes—as evident through our evaluation. // Further, the metapath2vec and metapath2vec++ models also differ from the Predictive Text Embedding (PTE) model ==[29]== in several ways. // First, PTE is a semi-supervised learning model that incorporates label information for text data.	ro	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[29]	['[29]  Jian Tang, Meng \x8b, and Qiaozhu Mei. 2015. PTE: Predictive Text Embedding \x8crough Large-scale Heterogeneous Text Networks. In KDD ’15. ACM, 1165– 1174. ']
498	111	[224]	Essentially, the raw input of PTE is words and its output is the embedding of each word, rather than multiple types of objects. // We summarize the differences of these methods in Table 1, which lists their input to learning algorithms, as well as the top-five similarity search results in the DBIS network for the same two queries used in ==[26]== (see Section 4 for details). // By modeling the heterogeneous neighborhood and further leveraging the heterogeneous negative sampling technique, metapath2vec++ is able to achieve the best top-five similar results for both types of queries.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[26]	['[26]  Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In VLDB ’11. 992–1003. ']
499	111	[159]	Data. // We use two heterogeneous networks, including the AMiner Computer Science (CS) dataset ==[31]== and the Database and Information Systems (DBIS) dataset [26].  // Both datasets and code are publicly available .	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[31]	['[31]  Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. ArnetMiner: Extraction and Mining of Academic Social Networks. In KDD ’08. 990–998. ']
500	111	[224]	Data. // We use two heterogeneous networks, including the AMiner Computer Science (CS) dataset [31] and the Database and Information Systems (DBIS) dataset ==[26]==.  // Both datasets and code are publicly available .	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[26]	['[26]  Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In VLDB ’11. 992–1003. ']
501	111	[228]	PROBLEM DEFINITION. // We formalize the representation learning problem in heterogeneous networks, which was first briefly introduced in ==[21]==. // In specific, we leverage the definition of heterogeneous networks in [25, 27] and present the learning problem with its inputs and outputs.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[21]	['[21]  Siddharth Pal, Yuxiao Dong, Bishal \x8capa, Nitesh V Chawla, Ananthram Swami, and Ram Ramanathan. 2016. Deep learning for network analysis: Problems, approaches and challenges. In Military Communications Conference, MILCOM 2016-2016. IEEE, 588–593. ']
502	111	[223, 225]	We formalize the representation learning problem in heterogeneous networks, which was first briefly introduced in [21]. // In specific, we leverage the definition of heterogeneous networks in ==[25, 27]== and present the learning problem with its inputs and outputs. // Definition.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[25, 27]	['[25]  Yizhou Sun and Jiawei Han. 2012. Mining Heterogeneous Information Networks: Principles and Methodologies. Morgan & Claypool Publishers. ', '[27]  Yizhou Sun, Brandon Norick, Jiawei Han, Xifeng Yan, Philip S. Yu, and Xiao Yu. 2012. Integrating Meta-path Selection with User-guided Object Clustering in Heterogeneous Information Networks. In KDD ’12. ACM, 1348–1356. ']
503	111	[37, 6, 1]	The main challenge of this problem comes from the network heterogeneity, wherein it is difficult to directly apply homogeneous language and network embedding methods. // The premise of network embedding models is to preserve the proximity between a node and its neighborhood (context) ==[8, 22, 30]==. // In a heterogeneous environment, how do we define and model this ‘node–neighborhood’ concept?	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8, 22, 30]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ', '[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ', '[30]  Jian Tang, Meng \x8b, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding.. In WWW ’15. ACM. ']
504	111	[35, 36]	We, rst, briey introduce the word2vec model and its application to homogeneous network embedding tasks. // Given a text corpus, Mikolov et al. proposed word2vec to learn the distributed representations of words in a corpus ==[17, 18]==. // Inspired by it, DeepWalk [22] and node2vec [8] aim to map the word-context concept in a text corpus into a network. Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[17, 18]	['[17]  Tomas Mikolov, Kai Chen, Greg Corrado, and Je\x82rey Dean. 2013. E\x81cient Estimation of Word Representations in Vector Space. CoRR abs/1301.3781 (2013). h\x8ap://arxiv.org/abs/1301.3781 ', '[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\x82 Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS ’13. 3111–3119. ']
505	111	[6]	Given a text corpus, Mikolov et al. proposed word2vec to learn the distributed representations of words in a corpus [17, 18]. // Inspired by it, DeepWalk ==[22]== and node2vec [8] aim to map the word-context concept in a text corpus into a network. // Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[22]	['[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ']
506	111	[37]	Given a text corpus, Mikolov et al. proposed word2vec to learn the distributed representations of words in a corpus [17, 18]. // Inspired by it, DeepWalk [22] and node2vec ==[8]== aim to map the word-context concept in a text corpus into a network. // Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
507	111	[37, 36, 6]	Both methods leverage random walks to achieve this and utilize the skip-gram model to learn the representation of a node that facilitates the prediction of its structural context—local neighborhoods—in a homogeneous network. // Usually, given a network G = (V, E), the objective is to maximize the network probability in terms of local structures ==[8, 18, 22]==, that is: arg max θ Y v ∈V Y c ∈N (v) p(c|v; θ ) (1) where N (v) is the neighborhood of node v in the network G, which can be defined in different ways such as v’s one-hop neighbors, and p(c|v; θ ) defines the conditional probability of having a context node c given a node v. // 3.2 Heterogeneous Network Embedding: metapath2vec.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8, 18, 22]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ', '[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\x82 Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS ’13. 3111–3119. ', '[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ']
508	111	[6]	Meta-Path-Based Random Walks. How to effectively transform the structure of a network into skip-gram? // In DeepWalk ==[22]== and node2vec [8], this is achieved by incorporating the node paths traversed by random walkers over a network into the neighborhood function. // Naturally, we can put random walkers in a heterogeneous network to generate paths of multiple types of nodes.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[22]	['[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ']
509	111	[37]	Meta-Path-Based Random Walks. How to effectively transform the structure of a network into skip-gram? // In DeepWalk [22] and node2vec ==[8]==, this is achieved by incorporating the node paths traversed by random walkers over a network into the neighborhood function. // Naturally, we can put random walkers in a heterogeneous network to generate paths of multiple types of nodes.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
510	111	[224]	The links represent dierent types of relationships among three sets of nodes— such as collaboration relationships on a paper. // The DBIS dataset was constructed and used by Sun et al. ==[26]==. // It covers 464 venues, their top-5000 authors, and corresponding 72,902 publications.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[26]	['[26]  Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In VLDB ’11. 992–1003. ']
511	111	[6]	We compare metapath2vec and metapath2vec++ with several recent network representation learning methods. // DeepWalk ==[22]== / node2vec [8]: With the same random walk path input (p=1 & q=1 in node2vec), we find that the choice between hierarchical softmax (DeepWalk) and negative sampling (node2vec) techniques does not yield significant differences. // Therefore we use p=1 and q=1 in node2vec for comparison.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[22]	['[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ']
512	111	[37]	We compare metapath2vec and metapath2vec++ with several recent network representation learning methods. // DeepWalk [22] / node2vec ==[8]==: With the same random walk path input (p=1 & q=1 in node2vec), we find that the choice between hierarchical softmax (DeepWalk) and negative sampling (node2vec) techniques does not yield significant differences. // Therefore we use p=1 and q=1 in node2vec for comparison.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
513	111	[37]	DeepWalk [22] / node2vec [8]: With the same random walk path input (p=1 & q=1 in node2vec), we find that the choice between hierarchical softmax (DeepWalk) and negative sampling (node2vec) techniques does not yield significant differences. // Therefore we use p=1 and q=1 ==[8]== in node2vec for comparison. // LINE [30]: We use the advanced version of LINE by considering both the 1st- and 2nd-order of node proximity.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
514	111	[1]	Therefore we use p=1 and q=1 [8] in node2vec for comparison. // LINE ==[30]==: We use the advanced version of LINE by considering both the 1st- and 2nd-order of node proximity. // PTE [29]: We construct three bipartite heterogeneous networks (author–author, author–venue, venue–venue) and restrain it as an unsupervised embedding method.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[30]	['[30]  Jian Tang, Meng \x8b, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding.. In WWW ’15. ACM. ']
515	111	[201]	LINE [30]: We use the advanced version of LINE by considering both the 1st- and 2nd-order of node proximity. // PTE ==[29]==: We construct three bipartite heterogeneous networks (author–author, author–venue, venue–venue) and restrain it as an unsupervised embedding method. // Spectral Clustering [33] / Graph Factorization [2]: With the same treatment to these methods in node2vec [8], we exclude them from our comparison, as previous studies have demonstrated that they are outperformed by DeepWalk and LINE.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[29]	['[29]  Jian Tang, Meng \x8b, and Qiaozhu Mei. 2015. PTE: Predictive Text Embedding \x8crough Large-scale Heterogeneous Text Networks. In KDD ’15. ACM, 1165– 1174. ']
516	111	[55]	PTE [29]: We construct three bipartite heterogeneous networks (author–author, author–venue, venue–venue) and restrain it as an unsupervised embedding method. // Spectral Clustering ==[33]== / Graph Factorization [2]: With the same treatment to these methods in node2vec [8], we exclude them from our comparison, as previous studies have demonstrated that they are outperformed by DeepWalk and LINE. // For all embedding methods, we use the same parameters listed below.	h-e	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[33]	['[33]  Lei Tang and Huan Liu. 2011. Leveraging social media networks for classi\x80cation. DMKD 23, 3 (2011), 447–478. ']
517	111	[154]	PTE [29]: We construct three bipartite heterogeneous networks (author–author, author–venue, venue–venue) and restrain it as an unsupervised embedding method. // Spectral Clustering [33] / Graph Factorization ==[2]==: With the same treatment to these methods in node2vec [8], we exclude them from our comparison, as previous studies have demonstrated that they are outperformed by DeepWalk and LINE. // For all embedding methods, we use the same parameters listed below.	h-e	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[2]	['[2]  Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J. Smola. 2013. Distributed Large-scale Natural Graph Factorization. In WWW ’13. ACM, 37–48. ']
518	111	[37]	PTE [29]: We construct three bipartite heterogeneous networks (author–author, author–venue, venue–venue) and restrain it as an unsupervised embedding method. // Spectral Clustering [33] / Graph Factorization [2]: With the same treatment to these methods in node2vec ==[8]==, we exclude them from our comparison, as previous studies have demonstrated that they are outperformed by DeepWalk and LINE. // For all embedding methods, we use the same parameters listed below.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
519	111	[224]	Our empirical results also show that this simple meta-path scheme “APVPA” can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks, suggesting its applicability to potential applications for academic search services. // We evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks, including multi-class node classification ==[13]==, node clustering [27], and similarity search [26]. // In addition, we also use the embedding projector in TensorFlow [1] to visualize the node embeddings learned from the heterogeneous academic networks.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[13]	['[13]  Ming Ji, Jiawei Han, and Marina Danilevsky. 2011. Ranking-based classi\x80cation of heterogeneous information networks. In KDD ’11. ACM, 1298–1306. ']
520	111	[225]	Our empirical results also show that this simple meta-path scheme “APVPA” can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks, suggesting its applicability to potential applications for academic search services. // We evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks, including multi-class node classification [13], node clustering ==[27]==, and similarity search [26]. // In addition, we also use the embedding projector in TensorFlow [1] to visualize the node embeddings learned from the heterogeneous academic networks.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[27]	['[27]  Yizhou Sun, Brandon Norick, Jiawei Han, Xifeng Yan, Philip S. Yu, and Xiao Yu. 2012. Integrating Meta-path Selection with User-guided Object Clustering in Heterogeneous Information Networks. In KDD ’12. ACM, 1348–1356. ']
521	111	[224]	Our empirical results also show that this simple meta-path scheme “APVPA” can lead to node embeddings that can be generalized to diverse heterogeneous academic mining tasks, suggesting its applicability to potential applications for academic search services. // We evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks, including multi-class node classification [13], node clustering [27], and similarity search ==[26]==. // In addition, we also use the embedding projector in TensorFlow [1] to visualize the node embeddings learned from the heterogeneous academic networks.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[26]	['[26]  Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. In VLDB ’11. 992–1003. ']
522	111	[111]	We evaluate the quality of the latent representations learned by different methods over three classical heterogeneous network mining tasks, including multi-class node classification [13], node clustering [27], and similarity search [26]. // In addition, we also use the embedding projector in TensorFlow ==[1]== to visualize the node embeddings learned from the heterogeneous academic networks. // Multi-Class Classication.	hoe	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[1]	['[1]  Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Je\x82rey Dean, Ma\x8ahieu Devin, Sanjay Ghemawat, Geo\x82rey Irving, and others. 2016. TensorFlow: A system for large-scale machine learning. In OSDI ’16. ']
523	111	[178, 157]	RELATED WORK. // Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks ==[10, 34]==, such as the application of factorization models for recommendation systems [14, 16], node classification [32], relational mining [19], and role discovery [9]. // This rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[10, 34]	['[10]  Peter D Ho\x82, Adrian E Ra\x89ery, and Mark S Handcock. 2002. Latent space approaches to social network analysis. Journal of the American Statistical association 97, 460 (2002), 1090–1098. ', '[34]  Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. 2007. Graph embedding and extensions: A general framework for dimensionality reduction. IEEE TPAMI 29, 1 (2007). ']
524	111	[229, 230]	RELATED WORK. // Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks [10, 34], such as the application of factorization models for recommendation systems ==[14, 16]==, node classification [32], relational mining [19], and role discovery [9]. // This rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[14, 16]	['[14]  Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative \x80ltering model. In KDD ’08. ACM, 426–434. ', '[16]  Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. 2011. Recommender systems with social regularization. In WSDM ’11. 287–296. ']
525	111	[33]	RELATED WORK. // Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks [10, 34], such as the application of factorization models for recommendation systems [14, 16], node classification ==[32]==, relational mining [19], and role discovery [9]. // This rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[32]	['[32]  Lei Tang and Huan Liu. 2009. Relational learning via latent social dimensions. In KDD ’09. 817–826. ']
526	111	[141]	RELATED WORK. // Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks [10, 34], such as the application of factorization models for recommendation systems [14, 16], node classification [32], relational mining ==[19]==, and role discovery [9]. // This rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[19]	['[19]  Jennifer Neville and David Jensen. 2005. Leveraging relational autocorrelation with latent group models. In Proceedings of the 4th international workshop on Multi-relational mining. ACM, 49–55. ']
527	111	[169]	RELATED WORK. // Network representation learning can be traced back to the usage of latent factor models for network analysis and graph mining tasks [10, 34], such as the application of factorization models for recommendation systems [14, 16], node classification [32], relational mining [19], and role discovery ==[9]==. // This rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[9]	['[9]  Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. 2012. Rolx: structural role extraction & mining in large graphs. In KDD ’12. ACM, 1231–1239. ']
528	111	[37]	This rich line of research focuses on factorizing the matrix/tensor format (e.g., the adjacency matrix) of a network, generating latent-dimension features for nodes or edges in this network. // However, the computational cost of decomposing a large-scale matrix/tensor is usually very expensive, and also suffers from its statistical performance drawback ==[8]==, making it neither practical nor effective for addressing tasks in big networks. // With the advent of deep learning techniques, significant effort has been devoted to designing neural network-based representation learning models.	ho	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
529	111	[35, 36]	With the advent of deep learning techniques, signicant effort has been devoted to designing neural network-based representation learning models. // For example, Mikolov et al. proposed the word2vec framework—a two-layer neural network—to learn the distributed representations of words in natural language ==[17, 18]==. // Building on word2vec, Perozzi et al. suggested that the “context” of a node can be denoted by their co-occurrence in a random walk path [22].	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[17, 18]	['[17]  Tomas Mikolov, Kai Chen, Greg Corrado, and Je\x82rey Dean. 2013. E\x81cient Estimation of Word Representations in Vector Space. CoRR abs/1301.3781 (2013). h\x8ap://arxiv.org/abs/1301.3781 ', '[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je\x82 Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS ’13. 3111–3119. ']
530	111	[6]	For example, Mikolov et al. proposed the word2vec framework—a two-layer neural network—to learn the distributed representations of words in natural language [17, 18]. // Building on word2vec, Perozzi et al. suggested that the “context” of a node can be denoted by their co-occurrence in a random walk path ==[22]==. // Formally, they put random walkers over networks to record their walking paths, each of which is composed of a chain of nodes that could be considered as a “sentence” of words in a text corpus.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[22]	['[22]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD ’14. ACM, 701–710. ']
531	111	[37]	Formally, they put random walkers over networks to record their walking paths, each of which is composed of a chain of nodes that could be considered as a “sentence” of words in a text corpus. // More recently, in order to diversify the neighborhood of a node, Grover & Leskovec presented biased random walkers—a mixture of breadth-first and width-first search procedures—over networks to produce paths of nodes ==[8]==. // With node paths generated, both works leveraged the skip-gram architecture in word2vec to model the structural correlations between nodes in a path.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. Node2Vec: Scalable Feature Learning for Networks. In KDD ’16. ACM, 855–864. ']
532	111	[109, 202, 29, 48, 231]	With node paths generated, both works leveraged the skip-gram architecture in word2vec to model the structural correlations between nodes in a path. // In addition, several other methods have been proposed for learning representations in networks ==[4, 5, 11, 20, 23]==. // In particular, to learn network embeddings, Tang et al. decomposed a node’s context into first-order (friends) and second-order (friends’ friends) proximity [30], which was further developed into a semi-supervised model PTE for embedding text data [29].	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[4, 5, 11, 20, 23]	['[4]  Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, and \x8comas S. Huang. 2015. Heterogeneous Network Embedding via Deep Architectures. In KDD ’15. ACM, 119–128. ', '[5]  Ting Chen and Yizhou Sun. 2017. Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identi\x80cation. In WSDM ’17. ACM. ', '[11]  Xiao Huang, Jundong Li, and Xia Hu. 2017. Label Informed A\x8aributed Network Embedding. In WSDM ’17. na. ', '[20]  Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric Transitivity Preserving Graph Embedding. In KDD ’16. ACM, 1105–1114. ', '[23]  Xiang Ren, Wenqi He, Meng \x8b, Clare R Voss, Heng Ji, and Jiawei Han. 2016. Label noise reduction in entity typing by heterogeneous partial-label embedding. In KDD ’16. ACM. ']
533	111	[1]	In addition, several other methods have been proposed for learning representations in networks [4, 5, 11, 20, 23]. // In particular, to learn network embeddings, Tang et al. decomposed a node’s context into first-order (friends) and second-order (friends’ friends) proximity ==[30]==, which was further developed into a semi-supervised model PTE for embedding text data [29]. //  CONCLUSION.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[30]	['[30]  Jian Tang, Meng \x8b, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding.. In WWW ’15. ACM. ']
534	111	[201]	In addition, several other methods have been proposed for learning representations in networks [4, 5, 11, 20, 23]. // In particular, to learn network embeddings, Tang et al. decomposed a node’s context into first-order (friends) and second-order (friends’ friends) proximity [30], which was further developed into a semi-supervised model PTE for embedding text data ==[29]==. //  CONCLUSION.	b	metapath2vec- Scalable Representation Learning for Heterogeneous Networks	[29]	['[29]  Jian Tang, Meng \x8b, and Qiaozhu Mei. 2015. PTE: Predictive Text Embedding \x8crough Large-scale Heterogeneous Text Networks. In KDD ’15. ACM, 1165– 1174. ']
535	26	[6]	This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. // We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. ==[20]== as well as the skip-gram model with negative sampling of Mikolov et al. [18] // We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization.	b	GraRep- Learning Graph Representations with Global Structural Information	[20]	['[20]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
536	26	[36]	This model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work, integrates global structural information of the graph into the learning process. // We also formally analyze the connections between our work and several previous research efforts, including the DeepWalk model of Perozzi et al. [20] as well as the skip-gram model with negative sampling of Mikolov et al. ==[18]== // We conduct experiments on a language network, a social network as well as a citation network and show that our learned global representations can be effectively used as features in tasks such as clustering, classification and visualization.	b	GraRep- Learning Graph Representations with Global Structural Information	[18]	['[18]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
537	26	[6]	Recently, there has been a surge of interest in learning graph representations from data. // For example, DeepWalk ==[20]==, one recent model, transforms a graph structure into a sample collection of linear sequences consisting of vertices using uniform sampling (which is also called truncated random walk). // The skip-gram model [18], originally designed for learning word representations from linear sequences, can also be used to learn the representations of vertices from such samples.	b	GraRep- Learning Graph Representations with Global Structural Information	[20]	['[20]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
538	26	[36]	For example, DeepWalk [20], one recent model, transforms a graph structure into a sample collection of linear sequences consisting of vertices using uniform sampling (which is also called truncated random walk). // The skip-gram model ==[18]==, originally designed for learning word representations from linear sequences, can also be used to learn the representations of vertices from such samples. // Although this method is empirically effective, it is not well understood what is the exact loss function defined over the graph involved in their learning process.	h-	GraRep- Learning Graph Representations with Global Structural Information	[18]	['[18]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
539	26	[1]	The above limitation is overcome in our proposed model through the preservation of different k-step relational information in distinct subspaces. //  Another recently proposed work is LINE ==[25]==, which has a loss function to capture both 1-step and 2-step local relational information. // To capture certain complex relations in such local information, they also learn non-linear transformations from such data.	b	GraRep- Learning Graph Representations with Global Structural Information	[25]	['[25]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015. ']
540	26	[36]	Currently, there are two mainstream methods for learning word representations: neural embedding methods and matrix factorization based approaches. Neural embedding methods employ a fixed slide window capturing context words of current word. // Models like skipgram ==[18]== are proposed, which provide an efficient approach to learning word representations. // While these methods may yield good performances on some tasks, they can poorly capture useful information since they use separate local context windows, instead of global co-occurrence counts [19].	h-	GraRep- Learning Graph Representations with Global Structural Information	[18]	['[18]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
541	26	[167]	Models like skipgram [18] are proposed, which provide an efficient approach to learning word representations. // While these methods may yield good performances on some tasks, they can poorly capture useful information since they use separate local context windows, instead of global co-occurrence counts ==[19]==. // On the other hand, the family of matrix factorization methods can utilize global statistics [5].	h+	GraRep- Learning Graph Representations with Global Structural Information	[19]	['[19]  J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. EMNLP, 12, 2014. ']
542	26	[232]	While these methods may yield good performances on some tasks, they can poorly capture useful information since they use separate local context windows, instead of global co-occurrence counts [19]. // On the other hand, the family of matrix factorization methods can utilize global statistics ==[5]==. // Previous work include Latent Semantic Analysis (LSA) [15], which decomposes termdocument matrix and yields latent semantic representations.	b	GraRep- Learning Graph Representations with Global Structural Information	[5]	['[5]  J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. BRM, 44(3):890–907, 2012. ']
543	26	[233]	On the other hand, the family of matrix factorization methods can utilize global statistics [5]. // Previous work include Latent Semantic Analysis (LSA) ==[15]==, which decomposes termdocument matrix and yields latent semantic representations. // Lund et al. [17] put forward Hyperspace Analogue to Language (HAL), factorizing a word-word co-occurrence counts matrix to generate word representations.	b	GraRep- Learning Graph Representations with Global Structural Information	[15]	['[15]  T. K. Landauer, P. W. Foltz, and D. Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259–284, 1998. ']
544	26	[234]	Previous work include Latent Semantic Analysis (LSA) [15], which decomposes termdocument matrix and yields latent semantic representations. // Lund et al. ==[17]== put forward Hyperspace Analogue to Language (HAL), factorizing a word-word co-occurrence counts matrix to generate word representations. // Levy et al. [4] presented matrix factorization over shifted positive Pointwise Mutual Information (PMI) matrix for learning word representations and showed that the Skip-Gram model with Negative Sampling (SGNS) can be regarded as a model that implicitly such a matrix [16].	b	GraRep- Learning Graph Representations with Global Structural Information	[17]	['[17]  K. Lund and C. Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence. BRMIC, 28(2):203–208, 1996. ']
545	26	[235]	Lund et al. [17] put forward Hyperspace Analogue to Language (HAL), factorizing a word-word co-occurrence counts matrix to generate word representations. // Levy et al. ==[4]== presented matrix factorization over shifted positive Pointwise Mutual Information (PMI) matrix for learning word representations and showed that the Skip-Gram model with Negative Sampling (SGNS) can be regarded as a model that implicitly such a matrix [16]. //  Graph Representation Approaches	b	GraRep- Learning Graph Representations with Global Structural Information	[4]	['[4]  J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: A computational study. BRM, 39(3):510–526, 2007. ']
546	26	[160]	Lund et al. [17] put forward Hyperspace Analogue to Language (HAL), factorizing a word-word co-occurrence counts matrix to generate word representations. // Levy et al. [4] presented matrix factorization over shifted positive Pointwise Mutual Information (PMI) matrix for learning word representations and showed that the Skip-Gram model with Negative Sampling (SGNS) can be regarded as a model that implicitly such a matrix ==[16]==. //  Graph Representation Approaches	b	GraRep- Learning Graph Representations with Global Structural Information	[16]	['[16]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177–2185, 2014. ']
547	26	[152]	Graph Representation Approaches. // There exist several classical approaches to learning low dimensional graph representations, such as multidimensional scaling (MDS) ==[8]==, IsoMap [28], LLE [21], and Laplacian Eigenmaps [3]. // Recently, Tang et al. [27] presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification.	b	GraRep- Learning Graph Representations with Global Structural Information	[8]	['[8]  T. F. Cox and M. A. Cox. Multidimensional scaling. CRC Press, 2000. ']
548	26	[3]	Graph Representation Approaches. // There exist several classical approaches to learning low dimensional graph representations, such as multidimensional scaling (MDS) [8], IsoMap ==[28]==, LLE [21], and Laplacian Eigenmaps [3]. // Recently, Tang et al. [27] presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification.	b	GraRep- Learning Graph Representations with Global Structural Information	[28]	['[28]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
549	26	[4]	Graph Representation Approaches. // There exist several classical approaches to learning low dimensional graph representations, such as multidimensional scaling (MDS) [8], IsoMap [28], LLE ==[21]==, and Laplacian Eigenmaps [3]. // Recently, Tang et al. [27] presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification.	b	GraRep- Learning Graph Representations with Global Structural Information	[21]	['[21]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ']
550	26	[5]	Graph Representation Approaches. // There exist several classical approaches to learning low dimensional graph representations, such as multidimensional scaling (MDS) [8], IsoMap [28], LLE [21], and Laplacian Eigenmaps ==[3]==. // Recently, Tang et al. [27] presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification.	b	GraRep- Learning Graph Representations with Global Structural Information	[3]	['[3]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
551	26	[33]	There exist several classical approaches to learning low dimensional graph representations, such as multidimensional scaling (MDS) [8], IsoMap [28], LLE [21], and Laplacian Eigenmaps [3]. // Recently, Tang et al. ==[27]== presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification. // Ahmed et al. [1] proposed a graph factorization method, which used stochastic gradient descent to optimize matrices from large graphs.	b	GraRep- Learning Graph Representations with Global Structural Information	[27]	['[27]  L. Tang and H. Liu. Relational learning via latent social dimensions. In SIGKDD, pages 817–826. ACM, 2009. ']
552	26	[154]	Recently, Tang et al. [27] presented methods for learning latent representational vectors of the graphs which can then be applied to social network classification. // Ahmed et al. ==[1]== proposed a graph factorization method, which used stochastic gradient descent to optimize matrices from large graphs. // Perozzi et al. [20] presented an approach, which transformed graph structure into several linear vertex sequences by using a truncated random walk algorithm and generated vertex representations by using skip-gram model. This is considered as an equally weighted linear combination of k-step information.	b	GraRep- Learning Graph Representations with Global Structural Information	[1]	['[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWW, pages 37–48. International World Wide Web Conferences Steering Committee, 2013. ']
553	26	[6]	hmed et al. [1] proposed a graph factorization method, which used stochastic gradient descent to optimize matrices from large graphs. // Perozzi et al. ==[20]== presented an approach, which transformed graph structure into several linear vertex sequences by using a truncated random walk algorithm and generated vertex representations by using skip-gram model. // This is considered as an equally weighted linear combination of k-step information.	b	GraRep- Learning Graph Representations with Global Structural Information	[20]	['[20]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
554	26	[1]	This is considered as an equally weighted linear combination of k-step information. // Tang et al. ==[25]== later proposed a large-scale information network embedding, which optimizes a loss function where both 1-step and 2-step relational information can be captured in the learning process. // GRAREP MODEL	b	GraRep- Learning Graph Representations with Global Structural Information	[25]	['[25]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015. ']
555	26	[36]	Our objective aims to maximize: 1) the probability that these pairs come from the graph, and 2) the probability that all other pairs do not come from the graph. // Motivated by the skip-gram model by Mikolov et al. ==[18]==, we employ noise contrastive estimation (NCE), which is proposed by Gutmann et al. [11], to define our objective function. // Following a similar discussion presented in [16], we first introduce our k-step loss function defined over the complete graph as follows: Lk = X w∈V Lk(w) where Lk(w) = X c∈V pk(c|w) log σ( ~w · ~c) ! +λEc 0∼pk(V )[log σ(−~w·c~0)] Here pk(c|w) describes the k-step relationship between w and c (the k-step transition probability from w to c), σ(·) is sigmoid function defined as σ(x) = (1 + e −x ) −1 , λ is a hyper-parameter indicating the number of negative samples, and pk(V ) is the distribution over the vertices in the graph.	ho	GraRep- Learning Graph Representations with Global Structural Information	[18]	['[18]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
556	26	[91]	Our objective aims to maximize: 1) the probability that these pairs come from the graph, and 2) the probability that all other pairs do not come from the graph. // Motivated by the skip-gram model by Mikolov et al. [18], we employ noise contrastive estimation (NCE), which is proposed by Gutmann et al. ==[11]==, to define our objective function. // Following a similar discussion presented in [16], we first introduce our k-step loss function defined over the complete graph as follows: Lk = X w∈V Lk(w) where Lk(w) = X c∈V pk(c|w) log σ( ~w · ~c) ! +λEc 0∼pk(V )[log σ(−~w·c~0)] Here pk(c|w) describes the k-step relationship between w and c (the k-step transition probability from w to c), σ(·) is sigmoid function defined as σ(x) = (1 + e −x ) −1 , λ is a hyper-parameter indicating the number of negative samples, and pk(V ) is the distribution over the vertices in the graph.	ho	GraRep- Learning Graph Representations with Global Structural Information	[11]	['[11]  M. U. Gutmann and A. Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. JMLR, 13(1):307–361, 2012. ']
557	26	[160]	Motivated by the skip-gram model by Mikolov et al. [18], we employ noise contrastive estimation (NCE), which is proposed by Gutmann et al. [11], to define our objective function. // Following ==[16]==, we define e = ~w · ~c, and setting ∂Lk ∂e = 0. This yields the following: ~w · ~c = log A k P w,c w0 Ak w0 ,c ! − log(β) where β = λ/N. // Here pk(c|w) describes the k-step relationship between w and c (the k-step transition probability from w to c), σ(·) is sigmoid function defined as σ(x) = (1 + e−x) −1 , λ is a hyper-parameter indicating the number of negative samples, and pk(V ) is the distribution over the vertices in the graph.	hoe	GraRep- Learning Graph Representations with Global Structural Information	[16]	['[16]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177–2185, 2014. ']
558	26	[160]	Optimization with Matrix Factorization. // Following the work of Levy et al. ==[16]==, to reduce noise, we replace all negative entries in Y k with 0. // This gives us a positive k-step log probabilistic matrix X k , where X k i,j = max(Y k i,j , 0) While various techniques for matrix factorization exist, in this work we focus on the popular singular value decomposition (SVD) method due to its simplicity.	hoe	GraRep- Learning Graph Representations with Global Structural Information	[16]	['[16]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177–2185, 2014. ']
559	26	[236, 237]	While various techniques for matrix factorization exist, in this work we focus on the popular singular value decomposition (SVD) method due to its simplicity. // SVD has been shown successful in several matrix factorization tasks ==[9, 14]==, and is regarded as one of the important methods that can be used for dimensionality reduction. // For the matrix Xk, SVD factorizes it as: Xk = U kΣk(Vk)T where U and V are orthonormal matrices and Σ is a diagonal matrix consisting of an ordered list of singular values.	h+	GraRep- Learning Graph Representations with Global Structural Information	[9, 14]	['[9]  C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936. ', '[14]  V. Klema and A. J. Laub. The singular value decomposition: Its computation and some applications. Automatic Control, 25(2):164–176, 1980. ']
560	26	[238, 232, 239]	This way, we can factorize our matrix X kas:Xk ≈ Xkd = WkCkwhereWk = Ukd (Σkd)12 , Ck = (Σkd)12 Vkd T. // The resulting Wk gives representations of current vertices as its column vectors, and Ck gives the representations of context vertices as its column vectors ==[6, 5, 30]==. // The final matrix Wk is returned from the algorithm as the low-d representations of the vertices which capture k-step global structural information in the graph.	b	GraRep- Learning Graph Representations with Global Structural Information	[6, 5, 30]	['[6]  J. Caron. Experiments with lsa scoring: Optimal rank and basis. In CIR, pages 157–169, 2001. ', '[5]  J. A. Bullinaria and J. P. Levy. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. BRM, 44(3):890–907, 2012. ', '[30]  P. D. Turney. Domain and function: A dual-space model of semantic relations and compositions. JAIR, pages 533–585, 2012. ']
561	26	[240]	Note that here we are essentially finding a projection from the row space of Xk to the row space of Wk with a lower rank. // Thus alternative approaches other than the popular SVD can also be exploited. Examples include incremental SVD ==[22]==, independent component analysis (ICA) [7, 13], and deep neural networks [12]. // Our focus in this work is on the novel model for learning graph representations, so we do not pursue any alternative methods.	ro	GraRep- Learning Graph Representations with Global Structural Information	[22]	['[22]  B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Incremental singular value decomposition algorithms for highly scalable recommender systems. In ICIS, pages 27–28. Citeseer, 2002. ']
562	26	[241, 242]	Note that here we are essentially finding a projection from the row space of Xk to the row space of Wk with a lower rank. // Thus alternative approaches other than the popular SVD can also be exploited. Examples include incremental SVD [22], independent component analysis (ICA) ==[7, 13]==, and deep neural networks [12]. // Our focus in this work is on the novel model for learning graph representations, so we do not pursue any alternative methods.	ro	GraRep- Learning Graph Representations with Global Structural Information	[7, 13]	['[7]  P. Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314, 1994. ', '[13]  C. Jutten and J. Herault. Blind separation of sources, part i: An adaptive algorithm based on neuromimetic architecture. Signal processing, 24(1):1–10, 1991. ']
563	26	[43]	Note that here we are essentially finding a projection from the row space of Xk to the row space of Wk with a lower rank. // Thus alternative approaches other than the popular SVD can also be exploited. Examples include incremental SVD [22], independent component analysis (ICA) [7, 13], and deep neural networks ==[12]==. // Our focus in this work is on the novel model for learning graph representations, so we do not pursue any alternative methods.	ro	GraRep- Learning Graph Representations with Global Structural Information	[12]	['[12]  G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006. ']
564	26	[160]	We plug these expected counts into the equation of Y E−SGNS i,j , and we arrive at: YE−SGNS w,c = log  #(w, c) · |D| #(w) · #(c) − log(λ) where D is the collection of all observed pairs in sequences, that is, |D| = γK. // To maintain the consistency with Levy et al. ==[16]==, we only employed SVD in this work. // This shows SGNS is essentially a special version of our GraRep model that deals with linear sequences which can be sampled from graphs.	ho	GraRep- Learning Graph Representations with Global Structural Information	[16]	['[16]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177–2185, 2014. ']
565	26	[1]	Baseline Algorithms. // LINE ==[25]==. LINE is a recently proposed method for learning graph representations on large-scale information networks. // LINE defines a loss function based on 1-step and 2-step relational information between vertices.	b	GraRep- Learning Graph Representations with Global Structural Information	[25]	['[25]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015. ']
566	26	[6]	LINE will get the best performance, if concatenating the representation of 1- step and 2-step relational information and tuning the threshold of maximum number of vertices. // DeepWalk ==[20]==. DeepWalk is a method that learns the representation of social networks. // The original model only works for unweighted graph.	b	GraRep- Learning Graph Representations with Global Structural Information	[20]	['[20]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
567	26	[73]	This method can be regarded as a special case of our model, where different representational vector of each k-step information is averaged. // Spectral Clustering ==[23]==. Spectral clustering is a reasonable baseline algorithm, which aims at minimizing Normalized Cut (NCut). // Spectral clustering also factorize a matrix, but it focuses on a different matrix of the graphs – the Laplacian Matrix. Essentially, the difference between spectral clustering and E-SGNS lies on their different loss function.	b	GraRep- Learning Graph Representations with Global Structural Information	[23]	['[23]  J. Shi and J. Malik. Normalized cuts and image segmentation. PAMI, 22(8):888–905, 2000. ']
568	26	[1]	Parameter Settings. // As suggested in ==[25]==, for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting value as 0.025, the number of negative samples as 5, and the total number of samples as 10 billion. // We also concatenate both 1-step and 2-step relational information to form the representations and employ the reconstruction strategy for vertices with small degrees to achieve the optimal performance.	hoe	GraRep- Learning Graph Representations with Global Structural Information	[25]	['[25]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015. ']
569	26	[6]	We also concatenate both 1-step and 2-step relational information to form the representations and employ the reconstruction strategy for vertices with small degrees to achieve the optimal performance. // As mentioned in ==[20]==, for DeepWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80. // According to [25], LINE yielded better results when the learned graph representations are L2 normalized, while DeepWalk and E-SGNS can achieve optimal performance without normalization.	hoe	GraRep- Learning Graph Representations with Global Structural Information	[20]	['[20]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
570	26	[1]	As mentioned in [20], for DeepWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80. // According to ==[25]==, LINE yielded better results when the learned graph representations are L2 normalized, while DeepWalk and E-SGNS can achieve optimal performance without normalization. //  For GraRep, we found the L2 normalization yielded better results.	ho	GraRep- Learning Graph Representations with Global Structural Information	[25]	['[25]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW. ACM, 2015. ']
571	48	[157]	However, graph representation in general remains an open problem. // Recently, the graph embedding paradigm ==[35]== is proposed to represent vertices of a graph in a low-dimensional vector space while the structures (i.e. edges and other high-order structures) of the graph can be reconstructed in the vector space. // With proper graph embedding, we can easily apply classic vector-based machine learning techniques to process graph data.	b	Asymmetric Transitivity Preserving Graph Embedding	[35]	['[35]  S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1):40–51, 2007. ']
572	48	[243]	The answer is no due to a fundamentally different characteristic of directed graphs: asymmetric transitivity. // Transitivity is a common characteristic of undirected and directed graphs ==[29]== (see Figure 1). // In undirected graphs, if there is an edge between vertices u and w, and one between w and v, then it is likely that u and v are connected by an edge.	ho	Asymmetric Transitivity Preserving Graph Embedding	[29]	['[29]  T. A. Snijders, P. E. Pattison, G. L. Robins, and M. S. Handcock. New specifications for exponential random graph models. Sociological methodology, 36(1):99–153, 2006. ']
573	48	[49, 244]	In undirected graphs, if there is an edge between vertices u and w, and one between w and v, then it is likely that u and v are connected by an edge. // Transitivity plays a key role in graph inference and analysis tasks, such as calculating similarities between nodes ==[16, 14]== and measuring the importance of nodes. // Transitivity is symmetric in undirected graphs.	b	Asymmetric Transitivity Preserving Graph Embedding	[16, 14]	['[16]  L. Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39–43, 1953. ', '[14]  G. Jeh and J. Widom. Simrank: a measure of structural-context similarity. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538–543. ACM, 2002. ']
574	48	[10, 245, 246, 26, 6]	However, how to preserve the asymmetric transitivity of directed graphs in a vector space is much more challenging. // Recently, a paucity of studies ==[37, 20, 3, 2, 24]== focus on directed graph embedding. // In order to reflect the asymmetric property in vector space, these methods design asymmetric metrics on the embedding vectors.	b	Asymmetric Transitivity Preserving Graph Embedding	[37, 20, 3, 2, 24]	['[37]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 487–494. ACM, 2007.', '[20]  K. Miller, M. I. Jordan, and T. L. Griffiths. Nonparametric latent feature models for link prediction. In Advances in neural information processing systems, pages 1276–1284, 2009. ', '[3]  S. Chang, G.-J. Qi, C. C. Aggarwal, J. Zhou, M. Wang, and T. S. Huang. Factorized similarity learning in networks. In Data Mining (ICDM), 2014 IEEE International Conference on, pages 60–69. IEEE, 2014. ', '[2]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891–900. ACM, 2015. ', '[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
575	48	[49]	From the graph embedding pserspective, the property of asymmetric transitivity leads to the assumption that the more and the shorter paths from vi to vj , the more similar should be vi’s source vector and vj ’s target vector. // This assumption coincides with high-order proximities of nodes in graphs, such as Katz ==[16]== and rooted pagerank [30]. // What is more, these high-order proximities are defined to be asymmetric in directed graphs. Thus, in its place, we propose to use highorder proximities of nodes as the target measure, resulting in a novel directed graph embedding algorithm, High Order Proximity preserved Embedding (HOPE).	b	Asymmetric Transitivity Preserving Graph Embedding	[16]	['[16]  L. Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39–43, 1953. ']
576	48	[50]	From the graph embedding pserspective, the property of asymmetric transitivity leads to the assumption that the more and the shorter paths from vi to vj , the more similar should be vi’s source vector and vj ’s target vector. // This assumption coincides with high-order proximities of nodes in graphs, such as Katz [16] and rooted pagerank ==[30]==. // What is more, these high-order proximities are defined to be asymmetric in directed graphs. Thus, in its place, we propose to use highorder proximities of nodes as the target measure, resulting in a novel directed graph embedding algorithm, High Order Proximity preserved Embedding (HOPE).	b	Asymmetric Transitivity Preserving Graph Embedding	[30]	['[30]  H. H. Song, T. W. Cho, V. Dave, Y. Zhang, and L. Qiu. Scalable proximity estimation and link prediction in online social networks. In Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 322–335. ACM, 2009. ']
577	48	[247, 4, 3, 5, 248]	Graph Embedding. // Graph embedding technology has been widely studied in the fields of dimensionality reduction ==[15, 27, 33, 1, 8]==, natural language processing [18], network analysis [11] and so on. // For dimensionality reduction, adjacency matrices of graphs are constructed from the feature similarity (distance) between samples [35]. And the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space.	b	Asymmetric Transitivity Preserving Graph Embedding	[15, 27, 33, 1, 8]	['[15]  I. Jolliffe. Principal component analysis. Wiley Online Library, 2002. ', '[27]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ', '[33]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ', '[1]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ', '[8]  X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face recognition using laplacianfaces. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(3):328–340, 2005. ']
578	48	[160]	Graph Embedding. // Graph embedding technology has been widely studied in the fields of dimensionality reduction [15, 27, 33, 1, 8], natural language processing ==[18]==, network analysis [11] and so on. // For dimensionality reduction, adjacency matrices of graphs are constructed from the feature similarity (distance) between samples [35]. And the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space.	b	Asymmetric Transitivity Preserving Graph Embedding	[18]	['[18]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185, 2014. ']
579	48	[178]	Graph Embedding. // Graph embedding technology has been widely studied in the fields of dimensionality reduction [15, 27, 33, 1, 8], natural language processing [18], network analysis ==[11]== and so on. // For dimensionality reduction, adjacency matrices of graphs are constructed from the feature similarity (distance) between samples [35]. And the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space.	b	Asymmetric Transitivity Preserving Graph Embedding	[11]	['[11]  P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis. Journal of the american Statistical association, 97(460):1090–1098, 2002. ']
580	48	[157]	Graph embedding technology has been widely studied in the fields of dimensionality reduction [15, 27, 33, 1, 8], natural language processing [18], network analysis [11] and so on. // For dimensionality reduction, adjacency matrices of graphs are constructed from the feature similarity (distance) between samples ==[35]==. // And the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space.	b	Asymmetric Transitivity Preserving Graph Embedding	[35]	['[35]  S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(1):40–51, 2007. ']
581	48	[5]	And the graph embedding algorithms aim to preserve the feature similarity in the embedded latent space. // For example, Laplacian Eigenmaps (LE) ==[1]== aims to learn the low-dimensional representation to expand the manifold where data lie. // Locality Preserving Projections (LPP) [8] is a linearization variant of LE which learns a linear projection from feature space to embedding space.	b	Asymmetric Transitivity Preserving Graph Embedding	[1]	['[1]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
582	48	[248]	For example, Laplacian Eigenmaps (LE) [1] aims to learn the low-dimensional representation to expand the manifold where data lie. // Locality Preserving Projections (LPP) ==[8]== is a linearization variant of LE which learns a linear projection from feature space to embedding space. // Besides, there are many other graph embedding algorithms for dimensionality reduction, including non-linear [33, 27], linear [15, 6], kernlized [28] and tensorized [36] algorithms.	b	Asymmetric Transitivity Preserving Graph Embedding	[8]	['[8]  X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face recognition using laplacianfaces. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(3):328–340, 2005. ']
583	48	[3, 4]	Locality Preserving Projections (LPP) [8] is a linearization variant of LE which learns a linear projection from feature space to embedding space. // Besides, there are many other graph embedding algorithms for dimensionality reduction, including non-linear ==[33, 27]==, linear [15, 6], kernlized [28] and tensorized [36] algorithms. All of these algorithms are based on undirected graphs derived from symmetric similarities. // Thus, they cannot preserve asymmetric transitivity.	h-	Asymmetric Transitivity Preserving Graph Embedding	[33, 27]	['[33]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ', '[27]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ']
584	48	[247, 249]	Locality Preserving Projections (LPP) [8] is a linearization variant of LE which learns a linear projection from feature space to embedding space. // Besides, there are many other graph embedding algorithms for dimensionality reduction, including non-linear [33, 27], linear ==[15, 6]==, kernlized [28] and tensorized [36] algorithms. All of these algorithms are based on undirected graphs derived from symmetric similarities. // Thus, they cannot preserve asymmetric transitivity.	h-	Asymmetric Transitivity Preserving Graph Embedding	[15, 6]	['[15]  I. Jolliffe. Principal component analysis. Wiley Online Library, 2002. ', '[6]  R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7(2):179–188, 1936. ']
585	48	[250]	Locality Preserving Projections (LPP) [8] is a linearization variant of LE which learns a linear projection from feature space to embedding space. // Besides, there are many other graph embedding algorithms for dimensionality reduction, including non-linear [33, 27], linear [15, 6], kernlized ==[28]== and tensorized [36] algorithms. All of these algorithms are based on undirected graphs derived from symmetric similarities. // Thus, they cannot preserve asymmetric transitivity.	h-	Asymmetric Transitivity Preserving Graph Embedding	[28]	['[28]  B. Scholkopft and K.-R. Mullert. Fisher discriminant analysis with kernels. Neural networks for signal processing IX, 1:1, 1999. ']
586	48	[251]	Locality Preserving Projections (LPP) [8] is a linearization variant of LE which learns a linear projection from feature space to embedding space. // Besides, there are many other graph embedding algorithms for dimensionality reduction, including non-linear [33, 27], linear [15, 6], kernlized [28] and tensorized ==[36]== algorithms. All of these algorithms are based on undirected graphs derived from symmetric similarities. // Thus, they cannot preserve asymmetric transitivity.	h-	Asymmetric Transitivity Preserving Graph Embedding	[36]	['[36]  J. Ye, R. Janardan, and Q. Li. Two-dimensional linear discriminant analysis. In Advances in neural information processing systems, pages 1569–1576, 2004. ']
587	48	[36, 160, 167]	Thus, they cannot preserve asymmetric transitivity. // In the field of natural language processing, the graph of words is often used to learn the representation of words ==[19, 18, 23]==. //  Mikolov et. al. [19] propose to ultilize the context of words to learn representation, which has been proved equivalent to factorizing word-context matrix [18].	b	Asymmetric Transitivity Preserving Graph Embedding	[19, 18, 23]	['[19]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013. ', '[18]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185, 2014. ', '[23]  J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532–1543, 2014. ']
588	48	[36]	In the field of natural language processing, the graph of words is often used to learn the representation of words [19, 18, 23]. //  Mikolov et. al. ==[19]== propose to ultilize the context of words to learn representation, which has been proved equivalent to factorizing word-context matrix [18]. // Pennington et. al. [23] exploit a word-word co-occurrance matrix.	b	Asymmetric Transitivity Preserving Graph Embedding	[19]	['[19]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013. ']
589	48	[160]	In the field of natural language processing, the graph of words is often used to learn the representation of words [19, 18, 23]. //  Mikolov et. al. [19] propose to ultilize the context of words to learn representation, which has been proved equivalent to factorizing word-context matrix ==[18]==. // Pennington et. al. [23] exploit a word-word co-occurrance matrix.	b	Asymmetric Transitivity Preserving Graph Embedding	[18]	['[18]  O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177–2185, 2014. ']
590	48	[178]	Pennington et. al. [23] exploit a word-word co-occurrance matrix. // In network analysis, Hoff et. al. ==[11]== first propose to learn latent space representation of vertexes in graph and the probability of a relation depends on the distance between vertexes in the latent space, and they apply it to link prediction problem [10]. // Handcock et. al. [7] propose to apply the latent space approach to clustering in graph.	b	Asymmetric Transitivity Preserving Graph Embedding	[11]	['[11]  P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis. Journal of the american Statistical association, 97(460):1090–1098, 2002. ']
591	48	[252]	Pennington et. al. [23] exploit a word-word co-occurrance matrix. // In network analysis, Hoff et. al. [11] first propose to learn latent space representation of vertexes in graph and the probability of a relation depends on the distance between vertexes in the latent space, and they apply it to link prediction problem ==[10]==. // Handcock et. al. [7] propose to apply the latent space approach to clustering in graph.	b	Asymmetric Transitivity Preserving Graph Embedding	[10]	['[10]  P. D. Hoff. Multiplicative latent factor models for description and prediction of social networks. Computational and Mathematical Organization Theory, 15(4):261–272, 2009. ']
592	48	[253]	In network analysis, Hoff et. al. [11] first propose to learn latent space representation of vertexes in graph and the probability of a relation depends on the distance between vertexes in the latent space, and they apply it to link prediction problem [10]. // Handcock et. al. ==[7]== propose to apply the latent space approach to clustering in graph. // And Zhu et. al. [37] propose to address the classification problem in graph with graph embedding model.	b	Asymmetric Transitivity Preserving Graph Embedding	[7]	['[7]  M. S. Handcock, A. E. Raftery, and J. M. Tantrum. Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301–354, 2007. ']
593	48	[10]	Handcock et. al. [7] propose to apply the latent space approach to clustering in graph. // And Zhu et. al. ==[37]== propose to address the classification problem in graph with graph embedding model. // While early graph embedding works focus on modeling the observed first-order relationship (i.e. edges in graph) between vertexes, some recent works try to model the directed higher order relationships between vertexes in sparse graphs [24, 2].	b	Asymmetric Transitivity Preserving Graph Embedding	[37]	['[37]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 487–494. ACM, 2007.']
594	48	[6, 26]	And Zhu et. al. [37] propose to address the classification problem in graph with graph embedding model. // While early graph embedding works focus on modeling the observed first-order relationship (i.e. edges in graph) between vertexes, some recent works try to model the directed higher order relationships between vertexes in sparse graphs ==[24, 2]==. // GraRep [2] is related to our work. But, it cannot fully capture transitivity.	b	Asymmetric Transitivity Preserving Graph Embedding	[24, 2]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ', '[2]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891–900. ACM, 2015. ']
595	48	[26]	While early graph embedding works focus on modeling the observed first-order relationship (i.e. edges in graph) between vertexes, some recent works try to model the directed higher order relationships between vertexes in sparse graphs [24, 2]. // GraRep ==[2]== is related to our work. But, it cannot fully capture transitivity. // Directed Graph.	h-	Asymmetric Transitivity Preserving Graph Embedding	[2]	['[2]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891–900. ACM, 2015. ']
596	48	[254]	So, modeling directed graph is a critical problem for graph analysis. // Holland et. al. ==[12]== propose the p1 distribution model to capture the structural properties in directed graph, including the atrractiveness and expansiveness of vertexes and the reciprocation of edges. // Besides these properties, Wang et. al. [34] take the group information of vertexes into consideration.	b	Asymmetric Transitivity Preserving Graph Embedding	[12]	['[12]  P. W. Holland and S. Leinhardt. An exponential family of probability distributions for directed graphs. Journal of the american Statistical association, 76(373):33–50, 1981. ']
597	48	[255]	Holland et. al. [12] propose the p1 distribution model to capture the structural properties in directed graph, including the atrractiveness and expansiveness of vertexes and the reciprocation of edges. // Besides these properties, Wang et. al. ==[34]== take the group information of vertexes into consideration. // Recently, some works adopt graph embedding [4, 26, 25, 21] to model directed graphs.	b	Asymmetric Transitivity Preserving Graph Embedding	[34]	['[34]  Y. J. Wang and G. Y. Wong. Stochastic blockmodels for directed graphs. Journal of the American Statistical Association, 82(397):8–19, 1987. ']
598	48	[256, 257, 258, 259]	Besides these properties, Wang et. al. [34] take the group information of vertexes into consideration. // Recently, some works adopt graph embedding ==[4, 26, 25, 21]== to model directed graphs. // Chen et. al. [4] learn the embedding vectors in Euclidean space with locality property preserved.	b	Asymmetric Transitivity Preserving Graph Embedding	[4, 26, 25, 21]	['[4]  M. Chen, Q. Yang, and X. Tang. Directed graph embedding. In IJCAI, pages 2707–2712, 2007. ', '[26]  D. C. Perrault-Joncas and M. Meila. Directed graph embedding: an algorithm based on continuous limits of laplacian-type operators. In Advances in Neural Information Processing Systems, pages 990–998, 2011. ', '[25]  D. Perrault-Joncas and M. Meila. Estimating vector fields on manifolds and the embedding of directed graphs. arXiv preprint arXiv:1406.0013, 2014. ', '[21]  S. Mousazadeh and I. Cohen. Embedding and function extension on directed graph. Signal Processing, 111:137–149, 2015. ']
599	48	[256]	Recently, some works adopt graph embedding [4, 26, 25, 21] to model directed graphs. // Chen et. al. ==[4]== learn the embedding vectors in Euclidean space with locality property preserved. // Perrault-Joncas et. al. [26, 25] and Mousazadeh et. al. [21] learn the embedding vectors based on Laplacian type operators and preserve the asymmetry property of edges in a vector field. However, all of these methods cannot preserve asymmetry property in embedding vector space.	h-	Asymmetric Transitivity Preserving Graph Embedding	[4]	['[4]  M. Chen, Q. Yang, and X. Tang. Directed graph embedding. In IJCAI, pages 2707–2712, 2007. ']
600	48	[257, 258]	Chen et. al. [4] learn the embedding vectors in Euclidean space with locality property preserved. // Perrault-Joncas et. al. ==[26, 25]== and Mousazadeh et. al. [21] learn the embedding vectors based on Laplacian type operators and preserve the asymmetry property of edges in a vector field. However, all of these methods cannot preserve asymmetry property in embedding vector space. //  HIGH-ORDER PROXIMITY PRESERVED EMBEDDING	h-	Asymmetric Transitivity Preserving Graph Embedding	[26, 25]	['[26]  D. C. Perrault-Joncas and M. Meila. Directed graph embedding: an algorithm based on continuous limits of laplacian-type operators. In Advances in Neural Information Processing Systems, pages 990–998, 2011. ', '[25]  D. Perrault-Joncas and M. Meila. Estimating vector fields on manifolds and the embedding of directed graphs. arXiv preprint arXiv:1406.0013, 2014. ']
601	48	[259]	Chen et. al. [4] learn the embedding vectors in Euclidean space with locality property preserved. // Perrault-Joncas et. al. [26, 25] and Mousazadeh et. al. ==[21]== learn the embedding vectors based on Laplacian type operators and preserve the asymmetry property of edges in a vector field. However, all of these methods cannot preserve asymmetry property in embedding vector space. //  HIGH-ORDER PROXIMITY PRESERVED EMBEDDING	h-	Asymmetric Transitivity Preserving Graph Embedding	[21]	['[21]  S. Mousazadeh and I. Cohen. Embedding and function extension on directed graph. Signal Processing, 111:137–149, 2015. ']
602	48	[260]	As the calculation of S is the efficiency bottleneck and S is just the intermediate product in our problem, we propose a novel algorithm to avoid the calculation of S and learn the embedding vectors directly. // As many proximity measurements have the general formulation in Equation (2), we transform the original SVD problem into a generalized SVD problem ==[22]== for proximity measurements with the general formulation. // According to [22], it is easy to derive the following theorem.	ho	Asymmetric Transitivity Preserving Graph Embedding	[22]	['[22]  C. C. Paige and M. A. Saunders. Towards a generalized singular value decomposition. SIAM Journal on Numerical Analysis, 18(3):398–405, 1981. ']
603	48	[261]	// Synthetic Data (Syn): We generate the synthetic data by the forest fire model ==[17]==. // The model can generate powerlaw graphs.	hoe	Asymmetric Transitivity Preserving Graph Embedding	[17]	['[17]  J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over time: densification laws, shrinking diameters and possible explanations. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 177–187. ACM, 2005. ']
604	48	[262]	We randomly generate ten synthetic datasets. // Cora1 ==[31]==: This is a citation network of academic papers. // The vertexes are academic papers and the directed edges are the citation relationship between papers.	b	Asymmetric Transitivity Preserving Graph Embedding	[31]	['[31]  L. Subelj and M. Bajec. Model of complex networks based ˇ on citation dynamics. In Proceedings of the 22nd international conference on World Wide Web companion, pages 527–530. International World Wide Web Conferences Steering Committee, 2013. ']
605	48	[263]	The vertexes are academic papers and the directed edges are the citation relationship between papers. // Twitter Social Network2 (SN-Twitter) ==[5]==: This dataset is a subnetwork of Twitter. // The vertexes are users of Twitter, and the directed edges are following relationships between users.	b	Asymmetric Transitivity Preserving Graph Embedding	[5]	['[5]  M. De Choudhury, Y.-R. Lin, H. Sundaram, K. S. Candan, L. Xie, A. Kelliher, et al. How does the data sampling strategy impact the discovery of information diffusion in social media? ICWSM, 10:34–41, 2010. ']
606	48	[1]	Baseline Methods. // LINE ==[32]==: This algorithm preserves the first-order and second-order proximity between vertexes, but it only can preserve symmetric second-order proximity when applied to directed graph. // We use vertex vectors as source vectors and context vectors as target vectors.	h-	Asymmetric Transitivity Preserving Graph Embedding	[32]	['[32]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
607	48	[6]	We use LINE1 to represent LINE preserving first-order proximity and LINE2 to represent LINE preserving second-order proximity. // DeepWalk ==[24]==: this algorithm first randomly walks on the graph, and assumes a pair of vertexes similar if they are close in the random path. // Then, the embedding is learned to preserve these pairwise similarities in the embedding.	b	Asymmetric Transitivity Preserving Graph Embedding	[24]	['[24]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
608	48	[50]	Then, the embedding is learned to preserve these pairwise similarities in the embedding. // PPE (Partial Proximity Embedding) ==[30]==: This algorithm first selects a small subset of vertexes as landmarks, and learns the embedding by approximating the proximity between vertexes and landmarks. // Common Neighbors.	b	Asymmetric Transitivity Preserving Graph Embedding	[30]	['[30]  H. H. Song, T. W. Cho, V. Dave, Y. Zhang, and L. Qiu. Scalable proximity estimation and link prediction in online social networks. In Proceedings of the 9th ACM SIGCOMM conference on Internet measurement conference, pages 322–335. ACM, 2009. ']
609	586	[109, 19]	Networks are ubiquitous in our daily lives and many real-life applications focus on mining information from the networks. // A fundamental problem in network mining is how to learn the desirable network representations ==[4,22]==. // To address this problem, network embedding is presented to learn the distributed representations of nodes in the network.	b	PPNE- Property Preserving Network Embedding	[4, 22]	['[4] . Chang, S., Han, W., Tang, J., Qi, G.J., Aggarwal, C.C., Huang, T.S.: Heterogeneous network embedding via deep architectures. In: The ACM SIGKDD International Conference, pp. 119–128 (2015) ', '[22] . Wang, D., Cui, P., Zhu, W.: Structural deep network embedding. In: The ACM SIGKDD International Conference (2016) ']
610	586	[37]	The main idea of network embedding is to ﬁnda dense, continuous, and low-dimensional vector for each node as its distributedrepresentation. //  Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification ==[6]==, link prediction [11] and recommendation [24,27]. // Most related works investigate the topology information for network embedding, such as DeepWalk [16], LINE [19], Node2Vec [6] and SDNE [22].	b	PPNE- Property Preserving Network Embedding	[6]	['[6] . Grover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: The ACM SIGKDD International Conference (2016) ']
611	586	[70]	The main idea of network embedding is to ﬁnda dense, continuous, and low-dimensional vector for each node as its distributedrepresentation. //  Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification [6], link prediction ==[11]== and recommendation [24,27]. // Most related works investigate the topology information for network embedding, such as DeepWalk [16], LINE [19], Node2Vec [6] and SDNE [22].	b	PPNE- Property Preserving Network Embedding	[11]	['[11] . Liben-Nowell, D., Kleinberg, J.: The link-prediction problem for social networks. J. Assoc. Inf. Sci. Technol. 54(7), 1345–1347 (2007) ']
612	586	[157, 264]	The main idea of network embedding is to ﬁnda dense, continuous, and low-dimensional vector for each node as its distributedrepresentation. //  Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification [6], link prediction [11] and recommendation ==[24,27]==. // Most related works investigate the topology information for network embedding, such as DeepWalk [16], LINE [19], Node2Vec [6] and SDNE [22].	b	PPNE- Property Preserving Network Embedding	[24, 27]	['[24] . Yan, S., Xu, D., Zhang, B., Zhang, H.-J., Yang, Q., Lin, S.: Graph embedding and extensions: a general framework for dimensionality reduction. IEEE Trans. Pattern Anal. Mach. Intell. 29(1), 40–51 (2007) ', '[27] . Zhang, H., Li, Z., Chen, Y., Zhang, X., Wang, S.: Exploit latent dirichlet allocation for one-class collaborative filtering. In: CIKM, pp. 1991–1994 (2014)']
613	586	[6]	Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification [6], link prediction [11] and recommendation [24,27]. // Most related works investigate the topology information for network embedding, such as DeepWalk ==[16]==, LINE [19], Node2Vec [6] and SDNE [22]. // The basic assumption of these topology-driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space.	b	PPNE- Property Preserving Network Embedding	[16]	['[16] . Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: online learning of social representations. In: The ACM SIGKDD International Conference, pp. 701–710. ACM (2014) ']
614	586	[1]	Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification [6], link prediction [11] and recommendation [24,27]. // Most related works investigate the topology information for network embedding, such as DeepWalk [16], LINE ==[19]==, Node2Vec [6] and SDNE [22]. // The basic assumption of these topology-driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space.	b	PPNE- Property Preserving Network Embedding	[19]	['[19] . Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: large-scale information network embedding. In: International Conference on World Wide Web, pp. 1067–1077 (2015) ']
615	586	[37]	Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification [6], link prediction [11] and recommendation [24,27]. // Most related works investigate the topology information for network embedding, such as DeepWalk [16], LINE [19], Node2Vec ==[6]== and SDNE [22]. // The basic assumption of these topology-driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space.	b	PPNE- Property Preserving Network Embedding	[6]	['[6] . Grover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: The ACM SIGKDD International Conference (2016) ']
616	586	[19]	Representing nodes into the distributed vectors can form up a potentially powerful basis to generate high-quality node features for many data mining and machine learning tasks, such as node classification [6], link prediction [11] and recommendation [24,27]. // Most related works investigate the topology information for network embedding, such as DeepWalk [16], LINE [19], Node2Vec [6] and SDNE ==[22]==. // The basic assumption of these topology-driven embedding methods is that nodes with similar topology context should be distributed closely in the learned low dimensional representation space.	b	PPNE- Property Preserving Network Embedding	[22]	['[22] . Wang, D., Cui, P., Zhu, W.: Structural deep network embedding. In: The ACM SIGKDD International Conference (2016) ']
617	586	[7]	Since the node properties potentially encode diﬀerent typesof information from the network topology, integrating them into the embeddingprocess is expected to achieve a better performance. // As the first attempt, TADW ==[25]== incorporates the text features of nodes into network embedding process under a framework of matrix factorization. // However, there are two limitations of TADW. Firstly,the very time andmemory consuming matrixfactorization processof TADWmakes it infeasible to scale up to large networks. Secondly, TADW only considersthe texts associated to each node, and it is diﬃcult to apply TADW to handle thenode properties of rich types in general.	h-	PPNE- Property Preserving Network Embedding	[25]	['[25] . Yang, C., Liu, Z., Zhao, D., Sun, M., Chang, E.Y.: Network representation learning with rich text information. In: International Conference on Artificial Intelligence, pp. 2111–2117 (2015) ']
618	586	[4]	Matrix factorization based methods first express the input network with a affinity matrix in which the entries represent the relationships between nodes, and then embed the affinity matrix into a low dimensional space using matrix factorization techniques. // Locally linear embedding ==[17]== seeks a lower-dimensional projection of the input affinity matrix which preserves distances within local neighborhoods. // Spectral Embedding [2] is one method to calculate the non-linear embeddings.	b	PPNE- Property Preserving Network Embedding	[17]	['[17] . Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear embedding. Science 290(5500), 2323–2326 (2000) PPNE: Property Preserving Network Embedding 179 ']
619	586	[265]	It ﬁnds a low dimensional representation of the input data usinga spectral decomposition of the graph Laplacian. // Sparse random projection ==[10]== reduces the dimensionality of data by projecting the original input space using a sparse random matrix. // However, matrix factorization based methods rely on the decomposition of the affinity matrix, which is too expensive to scale efficiently to large real-world networks.	h-	PPNE- Property Preserving Network Embedding	[10]	['[10] . Li, P., Hastie, T.J., Church, K.W.: Very sparse random projections. In: The ACM SIGKDD International Conference, pp. 287–296 (2006) ']
620	586	[6]	Recently neural network based models are introduced to solve the networkembedding problem. // As the first attempt, DeepWalk ==[16]== introduces an word embedding algorithm (Skip-Gram) [14] to learn the representation vectors of nodes. // Tang et al. propose LINE [19], which optimizes a carefully designed objec-tive function that preserves both the local and global structure.	b	PPNE- Property Preserving Network Embedding	[16]	['[16] . Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: online learning of social representations. In: The ACM SIGKDD International Conference, pp. 701–710. ACM (2014) ']
621	586	[36]	Recently neural network based models are introduced to solve the networkembedding problem. // As the first attempt, DeepWalk [16] introduces an word embedding algorithm (Skip-Gram) ==[14]== to learn the representation vectors of nodes. // Tang et al. propose LINE [19], which optimizes a carefully designed objec-tive function that preserves both the local and global structure.	b	PPNE- Property Preserving Network Embedding	[14]	['[14] . Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed representations of words and phrases and their compositionality. In: Advances in Neural Information Processing Systems, pp. 3111–3119 (2013) ']
622	586	[1]	As the ﬁrst attempt, DeepWalk [16] introduces an word embedding algorithm (Skip-Gram) [14] to learn the representation vectors ofnodes. // Tang et al. propose LINE ==[19]==, which optimizes a carefully designed objective function that preserves both the local and global structure. // Wang et al. propose SDNE [22], a deep embedding model to capture the highly non-linear network structure and preserve the global and local structures. SDNE exploits the first-order and second-order proximities to preserve the network structure.	b	PPNE- Property Preserving Network Embedding	[19]	['[19] . Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: large-scale information network embedding. In: International Conference on World Wide Web, pp. 1067–1077 (2015) ']
623	586	[19]	Tang et al. propose LINE [19], which optimizes a carefully designed objective function that preserves both the local and global structure. // Wang et al. propose SDNE ==[22]==, a deep embedding model to capture the highly non-linear network structure and preserve the global and local structures. // SDNE exploits the first-order and second-order proximities to preserve the network structure.	b	PPNE- Property Preserving Network Embedding	[22]	['[22] . Wang, D., Cui, P., Zhu, W.: Structural deep network embedding. In: The ACM SIGKDD International Conference (2016) ']
624	586	[37]	SDNE exploitsthe ﬁrst-order and second-order proximities to preserve the network structure. // Node2Vec ==[6]== learns a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving distances between network neighborhoods of nodes. // TADW [25] incorporates the text features of nodes into network embedding process under a framework of matrix factorization.	b	PPNE- Property Preserving Network Embedding	[6]	['[6] . Grover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: The ACM SIGKDD International Conference (2016) ']
625	586	[7]	Node2Vec [6] learns a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving distances between network neighborhoods of nodes. // TADW ==[25]== incorporates the text features of nodes into network embedding process under a framework of matrix factorization. // Compared to the matrix factorization based methods, neural network based methods are easier to generalize and own strong representation ability.	h+	PPNE- Property Preserving Network Embedding	[25]	['[25] . Yang, C., Liu, Z., Zhao, D., Sun, M., Chang, E.Y.: Network representation learning with rich text information. In: International Conference on Artificial Intelligence, pp. 2111–2117 (2015) ']
626	586	[266, 55, 55, 66]	Diﬀerent fromthe previous typology-only works, our work aims to propose a general propertypreserving network embedding model which integrate the rich types of nodeproperties in the network into the embedding process. // Finally, there is a body of works focus on the problem of node classification ==[8,20,21,26]== or link prediction [11,13]. // However, the objective of our work is totally different from these works. We aim to learn better representation vectors for nodes, while the node classification or link prediction tasks are only utilized to evaluate the quality of the embedding results.	h-	PPNE- Property Preserving Network Embedding	[8, 20, 21, 26]	['[8] . Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks (2016) ', '[20] . Tang, L., Liu, H.: Leveraging social media networks for classification. In: CIKM, pp. 1107–1116 (2009) ', '[21] . Tang, L., Liu, H.: Leveraging social media networks for classification. Data Min. Knowl. Discov. 23(3), 447–478 (2011) ', '[26] . Yang, Z., Cohen, W.W., Salakhutdinov, R.: Revisiting semi-supervised learning with graph embeddings. In: ICML (2016) ']
627	586	[70, 267]	Diﬀerent fromthe previous typology-only works, our work aims to propose a general propertypreserving network embedding model which integrate the rich types of nodeproperties in the network into the embedding process. // Finally, there is a body of works focus on the problem of node classification [8,20,21,26] or link prediction ==[11,13]==. // However, the objective of our work is totally different from these works. We aim to learn better representation vectors for nodes, while the node classification or link prediction tasks are only utilized to evaluate the quality of the embedding results.	h-	PPNE- Property Preserving Network Embedding	[11, 13]	['[11] . Liben-Nowell, D., Kleinberg, J.: The link-prediction problem for social networks. J. Assoc. Inf. Sci. Technol. 54(7), 1345–1347 (2007) ', '[13] . Menon, A.K., Elkan, C.: Link prediction via matrix factorization. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.) ECML PKDD 2011. LNCS, vol. 6912, pp. 437–452. Springer, Heidelberg (2011). doi:10.1007/ 978-3-642-23783-6 28 ']
628	586	[6]	Topology-Derived Objective Function. // Following the idea of DeepWalk ==[16]==, we assume that nodes with similar topology context tend to be similar. // With such an assumption, we aim to maximize the likelihood of the prediction of the center node given a specific contextual node.	ho	PPNE- Property Preserving Network Embedding	[16]	['[16] . Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: online learning of social representations. In: The ACM SIGKDD International Conference, pp. 701–710. ACM (2014) ']
629	586	[268]	In this subsection we present the details of the property-derived objective func-tion. // In natural language processing area, SWE ==[12]== and RC-NET [23] incorporate the semantic knowledges into the word embedding process. // Inspired by the above works, we propose two ways to extract constraints from the property similarity matrix P.	ho	PPNE- Property Preserving Network Embedding	[12]	['[12] . Liu, Q., Jiang, H., Wei, S., Ling, Z.-H., Hu, Y.: Learning semantic word embeddings based on ordinal knowledge constraints. In: ACL, pp. 1501–1511 (2015) ']
630	586	[269]	In this subsection we present the details of the property-derived objective func-tion. // In natural language processing area, SWE [12] and RC-NET ==[23]== incorporate the semantic knowledges into the word embedding process. // Inspired by the above works, we propose two ways to extract constraints from the property similarity matrix P.	ho	PPNE- Property Preserving Network Embedding	[23]	['[23] . Xu, C., Bai, Y., Bian, J., Gao, B., Wang, G., Liu, X., Liu, T.-Y.: Rc-net: a general framework for incorporating knowledge into word representations. In: CIKM, pp. 1219–1228 (2014) ']
631	586	[7]	Based on these constraints, we introduce the property-derivedobjective functions. // Related works incorporate the original property features of single type into the embedding process ==[25]==. // Diﬀerent from such works, in our approach theproperty matrix P∈R|V|×|V|contains the property similarity scores betweeneach pair of nodes, which is calculated by a predeﬁned similarity measurement.	h-	PPNE- Property Preserving Network Embedding	[25]	['[25] . Yang, C., Liu, Z., Zhao, D., Sun, M., Chang, E.Y.: Network representation learning with rich text information. In: International Conference on Artificial Intelligence, pp. 2111–2117 (2015) ']
632	586	[68]	In order to thoroughly evaluate the proposed methods, we conductexperiments on four paper citation networks and one social network with diﬀer-ent scale of nodes. // The four paper citation networks are Citeseer2, Cora (see Footnote 2), PubMed (see Footnote 2) ==[18]== and DBLP3 [9]. // In the paper citation networks, nodes refer to papers and links refer to the citation relationships among papers.	b	PPNE- Property Preserving Network Embedding	[18]	['[18] . Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., Eliassi-Rad, T.: Collective classification in network data. AI Mag. 29(3), 93 (2008) ']
633	586	[184]	In order to thoroughly evaluate the proposed methods, we conductexperiments on four paper citation networks and one social network with diﬀer-ent scale of nodes. // The four paper citation networks are Citeseer2, Cora (see Footnote 2), PubMed (see Footnote 2) [18] and DBLP3 ==[9]==. // In the paper citation networks, nodes refer to papers and links refer to the citation relationships among papers.	b	PPNE- Property Preserving Network Embedding	[9]	['[9] . Leskovec, J., Krevl, A.: SNAP datasets: Stanford large network dataset collection, June 2014. http://snap.stanford.edu/data ']
634	586	[6]	// DeepWalk: DeepWalk ==[16]== is a topology-only network embedding method, which introduces the Skip-Gram algorithm to learn the node representation vectors. // LINE: LINE [19] is a popular topology-only network embedding method, which considers the first-order and second-order proximities information.	b	PPNE- Property Preserving Network Embedding	[16]	['[16] . Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: online learning of social representations. In: The ACM SIGKDD International Conference, pp. 701–710. ACM (2014) ']
635	586	[1]	DeepWalk: DeepWalk [16] is a topology-only network embedding method, which introduces the Skip-Gram algorithm to learn the node representation vectors. // LINE: LINE ==[19]== is a popular topology-only network embedding method, which considers the first-order and second-order proximities information. // TADW: TADW [25] incorporates the text features of each node into the embedding process under a framework of matrix factorization.	b	PPNE- Property Preserving Network Embedding	[19]	['[19] . Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: large-scale information network embedding. In: International Conference on World Wide Web, pp. 1067–1077 (2015) ']
636	586	[7]	LINE: LINE [19] is a popular topology-only network embedding method, which considers the first-order and second-order proximities information. // TADW: TADW ==[25]== incorporates the text features of each node into the embedding process under a framework of matrix factorization. // PPNEineq : PPNEineq is the PPNE model with the inequality constrains.	b	PPNE- Property Preserving Network Embedding	[25]	['[25] . Yang, C., Liu, Z., Zhao, D., Sun, M., Chang, E.Y.: Network representation learning with rich text information. In: International Conference on Artificial Intelligence, pp. 2111–2117 (2015) ']
637	586	[208]	In LINE method,the parameters are set as follows: negative =5andsamples = 10 million. // In Property Features method, we reduce the dimension of node property features to 160 via SVD ==[7]== algorithm. // In TADW method the parameters are set to the same as given in the original paper.	ho	PPNE- Property Preserving Network Embedding	[7]	['[7] . Halko, N., Martinsson, P.G., Tropp, J.A.: Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev. 53(2), 217–288 (2010) ']
638	586	[270]	We utilize the representation vectors generated by various network embeddingmethods to perform multi-class node classiﬁcation task. // The representation vector of each node is treated as its feature vector, and then we use a linear support vector machine model ==[3]== to return the most likely category. // The classification model is implemented using scikit-learn.	ho	PPNE- Property Preserving Network Embedding	[3]	['[3] . Chang, C.-C., Lin, C.-J.: Libsvm: a library for support vector machines. ACM Trans. Intell. Syst. Technol. (TIST) 2(3), 27 (2011) ']
639	586	[271, 267]	Link Prediction. // Given a snapshot of the current network, the link prediction task refers to predicting the edges that will be added in the future time ==[1,13]==. // Link prediction can show the predictability of different network embedding methods.	b	PPNE- Property Preserving Network Embedding	[1, 13]	['[1] . Al Hasan, M., Zaki, M.J.: A survey of link prediction in social networks. In: Aggarwal, C.C. (ed.) Social Network Data Analytics, pp. 243–275. Springer, New York (2011) ', '[13] . Menon, A.K., Elkan, C.: Link prediction via matrix factorization. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.) ECML PKDD 2011. LNCS, vol. 6912, pp. 437–452. Springer, Heidelberg (2011). doi:10.1007/ 978-3-642-23783-6 28 ']
640	586	[272]	Given a node pair in the samples, the cosine similarityscore is calculated according to their representation vectors. // Area Under Curve (AUC) ==[5]== is used to evaluate the consistency between the labels and the similarity scores of the samples. // We also choose Common Neighbors as a baseline method because it has been proved as an effective method [11,15].	hoe	PPNE- Property Preserving Network Embedding	[5]	['[5] . Fawcett, T.: An introduction to roc analysis. Pattern Recogn. Lett. 27(8), 861–874 (2006) ']
641	586	[70, 273]	Area Under Curve(AUC) [5] is used to evaluate the consistency between the labels and the simi-larity scores of the samples. // We also choose Common Neighbors as a baseline method because it has been proved as an effective method ==[11,15]==. // Table 3 shows the experimental results.	h+e	PPNE- Property Preserving Network Embedding	[11, 15]	['[11] . Liben-Nowell, D., Kleinberg, J.: The link-prediction problem for social networks. J. Assoc. Inf. Sci. Technol. 54(7), 1345–1347 (2007) ', '[15] . Newman, M.E.: Clustering and preferential attachment in growing networks. Phys. Rev. E 64(2), 025102 (2001) ']
642	29	[274, 275]	// Attributed networks ==[18, 28]== are ubiquitous in a variety of real-world information systems, such as academic networks and health care systems. // Different from plain networks in which only node-to-node interactions and dependencies are observed, each node in an attributed network is often associated with a rich set of features.	b	Label Informed Attributed Network Embedding	[18, 28]	['[18]  T. M. V. Le and H. W. Lauw. Probabilistic latent document network embedding. ICDM, 2014. ', '[28]  G. Qi, C. Aggarwal, Q. Tian, H. Ji, and T. Huang. Exploring context and content links in social media: A latent space method. TPAMI, 2012. ']
643	29	[276, 277]	For instance, with the popularity of social networking services, people not only make friends with each other to form online communities but also actively share opinions and post comments. // In social science, social influence theories ==[22, 23]== have been studied that attributes of individuals can both reflect and affect their community structures [46]. // In addition, a number of data mining applications, such as sentiment analysis [12] and trust prediction [34], have been benefited by exploiting the correlations between geometrical structure and node attributes.	b	Label Informed Attributed Network Embedding	[22, 23]	['[22]  P. V. Marsden and N. E. Friedkin. Network studies of social influence. SAGE Focus Editions, 1994. ', '[23]  M. McPherson, L. Smith-Lovin, and J. M. Cook. Birds of a feather: Homophily in social networks. Annual Review of Sociology, 2001. ']
644	29	[278]	For instance, with the popularity of social networking services, people not only make friends with each other to form online communities but also actively share opinions and post comments. // In social science, social influence theories [22, 23] have been studied that attributes of individuals can both reflect and affect their community structures ==[46]==. // In addition, a number of data mining applications, such as sentiment analysis [12] and trust prediction [34], have been benefited by exploiting the correlations between geometrical structure and node attributes.	b	Label Informed Attributed Network Embedding	[46]	['[46]  Z. Yang, J. Guo, K. Cai, J. Tang, J. Li, L. Zhang, and Z. Su. Understanding retweeting behaviors in social networks. CIKM, 2010. ']
645	29	[279]	In social science, social influence theories [22, 23] have been studied that attributes of individuals can both reflect and affect their community structures [46]. // In addition, a number of data mining applications, such as sentiment analysis ==[12]== and trust prediction [34], have been benefited by exploiting the correlations between geometrical structure and node attributes. // Network embedding [36, 44], as an efficient computational tool for graph mining, aims at mapping the topological proximities of all nodes in a network into a continuous low-dimensional matrix representation.	b	Label Informed Attributed Network Embedding	[12]	['[12]  X. Hu, L. Tang, J. Tang, and H. Liu. Exploiting social relations for sentiment analysis in microblogging. WSDM, 2013. ']
646	29	[280]	In social science, social influence theories [22, 23] have been studied that attributes of individuals can both reflect and affect their community structures [46]. // In addition, a number of data mining applications, such as sentiment analysis [12] and trust prediction ==[34]==, have been benefited by exploiting the correlations between geometrical structure and node attributes. // Network embedding [36, 44], as an efficient computational tool for graph mining, aims at mapping the topological proximities of all nodes in a network into a continuous low-dimensional matrix representation.	b	Label Informed Attributed Network Embedding	[34]	['[34]  J. Tang, H. Gao, X. Hu, and H. Liu. Exploiting homophily effect for trust prediction. WSDM, 2013. ']
647	29	[1, 157]	In addition, a number of data mining applications, such as sentiment analysis [12] and trust prediction [34], have been benefited by exploiting the correlations between geometrical structure and node attributes. // Network embedding ==[36, 44]==, as an efficient computational tool for graph mining, aims at mapping the topological proximities of all nodes in a network into a continuous low-dimensional matrix representation. // The learned embedding representation paves the way for numerous applications such as node classification [33, 47], link prediction [10, 31], and network visualization [35].	h+	Label Informed Attributed Network Embedding	[36, 44]	['[36]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large scale Information Network Embedding. WWW, 2015. ', '[44]  S. Yan, D. Xu, B. Zhang, HJ. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: A general framework for dimensionality reduction. PAMI, 2007. ']
648	29	[281, 10]	Network embedding [36, 44], as an efficient computational tool for graph mining, aims at mapping the topological proximities of all nodes in a network into a continuous low-dimensional matrix representation. // The learned embedding representation paves the way for numerous applications such as node classification ==[33, 47]==, link prediction [10, 31], and network visualization [35]. // While this has been extensively studied, research on Attributed Network Embedding (ANE) [7] is still in its early stage. In contrast to network embedding that learns from pure networks, ANE targets at leveraging both network proximity and node attribute affinity.	b	Label Informed Attributed Network Embedding	[33, 47]	['[33]  J. Tang, C. Aggarwal, and H. Liu. Node classification in signed social networks. SDM, 2016. ', '[47]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. SIGIR, 2007.']
649	29	[13, 282]	Network embedding [36, 44], as an efficient computational tool for graph mining, aims at mapping the topological proximities of all nodes in a network into a continuous low-dimensional matrix representation. // The learned embedding representation paves the way for numerous applications such as node classification [33, 47], link prediction ==[10, 31]==, and network visualization [35]. // While this has been extensively studied, research on Attributed Network Embedding (ANE) [7] is still in its early stage. In contrast to network embedding that learns from pure networks, ANE targets at leveraging both network proximity and node attribute affinity.	b	Label Informed Attributed Network Embedding	[10, 31]	['[10]  S. Gao, L. Denoyer, and P. Gallinari. Temporal link prediction by integrating content and structure information. CIKM, 2011. ', '[31]  A. P. Singh and G. J. Gordon. Relational Learning via Collective Matrix Factorization. KDD, 2008. ']
650	29	[17]	Network embedding [36, 44], as an efficient computational tool for graph mining, aims at mapping the topological proximities of all nodes in a network into a continuous low-dimensional matrix representation. // The learned embedding representation paves the way for numerous applications such as node classification [33, 47], link prediction [10, 31], and network visualization ==[35]==. // While this has been extensively studied, research on Attributed Network Embedding (ANE) [7] is still in its early stage. In contrast to network embedding that learns from pure networks, ANE targets at leveraging both network proximity and node attribute affinity.	b	Label Informed Attributed Network Embedding	[35]	['[35]  J. Tang, J. Liu, M. Zhang, and Q. Mei. Visualizing large-scale and high-dimensional data. WWW, 2016. ']
651	29	[109]	The learned embedding representation paves the way for numerous applications such as node classification [33, 47], link prediction [10, 31], and network visualization [35]. // While this has been extensively studied, research on Attributed Network Embedding (ANE) ==[7]== is still in its early stage. In contrast to network embedding that learns from pure networks, ANE targets at leveraging both network proximity and node attribute affinity // In contrast to network embedding that learns from pure networks, ANE targets at leveraging both network proximity and node attribute affinity.	b	Label Informed Attributed Network Embedding	[7]	['[7]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. KDD, 2015. ']
652	29	[283, 284]	Papers published in the same research community usually share common topics. They also heavily cite others from the same community. // These facts can be explained by the homophily hypothesis ==[16, 21]==, i.e., individuals with the same label usually have similar social relations and similar node attributes. // Labels are strongly influenced by and inherently correlated to both of the network structure and attribute information.	b	Label Informed Attributed Network Embedding	[16, 21]	['[16]  D. B. Kandel. Homophily, selection, and socialization in adolescent friendships. American Journal of Sociology, 1978. ', '[21]  P. V. Marsden. Homogeneity in confiding relations. Social Networks, 1988. ']
653	29	[285]	There are two main challenges. First, the attributed network and label information could be sparse, incomplete and noisy. // For instance, in social networks, the number of single user’s friends is always exceedingly limited compared with the total number of users ==[1]==. // The proportion of active users who have specified their labels might also be quite small.	ho	Label Informed Attributed Network Embedding	[1]	['[1]  L. A. Adamic and B. A. Huberman. Zipf’s law and the internet. Glottometrics, 2002. ']
654	29	[283]	The second term Tr(U(Y )T U(G)U(G)T U(Y ) ) also measures the correlation between U(G) and U(Y ) . // It is beneficial to the label proximity learning since they are considered to be highly correlated ==[16]==. // Third, the noise in the learned latent representation U(Y ) could also be greatly reduced [24] and most information in the original label space is recoverable [41].	ho	Label Informed Attributed Network Embedding	[16]	['[16]  D. B. Kandel. Homophily, selection, and socialization in adolescent friendships. American Journal of Sociology, 1978. ']
655	29	[286]	It is beneficial to the label proximity learning since they are considered to be highly correlated [16]. // Third, the noise in the learned latent representation U(Y ) could also be greatly reduced ==[24]== and most information in the original label space is recoverable [41]. // Therefore, although label information might be incomplete and noisy, we are still able to fully capture the proximities of nodes in the label space.	h+	Label Informed Attributed Network Embedding	[24]	['[24]  A. I. Mees, P. E. Rapp, and L. S. Jennings. Singular-value decomposition and embedding dimension. Phys. Rev. A, 1987. ']
656	29	[287]	It is beneficial to the label proximity learning since they are considered to be highly correlated [16]. // Third, the noise in the learned latent representation U(Y ) could also be greatly reduced [24] and most information in the original label space is recoverable ==[41]==. // Therefore, although label information might be incomplete and noisy, we are still able to fully capture the proximities of nodes in the label space.	h+	Label Informed Attributed Network Embedding	[41]	['[41]  U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 2007. ']
657	29	[6]	The concatenating original feature space is used for both training and test group. // DeepWalk ==[27]==: It employs truncated random walks on the plain graph and involves language modeling techniques, i.e., word2vec, to analyze the walking tracks. // LINE [36]: It is one of the state-of-the-art embedding algorithms for large-scale networks. It preserves both first and second-order proximities between the nodes.	b	Label Informed Attributed Network Embedding	[27]	['[27]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online Learning of Social Representations. KDD, 2014. ']
658	29	[1]	DeepWalk [27]: It employs truncated random walks on the plain graph and involves language modeling techniques, i.e., word2vec, to analyze the walking tracks. // LINE ==[36]==: It is one of the state-of-the-art embedding algorithms for large-scale networks. // It preserves both first and second-order proximities between the nodes.	h+	Label Informed Attributed Network Embedding	[36]	['[36]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large scale Information Network Embedding. WWW, 2015. ']
659	29	[10]	It preserves both first and second-order proximities between the nodes. // LCMF ==[47]==: It conducts a joint matrix factorization on the linkage and attribute information, and maps them into a shared subspace. // It uses this subspace as the learned representation.	b	Label Informed Attributed Network Embedding	[47]	['[47]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. SIGIR, 2007.']
660	29	[288]	The corresponding top d eigenvectors are collected as the embedding representation. // MultiView ==[17]==: It considers the network, attributes, and labels as three views, and applies co-regularized spectral clustering on them collectively. // LANE on Net and LANE w/o Label: They are two variations of LANE, which have been described in Section 3.6.	b	Label Informed Attributed Network Embedding	[17]	['[17]  A. Kumar, P. Rai, and H. Daume. Co-regularized multi-view spectral clustering. NIPS, 2011. ']
661	29	[6, 1]	Experimental Settings. // Following a commonly adopted way ==[27, 36]==, we validate the effectiveness of different learned representations on node classification task [33, 47]. // This task is to predict which category or categories a new node belongs to based on the model learned from training data.	hoe	Label Informed Attributed Network Embedding	[27, 36]	['[27]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online Learning of Social Representations. KDD, 2014. ', '[36]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large scale Information Network Embedding. WWW, 2015. ']
662	29	[281, 10]	Experimental Settings. // Following a commonly adopted way [27, 36], we validate the effectiveness of different learned representations on node classification task ==[33, 47]==. // This task is to predict which category or categories a new node belongs to based on the model learned from training data.	hoe	Label Informed Attributed Network Embedding	[33, 47]	['[33]  J. Tang, C. Aggarwal, and H. Liu. Node classification in signed social networks. SDM, 2016. ', '[47]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. SIGIR, 2007.']
663	29	[289]	Network embedding enjoys increasing popularity in recent years. // Its pioneer work can be traced back to the graph embedding problem, which was introduced by Filotti et al. ==[9]== as a graph genus determining problem in 1979. // A family of more general graph embedding approaches [26, 38] were developed around the 2000s.	b	Label Informed Attributed Network Embedding	[9]	['[9]  I. S. Filotti, G. L. Miller, and J. Reif. On determining the genus of a graph in O(v O(g) ) steps. STOC, 1979. ']
664	29	[290, 3]	Its pioneer work can be traced back to the graph embedding problem, which was introduced by Filotti et al. [9] as a graph genus determining problem in 1979. // A family of more general graph embedding approaches ==[26, 38]== were developed around the 2000s. // They target at generating lowdimensional manifolds which can model the nonlinear geometry of data, including Isomap [38], Laplacian Eigenmaps [4] and spectral techniques [3, 8].	b	Label Informed Attributed Network Embedding	[26, 38]	['[26]  A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. NIPS, 2002. ', '[38]  J. Tenenbaum, V. Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 2000. ']
665	29	[3]	A family of more general graph embedding approaches [26, 38] were developed around the 2000s. // They target at generating lowdimensional manifolds which can model the nonlinear geometry of data, including Isomap ==[38]==, Laplacian Eigenmaps [4] and spectral techniques [3, 8]. // Up till now, due to the pervasiveness of networked data, a variety of network embedding algorithms [30, 36, 44] have been implemented.	b	Label Informed Attributed Network Embedding	[38]	['[38]  J. Tenenbaum, V. Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 2000. ']
666	29	[168]	A family of more general graph embedding approaches [26, 38] were developed around the 2000s. // They target at generating lowdimensional manifolds which can model the nonlinear geometry of data, including Isomap [38], Laplacian Eigenmaps ==[4]== and spectral techniques [3, 8]. // Up till now, due to the pervasiveness of networked data, a variety of network embedding algorithms [30, 36, 44] have been implemented.	b	Label Informed Attributed Network Embedding	[4]	['[4]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. NIPS, 2001. ']
667	29	[291, 292]	A family of more general graph embedding approaches [26, 38] were developed around the 2000s. // They target at generating lowdimensional manifolds which can model the nonlinear geometry of data, including Isomap [38], Laplacian Eigenmaps [4] and spectral techniques ==[3, 8]==. // Up till now, due to the pervasiveness of networked data, a variety of network embedding algorithms [30, 36, 44] have been implemented.	b	Label Informed Attributed Network Embedding	[3, 8]	['[3]  F. R. Bach and M. I. Jordan. Learning spectral clustering. NIPS, 2004. ', '[8]  F. R. Chung. Spectral graph theory. American Mathematical Soc., 1997. ']
668	29	[293, 1, 157]	They target at generating lowdimensional manifolds which can model the nonlinear geometry of data, including Isomap [38], Laplacian Eigenmaps [4] and spectral techniques [3, 8]. // Up till now, due to the pervasiveness of networked data, a variety of network embedding algorithms ==[30, 36, 44]== have been implemented. // Iwata et al. [13] applied probabilistic latent semantic analysis to embed document networks.	b	Label Informed Attributed Network Embedding	[30, 36, 44]	['[30]  B. Shaw and T. Jebara. Structure preserving embedding. ICML, 2009. ', '[36]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large scale Information Network Embedding. WWW, 2015. ', '[44]  S. Yan, D. Xu, B. Zhang, HJ. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: A general framework for dimensionality reduction. PAMI, 2007. ']
669	29	[294]	Up till now, due to the pervasiveness of networked data, a variety of network embedding algorithms [30, 36, 44] have been implemented. // Iwata et al. ==[13]== applied probabilistic latent semantic analysis to embed document networks. // Tang et al. [37] investigated the advantage of employing temporal information to analyze dynamic multi-mode networks.	b	Label Informed Attributed Network Embedding	[13]	['[13]  T. Iwata, T. Yamada, and N. Ueda. Probabilistic latent semantic visualization: topic model for visualizing documents. KDD, 2008. ']
670	29	[295]	Iwata et al. [13] applied probabilistic latent semantic analysis to embed document networks. // Tang et al. ==[37]== investigated the advantage of employing temporal information to analyze dynamic multi-mode networks. // Shaw and Jebara [30] exploited a semidefinite program to learn a low-dimensional representation that well preserves the global topological structure.	b	Label Informed Attributed Network Embedding	[37]	['[37]  L. Tang, H. Liu, J. Zhang, and Z. Nazeri. Community evolution in dynamic multi-mode networks. KDD, 2008. ']
671	29	[293]	Tang et al. [37] investigated the advantage of employing temporal information to analyze dynamic multi-mode networks. // Shaw and Jebara ==[30]== exploited a semidefinite program to learn a low-dimensional representation that well preserves the global topological structure. // Mei et al. [25] designed a harmonic regularization based embedding framework to tackle the problem of topic modeling with network structure.	b	Label Informed Attributed Network Embedding	[30]	['[30]  B. Shaw and T. Jebara. Structure preserving embedding. ICML, 2009. ']
672	29	[296]	Shaw and Jebara [30] exploited a semidefinite program to learn a low-dimensional representation that well preserves the global topological structure. // Mei et al. ==[25]== designed a harmonic regularization based embedding framework to tackle the problem of topic modeling with network structure. // Ahmed et al. [2] proposed an asynchronous distributed matrix factorization algorithm for large-scale graphs.	b	Label Informed Attributed Network Embedding	[25]	['[25]  Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic modeling with network regularization. WWW, 2008. ']
673	29	[154]	Mei et al. [25] designed a harmonic regularization based embedding framework to tackle the problem of topic modeling with network structure. // Ahmed et al. ==[2]== proposed an asynchronous distributed matrix factorization algorithm for large-scale graphs. // Bourigault et al. [5] projected the observed temporal dynamic into a latent space to better model the information diffusion in networks.	b	Label Informed Attributed Network Embedding	[2]	['[2]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. WWW, 2013. ']
674	29	[297]	Ahmed et al. [2] proposed an asynchronous distributed matrix factorization algorithm for large-scale graphs. // Bourigault et al. ==[5]== projected the observed temporal dynamic into a latent space to better model the information diffusion in networks. // Grover and Leskovec [11] further advanced the random walk based embedding algorithms by adding flexibility in exploiting neighborhoods.	b	Label Informed Attributed Network Embedding	[5]	['[5]  S. Bourigault, C. Lagnier, S. Lamprier, L. Denoyer, and P. Gallinari. Learning social network embeddings for predicting information diffusion. WSDM, 2014. ']
675	29	[37]	Bourigault et al. [5] projected the observed temporal dynamic into a latent space to better model the information diffusion in networks. // Grover and Leskovec ==[11]== further advanced the random walk based embedding algorithms by adding flexibility in exploiting neighborhoods. // To embed heterogeneous networks, Jacob et al. [14] extended the transductive models and deep learning techniques into the problem.	b	Label Informed Attributed Network Embedding	[11]	['[11]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. KDD, 2016. ']
676	29	[298]	Grover and Leskovec [11] further advanced the random walk based embedding algorithms by adding flexibility in exploiting neighborhoods. // To embed heterogeneous networks, Jacob et al. ==[14]== extended the transductive models and deep learning techniques into the problem. // Yang et al. [45] exploited a probabilistic model to conduct network embedding in a semisupervised manner.	b	Label Informed Attributed Network Embedding	[14]	['[14]  Y. Jacob, L. Denoyer, and P. Gallinari. Learning latent representations of nodes for classifying in heterogeneous social networks. WSDM, 2014. ']
677	29	[66]	To embed heterogeneous networks, Jacob et al. [14] extended the transductive models and deep learning techniques into the problem. // Yang et al. ==[45]== exploited a probabilistic model to conduct network embedding in a semisupervised manner. // Most recently, several deep learning based embedding algorithms [27, 42, 43] were proposed to further enhance the performance of learned representations. Attributed network analysis is put forward due to the fact that numerous networks are often associated with abundant content describing attributes of each node.	b	Label Informed Attributed Network Embedding	[45]	['[45]  Z. Yang, W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. ICML, 2016. ']
678	29	[6, 19, 299]	Yang et al. [45] exploited a probabilistic model to conduct network embedding in a semisupervised manner. // Most recently, several deep learning based embedding algorithms ==[27, 42, 43]== were proposed to further enhance the performance of learned representations. // Attributed network analysis is put forward due to the fact that numerous networks are often associated with abundant content describing attributes of each node.	b	Label Informed Attributed Network Embedding	[27, 42, 43]	['[27]  B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: Online Learning of Social Representations. KDD, 2014. ', '[42]  D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. KDD, 2016. ', '[43]  J. Weston, F. Ratle, H. Mobahi, and R. Collobert. Deep learning via semi-supervised embedding. Neural Networks: Tricks of the Trade, 2012. ']
679	29	[276, 278]	Attributed network analysis is put forward due to the fact that numerous networks are often associated with abundant content describing attributes of each node. // In these networks, it has been widely accepted that there exist correlations among geometrical structure and node attributes ==[22, 46]==. // Therefore, algorithms [10, 12, 34] exploiting them together could improve the overall learning performance.	ho	Label Informed Attributed Network Embedding	[22, 46]	['[22]  P. V. Marsden and N. E. Friedkin. Network studies of social influence. SAGE Focus Editions, 1994. ', '[46]  Z. Yang, J. Guo, K. Cai, J. Tang, J. Li, L. Zhang, and Z. Su. Understanding retweeting behaviors in social networks. CIKM, 2010. ']
680	29	[13, 279, 280]	In these networks, it has been widely accepted that there exist correlations among geometrical structure and node attributes [22, 46]. // Therefore, algorithms ==[10, 12, 34]== exploiting them together could improve the overall learning performance. // For instance, Tsur and Rappoport [40] advanced the prediction of spread of ideas by analyzing both social graph topology and content.	h+	Label Informed Attributed Network Embedding	[10, 12, 34]	['[10]  S. Gao, L. Denoyer, and P. Gallinari. Temporal link prediction by integrating content and structure information. CIKM, 2011. ', '[12]  X. Hu, L. Tang, J. Tang, and H. Liu. Exploiting social relations for sentiment analysis in microblogging. WSDM, 2013. ', '[34]  J. Tang, H. Gao, X. Hu, and H. Liu. Exploiting homophily effect for trust prediction. WSDM, 2013. ']
681	29	[300]	Therefore, algorithms [10, 12, 34] exploiting them together could improve the overall learning performance. // For instance, Tsur and Rappoport ==[40]== advanced the prediction of spread of ideas by analyzing both social graph topology and content. // In order to tackle the complex data structures, several efforts [28, 47] have been devoted to jointly embedding the two information sources into a unified space.	h+	Label Informed Attributed Network Embedding	[40]	['[40]  O. Tsur and A. Rappoport. What’s in a hashtag? content based prediction of the spread of ideas in microblogging communities. WSDM, 2012. ']
682	29	[275, 10]	For instance, Tsur and Rappoport [40] advanced the prediction of spread of ideas by analyzing both social graph topology and content. // In order to tackle the complex data structures, several efforts ==[28, 47]== have been devoted to jointly embedding the two information sources into a unified space. // Qi et al. [28] explored an effective algorithm that jointly embeds context and content in social media by constructing a latent space of semantic concepts.	b	Label Informed Attributed Network Embedding	[28, 47]	['[28]  G. Qi, C. Aggarwal, Q. Tian, H. Ji, and T. Huang. Exploring context and content links in social media: A latent space method. TPAMI, 2012. ', '[47]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. SIGIR, 2007.']
683	29	[275]	In order to tackle the complex data structures, several efforts [28, 47] have been devoted to jointly embedding the two information sources into a unified space. // Qi et al. ==[28]== explored an effective algorithm that jointly embeds context and content in social media by constructing a latent space of semantic concepts. // Le and Lauw [18] advocated a holistic framework for handling both document linkage and textual information and finding a unified lowdimensional representation. They achieved this via a joint probabilistic model.	b	Label Informed Attributed Network Embedding	[28]	['[28]  G. Qi, C. Aggarwal, Q. Tian, H. Ji, and T. Huang. Exploring context and content links in social media: A latent space method. TPAMI, 2012. ']
684	29	[274]	Qi et al. [28] explored an effective algorithm that jointly embeds context and content in social media by constructing a latent space of semantic concepts. // Le and Lauw ==[18]== advocated a holistic framework for handling both document linkage and textual information and finding a unified lowdimensional representation. // They achieved this via a joint probabilistic model.	b	Label Informed Attributed Network Embedding	[18]	['[18]  T. M. V. Le and H. W. Lauw. Probabilistic latent document network embedding. ICDM, 2014. ']
685	29	[301]	They achieved this via a joint probabilistic model. // Li et al. ==[19]== exploited the possibility of jointly learning latent factors in high-dimensional content data and link information via a streaming feature selection framework. //  Chang et al. [7] transformed content into another network and exploited a nonlinear multi-layered embedding model to learn the complex interactions between the constructed content network and original network.	b	Label Informed Attributed Network Embedding	[19]	['[19]  J. Li, X. Hu, J. Tang, and H. Liu. Unsupervised streaming feature selection in social media. CIKM, 2015. ']
686	29	[288, 302]	In many applications, data exhibits multiple facets of presentations, and these data are referred as multi-view data. // Multi-view learning ==[17, 20]== aims at learning a statistical model from multiple information sources. // A number of algorithms have been proposed in the literature.	b	Label Informed Attributed Network Embedding	[17, 20]	['[17]  A. Kumar, P. Rai, and H. Daume. Co-regularized multi-view spectral clustering. NIPS, 2011. ', '[20]  Y. Luo, J. Tang, J. Yan, C. Xu, and Z. Chen. Pre-trained multi-view word embedding using two-side neural network. AAAI, 2014. ']
687	29	[303]	A number of algorithms have been proposed in the literature. // Qian et al. ==[29]== investigated a reconstruction error based framework for handling multi-label and multi-view learning, which can explicitly quantify the performance of multiple labels or views merging. // Lou et al. [20] applied a two-side multimodal neural network to embed words based on multiple data sources.	b	Label Informed Attributed Network Embedding	[29]	['[29]  B. Qian, X. Wang, J. Ye, and I. Davidson. A reconstruction error based framework for multi-label and multi-view learning. TKDE, 2015. ']
688	29	[302]	Qian et al. [29] investigated a reconstruction error based framework for handling multi-label and multi-view learning, which can explicitly quantify the performance of multiple labels or views merging. // Lou et al. ==[20]== applied a two-side multimodal neural network to embed words based on multiple data sources. // A more detailed review of multi-view learning can be referred to [32]. The main differences between our work and multi-view learning are the facts that an attributed network can be seen as one specially constructed data source, and ANE itself is a challenging problem.	b	Label Informed Attributed Network Embedding	[20]	['[20]  Y. Luo, J. Tang, J. Yan, C. Xu, and Z. Chen. Pre-trained multi-view word embedding using two-side neural network. AAAI, 2014. ']
689	29	[304]	Lou et al. [20] applied a two-side multimodal neural network to embed words based on multiple data sources. // A more detailed review of multi-view learning can be referred to ==[32]==. // The main differences between our work and multi-view learning are the facts that an attributed network can be seen as one specially constructed da ta source, and ANE itself is a challenging problem.	b	Label Informed Attributed Network Embedding	[32]	['[32]  S. Sun. A survey of multi-view machine learning. Neural Computing and Applications, 2013. ']
690	42	[233]	A meaningful and discriminative representation for documents can help many text analysis tasks such as document classification, document clustering and information retrieval. // Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis ==[12]==, latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11].	b	Linked Document Embedding for Classification	[12]	['[12]  Thomas K Landauer, Peter W Foltz, and Darrell Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259–284, 1998. ']
691	42	[305]	A meaningful and discriminative representation for documents can help many text analysis tasks such as document classification, document clustering and information retrieval. // Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation ==[5]== and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11].	b	Linked Document Embedding for Classification	[5]	['[5]  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022, 2003. ']
692	42	[36, 35, 64]	A meaningful and discriminative representation for documents can help many text analysis tasks such as document classification, document clustering and information retrieval. // Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding ==[18, 17, 13]==. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11].	b	Linked Document Embedding for Classification	[18, 17, 13]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ', '[17]  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ', '[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
693	42	[36, 35]	Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram ==[18, 17]== and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11]. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps [10].” They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	h+	Linked Document Embedding for Classification	[18, 17]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ', '[17]  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
694	42	[64]	Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM ==[13]== have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11]. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps [10].” They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	h+	Linked Document Embedding for Classification	[13]	['[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
695	42	[36]	Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy ==[18]==, parsing [9], POS tagging [9], and sentiment analysis [11]. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps [10].” They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	b	Linked Document Embedding for Classification	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
696	42	[306]	Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing ==[9]==, POS tagging ==[9]==, and sentiment analysis [11]. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps [10].” They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	b	Linked Document Embedding for Classification	[9]	['[9]  Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537, 2011. ']
697	42	[306]	Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing ==[9]==, POS tagging ==[9]==, and sentiment analysis [11]. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps [10].” They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	b	Linked Document Embedding for Classification	[9]	['[9]  Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537, 2011. ']
698	42	[307]	Many document representation methods are proposed such as bag-of-words, N-gram, latent semantic analysis [12], latent Dirichlet allocation [5] and word/document embedding [18, 17, 13]. // Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis ==[11]==. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps [10].” They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	b	Linked Document Embedding for Classification	[11]	['[11]  Yoon Kim. Convolutional neural networks for sentence classification. EMNLP, 2014. ']
699	42	[156]	Among these algorithms, the recently proposed distributed representations of words and documents such as Skip-gram [18, 17] and PV-DM [13] have demonstrated superior performance in many tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11]. // The assumption behind these document/word embedding approaches is basically the distributional hypothesis that “you shall know a word by the company it keeps ==[10]==.” //  They embed words or documents into a low dimensional space, which can alleviate the curse of dimensionality and data sparsity problems suffered by traditional representations such as bag-of-words and N-gram.	ho	Linked Document Embedding for Classification	[10]	['[10]  John R Firth. A synopsis of linguistic theory 1930–55 (special volume of the philological society), 1957. ']
700	42	[10]	For example, web documents such as blogs and online news often contain hyperlinks to other web documents, and scientific articles commonly cite other articles. // A toy example of linked documents is illustrated in Figure 1 where {d1, d2, . . . , d5} are documents and {w1, w2, . . . , w8} are words in documents. In addition to content information, documents are linked and links suggest the inter-dependence of documents. Hence, the i.i.d. assumption of documents does not hold ==[33]==. // Additional link information of such documents has been shown to be useful in various text mining tasks such as document classification [33, 8], document clustering [14, 29] and feature selection [27]. Therefore, we propose to study the novel problem of linked document embedding following the distributional hypothesis.	ho	Linked Document Embedding for Classification	[33]	['[33]  Shenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong. Combining content and link for classification using matrix factorization. In SIGIR. ACM, 2007. ']
701	42	[10, 109]	Hence, the i.i.d. assumption of documents does not hold [33]. // Additional link information of such documents has been shown to be useful in various text mining tasks such as document classification ==[33, 8]==, document clustering [14, 29] and feature selection [27]. // Therefore, we propose to study the novel problem of linked document embedding following the distributional hypothesis.	b	Linked Document Embedding for Classification	[33, 8]	['[33]  Shenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong. Combining content and link for classification using matrix factorization. In SIGIR. ACM, 2007. ', '[8]  Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang. Heterogeneous network embedding via deep architectures. In Proceedings of KDD. ACM, 2015. ']
702	42	[308, 28]	Hence, the i.i.d. assumption of documents does not hold [33]. // Additional link information of such documents has been shown to be useful in various text mining tasks such as document classification [33, 8], document clustering ==[14, 29]== and feature selection [27]. // Therefore, we propose to study the novel problem of linked document embedding following the distributional hypothesis.	b	Linked Document Embedding for Classification	[14, 29]	['[14]  Bo Long, Zhongfei Mark Zhang, and Philip S Yu. A probabilistic framework for relational clustering. In KDD. ACM, 2007. ', '[29]  Suhang Wang, Jiliang Tang, Fred Morstatter, and Huan Liu. Paired restricted boltzmann machine for linked data. In CIKM. ACM, 2016. ']
703	42	[309]	Hence, the i.i.d. assumption of documents does not hold [33]. // Additional link information of such documents has been shown to be useful in various text mining tasks such as document classification [33, 8], document clustering [14, 29] and feature selection ==[27]==. // Therefore, we propose to study the novel problem of linked document embedding following the distributional hypothesis.	b	Linked Document Embedding for Classification	[27]	['[27]  Jiliang Tang and Huan Liu. Unsupervised feature selection for linked social media data. In KDD, 2012. ']
704	42	[36, 64, 7]	Therefore, we propose to study the novel problem of linked document embedding following the distributional hypothesis. // Most existing document embedding algorithms use unsupervised learning, such as those in ==[18, 13, 32]==. // The representations learned by these algorithms are very general and can be applied to various tasks.	b	Linked Document Embedding for Classification	[18, 13, 32]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ', '[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ', '[32]  Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In IJCAI, 2015. ']
705	42	[307]	However, they may not be optimal for some specialized tasks where label information is available such as y2 for d2 and y5 for d5 in Figure 1(a). // For example, deep learning algorithms such as convolutional neural networks ==[11]==, which use label information, often outperform text embeddings for classification tasks [23]. // Hence, in this paper we study the novel problem of linked document embedding for classification and investigate two specific problems: (1) how to capture link and label information mathematically; and (2) how to exploit them for document embedding.	h+	Linked Document Embedding for Classification	[11]	['[11]  Yoon Kim. Convolutional neural networks for sentence classification. EMNLP, 2014. ']
706	42	[201]	However, they may not be optimal for some specialized tasks where label information is available such as y2 for d2 and y5 for d5 in Figure 1(a). // For example, deep learning algorithms such as convolutional neural networks [11], which use label information, often outperform text embeddings for classification tasks ==[23]==. // Hence, in this paper we study the novel problem of linked document embedding for classification and investigate two specific problems: (1) how to capture link and label information mathematically; and (2) how to exploit them for document embedding.	h-	Linked Document Embedding for Classification	[23]	['[23]  Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 2015. ']
707	42	[201]	Document Representation. // Document representation is an important research area that receives great attention lately and can benefit many machine learning and data mining tasks such as document classification ==[23]==, information retrieval [31, 20] and sentiment analysis [13]. // Many different types of models have been proposed for document representation.	b	Linked Document Embedding for Classification	[23]	['[23]  Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 2015. ']
708	42	[310, 311]	Document Representation. // Document representation is an important research area that receives great attention lately and can benefit many machine learning and data mining tasks such as document classification [23], information retrieval ==[31, 20]== and sentiment analysis [13]. // Many different types of models have been proposed for document representation.	b	Linked Document Embedding for Classification	[31, 20]	['[31]  Xing Wei and W Bruce Croft. Lda-based document models for ad-hoc retrieval. In SIGIR. ACM, 2006. ', '[20]  Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval. arXiv preprint arXiv:1502.06922, 2015. ']
709	42	[64]	Document Representation. // Document representation is an important research area that receives great attention lately and can benefit many machine learning and data mining tasks such as document classification [23], information retrieval [31, 20] and sentiment analysis ==[13]==. // Many different types of models have been proposed for document representation.	b	Linked Document Embedding for Classification	[13]	['[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
710	42	[312]	Many different types of models have been proposed for document representation. // Bog-of-words ==[21]== is one of the most widely used one. // It is simple to implement, but not scalable since as the number of documents increases, the vocabulary size can become huge. At the same time, it suffers from data sparsity and curse of dimensionality problems and the semantic relatedness between different words is omitted.	h-	Linked Document Embedding for Classification	[21]	['[21]  Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5):513–523, 1988. ']
711	42	[233]	At the same time, it suffers from data sparsity and curse of dimensionality problems and the semantic relatedness between different words is omitted. // To mitigate the high dimensionality and data sparsity problems of BOW, Latent Semantic Analysis ==[12]== uses dimensionality reduction technique, i.e., SVD, to project the document-word matrix to a low dimension space. // Latent Dirichlet Allocation [5] is another low dimensional document representation algorithm. It is a generative model that assumes that each document has topic distribution and each word in the document is drawn from a topic with probability.	b	Linked Document Embedding for Classification	[12]	['[12]  Thomas K Landauer, Peter W Foltz, and Darrell Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259–284, 1998. ']
712	42	[305]	To mitigate the high dimensionality and data sparsity problems of BOW, Latent Semantic Analysis [12] uses dimensionality reduction technique, i.e., SVD, to project the document-word matrix to a low dimension space. // Latent Dirichlet Allocation ==[5]== is another low dimensional document representation algorithm. // It is a generative model that assumes that each document has topic distribution and each word in the document is drawn from a topic with probability.	b	Linked Document Embedding for Classification	[5]	['[5]  David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022, 2003. ']
713	42	[36, 35]	It is a generative model that assumes that each document has topic distribution and each word in the document is drawn from a topic with probability. // Recently, Mikolov et al. proposed the distributed representations of words, Skip-gram and CBOW, which learn the embeddings of words by utilizing word cooccurrence in the local context ==[18, 17]==. // It has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis [11].	h+	Linked Document Embedding for Classification	[18, 17]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ', '[17]  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
714	42	[36]	Recently, Mikolov et al. proposed the distributed representations of words, Skip-gram and CBOW, which learn the embeddings of words by utilizing word cooccurrence in the local context [18, 17]. // It has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy ==[18]==, parsing [9], POS tagging [9], and sentiment analysis [11]. // It is also scalable and can handle millions of documents. Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document.	h+	Linked Document Embedding for Classification	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
715	42	[306]	Recently, Mikolov et al. proposed the distributed representations of words, Skip-gram and CBOW, which learn the embeddings of words by utilizing word cooccurrence in the local context [18, 17]. // It has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy [18], parsing ==[9]==, POS tagging ==[9]==, and sentiment analysis [11]. // It is also scalable and can handle millions of documents. Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document.	h+	Linked Document Embedding for Classification	[9]	['[9]  Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537, 2011. ']
716	42	[306]	Recently, Mikolov et al. proposed the distributed representations of words, Skip-gram and CBOW, which learn the embeddings of words by utilizing word cooccurrence in the local context [18, 17]. // It has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy [18], parsing ==[9]==, POS tagging ==[9]==, and sentiment analysis [11]. // It is also scalable and can handle millions of documents. Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document.	h+	Linked Document Embedding for Classification	[9]	['[9]  Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537, 2011. ']
717	42	[307]	Recently, Mikolov et al. proposed the distributed representations of words, Skip-gram and CBOW, which learn the embeddings of words by utilizing word cooccurrence in the local context [18, 17]. // It has been proven to be powerful to capture the semantic and syntactic meanings of words and can benefit many natural language processing tasks such as word analogy [18], parsing [9], POS tagging [9], and sentiment analysis ==[11]==. // It is also scalable and can handle millions of documents. Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document.	h+	Linked Document Embedding for Classification	[11]	['[11]  Yoon Kim. Convolutional neural networks for sentence classification. EMNLP, 2014. ']
718	42	[64]	Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document. // Document embedding has also been proven to be powerful in many tasks such as sentiment analysis ==[13]==, machine translation [28] and information retrieve [20]. // Recently, predictive text embedding algorithm (PTE) is proposed in [23], which also utilizes label information to learn predictive text embeddings.	b	Linked Document Embedding for Classification	[13]	['[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
719	42	[313]	Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document. // Document embedding has also been proven to be powerful in many tasks such as sentiment analysis [13], machine translation ==[28]== and information retrieve [20]. // Recently, predictive text embedding algorithm (PTE) is proposed in [23], which also utilizes label information to learn predictive text embeddings.	b	Linked Document Embedding for Classification	[28]	['[28]  Mihaela Vela and Liling Tan. Predicting machine translation adequacy with document embeddings. EMNLP, page 402, 2015. ']
720	42	[311]	Based on the same distributed representation idea, [13] extended the word embedding model to document embedding (PV-DM, PV-DBOW) by finding document representations that are good at predicting words in the document. // Document embedding has also been proven to be powerful in many tasks such as sentiment analysis [13], machine translation [28] and information retrieve ==[20]==. // Recently, predictive text embedding algorithm (PTE) is proposed in [23], which also utilizes label information to learn predictive text embeddings.	b	Linked Document Embedding for Classification	[20]	['[20]  Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval. arXiv preprint arXiv:1502.06922, 2015. ']
721	42	[201]	Document embedding has also been proven to be powerful in many tasks such as sentiment analysis [13], machine translation [28] and information retrieve [20]. // Recently, predictive text embedding algorithm (PTE) is proposed in ==[23]==, which also utilizes label information to learn predictive text embeddings. // The proposed framework LDE is inherently different from PTE: (1) LDE is developed for linked documents while PTE still assumes documents to be i.i.d.; (2) LDE captures label information via modeling document-label information while PTE uses label information via word-label information; (3) in addition to label information, LDE also models link information among documents to learn document embeddings; and (4) the proposed formulations and optimization problems of LDE are also different from those of PTE.	b	Linked Document Embedding for Classification	[23]	['[23]  Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 2015. ']
722	42	[314, 309]	Documents in many real-world applications are inherently linked. For example, web pages are linked by hyperlinks and scientific papers are linked by citations. // Link information has been proven to be very effective for machine learning and data mining such as feature selection ==[26, 27]==, recommender systems [15, 16], and document classification/clustering [19, 3]. // The proposed framework LDE is inherently different from PTE: (1) LDE is developed for linked documents while PTE still assumes documents to be i.i.d.; (2) LDE captures label information via modeling document-label information while PTE uses label information via word-label information; (3) in addition to label information, LDE also models link information among documents to learn document embeddings; and (4) the proposed formulations and optimization problems of LDE are also different from those of PTE.	h+	Linked Document Embedding for Classification	[26, 27]	['[26]  Jiliang Tang and Huan Liu. Feature selection with linked data in social media. In SDM. SIAM, 2012. ', '[27]  Jiliang Tang and Huan Liu. Unsupervised feature selection for linked social media data. In KDD, 2012. ']
723	42	[315, 316]	Documents in many real-world applications are inherently linked. For example, web pages are linked by hyperlinks and scientific papers are linked by citations. // Link information has been proven to be very effective for machine learning and data mining such as feature selection [26, 27], recommender systems ==[15, 16]==, and document classification/clustering [19, 3]. // The proposed framework LDE is inherently different from PTE: (1) LDE is developed for linked documents while PTE still assumes documents to be i.i.d.; (2) LDE captures label information via modeling document-label information while PTE uses label information via word-label information; (3) in addition to label information, LDE also models link information among documents to learn document embeddings; and (4) the proposed formulations and optimization problems of LDE are also different from those of PTE.	h+	Linked Document Embedding for Classification	[15, 16]	['[15]  Hao Ma, Michael R Lyu, and Irwin King. Learning to recommend with trust and distrust relationships. In RecSys, pages 189–196. ACM, 2009. ', '[16]  Paolo Massa and Paolo Avesani. Trust-aware recommender systems. In RecSys. ACM, 2007. ']
724	42	[317, 318]	Documents in many real-world applications are inherently linked. For example, web pages are linked by hyperlinks and scientific papers are linked by citations. // Link information has been proven to be very effective for machine learning and data mining such as feature selection [26, 27], recommender systems [15, 16], and document classification/clustering ==[19, 3]==. // The proposed framework LDE is inherently different from PTE: (1) LDE is developed for linked documents while PTE still assumes documents to be i.i.d.; (2) LDE captures label information via modeling document-label information while PTE uses label information via word-label information; (3) in addition to label information, LDE also models link information among documents to learn document embeddings; and (4) the proposed formulations and optimization problems of LDE are also different from those of PTE.	h+	Linked Document Embedding for Classification	[19, 3]	['[19]  Jennifer Neville, Micah Adler, and David Jensen. Clustering relational data using attribute and link information. In Proceedings of the text mining and link analysis workshop, 18th international joint conference on artificial intelligence, pages 9–15, 2003. ', '[3]  Ralitsa Angelova and Stefan Siersdorfer. A neighborhood-based approach for clustering of linked document collections. In CIKM. ACM, 2006. ']
725	42	[319, 320, 7]	Link information has been proven to be very effective for machine learning and data mining such as feature selection [26, 27], recommender systems [15, 16], and document classification/clustering [19, 3]. // Based on the idea that two linked documents are likely to share similar topics, several works have been proposed to utilize link information for better document representations ==[7, 35, 32]==. // For example, RTM [7] extends LDA by considering link information for topic modeling; PMTLM [35] combines topic modeling with a variant of mixed-membership block model to model linked documents and TADW [32] learns linked document representations based on matrix factorization.	b	Linked Document Embedding for Classification	[7, 35, 32]	['[7]  Jonathan Chang and David M Blei. Relational topic models for document networks. In AISTAS, 2009. ', '[35]  Yaojia Zhu, Xiaoran Yan, Lise Getoor, and Cristopher Moore. Scalable text and link analysis with mixed-topic link models. In KDD, 2013', '[32]  Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In IJCAI, 2015. ']
726	42	[319]	Based on the idea that two linked documents are likely to share similar topics, several works have been proposed to utilize link information for better document representations [7, 35, 32]. // For example, RTM ==[7]== extends LDA by considering link information for topic modeling; PMTLM [35] combines topic modeling with a variant of mixed-membership block model to model linked documents and TADW [32] learns linked document representations based on matrix factorization. // However, the majority of the aforementioned works do not utilize label information; meanwhile most of them do not learn distributed document representations based on the distributional hypothesis; while LDE employs distributional hypothesis idea for document embedding by combining link and label information with content simultaneously.	h-	Linked Document Embedding for Classification	[7]	['[7]  Jonathan Chang and David M Blei. Relational topic models for document networks. In AISTAS, 2009. ']
727	42	[320]	Based on the idea that two linked documents are likely to share similar topics, several works have been proposed to utilize link information for better document representations [7, 35, 32]. // For example, RTM [7] extends LDA by considering link information for topic modeling; PMTLM ==[35]== combines topic modeling with a variant of mixed-membership block model to model linked documents and TADW [32] learns linked document representations based on matrix factorization. // However, the majority of the aforementioned works do not utilize label information; meanwhile most of them do not learn distributed document representations based on the distributional hypothesis; while LDE employs distributional hypothesis idea for document embedding by combining link and label information with content simultaneously.	h-	Linked Document Embedding for Classification	[35]	['[35]  Yaojia Zhu, Xiaoran Yan, Lise Getoor, and Cristopher Moore. Scalable text and link analysis with mixed-topic link models. In KDD, 2013']
728	42	[7]	Based on the idea that two linked documents are likely to share similar topics, several works have been proposed to utilize link information for better document representations [7, 35, 32]. // For example, RTM [7] extends LDA by considering link information for topic modeling; PMTLM [35] combines topic modeling with a variant of mixed-membership block model to model linked documents and TADW ==[32]== learns linked document representations based on matrix factorization. // However, the majority of the aforementioned works do not utilize label information; meanwhile most of them do not learn distributed document representations based on the distributional hypothesis; while LDE employs distributional hypothesis idea for document embedding by combining link and label information with content simultaneously.	h-	Linked Document Embedding for Classification	[32]	['[32]  Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In IJCAI, 2015. ']
729	42	[321]	Graph-based Classification. // Graph-based classification is to utilize the link information to design classifier for classification. Various graphbased classification algorithms are proposed. Label propagation ==[34]== is a classical graph-based methods, which performs classification by propagating label information from labeled data to unlabeled data through the graph. // However, label propagation doesn’t utilize the features of documents.	h-	Linked Document Embedding for Classification	[34]	['[34]  Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical report, Citeseer, 2002. ']
730	42	[322]	However, label propagation doesn’t utilize the features of documents. // GC ==[4]== is a more advanced graph-based classification method, which takes into account both link structure and documents’ content and can be combined with SVM classifiers. // Graffiti [2] is proposed to perform random walk on heterogeneous networks so as to capture the mutual influence of connected nodes for classification.	b	Linked Document Embedding for Classification	[4]	['[4]  Ralitsa Angelova and Gerhard Weikum. Graph-based text classification: learn from your neighbors. In SIGIR, pages 485–492. ACM, 2006. ']
731	42	[323]	GC [4] is a more advanced graph-based classification method, which takes into account both link structure and documents’ content and can be combined with SVM classifiers. // Graffiti ==[2]== is proposed to perform random walk on heterogeneous networks so as to capture the mutual influence of connected nodes for classification. // Abernethy et al. [1] incorporates the graph information into SVM classifier for web spam detection.	b	Linked Document Embedding for Classification	[2]	['[2]  Ralitsa Angelova, Gjergji Kasneci, and Gerhard Weikum. Graffiti: graph-based classification in heterogeneous networks. World Wide Web, 15(2):139–170, 2012. ']
732	42	[324]	Graffiti [2] is proposed to perform random walk on heterogeneous networks so as to capture the mutual influence of connected nodes for classification. // Abernethy et al. ==[1]== incorporates the graph information into SVM classifier for web spam detection. // The proposed method LDE is inherently different form the existing graph-based classification.	b	Linked Document Embedding for Classification	[1]	['[1]  Jacob Abernethy, Olivier Chapelle, and Carlos Castillo. Graph regularization methods for web spam detection. Machine Learning, 81(2):207–225, 2010. ']
733	42	[36]	The proposed method LDE is inherently different form the existing graph-based classification. // First, LDE learns both word embedding and document embedding, which can be used for other tasks, such as word analogy ==[18]== and visualization [23]; while existing graph-based classification methods don’t learn word/document representation. // Second, LDE utilizes the distributional hypothesis idea and considers word-word-document relations, while existing graph-based classification methods usually use BOW without considering the word-word relationship.	b	Linked Document Embedding for Classification	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
734	42	[201]	The proposed method LDE is inherently different form the existing graph-based classification. // First, LDE learns both word embedding and document embedding, which can be used for other tasks, such as word analogy [18] and visualization ==[23]==; while existing graph-based classification methods don’t learn word/document representation. // Second, LDE utilizes the distributional hypothesis idea and considers word-word-document relations, while existing graph-based classification methods usually use BOW without considering the word-word relationship.	b	Linked Document Embedding for Classification	[23]	['[23]  Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 2015. ']
735	42	[312]	The details of these algorithms are listed as follows. // BOW ==[21]==: the classical “bag-of-words” represent each document as a M-dimensional vector, where M is the size of the vocabulary and weight of each dimension is calculated by the TFIDF scheme. // RTM [7]: relational topic model is an extension of topic modeling that models document content and links between documents	b	Linked Document Embedding for Classification	[21]	['[21]  Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5):513–523, 1988. ']
736	42	[319]	BOW [21]: the classical “bag-of-words” represent each document as a M-dimensional vector, where M is the size of the vocabulary and weight of each dimension is calculated by the TFIDF scheme. // RTM ==[7]==: relational topic model is an extension of topic modeling that models document content and links between documents. // Skip-gram [18]: one of the state-of-the-art word embedding model and its training objective is to find word representations that are useful for predicting the surrounding words of a selected word in a sentence. After obtaining word embeddings by Skip-gram, we use Eq.(21) to get document representations.	b	Linked Document Embedding for Classification	[7]	['[7]  Jonathan Chang and David M Blei. Relational topic models for document networks. In AISTAS, 2009. ']
737	42	[36]	RTM [7]: relational topic model is an extension of topic modeling that models document content and links between documents. // Skip-gram ==[18]==: one of the state-of-the-art word embedding model and its training objective is to find word representations that are useful for predicting the surrounding words of a selected word in a sentence. After obtaining word embeddings by Skip-gram, we use Eq.(21) to get document representations. // CBOW [18]: another state-of-the-art word embedding model. Unlike Skip-gram, the training objective of CBOW is to find word representations that are useful for predicting the center word by its neighbors.	h+	Linked Document Embedding for Classification	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
738	42	[36]	Skip-gram [18]: one of the state-of-the-art word embedding model and its training objective is to find word representations that are useful for predicting the surrounding words of a selected word in a sentence. After obtaining word embeddings by Skip-gram, we use Eq.(21) to get document representations. // CBOW ==[18]==: another state-of-the-art word embedding model. Unlike Skip-gram, the training objective of CBOW is to find word representations that are useful for predicting the center word by its neighbors. // PV-DM [13]: the distributed memory version of paragraph vector which considers the order of the words. It aims at learning document embeddings that are good at predicting the next given context.	h+	Linked Document Embedding for Classification	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
739	42	[64]	CBOW [18]: another state-of-the-art word embedding model. Unlike Skip-gram, the training objective of CBOW is to find word representations that are useful for predicting the center word by its neighbors. // PV-DM ==[13]==: the distributed memory version of paragraph vector which considers the order of the words. It aims at learning document embeddings that are good at predicting the next given context. // PV-DBOW [13]: the distributed bag-of-words version of paragraph vector model proposed in [13].	b	Linked Document Embedding for Classification	[13]	['[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
740	42	[64]	PV-DM [13]: the distributed memory version of paragraph vector which considers the order of the words. It aims at learning document embeddings that are good at predicting the next given context. // PV-DBOW ==[13]==: the distributed bag-of-words version of paragraph vector model proposed in ==[13]==. // Unlike PV-DM, the word order is ignored in PV-DBOW. It aims to learn document representations that are good at predicting words in the document.	b	Linked Document Embedding for Classification	[13]	['[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
741	42	[64]	PV-DM [13]: the distributed memory version of paragraph vector which considers the order of the words. It aims at learning document embeddings that are good at predicting the next given context. // PV-DBOW ==[13]==: the distributed bag-of-words version of paragraph vector model proposed in ==[13]==. // Unlike PV-DM, the word order is ignored in PV-DBOW. It aims to learn document representations that are good at predicting words in the document.	b	Linked Document Embedding for Classification	[13]	['[13]  Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014. ']
742	42	[321]	Unlike PV-DM, the word order is ignored in PV-DBOW. It aims to learn document representations that are good at predicting words in the document. // LP ==[34]==: a traditional semi-supervised algorithm based on label propagation, which performs classification by propagating label information from labeled data to unlabeled data through the graph. // LP denotes a traditional method that utilizes both network information and label information for classification.	b	Linked Document Embedding for Classification	[34]	['[34]  Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical report, Citeseer, 2002. ']
743	42	[322]	LP [34]: a traditional semi-supervised algorithm based on label propagation, which performs classification by propagating label information from labeled data to unlabeled data through the graph. // GC ==[4]== a graph-based classification method which utilizes both document contents, link and label information into a probabilistic framework for classification. // CNN [11]: convolution neural network for classification. It uses word embeddings as input to train convolution neural network with label information3.	b	Linked Document Embedding for Classification	[4]	['[4]  Ralitsa Angelova and Gerhard Weikum. Graph-based text classification: learn from your neighbors. In SIGIR, pages 485–492. ACM, 2006. ']
744	42	[307]	GC [4] a graph-based classification method which utilizes both document contents, link and label information into a probabilistic framework for classification. // CNN ==[11]==: convolution neural network for classification. It uses word embeddings as input to train convolution neural network with label information3. // TADW [32]: text-associated DeepWalk is a matrix factorization based method that utilizes both link and document data4.	b	Linked Document Embedding for Classification	[11]	['[11]  Yoon Kim. Convolutional neural networks for sentence classification. EMNLP, 2014. ']
745	42	[7]	CNN [11]: convolution neural network for classification. It uses word embeddings as input to train convolution neural network with label information3. // TADW ==[32]==: text-associated DeepWalk is a matrix factorization based method that utilizes both link and document data4. // PTE [23]: predictive text embedding which considers label information to learn word embedding but cannot handle link information among documents.	b	Linked Document Embedding for Classification	[32]	['[32]  Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In IJCAI, 2015. ']
746	42	[201]	TADW [32]: text-associated DeepWalk is a matrix factorization based method that utilizes both link and document data4. // PTE ==[23]==: predictive text embedding which considers label information to learn word embedding but cannot handle link information among documents. // LDE-Word: the proposed framework trains both wordembedding and document embedding.	h-	Linked Document Embedding for Classification	[23]	['[23]  Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 2015. ']
747	42	[270]	Labels of testing data are held out and no algorithm can use labels of testing data during the representation learning phase. // During the classification phase, we use libsvm5 ==[6]== to train a SVM classifier using the learned document embeddings and the training data. The trained SVM classifier is then assessed on the testing data. // There are some parameters to set for the baseline algorithms.	ho	Linked Document Embedding for Classification	[6]	['[6]  Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM TIST, 2(3):27, 2011. ']
748	42	[36]	For a fair comparison, for Skip-gram, CBOW, PVDM, PV-DBOW, CNN, RTM and LDE, we set the embedding dimension to be 100. // For Skip-gram, CBOW, PV-DM, PV-DBOW and LDE, following the parameter setting suggestions in ==[18]==, we set the window size to be 7 and the number of negative samples also to be 7. // We follow the setting in [23] for PTE and we use the default setting in the code of TADW.	hoe	Linked Document Embedding for Classification	[18]	['[18]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
749	42	[201]	For Skip-gram, CBOW, PV-DM, PV-DBOW and LDE, following the parameter setting suggestions in [18], we set the window size to be 7 and the number of negative samples also to be 7. // We follow the setting in ==[23]== for PTE and we use the default setting in the code of TADW. // For the proposed model, we choose γ to be 0.0001.	hoe	Linked Document Embedding for Classification	[23]	['[23]  Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In KDD. ACM, 2015. ']
750	42	[307]	For the proposed model, we choose γ to be 0.0001. // As of CNN, we use the default architecture in ==[11]==. // For both datasets, we randomly select 60% as training data and the remaining 40% as testing data.	hoe	Linked Document Embedding for Classification	[11]	['[11]  Yoon Kim. Convolutional neural networks for sentence classification. EMNLP, 2014. ']
751	28	[325, 326, 109]	// Representation learning, which aims at learning low dimensional semantic representations of high-dimensional data, has proven to facilitate many machine learning and data mining tasks such as classification ==[13, 10, 2]==, clustering [17, 31] and information retrieval [7]. // In terms of the label availability, representation learning methods can be broadly classified into supervised [13, 10] and unsupervised methods [17, 29].	b	Paired Restricted Boltzmann Machine for Linked Data	[13, 10, 2]	['[13]  Jon D Mcauliffe and David M Blei. Supervised topic models. In NIPS, pages 121–128, 2008. ', '[10]  Ryan Kiros, Axel J Soto, Evangelos Milios, and Vlado Keselj. Representation learning for sparse, high dimensional multi-label classification, 2012. ', '[2]  Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang. Heterogeneous network embedding via deep architectures. In Proceedings of KDD. ACM, 2015. ']
752	28	[327, 328]	// Representation learning, which aims at learning low dimensional semantic representations of high-dimensional data, has proven to facilitate many machine learning and data mining tasks such as classification [13, 10, 2], clustering ==[17, 31]== and information retrieval [7]. // In terms of the label availability, representation learning methods can be broadly classified into supervised [13, 10] and unsupervised methods [17, 29].	b	Paired Restricted Boltzmann Machine for Linked Data	[17, 31]	['[17]  Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009. ', '[31]  Pengtao Xie, Yuntian Deng, and Eric Xing. Diversifying restricted boltzmann machine for document modeling. In Proceedings of KDD, pages 1315–1324. ACM, 2015. ']
753	28	[329]	// Representation learning, which aims at learning low dimensional semantic representations of high-dimensional data, has proven to facilitate many machine learning and data mining tasks such as classification [13, 10, 2], clustering [17, 31] and information retrieval ==[7]==. // In terms of the label availability, representation learning methods can be broadly classified into supervised [13, 10] and unsupervised methods [17, 29].	b	Paired Restricted Boltzmann Machine for Linked Data	[7]	['[7]  Geoffrey E Hinton and Ruslan R Salakhutdinov. Replicated softmax: an undirected topic model. In NIPS, pages 1607–1614, 2009. ']
754	28	[325, 326]	Representation learning, which aims at learning low dimensional semantic representations of high-dimensional data, has proven to facilitate many machine learning and data mining tasks such as classification [13, 10, 2], clustering [17, 31] and information retrieval [7]. // In terms of the label availability, representation learning methods can be broadly classified into supervised ==[13, 10]== and unsupervised methods [17, 29]. // As most data is unlabeled and it is very expensive to label the data, unsupervised representation learning has attracted increasing attention in recent years [17, 31].	b	Paired Restricted Boltzmann Machine for Linked Data	[13, 10]	['[13]  Jon D Mcauliffe and David M Blei. Supervised topic models. In NIPS, pages 121–128, 2008. ', '[10]  Ryan Kiros, Axel J Soto, Evangelos Milios, and Vlado Keselj. Representation learning for sparse, high dimensional multi-label classification, 2012. ']
755	28	[327, 330]	Representation learning, which aims at learning low dimensional semantic representations of high-dimensional data, has proven to facilitate many machine learning and data mining tasks such as classification [13, 10, 2], clustering [17, 31] and information retrieval [7]. // In terms of the label availability, representation learning methods can be broadly classified into supervised [13, 10] and unsupervised methods ==[17, 29]==. // As most data is unlabeled and it is very expensive to label the data, unsupervised representation learning has attracted increasing attention in recent years [17, 31].	b	Paired Restricted Boltzmann Machine for Linked Data	[17, 29]	['[17]  Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009. ', '[29]  Yilin Wang, Suhang Wang, Jiliang Tang, Huan Liu, and Baoxin Li. Unsupervised sentiment analysis for social media images. In IJCAI, 2015. ']
756	28	[327, 328]	In terms of the label availability, representation learning methods can be broadly classified into supervised [13, 10] and unsupervised methods [17, 29]. // As most data is unlabeled and it is very expensive to label the data, unsupervised representation learning has attracted increasing attention in recent years ==[17, 31]==. // Restricted Boltzmann Machine (RBM) is one of the most widely used unsupervised representation learning methods.	b	Paired Restricted Boltzmann Machine for Linked Data	[17, 31]	['[17]  Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009. ', '[31]  Pengtao Xie, Yuntian Deng, and Eric Xing. Diversifying restricted boltzmann machine for document modeling. In Proceedings of KDD, pages 1315–1324. ACM, 2015. ']
757	28	[331, 332]	Restricted Boltzmann Machine (RBM) is one of the most widely used unsupervised representation learning methods. // RBM is very powerful in learning meaningful nonlinear latent features, which has powered many applications such as collaborative filtering ==[18, 5]==, link prediction [12], document representation [7, 31] and social behavior prediction [16]. // In recent years, linked data has become pervasively available in various domains.	h+	Paired Restricted Boltzmann Machine for Linked Data	[18, 5]	['[18]  Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for collaborative filtering. In Proceedings of ICML, pages 791–798. ACM, 2007. ', '[5]  Asela Gunawardana and Christopher Meek. Tied boltzmann machines for cold start recommendations. In Proceedings of RecSys, pages 19–26. ACM, 2008. ']
758	28	[174]	Restricted Boltzmann Machine (RBM) is one of the most widely used unsupervised representation learning methods. // RBM is very powerful in learning meaningful nonlinear latent features, which has powered many applications such as collaborative filtering [18, 5], link prediction ==[12]==, document representation [7, 31] and social behavior prediction [16]. // In recent years, linked data has become pervasively available in various domains.	h+	Paired Restricted Boltzmann Machine for Linked Data	[12]	['[12]  Xiaoyi Li, Nan Du, Hui Li, Kang Li, Jing Gao, and Aidong Zhang. A deep learning approach to link prediction in dynamic networks. In Proceedings of SDM, pages 289–297, 2014. ']
759	28	[329, 328]	Restricted Boltzmann Machine (RBM) is one of the most widely used unsupervised representation learning methods. // RBM is very powerful in learning meaningful nonlinear latent features, which has powered many applications such as collaborative filtering [18, 5], link prediction [12], document representation ==[7, 31]== and social behavior prediction [16]. // In recent years, linked data has become pervasively available in various domains.	h+	Paired Restricted Boltzmann Machine for Linked Data	[7, 31]	['[7]  Geoffrey E Hinton and Ruslan R Salakhutdinov. Replicated softmax: an undirected topic model. In NIPS, pages 1607–1614, 2009. ', '[31]  Pengtao Xie, Yuntian Deng, and Eric Xing. Diversifying restricted boltzmann machine for document modeling. In Proceedings of KDD, pages 1315–1324. ACM, 2015. ']
760	28	[333]	Restricted Boltzmann Machine (RBM) is one of the most widely used unsupervised representation learning methods. // RBM is very powerful in learning meaningful nonlinear latent features, which has powered many applications such as collaborative filtering [18, 5], link prediction [12], document representation [7, 31] and social behavior prediction ==[16]==. // In recent years, linked data has become pervasively available in various domains.	h+	Paired Restricted Boltzmann Machine for Linked Data	[16]	['[16]  NhatHai Phan, Dejing Dou, Brigitte Piniewski, and David Kil. Social restricted boltzmann machine: Human behavior prediction in health social networks. In Proceedings of ASONAM. ACM, 2015. ']
761	28	[314]	In recent years, linked data has become pervasively available in various domains. // For example, social media data is inherently linked via social context ==[22]==, web data is networked via hyperlinks [15], and biological data is embedded in correlation or interaction networks [4]. // Link information can be represented as a network as shown in Figure 1(a), where nodes are data instances.	b	Paired Restricted Boltzmann Machine for Linked Data	[22]	['[22]  Jiliang Tang and Huan Liu. Feature selection with linked data in social media. In SDM, pages 118–128. SIAM, 2012. ']
762	28	[334]	In recent years, linked data has become pervasively available in various domains. // For example, social media data is inherently linked via social context [22], web data is networked via hyperlinks ==[15]==, and biological data is embedded in correlation or interaction networks [4]. // Link information can be represented as a network as shown in Figure 1(a), where nodes are data instances.	b	Paired Restricted Boltzmann Machine for Linked Data	[15]	['[15]  Panagiotis Papadimitriou, Ali Dasdan, and Hector Garcia-Molina. Web graph similarity for anomaly detection. Journal of Internet Services and Applications, 1(1):19–30, 2010. ']
763	28	[335]	In recent years, linked data has become pervasively available in various domains. // For example, social media data is inherently linked via social context [22], web data is networked via hyperlinks [15], and biological data is embedded in correlation or interaction networks ==[4]==. // Link information can be represented as a network as shown in Figure 1(a), where nodes are data instances.	b	Paired Restricted Boltzmann Machine for Linked Data	[4]	['[4]  Alexei V´azqueza Alessandro Flamminia, Amos Maritana, and Alessandro Vespignanib. Modeling of protein interaction networks. ComPlexUs, 1:38–44, 2003. ']
764	28	[314]	In addition, linked data provides link information as demonstrated in Figure 1(c). // Linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection ==[22]==, sentiment analysis [8], topic modeling [1] and document classification [28]. // Therefore, it has potential to advance RBMs for better representation learning.	h+	Paired Restricted Boltzmann Machine for Linked Data	[22]	['[22]  Jiliang Tang and Huan Liu. Feature selection with linked data in social media. In SDM, pages 118–128. SIAM, 2012. ']
765	28	[279]	In addition, linked data provides link information as demonstrated in Figure 1(c). // Linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection [22], sentiment analysis ==[8]==, topic modeling [1] and document classification [28]. // Therefore, it has potential to advance RBMs for better representation learning.	h+	Paired Restricted Boltzmann Machine for Linked Data	[8]	['[8]  Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. Exploiting social relations for sentiment analysis in microblogging. In Proceedings of WSDM, pages 537–546. ACM, 2013. ']
766	28	[319]	In addition, linked data provides link information as demonstrated in Figure 1(c). // Linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection [22], sentiment analysis [8], topic modeling ==[1]== and document classification [28]. // Therefore, it has potential to advance RBMs for better representation learning.	h+	Paired Restricted Boltzmann Machine for Linked Data	[1]	['[1]  Jonathan Chang and David M Blei. Relational topic models for document networks. In International conference on artificial intelligence and statistics, pages 81–88, 2009. ']
767	28	[42]	In addition, linked data provides link information as demonstrated in Figure 1(c). // Linked information is complementary to attribute information and has been proven to enhance a variety of applications such as feature selection [22], sentiment analysis [8], topic modeling [1] and document classification ==[28]==. // Therefore, it has potential to advance RBMs for better representation learning.	h+	Paired Restricted Boltzmann Machine for Linked Data	[28]	['[28]  Suhang Wang, Jiliang Tang, Charu Aggarwal, and Huan Liu. Linked docuent embedding for classification. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management. ACM, 2015. ']
768	28	[336]	// PCA: Principle Component Analysis ==[9]== performs dimensionality reduction by seeking orthogonal projections of the data onto a low-dimensional linear space such that the variance of the projected data is maximized. // It is a popular and effective linear feature learning algorithm.	h+	Paired Restricted Boltzmann Machine for Linked Data	[9]	['[9]  Ian Jolliffe. Principal component analysis. Wiley Online Library, 2002. ']
769	28	[337]	It is a popular and effective linear feature learning algorithm. // DAE: Denoising autoencoder ==[26]== is a variant of autoencoder that is to learn a feature representation that is able to reconstruct the input data. // Specifically, DAE is trained to reconstruct a clean “repaired” input from a corrupted version, which makes it able to extract more robust features.	b	Paired Restricted Boltzmann Machine for Linked Data	[26]	['[26]  Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of ICML, pages 1096–1103. ACM, 2008. ']
770	28	[47]	The encoded feature is used to perform clustering. // SDAE: Stacked denoising autoencoder ==[27]== is a deep network based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. // Compared with the denoising autoencoder, features learned in a purely unsupervised fashion by SDAE are higher-level and could boost the performance of clustering.	h+	Paired Restricted Boltzmann Machine for Linked Data	[27]	['[27]  Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371–3408, 2010. ']
771	28	[338]	SDAE is used as a representative deep learning algorithm for unsupervised representation learning. // RBM: Restricted Boltzmann machine ==[3]== is an undirected graphical model which defines a probability distribution over a vector of observed and a vector of latent variables. // The learned latent variable is used for clustering in our experiment.	b	Paired Restricted Boltzmann Machine for Linked Data	[3]	['[3]  Asja Fischer and Christian Igel. An introduction to restricted boltzmann machines. In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, pages 14–36. Springer, 2012. ']
772	28	[319]	RBM can be seen as pRBM without link information. // RTM: Relational Topic Model ==[1]== is a variant of Latent Dirichlet Allocation (LDA), which takes attribute and link information into consideration for learning topic distributions. // The learned topic distributions of documents are treated as the representations.	b	Paired Restricted Boltzmann Machine for Linked Data	[1]	['[1]  Jonathan Chang and David M Blei. Relational topic models for document networks. In International conference on artificial intelligence and statistics, pages 81–88, 2009. ']
773	28	[7]	The learned topic distributions of documents are treated as the representations. // TADE: Text-associated DeepWalk ==[32]== incorporates both attribute and link information into the matrix factorization framework to learn representations of each nodes. //  It is state-of-the-art representation learning algorithm for network with rich attributes.	b	Paired Restricted Boltzmann Machine for Linked Data	[32]	['[32]  Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation learning with rich text information. In Proceedings of IJCAI, pages 2111–2117, 2015.']
774	28	[173]	It is state-of-the-art representation learning algorithm for network with rich attributes. // LRBM: LRBM ==[11]== combines graph factorization and conditional RBM using four-way tensor for linked data. // It is the closest work to ours and their differences will be detailed in the related work section.	b	Paired Restricted Boltzmann Machine for Linked Data	[11]	['[11]  Kang Li, Jing Gao, Suxin Guo, Nan Du, Xiaoyi Li, and Aidong Zhang. Lrbm: A restricted boltzmann machine based approach for representation learning on linked data. In ICDM. IEEE, 2014. ']
775	28	[339]	As mentioned earlier, for training RBM and pRBM, calculation of the negative gradient is intractable. // Contrastive divergence (CD) ==[6]== is the first practical method for training RBMs. // Instead of running a Gibbs chain until equilibrium to draw samples for negative gradient, contrastive divergence approximates the negative gradient using samples obtained by starting a Gibbs chain at a training vector and running it for a few steps.	b	Paired Restricted Boltzmann Machine for Linked Data	[6]	['[6]  Geoffrey Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771–1800, 2002. ']
776	28	[340]	Instead of running a Gibbs chain until equilibrium to draw samples for negative gradient, contrastive divergence approximates the negative gradient using samples obtained by starting a Gibbs chain at a training vector and running it for a few steps. // Though it has been proven that the resulting gradient estimate is not the gradient of any function ==[20]==, it is extensively used for training energy-based models such as RBMs, and the performance is good. // However, CD learning has a problem that it provides biased estimates of the gradient.	ho	Paired Restricted Boltzmann Machine for Linked Data	[20]	['[20]  Ilya Sutskever and Tijmen Tieleman. On the convergence properties of contrastive divergence. In AISTAS, pages 789–795, 2010. ']
777	28	[341]	However, CD learning has a problem that it provides biased estimates of the gradient. // The Persistent Contrastive Divergence (PCD) addressed this problem ==[25]==. Instead of running a new chain for each parameter, PCD maintains a single persistent chain. The update at time t takes the state of Gibbs chain at time t − 1, perform one round of Gibbs sampling and uses this state in the negative gradient estimate. // PCD has been proven to have good performance in practice.	h+	Paired Restricted Boltzmann Machine for Linked Data	[25]	['[25]  Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of ICML, pages 1064–1071. ACM, 2008. ']
778	28	[332, 331]	Variants of RBMs. // RBM is very powerful for unsupervised representation learning, which has powered many applications such as collaborative filtering ==[5, 18]==, document representation [7, 31] and social behavior prediction [16]. // In [31], RBM is used for representation learning of documents by considering the diversity.	h+	Paired Restricted Boltzmann Machine for Linked Data	[5, 18]	['[5]  Asela Gunawardana and Christopher Meek. Tied boltzmann machines for cold start recommendations. In Proceedings of RecSys, pages 19–26. ACM, 2008. ', '[18]  Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for collaborative filtering. In Proceedings of ICML, pages 791–798. ACM, 2007. ']
779	28	[329, 328]	Variants of RBMs. // RBM is very powerful for unsupervised representation learning, which has powered many applications such as collaborative filtering [5, 18], document representation ==[7, 31]== and social behavior prediction [16]. // In [31], RBM is used for representation learning of documents by considering the diversity.	h+	Paired Restricted Boltzmann Machine for Linked Data	[7, 31]	['[7]  Geoffrey E Hinton and Ruslan R Salakhutdinov. Replicated softmax: an undirected topic model. In NIPS, pages 1607–1614, 2009. ', '[31]  Pengtao Xie, Yuntian Deng, and Eric Xing. Diversifying restricted boltzmann machine for document modeling. In Proceedings of KDD, pages 1315–1324. ACM, 2015. ']
780	28	[333]	Variants of RBMs. // RBM is very powerful for unsupervised representation learning, which has powered many applications such as collaborative filtering [5, 18], document representation [7, 31] and social behavior prediction ==[16]==. // In [31], RBM is used for representation learning of documents by considering the diversity.	h+	Paired Restricted Boltzmann Machine for Linked Data	[16]	['[16]  NhatHai Phan, Dejing Dou, Brigitte Piniewski, and David Kil. Social restricted boltzmann machine: Human behavior prediction in health social networks. In Proceedings of ASONAM. ACM, 2015. ']
781	28	[328]	RBM is very powerful for unsupervised representation learning, which has powered many applications such as collaborative filtering [5, 18], document representation [7, 31] and social behavior prediction [16]. // In ==[31]==, RBM is used for representation learning of documents by considering the diversity. // In [30], RBM and conditional RBM are applied to the task of learning Drug-Target relations on multidimensional networks.	h+	Paired Restricted Boltzmann Machine for Linked Data	[31]	['[31]  Pengtao Xie, Yuntian Deng, and Eric Xing. Diversifying restricted boltzmann machine for document modeling. In Proceedings of KDD, pages 1315–1324. ACM, 2015. ']
782	28	[342]	In [31], RBM is used for representation learning of documents by considering the diversity. // In ==[30]==, RBM and conditional RBM are applied to the task of learning Drug-Target relations on multidimensional networks. // In [17] Deep Boltzmann Machines (DBM) are proposed with multiple hidden layers by stacking RBMs [17]. RBM is also used for modeling networks.	b	Paired Restricted Boltzmann Machine for Linked Data	[30]	['[30]  Yuhao Wang and Jianyang Zeng. Predicting drug-target interactions using restricted boltzmann machines. Bioinformatics, 29(13):i126–i134, 2013. ']
783	28	[327]	In [30], RBM and conditional RBM are applied to the task of learning Drug-Target relations on multidimensional networks. // In ==[17]== Deep Boltzmann Machines (DBM) are proposed with multiple hidden layers by stacking RBMs ==[17]==. // RBM is also used for modeling networks.	b	Paired Restricted Boltzmann Machine for Linked Data	[17]	['[17]  Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009. ']
784	28	[327]	In [30], RBM and conditional RBM are applied to the task of learning Drug-Target relations on multidimensional networks. // In ==[17]== Deep Boltzmann Machines (DBM) are proposed with multiple hidden layers by stacking RBMs ==[17]==. // RBM is also used for modeling networks.	b	Paired Restricted Boltzmann Machine for Linked Data	[17]	['[17]  Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009. ']
785	28	[174]	RBM is also used for modeling networks. // In ==[12]==, an advanced model named ctRBM is proposed to do link prediction on dynamic data. // However, they only use the link structure without considering the attributes of the nodes.	h-	Paired Restricted Boltzmann Machine for Linked Data	[12]	['[12]  Xiaoyi Li, Nan Du, Hui Li, Kang Li, Jing Gao, and Aidong Zhang. A deep learning approach to link prediction in dynamic networks. In Proceedings of SDM, pages 289–297, 2014. ']
786	28	[173]	However, they only use the link structure without considering the attributes of the nodes. // In ==[11]==, LRBM is proposed to learn feature representation from both attributes and the link structure for node classification and link prediction. //  LRBM combines graph factorization and conditional RBM using a four-way tensor for linked data.	b	Paired Restricted Boltzmann Machine for Linked Data	[11]	['[11]  Kang Li, Jing Gao, Suxin Guo, Nan Du, Xiaoyi Li, and Aidong Zhang. Lrbm: A restricted boltzmann machine based approach for representation learning on linked data. In ICDM. IEEE, 2014. ']
787	39	[3, 298, 6, 1, 201]	Hence, network embedding, which is used to represent each vertex of a network with a lowdimensional vector that can preserve the similarities between them, has attracted continuous attention and has been successfully used in various applications, including image processing, knowledge graph, recommendation, etc. // Along with the increasing requirements, a variety of researchers have studied the network embedding construction problem from different aspects ==[10, 3, 7, 9, 8]==. // Classical methods (e.g., IsoMap [10] and Laplacian eigenmaps [1]) usually transform this task into a constrained optimization problem.	b	Incorporate Group Information to Enhance Network Embedding	[10, 3, 7, 9, 8]	['[10]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ', '[3]  Y. Jacob, L. Denoyer, and P. Gallinari. Learning latent representations of nodes for classifying in heterogeneous social networks. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 373–382. ACM, 2014. ', '[7]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ', '[9]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ', '[8]  J. Tang, M. Qu, and Q. Mei. Pte: Predictive text embedding through large-scale heterogeneous text networks. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1165–1174. ACM, 2015. ']
788	39	[3]	Along with the increasing requirements, a variety of researchers have studied the network embedding construction problem from different aspects [10, 3, 7, 9, 8]. // Classical methods (e.g., IsoMap ==[10]== and Laplacian eigenmaps [1]) usually transform this task into a constrained optimization problem. // Hence, the usefulness of these methods may be heavily impacted by the computation consumption of processing hundreds of millions of nodes.	h-	Incorporate Group Information to Enhance Network Embedding	[10]	['[10]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
789	39	[5]	Along with the increasing requirements, a variety of researchers have studied the network embedding construction problem from different aspects [10, 3, 7, 9, 8]. // Classical methods (e.g., IsoMap [10] and Laplacian eigenmaps ==[1]==) usually transform this task into a constrained optimization problem. // Hence, the usefulness of these methods may be heavily impacted by the computation consumption of processing hundreds of millions of nodes.	h-	Incorporate Group Information to Enhance Network Embedding	[1]	['[1]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
790	39	[6]	Hence, the usefulness of these methods may be heavily impacted by the computation consumption of processing hundreds of millions of nodes. // To process largescale networks, DeepWalk ==[7]== uses a shallow neural network architecture. // LINE [9] uses both first-order and second-order proximity to train the embedding, and negative sampling methods to reduce the computational requirement.	b	Incorporate Group Information to Enhance Network Embedding	[7]	['[7]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
791	39	[1]	To process largescale networks, DeepWalk [7] uses a shallow neural network architecture. // LINE ==[9]== uses both first-order and second-order proximity to train the embedding, and negative sampling methods to reduce the computational requirement. // However, most of the previous studies focused on a classical network and took only vertices and edges into consideration.	h-	Incorporate Group Information to Enhance Network Embedding	[9]	['[9]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
792	39	[343]	For example, in social media (e.g., Youtube and Facebook), users can create groups that other users can join. // Previous literatures ==[11]== also show that this kind of network is common in real-world social, collaboration, information, and many other kinds of networks. // More than 200 different kinds of large real-world networks where nodes explicitly state their group memberships were studied in the work done by Yang and Leskovec [11].	b	Incorporate Group Information to Enhance Network Embedding	[11]	['[11]  J. Yang and J. Leskovec. Defining and evaluating network communities based on ground-truth. In 2012 IEEE 12th International Conference on Data Mining, pages 745–754. IEEE, 2012.']
793	39	[343]	Previous literatures [11] also show that this kind of network is common in real-world social, collaboration, information, and many other kinds of networks. // More than 200 different kinds of large real-world networks where nodes explicitly state their group memberships were studied in the work done by Yang and Leskovec ==[11]==. // Although communities or groups in networks can provide valuable information, previous network embedding studies rarely took this information into consideration.	h-	Incorporate Group Information to Enhance Network Embedding	[11]	['[11]  J. Yang and J. Leskovec. Defining and evaluating network communities based on ground-truth. In 2012 IEEE 12th International Conference on Data Mining, pages 745–754. IEEE, 2012.']
794	39	[6]	THE PROPOSED METHODS. // Inspired by the work of DeepWalk ==[7]== and the idea of modelling document [4, 2] in natural language processing, our model contains two main stages, sampling and training. // We will then illustrate the two steps in details.	ho	Incorporate Group Information to Enhance Network Embedding	[7]	['[7]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
795	39	[64, 65]	THE PROPOSED METHODS. // Inspired by the work of DeepWalk [7] and the idea of modelling document ==[4, 2]== in natural language processing, our model contains two main stages, sampling and training. // We will then illustrate the two steps in details.	ho	Incorporate Group Information to Enhance Network Embedding	[4, 2]	['[4]  Q. Le and T. Mikolov. Distributed representations of sentences and documents. In Proceedings of The 31st International Conference on Machine Learning, pages 1188–1196, 2014. ', '[2]  N. Djuric, H. Wu, V. Radosavljevic, M. Grbovic, and N. Bhamidipati. Hierarchical neural language models for joint representation of streaming documents and their content. In Proceedings of the 24th International Conference on World Wide Web, pages 248–255. International World Wide Web Conferences Steering Committee, 2015. ']
796	39	[36]	millions of vertices, so it is impractical to compute these gradients directly. // To address this problem, we adopt the approach of negative sampling proposed in ==[5]== to reduce the computational requirement. // We then give a brief explanation to our proposed model. Since we adopt random walks starting from different vertices in the network, and there should be similar walks for the vertices sharing similar neighbours, thus, those vertices will be placed closely	ho	Incorporate Group Information to Enhance Network Embedding	[5]	['[5]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013. ']
797	39	[343]	We use two large real world datasets for evaluating the proposed methods. // Amazon: This network is provided by Yang ==[11]== and collected by crawling Amazon website. // In this network, if a product i is frequently co-purchased with product j, the graph contains an undirected edge from i to j. Each product category provided by Amazon defines each ground-truth community.	ho	Incorporate Group Information to Enhance Network Embedding	[11]	['[11]  J. Yang and J. Leskovec. Defining and evaluating network communities based on ground-truth. In 2012 IEEE 12th International Conference on Data Mining, pages 745–754. IEEE, 2012.']
798	39	[344]	Each product category provided by Amazon defines each ground-truth community. // Youtube: This network is provided by Alan Mislove ==[6]==. // Youtube is a video-sharing web site that includes a social network. In the Youtube social network, users form friendship each other and users can create groups which other users can join.	b	Incorporate Group Information to Enhance Network Embedding	[6]	['[6]  A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee. Measurement and analysis of online social networks. In Proceedings of the 7th ACM SIGCOMM conference on Internet measurement, pages 29–42. ACM, 2007. ']
799	39	[6]	// DeepWalk ==[7]==: It uses information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. // LINE [9]: It is a network embedding method, which tries to preserve both the local and global network structures.	b	Incorporate Group Information to Enhance Network Embedding	[7]	['[7]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
800	39	[1]	DeepWalk [7]: It uses information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. // LINE ==[9]==: It is a network embedding method, which tries to preserve both the local and global network structures. // GroupWalk: It only takes the labeled random walks cross groups mentioned above into consideration.	b	Incorporate Group Information to Enhance Network Embedding	[9]	['[9]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
801	9	[235, 36]	One of effective means towards organizing such information associated with the potentially large and complex graph is to learn the graph representations, which assign to each vertex of the graph a low-dimensional dense vector representation, encoding meaningful information conveyed by the graph. // Recently, there has been significant interest in the work of learning word embeddings ==[5, 22]==. // Their goal is to learn for each natural language word a low-dimensional vector representation based on their contexts, from a large amount of natural language texts.	b	Deep neural networks for learning graph representations	[5, 22]	['[5]  Bullinaria, J. A., and Levy, J. P. 2007. Extracting semantic rep- resentations from word co-occurrence statistics: A computational study. Behavior research methods 39(3):510–526. ', '[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ']
802	9	[36, 167]	The resulting compact, low-dimensional vector representations are believed to be able to capture rich semantic information and are proven to be useful for various natural language processing tasks. // While efficient and effective methods for learning good representations for linear structures have been identified ==[22, 25]==, dealing with general graph structures with rich topologies is more complicated. // To this end, one natural approach is to identify effective means to cast the task of learning vertex representations for general graph structures into the task of learning representations from linear structures.	ro	Deep neural networks for learning graph representations	[22, 25]	['[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ', '[25]  Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP, 1532–1543. ']
803	9	[167]	To this end, one natural approach is to identify effective means to cast the task of learning vertex representations for general graph structures into the task of learning representations from linear structures. // DeepWalk proposed by ==[25]== presented an idea to transform unweighted graphs into collections of linear sequences by a uniform sampling method known as truncated random walk. // In their approach, sampled vertex sequences characterize the connections between vertices in a graph.	b	Deep neural networks for learning graph representations	[25]	['[25]  Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP, 1532–1543. ']
804	9	[36]	This step can be understood as a process for converting a general graph structure into a large collection of linear structures. // Next, they utilized the skip-gram model proposed by ==[22]== to learn low-dimensional representations for vertices from such linear structures. // The learned vertex representations were shown to be effective across a few tasks, outperforming several previous approaches such as spectral clustering and modularity method.	h+	Deep neural networks for learning graph representations	[22]	['[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ']
805	9	[33, 56, 55, 137]	Next, they utilized the skip-gram model proposed by [22] to learn low-dimensional representations for vertices from such linear structures. // The learned vertex representations were shown to be effective across a few tasks, outperforming several previous approaches such as spectral clustering and modularity method ==[29, 30, 31, 21]==. //	h-e	Deep neural networks for learning graph representations	[29, 30, 31, 21]	['[29]  Tang, L., and Liu, H. 2009a. Relational learning via latent social dimensions. In SIGKDD, 817–826. ', '[30]  Tang, L., and Liu, H. 2009b. Scalable learning of collective behav- ior based on sparse social dimensions. In CIKM, 1107–1116. ', '[31]  Tang, L., and Liu, H. 2011. Leveraging social media networks for classification. Data Mining and Knowledge Discovery 23(3):447– 478. ', '[21]  Macskassy, S. A., and Provost, F. 2003. A simple relational classi- fier. Technical report, DTIC Document. ']
806	9	[235]	To answer the first question, we design a random surfing model suitable for weighted graphs, which can directly yield a probabilistic co-occurrence matrix. // Such a matrix is similar to the cooccurrence matrix obtained by sampling linear sequences from graphs ==[5]==, but no sampling process is required in our approach. // To answer the secProceedings of the Thirtieth AAAI Conference on Artificial Intelligence and question, we first revisit one popular existing method used for learning vertex representations for linear structures.	b	Deep neural networks for learning graph representations	[5]	['[5]  Bullinaria, J. A., and Levy, J. P. 2007. Extracting semantic rep- resentations from word co-occurrence statistics: A computational study. Behavior research methods 39(3):510–526. ']
807	9	[160]	To answer the secProceedings of the Thirtieth AAAI Conference on Artificial Intelligence and question, we first revisit one popular existing method used for learning vertex representations for linear structures. // A recent study by ==[17]== showed that optimizing the objective function associated with the skipgram with negative sampling method [22] has a intrinsic relation with factorzing a shifted positive pointwise mutual information (PPMI) matrix [5] of the words and their contexts. // Specifically, they showed that it was possible to use the standard singular value decomposition (SVD) method to factorize the PPMI matrix to induce the vertex/word representations from the decomposed matrices.	b	Deep neural networks for learning graph representations	[17]	['[17]  Levy, O., and Goldberg, Y. 2014. Neural word embedding as im- plicit matrix factorization. In NIPS, 2177–2185. ']
808	9	[36]	To answer the secProceedings of the Thirtieth AAAI Conference on Artificial Intelligence and question, we first revisit one popular existing method used for learning vertex representations for linear structures. // A recent study by [17] showed that optimizing the objective function associated with the skipgram with negative sampling method ==[22]== has a intrinsic relation with factorzing a shifted positive pointwise mutual information (PPMI) matrix [5] of the words and their contexts. // Specifically, they showed that it was possible to use the standard singular value decomposition (SVD) method to factorize the PPMI matrix to induce the vertex/word representations from the decomposed matrices.	b	Deep neural networks for learning graph representations	[22]	['[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ']
809	9	[235]	To answer the secProceedings of the Thirtieth AAAI Conference on Artificial Intelligence and question, we first revisit one popular existing method used for learning vertex representations for linear structures. // A recent study by [17] showed that optimizing the objective function associated with the skipgram with negative sampling method [22] has a intrinsic relation with factorzing a shifted positive pointwise mutual information (PPMI) matrix ==[5]== of the words and their contexts. // Specifically, they showed that it was possible to use the standard singular value decomposition (SVD) method to factorize the PPMI matrix to induce the vertex/word representations from the decomposed matrices.	b	Deep neural networks for learning graph representations	[5]	['[5]  Bullinaria, J. A., and Levy, J. P. 2007. Extracting semantic repre- sentations from word co-occurrence statistics: A computational study. Behavior research methods 39(3):510–526. ']
810	9	[26]	Specifically, they showed that it was possible to use the standard singular value decomposition (SVD) method to factorize the PPMI matrix to induce the vertex/word representations from the decomposed matrices. // Our recent approach called GraRep ==[7]== has been shown to achieve good empirical results on the task of learning graph representations. // However, the approach employs SVD for performing linear dimension reduction, while better non-linear dimension reduction techniques were not explored.	h-	Deep neural networks for learning graph representations	[7]	['[7]  Cao, S.; Lu, W.; and Xu, Q. 2015. Grarep: Learning graph repre- sentations with global structural information. In CIKM, 891–900. ']
811	9	[345]	While the SVD step for inducing the final word representations was shown effective, Levy et al. (2015) also demonstrated the effectiveness of using the PPMI matrix itself as the word representations. // Interestingly, as shown by the authors, the representations learned from the SVD method cannot completely outperform the representations from the PPMI matrix itself ==[18]==. //  Since our final goal is to learn good vertex representations that are effective in capturing the graph’s information, it is essential to investigate better ways of recovering the vertex representations from the PPMI matrix, where potentially complex, non-linear relations amongst different vertices can be captured.	ho	Deep neural networks for learning graph representations	[18]	['[18]  Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distri- butional similarity with lessons learned from word embeddings. TACL 3:211–225. ']
812	9	[150]	Since our final goal is to learn good vertex representations that are effective in capturing the graph’s information, it is essential to investigate better ways of recovering the vertex representations from the PPMI matrix, where potentially complex, non-linear relations amongst different vertices can be captured. // Deep learning sheds light on the path of modeling nonlinear complex phenomena, which has many successful applications in different domains, such as speech recognition ==[9]== and computer vision [16]. // Deep neural networks (DNN), e.g., the stacked autoencoders, can be regarded as an effective method for learning high level abstractions from low level features.	h+	Deep neural networks for learning graph representations	[9]	['[9]  Dahl, G. E.; Yu, D.; Deng, L.; and Acero, A. 2012. Context- dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on 20(1):30–42. ']
813	9	[149]	Since our final goal is to learn good vertex representations that are effective in capturing the graph’s information, it is essential to investigate better ways of recovering the vertex representations from the PPMI matrix, where potentially complex, non-linear relations amongst different vertices can be captured. // Deep learning sheds light on the path of modeling nonlinear complex phenomena, which has many successful applications in different domains, such as speech recognition [9] and computer vision ==[16]==. // Deep neural networks (DNN), e.g., the stacked autoencoders, can be regarded as an effective method for learning high level abstractions from low level features.	h+	Deep neural networks for learning graph representations	[16]	['[16]  Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In NIPS, 1097–1105. ']
814	9	[6]	We apply the deep learning method to the PPMI matrix instead of the Laplacian matrix used in their model. // The former has been shown to have the potential to yield better representations than the latter ==[26]==. // To enhance our model’s robustness, we also employ the stacked denoising autoencoders for learning multiple layers of representations.	h+	Deep neural networks for learning graph representations	[26]	['[26]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In SIGKDD, 701–710. ']
815	9	[234]	Such approaches learn representations based on statistics of global co-occurrence counts, and can outperform neural network method based on separate local context windows in certain prediction tasks. // An example of a matrix factorization method is hyperspace analogue analysis ==[20]== which factorizes a word-word co-occurrence matrix to yield word representations. // A major shortcoming of such an approach and related methods is that frequent words with relatively little semantic value such as stop words have a disproportionate effect on the word representations generated.	h-	Deep neural networks for learning graph representations	[20]	['[20]  Lund, K., and Burgess, C. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers 28(2):203–208. ']
816	9	[346]	A major shortcoming of such an approach and related methods is that frequent words with relatively little semantic value such as stop words have a disproportionate effect on the word representations generated. // Church and Hanks’s pointwise mutual information matrix ==[8]== was proposed to address this problem and has since been proven to provide better word representations [5]. // A common approach to improving performance is to assign each negative value to 0.	h+	Deep neural networks for learning graph representations	[8]	['[8]  Church, K. W., and Hanks, P. 1990. Word association norms, mutual information, and lexicography. Computational linguistics 16(1):22–29. ']
817	9	[235]	A major shortcoming of such an approach and related methods is that frequent words with relatively little semantic value such as stop words have a disproportionate effect on the word representations generated. // Church and Hanks’s pointwise mutual information matrix [8] was proposed to address this problem and has since been proven to provide better word representations ==[5]==. // A common approach to improving performance is to assign each negative value to 0.	h+	Deep neural networks for learning graph representations	[5]	['[5]  Bullinaria, J. A., and Levy, J. P. 2007. Extracting semantic rep- resentations from word co-occurrence statistics: A computational study. Behavior research methods 39(3):510–526. ']
818	9	[150, 149]	Deep Neural Networks. // Deep neural networks, which can be used to learn multiple levels of feature representations, has achieved successful results in different fields ==[9, 16]==. // Training such networks were shown difficult.	h+	Deep neural networks for learning graph representations	[9, 16]	['[9]  Dahl, G. E.; Yu, D.; Deng, L.; and Acero, A. 2012. Context- dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on 20(1):30–42. ', '[16]  Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. In NIPS, 1097–1105. ']
819	9	[43, 347]	Training such networks were shown difficult. // One effective solution, proposed in ==[13, 3]==, is to use the greedy layer-wise unsupervised pre-training. // This strategy aims at learning useful representations of each layer at a time.	h+	Deep neural networks for learning graph representations	[13, 3]	['[13]  Hinton, G. E., and Salakhutdinov, R. R. 2006. Reducing the dimen- sionality of data with neural networks. Science 313(5786):504– 507. ', '[3]  Bengio, Y.; Lamblin, P.; Popovici, D.; Larochelle, H.; et al. 2007. Greedy layer-wise training of deep networks. In NIPS, 153–160. ']
820	9	[348, 349]	Neural networks typically employ non-linear activation function such as sigmoid or tanh to capture complex, non-linear projections from the inputs to the outputs. // To train a deep architecture that involves multiple layers of feature representations, autoencoders have emerged as one of the commonly used building blocks ==[4, 14]==. // An autoencoder performs two actions – an encoding step, followed by a decoding step.	b	Deep neural networks for learning graph representations	[4, 14]	['[4]  Bourlard, H., and Kamp, Y. 1988. Auto-association by multilayer perceptrons and singular value decomposition. Biological cyber- netics 59(4-5):291–294. ', '[14]  Hinton, G. E., and Zemel, R. S. 1994. Autoencoders, minimum description length, and helmholtz free energy. In NIPS, 3–10. ']
821	9	[36]	Thus, it is reasonable to weigh the importance of contextual nodes based on their relative distance to the current node. // Such weighting strategies were implemented in both word2vec ==[22]== and GloVe [25] and were found important for achieving good empirical results [18]. //  Based on this fact, we can see that ideally the representation for the i-th vertex should be constructed in the following way: r = K k=1 w(k) · p∗ k where w(·) is a decreasing function, i.e., w(t + 1) < w(t).	h+	Deep neural networks for learning graph representations	[22]	['[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ']
822	9	[167]	Thus, it is reasonable to weigh the importance of contextual nodes based on their relative distance to the current node. // Such weighting strategies were implemented in both word2vec [22] and GloVe ==[25]== and were found important for achieving good empirical results [18]. //  Based on this fact, we can see that ideally the representation for the i-th vertex should be constructed in the following way: r = K k=1 w(k) · p∗ k where w(·) is a decreasing function, i.e., w(t + 1) < w(t).	h+	Deep neural networks for learning graph representations	[25]	['[25]  Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP, 1532–1543. ']
823	9	[345]	Thus, it is reasonable to weigh the importance of contextual nodes based on their relative distance to the current node. // Such weighting strategies were implemented in both word2vec [22] and GloVe [25] and were found important for achieving good empirical results ==[18]==. //  Based on this fact, we can see that ideally the representation for the i-th vertex should be constructed in the following way: r = K k=1 w(k) · p∗ k where w(·) is a decreasing function, i.e., w(t + 1) < w(t).	h+	Deep neural networks for learning graph representations	[18]	['[18]  Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distri- butional similarity with lessons learned from word embeddings. TACL 3:211–225. ']
824	9	[36]	To demonstrate the effectiveness of deep learning as compared to SVD, we chose a dictionary of 10,000 most frequent words (10,000 selected due to the time complexity of the SVD algorithm). // To evaluate the performance of word representations generated by each algorithm, we conducted experiments on word similarities ==[22]== on 4 datasets, including the popular WordSim353 [11], WordSim Similarity and WordSim Relatedness [1] and MC [23], as used in [25, 18]. //	hoe	Deep neural networks for learning graph representations	[22]	['[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ']
825	9	[350]	To demonstrate the effectiveness of deep learning as compared to SVD, we chose a dictionary of 10,000 most frequent words (10,000 selected due to the time complexity of the SVD algorithm). // To evaluate the performance of word representations generated by each algorithm, we conducted experiments on word similarities [22] on 4 datasets, including the popular WordSim353 ==[11]==, WordSim Similarity and WordSim Relatedness [1] and MC [23], as used in [25, 18]. //	hoe	Deep neural networks for learning graph representations	[11]	['[11]  Finkelstein, L.; Gabrilovich, E.; Matias, Y.; Rivlin, E.; Solan, Z.; Wolfman, G.; and Ruppin, E. 2001. Placing search in context: The concept revisited. In WWW, 406–414. ']
826	9	[351]	To demonstrate the effectiveness of deep learning as compared to SVD, we chose a dictionary of 10,000 most frequent words (10,000 selected due to the time complexity of the SVD algorithm). // To evaluate the performance of word representations generated by each algorithm, we conducted experiments on word similarities [22] on 4 datasets, including the popular WordSim353 [11], WordSim Similarity and WordSim Relatedness ==[1]== and MC [23], as used in [25, 18]. //	hoe	Deep neural networks for learning graph representations	[1]	['[1]  Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pas ̧ca, M.; and Soroa, A. 2009. A study on similarity and relatedness using distri- butional and wordnet-based approaches. In NAACL, 19–27. ']
827	9	[352]	To demonstrate the effectiveness of deep learning as compared to SVD, we chose a dictionary of 10,000 most frequent words (10,000 selected due to the time complexity of the SVD algorithm). // To evaluate the performance of word representations generated by each algorithm, we conducted experiments on word similarities [22] on 4 datasets, including the popular WordSim353 [11], WordSim Similarity and WordSim Relatedness [1] and MC ==[23]==, as used in [25, 18]. //	hoe	Deep neural networks for learning graph representations	[23]	['[23]  Miller, G. A., and Charles, W. G. 1991. Contextual correlates of semantic similarity. Language and cognitive processes 6(1):1–28. ']
828	9	[167, 345]	To demonstrate the effectiveness of deep learning as compared to SVD, we chose a dictionary of 10,000 most frequent words (10,000 selected due to the time complexity of the SVD algorithm). // To evaluate the performance of word representations generated by each algorithm, we conducted experiments on word similarities [22] on 4 datasets, including the popular WordSim353 [11], WordSim Similarity and WordSim Relatedness [1] and MC [23], as used in ==[25, 18]==. //	hoe	Deep neural networks for learning graph representations	[25, 18]	['[25]  Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In EMNLP, 1532–1543. ', '[18]  Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distri- butional similarity with lessons learned from word embeddings. TACL 3:211–225. ']
829	9	[6]	// DeepWalk ==[26]== is a recently proposed method to learn representations of networks. // It transforms a graph structure into linear sequences by truncated random walks and processes the sequences using skip-gram with hierarchical softmax.	b	Deep neural networks for learning graph representations	[26]	['[26]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In SIGKDD, 701–710. ']
830	9	[36]	It transforms a graph structure into linear sequences by truncated random walks and processes the sequences using skip-gram with hierarchical softmax. // SGNS ==[22]== is proposed in word2vec. // It is suitable for capturing linear structures. SGNS has been proven to be an effective model to learn word representations both theoretically and empircally	h+	Deep neural networks for learning graph representations	[22]	['[22]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111–3119. ']
831	9	[235]	SGNS has been proven to be an effective model to learn word representations both theoretically and empircally. // PPMI ==[5]== is a measure often used in information theory. // PPMI was used for word representations in [18] and is a sparse high dimensional representation.	b	Deep neural networks for learning graph representations	[5]	['[5]  Bullinaria, J. A., and Levy, J. P. 2007. Extracting semantic rep- resentations from word co-occurrence statistics: A computational study. Behavior research methods 39(3):510–526. ']
832	9	[345]	PPMI [5] is a measure often used in information theory. // PPMI was used for word representations in ==[18]== and is a sparse high dimensional representation. // SVD [17] is a common matrix factorization method that is used to reduce dimensions or extract features.	b	Deep neural networks for learning graph representations	[18]	['[18]  Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distri- butional similarity with lessons learned from word embeddings. TACL 3:211–225. ']
833	9	[160]	PPMI was used for word representations in [18] and is a sparse high dimensional representation. // SVD ==[17]== is a common matrix factorization method that is used to reduce dimensions or extract features. // Following [17], we used SVD to compress the PPMI matrix to obtain low dimensional representations.	ho	Deep neural networks for learning graph representations	[17]	['[17]  Levy, O., and Goldberg, Y. 2014. Neural word embedding as im- plicit matrix factorization. In NIPS, 2177–2185. ']
834	9	[160]	SVD [17] is a common matrix factorization method that is used to reduce dimensions or extract features. // Following ==[17]==, we used SVD to compress the PPMI matrix to obtain low dimensional representations. // Parameters.	ho	Deep neural networks for learning graph representations	[17]	['[17]  Levy, O., and Goldberg, Y. 2014. Neural word embedding as im- plicit matrix factorization. In NIPS, 2177–2185. ']
835	32	[5, 4, 3]	These representations can then be used as features for common tasks on graphs such as multi-label classification, clustering, and link prediction. // Traditional methods for graph dimensionality reduction ==[2, 15, 19]== perform well on small graphs. // However, the time complexity of these methods are at least quadratic in the number of graph nodes, makes them impossible to run on large-scale networks.	h-	HARP: Hierarchical Representation Learning for Networks	[2, 15, 19]	['[2]  Belkin, M., and Niyogi, P. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, 585–591. ', '[15]  Roweis, S. T., and Saul, L. K. 2000. Nonlinear dimen- sionality reduction by locally linear embedding. Science 290(5500):2323–2326. ', '[19]  Tenenbaum, J. B.; De Silva, V.; and Langford, J. C. 2000. A global geometric framework for nonlinear dimensionality reduction. Science 290(5500):2319–2323. ']
836	32	[6]	However, the time complexity of these methods are at least quadratic in the number of graph nodes, makes them impossible to run on large-scale networks. // A recent advancement in graph representation learning, DeepWalk ==[13]== proposed online learning methods using neural networks to address this scalability limitation. // Much work has since followed [3, 8, 1, 18]. These neural network-based methods have proven both highly scalable and performant, achieving strong results on classification and link prediction tasks in large networks.	b	HARP: Hierarchical Representation Learning for Networks	[13]	['[13]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowl- edge discovery and data mining, 701–710. ACM. ']
837	32	[26, 37, 353, 1]	A recent advancement in graph representation learning, DeepWalk [13] proposed online learning methods using neural networks to address this scalability limitation.  // Much work has since followed ==[3, 8, 1, 18]==. // These neural network-based methods have proven both highly scalable and performant, achieving strong results on classification and link prediction tasks in large networks.	h+	HARP: Hierarchical Representation Learning for Networks	[3, 8, 1, 18]	['[3]  Cao, S.; Lu, W.; and Xu, Q. 2015. Grarep: Learning graph representations with global structural information. In Pro- ceedings of the 24th ACM International on Conference on Information and Knowledge Management, 891–900. ACM. ', '[8]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable fea- ture learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discov- ery and Data Mining. ', '[1]  Abu-El-Haija, S.; Perozzi, B.; and Al-Rfou, R. 2017. Learn- ing edge representations via low-rank asymmetric projec- tions. arXiv preprint arXiv:1705.05615. ', '[18]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embed- ding. In Proceedings of the 24th International Conference on World Wide Web, 1067–1077. International World Wide Web Conferences Steering Committee. ']
838	32	[204, 36]	This focus on local structure implicitly ignores long-distance global relationships, and the learned representations can fail to uncover important global structural patterns. // Secondly, they all rely on a non-convex optimization goal solved using stochastic gradient descent ==[7, 11]== which can become stuck in a local minima(e.g. perhaps as a result of a poor initialization). // In other words, all previously proposed techniques for graph representation learning can accidentally learn embedding configurations which disregard important structural features of their input graph.	h-	HARP: Hierarchical Representation Learning for Networks	[7, 11]	['[7]  Goldberg, Y., and Levy, O. 2014. word2vec explained: de- riving mikolov et al.’s negative-sampling word-embedding method. arXiv preprint arXiv:1402.3722. ', '[11]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111–3119. ']
839	32	[354]	New Representation Learning Paradigm. // We propose HARP, a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing ==[6]== and graph representation learning [13, 18, 8] communities to build substantially better graph embeddings. // Improved Optimization Primitives.	ho	HARP: Hierarchical Representation Learning for Networks	[6]	['[6]  Fruchterman, T. M., and Reingold, E. M. 1991. Graph draw- ing by force-directed placement. Software: Practice and ex- perience 21(11):1129–1164. ']
840	32	[6, 1, 37]	New Representation Learning Paradigm. // We propose HARP, a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing [6] and graph representation learning ==[13, 18, 8]== communities to build substantially better graph embeddings. // Improved Optimization Primitives.	ho	HARP: Hierarchical Representation Learning for Networks	[13, 18, 8]	['[13]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowl- edge discovery and data mining, 701–710. ACM. ', '[18]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embed- ding. In Proceedings of the 24th International Conference on World Wide Web, 1067–1077. International World Wide Web Conferences Steering Committee. ', '[8]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable fea- ture learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discov- ery and Data Mining. ']
841	32	[354]	New Representation Learning Paradigm. // We propose HARP, a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing ==[6]== and graph representation learning [13, 18, 8] communities to build substantially better graph embeddings. // Improved Optimization Primitives.	ho	HARP: Hierarchical Representation Learning for Networks	[6]	['[6]  Fruchterman, T. M., and Reingold, E. M. 1991. Graph draw- ing by force-directed placement. Software: Practice and ex- perience 21(11):1129–1164. ']
842	32	[6, 1, 37]	New Representation Learning Paradigm. // We propose HARP, a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing [6] and graph representation learning ==[13, 18, 8]== communities to build substantially better graph embeddings. // Improved Optimization Primitives.	ho	HARP: Hierarchical Representation Learning for Networks	[13, 18, 8]	['[13]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowl- edge discovery and data mining, 701–710. ACM. ', '[18]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embed- ding. In Proceedings of the 24th International Conference on World Wide Web, 1067–1077. International World Wide Web Conferences Steering Committee. ', '[8]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable fea- ture learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discov- ery and Data Mining. ']
843	32	[353]	Table 1: Statistics of the graphs used in our experiments. // DBLP ==[1]== – DBLP is a co-author graph of researchers in computer science. //  The labels indicate the research areas a researcher publishes his work in.	b	HARP: Hierarchical Representation Learning for Networks	[1]	['[1]  Abu-El-Haija, S.; Perozzi, B.; and Al-Rfou, R. 2017. Learn- ing edge representations via low-rank asymmetric projec- tions. arXiv preprint arXiv:1705.05615. ']
844	32	[56]	The labels indicate the research areas a researcher publishes his work in. // BlogCatalog ==[17]== – BlogCatalog is a network of social relationships between users on the BlogCatalog website. // The labels represent the categories a blogger publishes in.	b	HARP: Hierarchical Representation Learning for Networks	[17]	['[17]  Tang, L., and Liu, H. 2009. Relational learning via latent so- cial dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, 817–826. ACM. ']
845	32	[68]	The labels represent the categories a blogger publishes in. // CiteSeer ==[16]== – CiteSeer is a citation network between publications in computer science. // The labels indicate the research areas a paper belongs to.	b	HARP: Hierarchical Representation Learning for Networks	[16]	['[16]  Sen, P.; Namata, G. M.; Bilgic, M.; Getoor, L.; Gallagher, B.; and Eliassi-Rad, T. 2008. Collective classification in network data. AI Magazine 29(3):93–106. ']
846	32	[5, 355, 3]	Graph Representation Learning. // Most early methods treated representation learning as performing dimension reduction on the Laplacian and adjacency matrices ==[2, 4, 19]==. // These methods work well on small graphs, but the time complexity of these algorithms is too high for the large-scale graphs commonly encountered today.	h-	HARP: Hierarchical Representation Learning for Networks	[2, 4, 19]	['[2]  Belkin, M., and Niyogi, P. 2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, 585–591. ', '[4]  Cox, T. F., and Cox, M. A. 2000. Multidimensional scaling. CRC press. ', '[19]  Tenenbaum, J. B.; De Silva, V.; and Langford, J. C. 2000. A global geometric framework for nonlinear dimensionality reduction. Science 290(5500):2319–2323. ']
847	32	[6]	These methods work well on small graphs, but the time complexity of these algorithms is too high for the large-scale graphs commonly encountered today. // Deepwalk ==[13]== presents a two-phase algorithm for graph representation learning. In the first phase, Deepwalk samples sequences of neighboring nodes of each node by random walking on the graph. // In the first phase, Deepwalk samples sequences of neighboring nodes of each node by random walking on the graph.	b	HARP: Hierarchical Representation Learning for Networks	[13]	['[13]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowl- edge discovery and data mining, 701–710. ACM. ']
848	32	[36]	In the first phase, Deepwalk samples sequences of neighboring nodes of each node by random walking on the graph. // Then, the node representation is learned by training a Skip-gram model ==[11]== on the random walks. // A number of methods have been proposed which extend this idea.	b	HARP: Hierarchical Representation Learning for Networks	[11]	['[11]  Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, 3111–3119. ']
849	32	[1]	First, several methods use different strategies for sampling neighboring nodes. // LINE ==[18]== learns graph embeddings which preserve both the first-order and second-order proximities in a graph. // Walklets [1] captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix.	b	HARP: Hierarchical Representation Learning for Networks	[18]	['[18]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embed- ding. In Proceedings of the 24th International Conference on World Wide Web, 1067–1077. International World Wide Web Conferences Steering Committee. ']
850	32	[353]	LINE [18] learns graph embeddings which preserve both the first-order and second-order proximities in a graph. // Walklets ==[1]== captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix. // Node2vec [8] combines DFS-like and BFS-like exploration within the random walk framework.	b	HARP: Hierarchical Representation Learning for Networks	[1]	['[1]  Abu-El-Haija, S.; Perozzi, B.; and Al-Rfou, R. 2017. Learn- ing edge representations via low-rank asymmetric projec- tions. arXiv preprint arXiv:1705.05615. ']
851	32	[37]	Walklets [1] captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix. // Node2vec ==[8]== combines DFS-like and BFS-like exploration within the random walk framework. // Second, matrix factorization methods and deep neural networks have also been proposed [3, 12, 23, 1] as alternatives to the Skip-gram model for learning the latent representations.	b	HARP: Hierarchical Representation Learning for Networks	[8]	['[8]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable fea- ture learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discov- ery and Data Mining. ']
852	32	[26, 48, 19, 353]	Node2vec [8] combines DFS-like and BFS-like exploration within the random walk framework. // Second, matrix factorization methods and deep neural networks have also been proposed ==[3, 12, 23, 1]== as alternatives to the Skip-gram model for learning the latent representations. // Although these methods are highly scalable, they all rely on optimizing a non-convex objective function. With no prior knowledge of the graph, the latent representations are usually initialized with random numbers or zero.	h-	HARP: Hierarchical Representation Learning for Networks	[3, 12, 23, 1]	['[3]  Cao, S.; Lu, W.; and Xu, Q. 2015. Grarep: Learning graph representations with global structural information. In Pro- ceedings of the 24th ACM International on Conference on Information and Knowledge Management, 891–900. ACM. ', '[12]  Ou, M.; Cui, P.; Pei, J.; and Zhu, W. 2016. Asymmetric tran- sitivity preserving graph embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining. ', '[23]  Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discov- ery and Data Mining. ', '[1]  Abu-El-Haija, S.; Perozzi, B.; and Al-Rfou, R. 2017. Learn- ing edge representations via low-rank asymmetric projec- tions. arXiv preprint arXiv:1705.05615. ']
853	32	[354, 356, 357]	Graph Drawing. // Multilevel layout algorithms are popular methods in the graph drawing community, where a hierarchy of approximations is used to solve the original layout problem ==[6, 9, 21]==. // Using an approximation of the original graph has two advantages - not only is the approximation usually simpler to solve, it can also be extended as a good initialization for solving the original problem.	b	HARP: Hierarchical Representation Learning for Networks	[6, 9, 21]	['[6]  Fruchterman, T. M., and Reingold, E. M. 1991. Graph draw- ing by force-directed placement. Software: Practice and ex- perience 21(11):1129–1164. ', '[9]  Hu, Y. 2005. Efficient, high-quality force-directed graph drawing. Mathematica Journal 10(1):37–71. ', '[21]  Walshaw, C. 2003. A multilevel algorithm for force-directed graph-drawing. Journal of Graph Algorithms Applications 7(3):253–285. ']
854	32	[358]	Using an approximation of the original graph has two advantages - not only is the approximation usually simpler to solve, it can also be extended as a good initialization for solving the original problem. // In addition to force-directed graph drawing, the multilevel framework ==[22]== has been proved successful in various graph theory problems, including the traveling salesman problem [20], and graph partitioning [10]. // HARP extends the idea of the multilevel layout to neural representation learning methods.	h+	HARP: Hierarchical Representation Learning for Networks	[22]	['[22]  Walshaw, C. 2004. Multilevel refinement for combinato- rial optimisation problems. Annals of Operations Research 131(1-4):325–372. ']
855	32	[359]	Using an approximation of the original graph has two advantages - not only is the approximation usually simpler to solve, it can also be extended as a good initialization for solving the original problem. // In addition to force-directed graph drawing, the multilevel framework [22] has been proved successful in various graph theory problems, including the traveling salesman problem ==[20]==, and graph partitioning [10]. // HARP extends the idea of the multilevel layout to neural representation learning methods.	h+	HARP: Hierarchical Representation Learning for Networks	[20]	['[20]  Walshaw, C. 2001. A multilevel Lin-Kernighan-Helsgaun algorithm for the travelling salesman problem. Citeseer. ']
856	32	[360]	Using an approximation of the original graph has two advantages - not only is the approximation usually simpler to solve, it can also be extended as a good initialization for solving the original problem. // In addition to force-directed graph drawing, the multilevel framework [22] has been proved successful in various graph theory problems, including the traveling salesman problem [20], and graph partitioning ==[10]==. // HARP extends the idea of the multilevel layout to neural representation learning methods.	h+	HARP: Hierarchical Representation Learning for Networks	[10]	['[10]  Karypis, G., and Kumar, V. 1998. A parallel algorithm for multilevel graph partitioning and sparse matrix ordering. Journal of Parallel and Distributed Computing 48(1):71–95. ']
857	44	[13]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction ==[7]==, node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[7]	['[7]  Gao, S.; Denoyer, L.; and Gallinari, P. 2011. Temporal link prediction by integrating content and structure information. In CIKM, 1169–1174. ACM. ']
858	44	[281]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification ==[24]==, recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[24]	['[24]  Tang, J.; Aggarwal, C.; and Liu, H. 2016. Node classification in signed social networks. In Proceedings of the 2016 SIAM International Conference on Data Mining, 54–62. SIAM. ']
859	44	[151]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation ==[33]==, visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[33]	['[33]  Yu, X.; Ren, X.; Sun, Y.; Gu, Q.; Sturt, B.; Khandelwal, U.; Norick, B.; and Han, J. 2014. Personalized entity recom- mendation: A heterogeneous information network approach. In WSDM, 283–292. ACM. ']
860	44	[78]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization ==[17]==, knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[17]	['[17]  Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using t- sne. Journal of Machine Learning Research 9(Nov):2579–2605. ']
861	44	[18]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation ==[14]==, clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[14]	['[14]  Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015. Learning entity and relation embeddings for knowledge graph comple- tion. In AAAI, 2181–2187. ']
862	44	[176]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering ==[27]==, text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[27]	['[27]  Tian, F.; Gao, B.; Cui, Q.; Chen, E.; and Liu, T.-Y. 2014. Learn- ing deep representations for graph clustering. In AAAI, 1293– 1299. ']
863	44	[1]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding ==[25]==, and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[25]	['[25]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In WWW, 1067–1077. International World Wide Web Conferences Steering Committee. ']
864	44	[361]	Graph representation learning, also known as network embedding, aims to represent each vertex in a graph (network) as a low-dimensional vector, which could facilitate tasks of network analysis and prediction over vertices and edges. // Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis ==[16]==. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[16]	['[16]  Liu, L.; Cheung, W. K.; Li, X.; and Liao, L. 2016. Aligning users across social networks using network embedding. In IJ- CAI, 1774–1780. ']
865	44	[37]	Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs ==[9]==, directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity [20].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[9]	['[9]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In KDD, 855–864. ACM. ']
866	44	[40]	Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs ==[11]==, signed graphs [12], heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity [20].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[11]	['[11]  Li, C.; Li, Z.; Wang, S.; Yang, Y.; Zhang, X.; and Zhou, J. 2017a. Semi-supervised network embedding. In International Confer- ence on Database Systems for Advanced Applications, 131–147. Springer. ']
867	44	[30]	Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs ==[12]==, heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity [20].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[12]	['[12]  Li, C.; Wang, S.; Yang, D.; Li, Z.; Yang, Y.; Zhang, X.; and Zhou, J. 2017b. Ppne: Property preserving network embedding. In International Conference on Database Systems for Advanced Applications, 163–179. Springer. ']
868	44	[117]	Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs ==[31]==, and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity [20].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[31]	['[31]  Wang, H.; Zhang, F.; Hou, M.; Xie, X.; Guo, M.; and Liu, Q. 2018. Shine: Signed heterogeneous information network em- bedding for sentiment link prediction. In WSDM. ACM. ']
869	44	[29]	Learned embeddings are capable to benefit a wide range of real-world applications such as link prediction [7], node classification [24], recommendation [33], visualization [17], knowledge graph representation [14], clustering [27], text embedding [25], and social network analysis [16]. // Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs ==[10]==. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity [20].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[10]	['[10]  Huang, X.; Li, J.; and Hu, X. 2017. Label informed attributed network embedding. In WSDM, 731–739. ACM. ']
870	44	[19]	Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures ==[32]==, community structures [30], group information [3], and asymmetric transitivity [20]. // Arguably, most existing methods of graph representation learning can be classified into two categories. The first is generative graph representation learning model [21, 9, 11, 6, 11].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[32]	['[32]  Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deep network embedding. In KDD, 1225–1234. ACM. ']
871	44	[30]	Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures ==[30]==, group information [3], and asymmetric transitivity [20]. // Arguably, most existing methods of graph representation learning can be classified into two categories. The first is generative graph representation learning model [21, 9, 11, 6, 11].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[30]	['[30]  Wang, X.; Cui, P.; Wang, J.; Pei, J.; Zhu, W.; and Yang, S. 2017c. Community preserving network embedding. In AAAI, 203–209. ']
872	44	[39]	Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information ==[3]==, and asymmetric transitivity [20]. // Arguably, most existing methods of graph representation learning can be classified into two categories. The first is generative graph representation learning model [21, 9, 11, 6, 11].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[3]	['[3]  Chen, J.; Zhang, Q.; and Huang, X. 2016. Incorporate group information to enhance network embedding. In CIKM, 1901– 1904. ACM. ']
873	44	[48]	Recently, researchers have examined applying representation learning methods to various types of graphs, such as weighted graphs [9], directed graphs [11], signed graphs [12], heterogeneous graphs [31], and attributed graphs [10]. // In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity ==[20]==. // Arguably, most existing methods of graph representation learning can be classified into two categories. The first is generative graph representation learning model [21, 9, 11, 6, 11].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[20]	['[20]  Ou, M.; Cui, P.; Pei, J.; Zhang, Z.; and Zhu, W. 2016. Asym- metric transitivity preserving graph embedding. In KDD, 1105– 1114. ']
874	44	[6, 37, 40, 111, 40]	In addition, several prior works also try to preserve specific properties during the learning process, such as global structures [32], community structures [30], group information [3], and asymmetric transitivity [20]. // Arguably, most existing methods of graph representation learning can be classified into two categories. The first is generative graph representation learning model ==[21, 9, 11, 6, 11]==. // Similar to classic generative models such as Gaussian Mixture Model [15] or Latent Dirichlet Allocation [1], generative graph representation learning models assume that, for each vertex vc, there exists an underlying true connectivity distribution ptrue, which implies vc’s connectivity preference over all other vertices in the graph.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[21, 9, 11, 6, 11]	['[21]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: On- line learning of social representations. In KDD, 701–710. ACM. ', '[9]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In KDD, 855–864. ACM. ', '[11]  Li, C.; Li, Z.; Wang, S.; Yang, Y.; Zhang, X.; and Zhou, J. 2017a. Semi-supervised network embedding. In International Confer- ence on Database Systems for Advanced Applications, 131–147. Springer. ', '[6]  Dong, Y.; Chawla, N. V.; and Swami, A. 2017. metapath2vec: Scalable representation learning for heterogeneous networks. In KDD, 135–144. ACM. ', '[11]  Li, C.; Li, Z.; Wang, S.; Yang, Y.; Zhang, X.; and Zhou, J. 2017a. Semi-supervised network embedding. In International Confer- ence on Database Systems for Advanced Applications, 131–147. Springer. ']
875	44	[362]	The first is generative graph representation learning model [21, 9, 11, 6, 11]. // Similar to classic generative models such as Gaussian Mixture Model ==[15]== or Latent Dirichlet Allocation [1], generative graph representation learning models assume that, for each vertex vc, there exists an underlying true connectivity distribution ptrue, which implies vc’s connectivity preference over all other vertices in the graph. // The edges in the graph can thus be viewed as observed samples generated by these conditional distributions, and these generative models learn vertex embeddings by maximizing the likelihood of edges in the graph.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[15]	['[15]  Lindsay, B. G. 1995. Mixture models: theory, geometry and applications. In NSF-CBMS regional conference series in prob- ability and statistics, i–163. JSTOR. ']
876	44	[305]	The first is generative graph representation learning model [21, 9, 11, 6, 11]. // Similar to classic generative models such as Gaussian Mixture Model [15] or Latent Dirichlet Allocation ==[1]==, generative graph representation learning models assume that, for each vertex vc, there exists an underlying true connectivity distribution ptrue, which implies vc’s connectivity preference over all other vertices in the graph. // The edges in the graph can thus be viewed as observed samples generated by these conditional distributions, and these generative models learn vertex embeddings by maximizing the likelihood of edges in the graph.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[1]	['[1]  Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent dirichlet allocation. Journal of machine Learning research 3(Jan):993– 1022. ']
877	44	[6]	The edges in the graph can thus be viewed as observed samples generated by these conditional distributions, and these generative models learn vertex embeddings by maximizing the likelihood of edges in the graph. // For example, DeepWalk ==[21]== uses random walk to sample “context” vertices for each vertex, and tries to maximize the log-likelihood of observing context vertices for the given vertex. // Node2vec [9] further extends the idea by proposing a biased random walk procedure, which provides more flexibility when generating the context for a given vertex.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[21]	['[21]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: On- line learning of social representations. In KDD, 701–710. ACM. ']
878	44	[37]	For example, DeepWalk [21] uses random walk to sample “context” vertices for each vertex, and tries to maximize the log-likelihood of observing context vertices for the given vertex. // Node2vec ==[9]== further extends the idea by proposing a biased random walk procedure, which provides more flexibility when generating the context for a given vertex. // The second kind of graph representation learning method is the discriminative model [31, 2, 32, 12].	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[9]	['[9]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In KDD, 855–864. ACM. ']
879	44	[117, 9, 19, 30]	Node2vec [9] further extends the idea by proposing a biased random walk procedure, which provides more flexibility when generating the context for a given vertex. // The second kind of graph representation learning method is the discriminative model ==[31, 2, 32, 12]==. // Different from generative models, discriminative graph representation learning models do not treat edges as generated from an underlying conditional distribution, but aim to learn a classifier for predicting the existence of edges directly.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[31, 2, 32, 12]	['[31]  Wang, H.; Zhang, F.; Hou, M.; Xie, X.; Guo, M.; and Liu, Q. 2018. Shine: Signed heterogeneous information network em- bedding for sentiment link prediction. In WSDM. ACM. ', '[2]  Cao, S.; Lu, W.; and Xu, Q. 2016. Deep neural networks for learning graph representations. In AAAI, 1145–1152. ', '[32]  Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deep network embedding. In KDD, 1225–1234. ACM. ', '[12]  Li, C.; Wang, S.; Yang, D.; Li, Z.; Yang, Y.; Zhang, X.; and Zhou, J. 2017b. Ppne: Property preserving network embedding. In International Conference on Database Systems for Advanced Applications, 163–179. Springer. ']
880	44	[19]	Typically, discriminative models consider two vertices vi and vj jointly as features, and predict the probability of an edge existing between the two vertices, i.e., p edge|(vi, vj ), based on the training data in the graph. // For instance, SDNE ==[32]== uses the sparse adjacency vector of vertices as raw features for each vertex, and applies an autoencoder to extract short and condense features for vertices under the supervision of edge existence. // PPNE [12] directly learns vertex embeddings with supervised learning on positive samples and negative samples, also preserving the inherent properties of vertices during the learning process.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[32]	['[32]  Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deep network embedding. In KDD, 1225–1234. ACM. ']
881	44	[30]	For instance, SDNE [32] uses the sparse adjacency vector of vertices as raw features for each vertex, and applies an autoencoder to extract short and condense features for vertices under the supervision of edge existence. // PPNE ==[12]== directly learns vertex embeddings with supervised learning on positive samples and negative samples, also preserving the inherent properties of vertices during the learning process. // Although generative and discriminative models are generally two disjoint classes of graph representation learning methods, they can be considered two sides of the same coin.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[12]	['[12]  Li, C.; Wang, S.; Yang, D.; Li, Z.; Yang, Y.; Zhang, X.; and Zhou, J. 2017b. Ppne: Property preserving network embedding. In International Conference on Database Systems for Advanced Applications, 163–179. Springer. ']
882	44	[40]	PPNE [12] directly learns vertex embeddings with supervised learning on positive samples and negative samples, also preserving the inherent properties of vertices during the learning process. // Although generative and discriminative models are generally two disjoint classes of graph representation learning methods, they can be considered two sides of the same coin ==[11]==. // In fact, LINE [25] has done a preliminary trial on implicitly combining these two objectives.	ho	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[11]	['[11]  Li, C.; Li, Z.; Wang, S.; Yang, Y.; Zhang, X.; and Zhou, J. 2017a. Semi-supervised network embedding. In International Confer- ence on Database Systems for Advanced Applications, 131–147. Springer. ']
883	44	[1]	Although generative and discriminative models are generally two disjoint classes of graph representation learning methods, they can be considered two sides of the same coin [11]. // In fact, LINE ==[25]== has done a preliminary trial on implicitly combining these two objectives. // Recently, Generative Adversarial Nets [8] have received a great deal of attention.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[25]	['[25]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In WWW, 1067–1077. International World Wide Web Conferences Steering Committee. ']
884	44	[45]	In fact, LINE [25] has done a preliminary trial on implicitly combining these two objectives. // Recently, Generative Adversarial Nets ==[8]== have received a great deal of attention. //  By designing a game-theoretical minimax game to combine generative and discriminative models, GAN and its variants achieve success in various applications, such as image generation, sequence generation, dialogue generation, information retrieval, and domain adaption.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[8]	['[8]  Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde- Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Gen- erative adversarial nets. In NIPS, 2672–2680. ']
885	44	[363]	Recently, Generative Adversarial Nets [8] have received a great deal of attention. //  By designing a game-theoretical minimax game to combine generative and discriminative models, GAN and its variants achieve success in various applications, such as image generation ==[5]==, sequence generation [28], dialogue generation [13], information retrieval [11], and domain adaption [35]. // Inspired by GAN, in this paper we propose GraphGAN, a novel framework that unifies generative and discriminative thinking for graph representation learning.	h+	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[5]	['[5]  Denton, E. L.; Chintala, S.; Fergus, R.; et al. 2015. Deep gen- erative image models using a laplacian pyramid of adversarial networks. In NIPS, 1486–1494. ']
886	44	[364]	Recently, Generative Adversarial Nets [8] have received a great deal of attention. //  By designing a game-theoretical minimax game to combine generative and discriminative models, GAN and its variants achieve success in various applications, such as image generation [5], sequence generation ==[28]==, dialogue generation [13], information retrieval [11], and domain adaption [35]. // Inspired by GAN, in this paper we propose GraphGAN, a novel framework that unifies generative and discriminative thinking for graph representation learning.	h+	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[28]	['[28]  Wang, J.; Yu, L.; Zhang, W.; Gong, Y.; Xu, Y.; Wang, B.; Zhang, P.; and Zhang, D. 2017a. Irgan: A minimax game for unifying generative and discriminative information retrieval models. In SIGIR. ACM. ']
887	44	[365]	Recently, Generative Adversarial Nets [8] have received a great deal of attention. //  By designing a game-theoretical minimax game to combine generative and discriminative models, GAN and its variants achieve success in various applications, such as image generation [5], sequence generation [28], dialogue generation ==[13]==, information retrieval [11], and domain adaption [35]. // Inspired by GAN, in this paper we propose GraphGAN, a novel framework that unifies generative and discriminative thinking for graph representation learning.	h+	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[13]	['[13]  Li, J.; Monroe, W.; Shi, T.; Ritter, A.; and Jurafsky, D. 2017c. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547. ']
888	44	[40]	Recently, Generative Adversarial Nets [8] have received a great deal of attention. //  By designing a game-theoretical minimax game to combine generative and discriminative models, GAN and its variants achieve success in various applications, such as image generation [5], sequence generation [28], dialogue generation [13], information retrieval ==[11]==, and domain adaption [35]. // Inspired by GAN, in this paper we propose GraphGAN, a novel framework that unifies generative and discriminative thinking for graph representation learning.	h+	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[11]	['[11]  Li, C.; Li, Z.; Wang, S.; Yang, Y.; Zhang, X.; and Zhou, J. 2017a. Semi-supervised network embedding. In International Confer- ence on Database Systems for Advanced Applications, 131–147. Springer. ']
889	44	[366]	Recently, Generative Adversarial Nets [8] have received a great deal of attention. //  By designing a game-theoretical minimax game to combine generative and discriminative models, GAN and its variants achieve success in various applications, such as image generation [5], sequence generation [28], dialogue generation [13], information retrieval [11], and domain adaption ==[35]==. // Inspired by GAN, in this paper we propose GraphGAN, a novel framework that unifies generative and discriminative thinking for graph representation learning.	h+	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[35]	['[35]  Zhang, Y.; Barzilay, R.; and Jaakkola, T. 2017. Aspect- augmented adversarial networks for domain adaptation. arXiv preprint arXiv:1701.00188. ']
890	44	[6]	MovieLens-1M6 is a bipartite graph consisting of approximately 1 million ratings (edges) with 6,040 users and 3,706 movies in MovieLens website. // DeepWalk ==[21]== adopts random walk and Skip-Gram to learn vertex embeddings. // LINE [25] preserves the first-order and second-order proximity among vertices in the graph.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[21]	['[21]  Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk: On- line learning of social representations. In KDD, 701–710. ACM. ']
891	44	[1]	DeepWalk [21] adopts random walk and Skip-Gram to learn vertex embeddings. // LINE ==[25]== preserves the first-order and second-order proximity among vertices in the graph. // Node2vec [9] is a variant of DeepWalk and designs a biased random walk to learn vertex embeddings.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[25]	['[25]  Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei, Q. 2015. Line: Large-scale information network embedding. In WWW, 1067–1077. International World Wide Web Conferences Steering Committee. ']
892	44	[37]	LINE [25] preserves the first-order and second-order proximity among vertices in the graph. // Node2vec ==[9]== is a variant of DeepWalk and designs a biased random walk to learn vertex embeddings. // Struc2vec [22] captures the structural identity of vertices in a graph.	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[9]	['[9]  Grover, A., and Leskovec, J. 2016. node2vec: Scalable feature learning for networks. In KDD, 855–864. ACM. ']
893	44	[52]	Node2vec [9] is a variant of DeepWalk and designs a biased random walk to learn vertex embeddings. // Struc2vec ==[22]== captures the structural identity of vertices in a graph. //	b	GraphGAN: Graph Representation Learning with Generative Adversarial Nets	[22]	['[22]  Ribeiro, L. F.; Saverese, P. H.; and Figueiredo, D. R. 2017. struc2vec: Learning node representations from structural iden- tity. In KDD, 385–394. ACM. ']
894	24	[367]	Thus, distributing different nodes in different shards or servers often causes demandingly high communication cost among servers, and holds back speed-up ratio. // Although some limited progress is made on graph parallelization by subtly segmenting large-scale graphs ==[1]==, the luck of these methods heavily depends on the topological characteristics of the underlying graphs. Inapplicability of machine learning methods. // Inapplicability of machine learning methods.	h-	A Survey on Network Embedding	[1]	['[1]  C. L. Staudt, A. Sazonovs, and H. Meyerhenke, “Networkit: A tool suite for large-scale network analysis,” Netw. Sci., Cambridge University Press, vol. 4, pp. 508–530, 2016. ']
895	24	[6]	Fig. 1. // An example of network embedding on a karate network. Images are extracted from DeepWalk ==[3]==. // (i.e., the nodes) are dependant to each other to some degree determined by E.	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
896	24	[68]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification ==[2]==, [3], node clustering [4], network visualization [5], [6] and link prediction [7], [8]. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[2]	['[2]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, “Collective classification in network data,” AI Mag., vol. 29, no. 3, 2008, Art. no. 93. ']
897	24	[6]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification [2], ==[3]==, node clustering [4], network visualization [5], [6] and link prediction [7], [8]. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
898	24	[30]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification [2], [3], node clustering ==[4]==, network visualization [5], [6] and link prediction [7], [8]. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[4]	['[4]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proc. 31th AAAI Conf. Artif. Intell., AAAI Press, 2017, pp. 203–209. ']
899	24	[77]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification [2], [3], node clustering [4], network visualization ==[5]==, [6] and link prediction [7], [8]. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[5]	['[5]  I. Herman, G. Melanc¸on, and M. S. Marshall, “Graph visualization and navigation in information visualization: A survey,” IEEE Trans. Vis. Comput. Graph., vol. 6, no. 1, pp. 24–43, Jan.-Mar. 2000. ']
900	24	[19]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification [2], [3], node clustering [4], network visualization [5], ==[6]== and link prediction [7], [8]. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
901	24	[70]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification [2], [3], node clustering [4], network visualization [5], [6] and link prediction ==[7]==, [8]. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[7]	['[7]  D. Liben-Nowell and J. Kleinberg, “The link-prediction problem for social networks,” J. Assoc. Inf. Sci. Technol., vol. 58, no. 7, pp. 1019–1031, 2007. ']
902	24	[48]	After embedding the karate club network into a two-dimensional space, the similar nodes marked by the same color are close to each other in the embedding space, demonstrating that the network structure can be well modeled in the two-dimensional embedding space. // Network embedding, as a promising way of network representation, is capable of supporting subsequent network processing and analysis tasks such as node classification [2], [3], node clustering [4], network visualization [5], [6] and link prediction [7], ==[8]==. // If this goal is fulfilled, the advantages of network embedding over traditional network representation methods are apparent, as shown in Fig. 2.	b	A Survey on Network Embedding	[8]	['[8]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 672–681. ']
903	24	[368]	We will briefly review those methods in Section 3. // Fu and Ma ==[9]== present a more detailed survey. // In this paper, we focus on the recently proposed network embedding methods aiming to address the goal of network inference.	b	A Survey on Network Embedding	[9]	['[9]  Y. Fu and Y. Ma, Graph Embedding for Pattern Analysis. New York, NY, USA: Springer Science & Business Media, 2012. ']
904	24	[1]	However, as mentioned before, directly conducting these tasks based on network topology has a series of problems, and thus poses a question that whether we can learn a network embedding space purely based on the network topology information, such that these tasks can be well supported in this low dimensional space. // Motivated by this, attempts are proposed to preserve rich structural information into network embedding, from nodes and links ==[10]== to neighborhood structure [3], high-order proximities of nodes [6], and community structures [4]. // All these types of structural information have been demonstrated useful and necessary in various network analysis tasks.	b	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
905	24	[6]	However, as mentioned before, directly conducting these tasks based on network topology has a series of problems, and thus poses a question that whether we can learn a network embedding space purely based on the network topology information, such that these tasks can be well supported in this low dimensional space. // Motivated by this, attempts are proposed to preserve rich structural information into network embedding, from nodes and links [10] to neighborhood structure ==[3]==, high-order proximities of nodes [6], and community structures [4]. // All these types of structural information have been demonstrated useful and necessary in various network analysis tasks.	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
906	24	[19]	However, as mentioned before, directly conducting these tasks based on network topology has a series of problems, and thus poses a question that whether we can learn a network embedding space purely based on the network topology information, such that these tasks can be well supported in this low dimensional space. // Motivated by this, attempts are proposed to preserve rich structural information into network embedding, from nodes and links [10] to neighborhood structure [3], high-order proximities of nodes ==[6]==, and community structures [4]. // All these types of structural information have been demonstrated useful and necessary in various network analysis tasks.	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
907	24	[30]	However, as mentioned before, directly conducting these tasks based on network topology has a series of problems, and thus poses a question that whether we can learn a network embedding space purely based on the network topology information, such that these tasks can be well supported in this low dimensional space. // Motivated by this, attempts are proposed to preserve rich structural information into network embedding, from nodes and links [10] to neighborhood structure [3], high-order proximities of nodes [6], and community structures ==[4]==. // All these types of structural information have been demonstrated useful and necessary in various network analysis tasks.	b	A Survey on Network Embedding	[4]	['[4]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proc. 31th AAAI Conf. Artif. Intell., AAAI Press, 2017, pp. 203–209. ']
908	24	[369]	Besides this structural information, network properties in the original network space are not ignorable in modeling the formation and evolution of networks. // To name a few, network transitivity (i.e. triangle closure) is the driving force of link formation in networks ==[11]==, and structural balance property plays an important role in the evolution of signed networks [12]. Preserving these properties in a network embedding space is, however, challenging due to the inhomogeneity between the network space and the embedding vector space. // Some recent studies begin to look into this Fig. 3. An overview of different settings of network embedding. CUI ET AL.: A SURVEY ON NETWORK EMBEDDING 835 problem and demonstrate the possibility of aligning these two spaces at the property level [8], [13].	ho	A Survey on Network Embedding	[11]	['[11]  H. Huang, J. Tang, S. Wu, L. Liu, et al., “Mining triadic closure patterns in social networks,” in Proc. 23rd Int. Conf. World Wide Web, 2014, pp. 499–504. ']
909	24	[370]	Besides this structural information, network properties in the original network space are not ignorable in modeling the formation and evolution of networks. // To name a few, network transitivity (i.e. triangle closure) is the driving force of link formation in networks [11], and structural balance property plays an important role in the evolution of signed networks ==[12]==. Preserving these properties in a network embedding space is, however, challenging due to the inhomogeneity between the network space and the embedding vector space. // Some recent studies begin to look into this Fig. 3. An overview of different settings of network embedding. CUI ET AL.: A SURVEY ON NETWORK EMBEDDING 835 problem and demonstrate the possibility of aligning these two spaces at the property level [8], [13].	ho	A Survey on Network Embedding	[12]	['[12]  D. Cartwright and F. Harary, “Structural balance: A generalization of heider’s theory,” Psychological Rev., vol. 63, no. 5, 1956, Art. no. 277. ']
910	24	[48]	To name a few, network transitivity (i.e. triangle closure) is the driving force of link formation in networks [11], and structural balance property plays an important role in the evolution of signed networks [12]. Preserving these properties in a network embedding space is, however, challenging due to the inhomogeneity between the network space and the embedding vector space. // Some recent studies begin to look into this Fig. 3. An overview of different settings of network embedding. CUI ET AL.: A SURVEY ON NETWORK EMBEDDING 835 problem and demonstrate the possibility of aligning these two spaces at the property level ==[8]==, [13]. // Network Embedding with Side Information	b	A Survey on Network Embedding	[8]	['[8]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 672–681. ']
911	24	[118]	To name a few, network transitivity (i.e. triangle closure) is the driving force of link formation in networks [11], and structural balance property plays an important role in the evolution of signed networks [12]. Preserving these properties in a network embedding space is, however, challenging due to the inhomogeneity between the network space and the embedding vector space. // Some recent studies begin to look into this Fig. 3. An overview of different settings of network embedding. CUI ET AL.: A SURVEY ON NETWORK EMBEDDING 835 problem and demonstrate the possibility of aligning these two spaces at the property level [8], ==[13]==. // Network Embedding with Side Information	b	A Survey on Network Embedding	[13]	['[13]  S. Wang, J. Tang, C. Aggarwal, Y. Chang, and H. Liu, “Signed network embedding in social media,” in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 327–335. ']
912	24	[62]	Network Embedding with Side Information. // Besides network topology, some types of networks are accompanied with rich side information, such as node content or labels in information networks ==[14]==, node and edge attributes in social networks [15], as well as node types in heterogeneous networks [16]. // Side information provides useful clues for characterizing relationships among network nodes, and thus is helpful in learning embedding vector spaces.	b	A Survey on Network Embedding	[14]	['[14]  C. Tu, W. Zhang, Z. Liu, and M. Sun, “Max-margin deepwalk: Discriminative learning of network representation,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 3889–3895. ']
913	24	[7]	Network Embedding with Side Information. // Besides network topology, some types of networks are accompanied with rich side information, such as node content or labels in information networks [14], node and edge attributes in social networks ==[15]==, as well as node types in heterogeneous networks [16]. // Side information provides useful clues for characterizing relationships among network nodes, and thus is helpful in learning embedding vector spaces.	b	A Survey on Network Embedding	[15]	['[15]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proc. 24th Int. Joint Conf. Artif. Intell., 2015, pp. 2111–2117. ']
914	24	[109]	Network Embedding with Side Information. // Besides network topology, some types of networks are accompanied with rich side information, such as node content or labels in information networks [14], node and edge attributes in social networks [15], as well as node types in heterogeneous networks ==[16]==. // Side information provides useful clues for characterizing relationships among network nodes, and thus is helpful in learning embedding vector spaces.	b	A Survey on Network Embedding	[16]	['[16]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang, “Heterogeneous network embedding via deep architectures,” in Proc. 21th ACM SIGKDD Int. Conf. Knowl Discovery Data Mining, 2015, pp. 119–128. ']
915	24	[7]	In the cases where the network topology is relatively sparse, the importance of the side information as complementary information sources is even more substantial. Methodologically, the main challenge is how to integrate and balance the topological and side information in network embedding. // Some multimodal and multisource fusion techniques are explored in this line of research ==[15]==, [17]. // Advanced Information Preserving Network Embedding	b	A Survey on Network Embedding	[15]	['[15]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proc. 24th Int. Joint Conf. Artif. Intell., 2015, pp. 2111–2117. ']
916	24	[34]	In the cases where the network topology is relatively sparse, the importance of the side information as complementary information sources is even more substantial. Methodologically, the main challenge is how to integrate and balance the topological and side information in network embedding. // Some multimodal and multisource fusion techniques are explored in this line of research [15], ==[17]==. // Advanced Information Preserving Network Embedding	b	A Survey on Network Embedding	[17]	['[17]  N. Natarajan and I. S. Dhillon, “Inductive matrix completion for predicting gene–disease associations,” Bioinf., vol. 30, no. 12, pp. i60–i68, 2014. ']
917	24	[371]	Realizing this idea leads to supervised or pseudo supervised information (i.e. the advanced information) in the target scenarios. // Directly designing a framework of representation learning for a particular target scenario is also known as an end-to-end solution ==[18]==, where highquality supervised information is exploited to learn the latent representation space from scratch. // End-to-end solutions have demonstrated their advantages in some fields, such as computer vision [19] and natural language processing (NLP) [20]. Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification.	b	A Survey on Network Embedding	[18]	['[18]  C. Li, J. Ma, X. Guo, and Q. Mei, “Deepcas: An end-to-end predictor of information cascades,” in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 577–586. ']
918	24	[372]	Directly designing a framework of representation learning for a particular target scenario is also known as an end-to-end solution [18], where highquality supervised information is exploited to learn the latent representation space from scratch. // End-to-end solutions have demonstrated their advantages in some fields, such as computer vision ==[19]== and natural language processing (NLP) [20]. Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification. // Some recent works demonstrate the feasibility in applications such as cascading prediction [18], anomaly detection [21], network alignment [22] and collaboration prediction [23]. In general, network structures and properties are the fundamental factors that need to be considered in network embedding.	b	A Survey on Network Embedding	[19]	['[19]  S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, “End-to-end learning of action detection from frame glimpses in videos,” in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2016, pp. 2678– 2687. ']
919	24	[373]	Directly designing a framework of representation learning for a particular target scenario is also known as an end-to-end solution [18], where highquality supervised information is exploited to learn the latent representation space from scratch. // End-to-end solutions have demonstrated their advantages in some fields, such as computer vision [19] and natural language processing (NLP) ==[20]==. Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification. // Some recent works demonstrate the feasibility in applications such as cascading prediction [18], anomaly detection [21], network alignment [22] and collaboration prediction [23]. In general, network structures and properties are the fundamental factors that need to be considered in network embedding.	b	A Survey on Network Embedding	[20]	['[20]  X. Yang, Y.-N. Chen, D. Hakkani-Tur, P. Crook, X. Li, J. Gao, and € L. Deng, “End-to-end joint learning of natural language understanding and dialogue manager,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., 2017, pp. 5690–5694. ']
920	24	[371]	Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification. // Some recent works demonstrate the feasibility in applications such as cascading prediction ==[18]==, anomaly detection [21], network alignment [22] and collaboration prediction [23]. // In general, network structures and properties are the fundamental factors that need to be considered in network embedding.	b	A Survey on Network Embedding	[18]	['[18]  C. Li, J. Ma, X. Guo, and Q. Mei, “Deepcas: An end-to-end predictor of information cascades,” in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 577–586. ']
921	24	[374]	Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification. // Some recent works demonstrate the feasibility in applications such as cascading prediction [18], anomaly detection ==[21]==, network alignment [22] and collaboration prediction [23]. // In general, network structures and properties are the fundamental factors that need to be considered in network embedding.	b	A Survey on Network Embedding	[21]	['[21]  R. Hu, C. C. Aggarwal, S. Ma, and J. Huai, “An embedding approach to anomaly detection,” in Proc. IEEE 32nd Int. Conf. Data Eng., 2016, pp. 385–396. ']
922	24	[375]	Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification. // Some recent works demonstrate the feasibility in applications such as cascading prediction [18], anomaly detection [21], network alignment ==[22]== and collaboration prediction [23]. // In general, network structures and properties are the fundamental factors that need to be considered in network embedding.	b	A Survey on Network Embedding	[22]	['[22]  T. Man, H. Shen, S. Liu, X. Jin, and X. Cheng, “Predict anchor links across social networks via an embedding approach,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 1823–1829. ']
923	24	[202]	Similar ideas are also feasible for network applications. Taking the network node classification problem as an example, if we have the labels of some network nodes, we can design a solution with network structure as input, node labels as supervised information, and embedding representation as latent middle layer, and the resulted network embedding is specific for node classification. // Some recent works demonstrate the feasibility in applications such as cascading prediction [18], anomaly detection [21], network alignment [22] and collaboration prediction ==[23]==. // In general, network structures and properties are the fundamental factors that need to be considered in network embedding.	b	A Survey on Network Embedding	[23]	['[23]  T. Chen and Y. Sun, “Task-guided and path-augmented heterogeneous network embedding for author identification,” in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 295–304. ']
924	24	[48]	In this sense, matrix factorization methods, with the same goal of learning lowrank space for the original matrix, can naturally be applied to solve this problem. // In the series of matrix factorization models, Singular Value Decomposition (SVD) is commonly used in network embedding due to its optimality for low-rank approximation ==[8]==. // Non-negative matrix factorization is often used because of its advantages as an additive model [4].	b	A Survey on Network Embedding	[8]	['[8]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 672–681. ']
925	24	[30]	In the series of matrix factorization models, Singular Value Decomposition (SVD) is commonly used in network embedding due to its optimality for low-rank approximation [8]. // Non-negative matrix factorization is often used because of its advantages as an additive model ==[4]==. // Random Walk.	h+	A Survey on Network Embedding	[4]	['[4]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proc. 31th AAAI Conf. Artif. Intell., AAAI Press, 2017, pp. 203–209. ']
926	24	[36]	In the field of natural language processing, the word representation also suffers from similar drawbacks. // The development of Word2Vector ==[24]== significantly improves the effectiveness of word representation by transforming sparse, discrete and high-dimensional vectors into dense, continuous and low-dimensional vectors. // The intuition of Word2Vector is that a word vector should be able to reconstruct the vectors of its neighborhood words which are defined by co-occurence rate.	h+	A Survey on Network Embedding	[24]	['[24]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119. ']
927	24	[6]	By regarding a node as a word, we can regard a random path as a sentence, and the node neighborhood can be identified by co-occurence rate as in Word2Vector.  // Some representative methods include DeepWalk ==[3]== and Node2Vec [25]. // Deep Neural Networks	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
928	24	[37]	By regarding a node as a word, we can regard a random path as a sentence, and the node neighborhood can be identified by co-occurence rate as in Word2Vector.  // Some representative methods include DeepWalk [3] and Node2Vec ==[25]==. // Deep Neural Networks	b	A Survey on Network Embedding	[25]	['[25]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
929	24	[19]	The key challenges are how to make deep models fit network data, and how to impose network structure and property-level constraints on deep models. // Some representative methods, such as SDNE ==[6]==, SDAE [26], and SiNE [13], propose deep learning models for network embedding to address these challenges. // At the same time, deep neural networks are also well known for their advantages in providing end-to-end solutions.	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
930	24	[9]	The key challenges are how to make deep models fit network data, and how to impose network structure and property-level constraints on deep models. // Some representative methods, such as SDNE [6], SDAE ==[26]==, and SiNE [13], propose deep learning models for network embedding to address these challenges. // At the same time, deep neural networks are also well known for their advantages in providing end-to-end solutions.	b	A Survey on Network Embedding	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 1145–1152. ']
931	24	[118]	The key challenges are how to make deep models fit network data, and how to impose network structure and property-level constraints on deep models. // Some representative methods, such as SDNE [6], SDAE [26], and SiNE ==[13]==, propose deep learning models for network embedding to address these challenges. // At the same time, deep neural networks are also well known for their advantages in providing end-to-end solutions.	b	A Survey on Network Embedding	[13]	['[13]  S. Wang, J. Tang, C. Aggarwal, Y. Chang, and H. Liu, “Signed network embedding in social media,” in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 327–335. ']
932	24	[371]	Therefore, in the problems where advanced information is available, it is natural to exploit deep models to come up with an end-to-end network embedding solution. // For instance, some deep model based end-to-end solutions are proposed for cascade prediction ==[18]== and network alignment [22]. // The network embedding models are not limited to those mentioned in this subsection.	b	A Survey on Network Embedding	[18]	['[18]  C. Li, J. Ma, X. Guo, and Q. Mei, “Deepcas: An end-to-end predictor of information cascades,” in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 577–586. ']
933	24	[375]	Therefore, in the problems where advanced information is available, it is natural to exploit deep models to come up with an end-to-end network embedding solution. // For instance, some deep model based end-to-end solutions are proposed for cascade prediction [18] and network alignment ==[22]==. // The network embedding models are not limited to those mentioned in this subsection.	b	A Survey on Network Embedding	[22]	['[22]  T. Man, H. Shen, S. Liu, X. Jin, and X. Cheng, “Predict anchor links across social networks via an embedding approach,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 1823–1829. ']
934	24	[376]	NETWORK EMBEDDING VERSUS GRAPH EMBEDDING. // The goal of graph embedding is similar as network embedding, that is, to embed a graph into a low-dimensional vector space ==[27]==. // There is a rich literature in graph embedding.	b	A Survey on Network Embedding	[27]	['[27]  S. Yan, D. Xu, B. Zhang, and H.-J. Zhang, “Graph embedding: A general framework for dimensionality reduction,” in Proc. IEEE Comput. Soc. Conf. Comput. Vision Pattern Recognit., 2005, vol. 2, pp. 830–837. ']
935	24	[368]	There is a rich literature in graph embedding. // Fu and Ma ==[9]== provide a thorough review on the traditional graph embedding methods. // Here we only present some representative and classical methods on graph embedding, aiming to demonstrate the critical differences between graph embedding and the current network embedding.	b	A Survey on Network Embedding	[9]	['[9]  Y. Fu and Y. Ma, Graph Embedding for Pattern Analysis. New York, NY, USA: Springer Science & Business Media, 2012. ']
936	24	[3]	Graph embedding methods are originally studied as dimensionreduction techniques. // A graph is usually constructed from a feature represented data set, like image data set. Isomap ==[28]== first constructs a neighborhood graph G using connectivity algorithms such as K nearest neighbors (KNN), i.e., connecting data entries i and j if i is one of the K nearest neighbors of j. // Then based on G, the shortest path dG ij of entries i and j in G can be computed. Consequently, for all the N data entries in the data set, we have the matrix of graph distances DG ¼ fdG ijg.	b	A Survey on Network Embedding	[28]	['[28]  J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” Sci., vol. 290, no. 5500, pp. 2319–2323, 2000. ']
937	24	[4]	Indeed, Isomap learns the representation ui of entry i, which approximately preserves the geodesic distances of the entry pairs in the low-dimensional space. // The key problem of Isomap is its high complexity due to the computing of pair-wise shortest pathes. Locally linear embedding (LLE) ==[29]== is proposed to eliminate the need to estimate the pairwise distances between widely separated entries. // LLE assumes that each entry and its neighbors lie on or close to a locally linear patch of a mainfold. T	b	A Survey on Network Embedding	[29]	['[29]  S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by locally linear embedding,” Sci., vol. 290, no. 5500, pp. 2323– 2326, 2000. ']
938	24	[5]	By optimizing the above function, the low-dimensional representation matrix U, which preserves the neighborhood structure, can be obtained. // Laplacian eigenmaps (LE) ==[30]== also begins with constructing a graph using -neighborhoods or K nearest neighbors. // Then the heat kernel [31] is utilized to choose the weight Wij of nodes i and j in the graph.	b	A Survey on Network Embedding	[30]	['[30]  M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in Proc. Adv. Neural Inf. Process. Syst., 2002, pp. 585–591. ']
939	24	[368]	(LE) [30] also begins with constructing a graph using -neighborhoods or K nearest neighbors. // These methods are extended in the rich literature of graph embedding by considering different characteristics of the constructed graphs ==[9]==. // Major Differences	b	A Survey on Network Embedding	[9]	['[9]  Y. Fu and Y. Ma, Graph Embedding for Pattern Analysis. New York, NY, USA: Springer Science & Business Media, 2012. ']
940	24	[377]	// Furthermore, the locality preserving projection (LPP) ==[32]==, a linear approximation of the nonlinear LE, is proposed. // Also, it introduces a transformation matrix A such that the representation ui of entry xi is ui ¼ AT xi.	b	A Survey on Network Embedding	[32]	['[32]  X. He and P. Niyogi, “Locality preserving projections,” in Proc. Adv. Neural Inf. Process. Syst., 2004, pp. 153–160. ']
941	24	[36]	DeepWalk discovers that the distribution of nodes appearing in short random walks is similar to the distribution of words in natural language. // Motivated by this observation, Skip-Gram model ==[24]==, a widely used word representation learning model, is adopted by DeepWalk to learn the representations of nodes. // Specifically, as shown in Fig.	b	A Survey on Network Embedding	[24]	['[24]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119. ']
942	24	[6]	The commonly exploited network structures in network embedding include neighborhood structure, high-order node proximity and network communities. // DeepWalk ==[3]== is proposed for learning the representations of nodes in a network, which is able to preserve the neighbor structures of nodes. // DeepWalk discovers that the distribution of nodes appearing in short random walks is similar to the distribution of words in natural language.	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
943	24	[35]	where w is the window size, fðviÞ is the current representation of vi and fviw; :::; viþwgnvi is the local context nodes of vi. // Finally, hierarchical soft-max ==[33]== is used to efficiently infer the embeddings. // Node2vec [25] demonstrates that DeepWalk is not expressive enough to capture the diversity of connectivity patterns in a network.	h+	A Survey on Network Embedding	[33]	['[33]  T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” in Proc. ICLR Workshops Track, 2013. ']
944	24	[37]	Finally, hierarchical soft-max [33] is used to efficiently infer the embeddings.  // Node2vec ==[25]== demonstrates that DeepWalk is not expressive enough to capture the diversity of connectivity patterns in a network. // Node2vec defines a flexible notion of a node’s network neighborhood and designs a second order random walk strategy to sample the neighborhood nodes, which can smoothly interpolate between breadth-first sampling (BFS) and depth-first sampling (DFS).	ho	A Survey on Network Embedding	[25]	['[25]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
945	24	[1]	Node2vec is able to learn the representations that embed nodes with same network community closely, and to learn representations where nodes sharing similar roles have similar embeddings. // LINE ==[10]== is proposed for large scale network embedding, and can preserve the first and second order proximities. // The first order proximity is the observed pairwise proximity between two nodes, such as the observed edge between nodes 6 and 7 in Fig. 5.	b	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
946	24	[26]	By minimizing the KL-divergence of the two distributions and the empirical distributions respectively, the representations of nodes that are able to preserve the first and second order proximities can be obtained. // Considering that LINE only preserves the first-order and second-order proximities, GraRep ==[34]== demonstrates that k-step (k > 2) proximities should also be captured when constructing the global representations of nodes. // Given the adjacency matrix A, the k-step probability transition matrix can be computed by Ak ¼ A:::A |fflffl{zfflffl} k , whose element Ak ij refers to the transition probability pkðjjiÞ from a current node i to a context node j and the transition consists of k steps.	b	A Survey on Network Embedding	[34]	['[34]  S. Cao, W. Lu, and Q. Xu, “Grarep: Learning graph representations with global structural information,” in Proc. 24th ACM Int. Conf. Inf. Knowl. Manage, 2015, pp. 891–900. ']
947	24	[36]	Given the adjacency matrix A, the k-step probability transition matrix can be computed by Ak ¼ |Afflffl{::z:Afflffl} k, whose element Ak ij refers to the transition probability pkðjjiÞ from a current node i to a context node j and the transition consists of k steps. // Moreover, motivated by the Skip-Gram model ==[24]==, the k-step loss function of node i is defined aswhere sðxÞ¼ð1 þ exÞ 1 , pkðV Þ is the distribution over the nodes in the network and j0 is the node obtained from negative sampling. // Furthermore, GraRep reformulates the loss function as the matrix factorization problem, for each k-step loss function, SVD can be directly used to infer the representations of nodes.	b	A Survey on Network Embedding	[24]	['[24]  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119. ']
948	24	[30]	By concentrating the representations learned from each function, the global representations can be obtained. // Wang et al. ==[4]== propose a modularized nonnegative matrix factorization (M-NMF) model for network embedding, which aims to preserve both the microscopic structure, i.e., the first-order and second-order proximities of nodes, and the mesoscopic community structure [35]. // To preserve the microscopic structure, they adopt the NMFmodel [36] to factorize the pairwise node similarity matrix and learn the representations of nodes.	b	A Survey on Network Embedding	[4]	['[4]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proc. 31th AAAI Conf. Artif. Intell., AAAI Press, 2017, pp. 203–209. ']
949	24	[27]	By concentrating the representations learned from each function, the global representations can be obtained. // Wang et al. [4] propose a modularized nonnegative matrix factorization (M-NMF) model for network embedding, which aims to preserve both the microscopic structure, i.e., the first-order and second-order proximities of nodes, and the mesoscopic community structure ==[35]==. // To preserve the microscopic structure, they adopt the NMFmodel [36] to factorize the pairwise node similarity matrix and learn the representations of nodes.	b	A Survey on Network Embedding	[35]	['[35]  M. Girvan and M. E. Newman, “Community structure in social and biological networks,” Proc. Nat. Academy Sci. USA, vol. 99, no. 12, pp. 7821–7826, 2002. ']
950	24	[378]	Wang et al. [4] propose a modularized nonnegative matrix factorization (M-NMF) model for network embedding, which aims to preserve both the microscopic structure, i.e., the first-order and second-order proximities of nodes, and the mesoscopic community structure [35]. // To preserve the microscopic structure, they adopt the NMFmodel ==[36]== to factorize the pairwise node similarity matrix and learn the representations of nodes. // Meanwhile, the community structure is detected by modularity maximization [37].	b	A Survey on Network Embedding	[36]	['[36]  D. D. Lee and H. S. Seung, “Algorithms for non-negative matrix factorization,” in Proc. Adv. Neural Inf. Process. Syst., 2001, pp. 556– 562. ']
951	24	[379]	To preserve the microscopic structure, they adopt the NMFmodel [36] to factorize the pairwise node similarity matrix and learn the representations of nodes. // Meanwhile, the community structure is detected by modularity maximization ==[37]==. // Then, based on the assumption that if the representation of a node is similar to that of a community, the node may have a high propensity to be in this community, they introduce an auxiliary community representation matrix to bridge the representations of nodes with the community structure.	b	A Survey on Network Embedding	[37]	['[37]  M. E. Newman, “Finding community structure in networks using the eigenvectors of matrices,” Phys. Rev. E, vol. 74, no. 3, 2006, Art. no. 036104. ']
952	24	[19]	The aforementioned methods mainly adopt the shallow models, consequently, the representation ability is limited. // SDNE ==[6]== proposes a deep model for network embedding, so as to address the high non-linearity, structure-preserving, and sparsity issues. // The framework is shown in Fig. 6.	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
953	24	[5]	To impose more penalty to the reconstruction error of the non-zero elements than that of zero elements, SDNE introduces the penalty vector bi ¼ fbijgn j¼1 (bij is larger than a threshold if there is an edge between nodes i and j) and gives rise to the following function that can preserve the second-order proximity L2nd ¼X i kð^xi  xiÞ  bik2: // To preserve the first-order proximity of nodes, the idea of Laplacian eigenmaps ==[30]== is adopted. // By exploiting the firstorder and second-order proximities jointly into the learning process, the representations of nodes can be finally obtained.	b	A Survey on Network Embedding	[30]	['[30]  M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in Proc. Adv. Neural Inf. Process. Syst., 2002, pp. 585–591. ']
954	24	[9]	Laplacian eigenmaps [30] is adopted. By exploiting the firstorder and second-order proximities jointly into the learning process, the representations of nodes can be finally obtained. // Cao et al. ==[26]== propose a network embedding method to capture the weighted graph structure and represent nodes of non-linear structures. // As shown in Fig. 7, instead of adopting the previous sampling strategy that needs to determine certain hyper parameters, they considers a random surfing model motivated by the PageRank model.	b	A Survey on Network Embedding	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 1145–1152. ']
955	24	[160]	As shown in Fig. 7, instead of adopting the previous sampling strategy that needs to determine certain hyper parameters, they considers a random surfing model motivated by the PageRank model. Based on this random surfing model, the representation of a node can be initiatively constructed by combining the weighted transition probability matrix. // After that, the PPMI matrix ==[38]== can be computed. // Finally, the stacked denoisingautoencoders [39] that partially corrupt the input data before taking the training step are applied to learn the latent representations.	b	A Survey on Network Embedding	[38]	['[38]  O. Levy and Y. Goldberg, “Neural word embedding as implicit matrix factorization,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2177–2185. ']
956	24	[47]	After that, the PPMI matrix [38] can be computed. // Finally, the stacked denoisingautoencoders ==[39]== that partially corrupt the input data before taking the training step are applied to learn the latent representations. // In order to make a general framework on network embedding, Chen et al. [40] propose a network embedding framework that unifies some of the previous algorithms, such as LE, DeepWalk and Node2vec.	b	A Survey on Network Embedding	[39]	['[39]  P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,” J. Mach. Learn. Res., vol. 11, no. Dec, pp. 3371–3408, 2010. ']
957	24	[380]	Finally, the stacked denoisingautoencoders [39] that partially corrupt the input data before taking the training step are applied to learn the latent representations. // In order to make a general framework on network embedding, Chen et al. ==[40]== propose a network embedding framework that unifies some of the previous algorithms, such as LE, DeepWalk and Node2vec. // The proposed framework, denoted by GEM-D ½hðÞ; gðÞ; dð; Þ, involves three important building blocks: hðÞ is a node proximity function based on the adjacency matrix; gðÞ is a warping function that warps the inner products of network embeddings; and dð; Þ measures the differences between h and g.	b	A Survey on Network Embedding	[40]	['[40]  S. Chen, S. Niu, L. Akoglu, J. Kovacevi c, and C. Faloutsos, “Fast, warped graph embedding: Unifying framework and one-click algorithm,” arXiv preprint arXiv:1702.05764, 2017. ']
958	24	[381]	Therefore, the representations of these subgraphs can be learned and the similarities of different networks can be captured. // PATCHY-SAN ==[42]== is proposed to learn the embedding for a whole graph based on convolutional neural network (CNN) [43], so as to deal with the whole graph related tasks. // In order to make the traditional CNN compatible with the network data, they elaborately design several network data preprocessing steps, such as node sequence selection and graph normalization. In this way, the network topology can be transformed to the formation for CNN.	b	A Survey on Network Embedding	[42]	['[42]  M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural networks for graphs,” in Proc. Int. Conf. Mach. Learn., 2016, pp. 2014–2023. ']
959	24	[222]	Therefore, the representations of these subgraphs can be learned and the similarities of different networks can be captured. // PATCHY-SAN [42] is proposed to learn the embedding for a whole graph based on convolutional neural network (CNN) ==[43]==, so as to deal with the whole graph related tasks. // In order to make the traditional CNN compatible with the network data, they elaborately design several network data preprocessing steps, such as node sequence selection and graph normalization. In this way, the network topology can be transformed to the formation for CNN.	b	A Survey on Network Embedding	[43]	['[43]  Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, 2015, Art. no. 436. ']
960	24	[382]	Specifically, most of the existing property preserving network embedding methods focus on network transitivity in all types of networks and the structural balance property in signed networks. // Ou et al. ==[44]== aim to preserve the non-transitivity property via latent similarity components. // The non-transitivity property declares that, for nodes A, B and C in a network where ðA; BÞ and ðB; CÞ are similar pairs, ðA; CÞ may be a dissimilar pair.	b	A Survey on Network Embedding	[44]	['[44]  M. Ou, P. Cui, F. Wang, J. Wang, and W. Zhu, “Non-transitive hashing with latent similarity components,” in Proc. 21th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2015, pp. 895– 904. ']
961	24	[48]	Finally they approximate the aggregated similarity to the semantic similarity based on the observation that if two nodes have a large semantic similarity, at least one of the similarities Sm ij from the hash tables is large, otherwise, all of the similarities are small. // Preserving the asymmetric transitivity property of directed network is considered by HOPE ==[8]==. // Asymmetric transitivity indicates that, if there is a directed edge from node i to node j and a directed edge from j to v, there is likely a directed edge from i to v, but not from v to i.	b	A Survey on Network Embedding	[8]	['[8]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 672–681. ']
962	24	[49]	Asymmetric transitivity indicates that, if there is a directed edge from node i to node j and a directed edge from j to v, there is likely a directed edge from i to v, but not from v to i. // In order to measure this high-order proximity, HOPE summarizes four measurements in a general formulation, that is, Katz Index ==[45]==, Rooted PageRank [7], Common Neighbors [7], and Adamic-Adar [46]. // With the high-order proximity, SVD can be directly applied to obtain the low dimensional representations.	b	A Survey on Network Embedding	[45]	['[45]  L. Katz, “A new status index derived from sociometric analysis,” Psychometrika, vol. 18, no. 1, pp. 39–43, 1953. ']
963	24	[70]	Asymmetric transitivity indicates that, if there is a directed edge from node i to node j and a directed edge from j to v, there is likely a directed edge from i to v, but not from v to i. // In order to measure this high-order proximity, HOPE summarizes four measurements in a general formulation, that is, Katz Index [45], Rooted PageRank ==[7]==, Common Neighbors ==[7]==, and Adamic-Adar [46]. // With the high-order proximity, SVD can be directly applied to obtain the low dimensional representations.	b	A Survey on Network Embedding	[7]	['[7]  D. Liben-Nowell and J. Kleinberg, “The link-prediction problem for social networks,” J. Assoc. Inf. Sci. Technol., vol. 58, no. 7, pp. 1019–1031, 2007. ']
964	24	[70]	Asymmetric transitivity indicates that, if there is a directed edge from node i to node j and a directed edge from j to v, there is likely a directed edge from i to v, but not from v to i. // In order to measure this high-order proximity, HOPE summarizes four measurements in a general formulation, that is, Katz Index [45], Rooted PageRank ==[7]==, Common Neighbors ==[7]==, and Adamic-Adar [46]. // With the high-order proximity, SVD can be directly applied to obtain the low dimensional representations.	b	A Survey on Network Embedding	[7]	['[7]  D. Liben-Nowell and J. Kleinberg, “The link-prediction problem for social networks,” J. Assoc. Inf. Sci. Technol., vol. 58, no. 7, pp. 1019–1031, 2007. ']
965	24	[185]	Asymmetric transitivity indicates that, if there is a directed edge from node i to node j and a directed edge from j to v, there is likely a directed edge from i to v, but not from v to i. // In order to measure this high-order proximity, HOPE summarizes four measurements in a general formulation, that is, Katz Index [45], Rooted PageRank [7], Common Neighbors [7], and Adamic-Adar ==[46]==. // With the high-order proximity, SVD can be directly applied to obtain the low dimensional representations.	b	A Survey on Network Embedding	[46]	['[46]  L. A. Adamic and E. Adar, “Friends and neighbors on the web,” Soc. Netw., vol. 25, no. 3, pp. 211–230, 2003. ']
966	24	[260]	With the high-order proximity, SVD can be directly applied to obtain the low dimensional representations. // Furthermore, the general formulation of high-order proximity enables HOPE to transform the original SVD problem into a generalized SVD problem ==[47]==, such that the time complexity of HOPE is largely reduced, which means HOPE is scalable for large scale networks. // SiNE [13] is proposed for signed network embedding, which considers both positive and negative edges in a network.	b	A Survey on Network Embedding	[47]	['[47]  C. C. Paige and M. A. Saunders, “Towards a generalized singular value decomposition,” SIAM J. Numerical Anal., vol. 18, no. 3, pp. 398–405, 1981. ']
967	24	[118]	Furthermore, the general formulation of high-order proximity enables HOPE to transform the original SVD problem into a generalized SVD problem [47], such that the time complexity of HOPE is largely reduced, which means HOPE is scalable for large scale networks. // SiNE ==[13]== is proposed for signed network embedding, which considers both positive and negative edges in a network. // Due to the negative edges, the social theories on signed network, such as structural balance theory [12], [48], are very different from the unsigned network.	b	A Survey on Network Embedding	[13]	['[13]  S. Wang, J. Tang, C. Aggarwal, Y. Chang, and H. Liu, “Signed network embedding in social media,” in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 327–335. ']
968	24	[370]	SiNE [13] is proposed for signed network embedding, which considers both positive and negative edges in a network. // Due to the negative edges, the social theories on signed network, such as structural balance theory ==[12]==, [48], are very different from the unsigned network. // The structural balance theory demonstrates that users in a signed social network should be able to have their “friends” closer than their “foes”.	b	A Survey on Network Embedding	[12]	['[12]  D. Cartwright and F. Harary, “Structural balance: A generalization of heider’s theory,” Psychological Rev., vol. 63, no. 5, 1956, Art. no. 277. ']
969	24	[383]	SiNE [13] is proposed for signed network embedding, which considers both positive and negative edges in a network. // Due to the negative edges, the social theories on signed network, such as structural balance theory [12], ==[48]==, are very different from the unsigned network. // The structural balance theory demonstrates that users in a signed social network should be able to have their “friends” closer than their “foes”.	b	A Survey on Network Embedding	[48]	['[48]  M. Cygan, M. Pilipczuk, M. Pilipczuk, and J. O. Wojtaszczyk, “Sitting closer to friends than enemies, revisited,” Theory Comput. Syst., vol. 56, no. 2, pp. 394–405, 2015. ']
970	24	[62]	How to combine them with the network topology in network embedding arouses considerable research interests. // Tu et al. ==[14]== propose a semi-supervised network embedding algorithm, MMDW, by leveraging labeling information of nodes. MMDW is also based on the DeepWalk-derived matrix factorization. // MMDW is also based on the DeepWalk-derived matrix factorization.	b	A Survey on Network Embedding	[14]	['[14]  C. Tu, W. Zhang, Z. Liu, and M. Sun, “Max-margin deepwalk: Discriminative learning of network representation,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 3889–3895. ']
971	24	[24]	MMDW is also based on the DeepWalk-derivedmatrix factorization. // MMDW adopts support vector machines (SVM) ==[49]== and incorporates the label information to find an optimal classifying boundary. // By optimizing the max-margin classifier of SVM and matrix factorization based DeepWalk simultaneously, the representations of nodes that have more discriminative ability can be learned.	b	A Survey on Network Embedding	[49]	['[49]  M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf, “Support vector machines,” IEEE Intell. Syst. Appl., vol. 13, no. 4, pp. 18–28, July-Aug. 1988. ']
972	24	[274]	By optimizing the max-margin classifier of SVM and matrix factorization based DeepWalk simultaneously, the representations of nodes that have more discriminative ability can be learned. // Le et al. ==[50]== propose a generative model for document network embedding, where the words associated with each documents and the relationships between documents are both considered. // For each node, they learn its low-rank representation ui in a low dimensional vector space, which can reconstruct the network structure.	b	A Survey on Network Embedding	[50]	['[50]  T. M. Le and H. W. Lauw, “Probabilistic latent document network embedding,” in Proc. IEEE Int. Conf. Data Mining, 2014, pp. 270– 279. ']
973	24	[319]	For each node, they learn its low-rankrepresentation ui in a low dimensional vector space, which can reconstruct the network structure. // Also, they learn the representation of nodes in the topic space based on the Relational Topic Model (RTM) ==[51]==, where each topic z is associated with a probability distribution over words. // To integrate the two aspects, they associate each topic z with a representation z in the same low dimensional vector space and then have the following function:PðzjviÞ ¼expð 12 kui  ’zk2 P Þz expð 12 kui  ’zk2Þ: (11) Finally, in a unified generative process, the representations of nodes U can be learned.	b	A Survey on Network Embedding	[51]	['[51]  J. Chang and D. M. Blei, “Relational topic models for document networks,” in Proc. Int. Conf. Artif. Intell. Statist., 2009, pp. 81–88. ']
974	24	[7]	Finally, in a unified generative process, the representations of nodes U can be learned. // Besides network structures, Yang et al. ==[15]== propose TADW that takes the rich information (e.g., text) associated with nodes into account when they learn the low dimensional representations of nodes. // DeepWalk is equivalent to factorizing the matrix M whose element Mij ¼ log ð½eiðA þ A2 þ ::: þ AtÞj=tÞ, where A is the adjacency matrix, t denotes the t steps in a random walk and ei is a row vector where all entries are 0 except the i-th entry is 1.	b	A Survey on Network Embedding	[15]	['[15]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang, “Network representation learning with rich text information,” in Proc. 24th Int. Joint Conf. Artif. Intell., 2015, pp. 2111–2117. ']
975	24	[34]	DeepWalk is equivalent to factorizing the matrix M whose element Mij ¼ log ð½eiðA þ A2 þ ::: þ AtÞj=tÞ, where A is the adjacency matrix, t denotes the t steps in a random walk and ei is a row vector where all entries are 0 except the i-th entry is 1. // Then, based on the DeepWalk-derived matrix factorization and motivated by the inductive matrix completion ==[17]==, they incorporate rich text information T into network embedding as follows: min W;H kM WTHTk2 F þ 2 ðkWk2 F þ kHk2 F Þ: (12) Finally, they concatenate the optimal W and HT as the representations of nodes. // TADW suffers from high computational cost and the node attributes just simply incorporated as unordered features lose the much semantic information.	b	A Survey on Network Embedding	[17]	['[17]  N. Natarajan and I. S. Dhillon, “Inductive matrix completion for predicting gene–disease associations,” Bioinf., vol. 30, no. 12, pp. i60–i68, 2014. ']
976	24	[384]	TADW suffers from high computational cost and the node attributes just simply incorporated as unordered features lose the much semantic information. // Sun et al. ==[52]== consider the content as a special kind of nodes, and give rise to an augmented network, as shown in Fig. 9. //  With this augmented network, they are able to model the node-node links and node-content links in the latent vector space.	b	A Survey on Network Embedding	[52]	['[52]  X. Sun, J. Guo, X. Ding, and T. Liu, “A general framework for content-enhanced network representation learning,” arXiv:1610.02906, 2016. ']
977	24	[41]	They use a logistic function to model the relationship in the new augmented network, and by combining with negative sampling, they can learn the representations of nodes in a joint objective function, such that the representations can preserve the network structure as well as the relationship between the node and content. // Pan et al. ==[53]== propose a coupled deep model that incorporates network structure, node attributes and node labels into network embedding. // The architecture of the proposed model is shown in Fig. 10.	b	A Survey on Network Embedding	[53]	['[53]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” Netw., vol. 11, no. 9, 2016, Art. no. 12. ']
978	24	[24]	As a result, the learned representations is enhanced by network structure, node content, and node labels. // LANE ==[54]== is also proposed to incorporate the label information into the attributed network embedding. // LANE is mainly based on spectral techniques [55]. LANE adopts the cosine similarity to construct the corresponding affinity matrices of the node attributes, network structure, and labels.	b	A Survey on Network Embedding	[54]	['[54]  X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 731–739. 850 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 31, NO. 5, MAY 2019 ']
979	24	[292]	LANE [54] is also proposed to incorporate the label information into the attributed network embedding. // LANE is mainly based on spectral techniques ==[55]==. LANE adopts the cosine similarity to construct the corresponding affinity matrices of the node attributes, network structure, and labels. // Then, based on the corresponding Laplacian matrices, LANE is able to map the three different sources into different latent representations, respectively.	b	A Survey on Network Embedding	[55]	['[55]  F. R. K. Chung and F. C. Graham, “Spectral graph theory,” American Mathematical Soc., no. 92, 1997. ']
980	24	[385]	The learned representations of nodes are able to capture the structure proximities as well as the correlations in the label informed attributed network. // Huang et al. ==[56]== pay more attentions on the scalability of attributed network embedding. // The proposed method, named AANE, is based on the decomposition of attribute affinity matrix and the penalty of embedding difference between linked nodes.	b	A Survey on Network Embedding	[56]	['[56]  X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 633– 641. ']
981	24	[386]	AANE provides a distributed optimization algorithm to process each node efficiently. // Wei et al. ==[57]== study the problem of cross view link prediction (CVLP) based on attributed network embedding, i.e., to recommend nodes with only links to nodes with only attributes (or vice versa). // The proposed model learns the link-based and attribute-based representations, and utilize the consensus to establish the relations between them.	b	A Survey on Network Embedding	[57]	['[57]  X. Wei, L. Xu, B. Cao, and P. S. Yu, “Cross view link prediction by learning noise-resilient representation consensus,” in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 1611–1619. ']
982	24	[298]	Howto unify the heterogeneous types of nodes and links in network embedding is also an interesting and challenging problem. // Yann et al. ==[58]== propose a heterogeneous social network embedding algorithm for classifying nodes. // They learn the representations of all types of nodes in a common vector space, and perform the inference in this space.	b	A Survey on Network Embedding	[58]	['[58]  Y. Jacob, L. Denoyer, and P. Gallinari, “Learning latent representations of nodes for classifying in heterogeneous social networks,” in Proc. 7th ACM Int. Conf. Web Search Data Mining, 2014, pp. 373– 382. ']
983	24	[109]	A stochastic gradient descent method is used here to learn the representations of nodes in a heterogeneous network for classifying. // Chang et al. ==[16]== propose a deep embedding algorithm for heterogeneous networks, whose nodes have various types. // The main goal of the heterogeneous network embedding is to learn the representations of nodes with different types such that the heterogeneous network structure can be well preserved.	b	A Survey on Network Embedding	[16]	['[16]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang, “Heterogeneous network embedding via deep architectures,” in Proc. 21th ACM SIGKDD Int. Conf. Knowl Discovery Data Mining, 2015, pp. 119–128. ']
984	24	[110]	In the common space, the similarities between data from different modalities can be directly measured, so that if there is an edge in the original heterogeneous network, the pair of data has similar representations. // Huang and Mamoulis ==[59]== propose a meta path similarity preserving heterogeneous information network embedding algorithm. // To model a particular relationship, a meta path [60] is a sequence of object types with edge types in between. They develop a fast dynamic programming approach to calculate the truncated meta path based proximities, whose time complexity is linear to the size of the network.	b	A Survey on Network Embedding	[59]	['[59]  Z. Huang and N. Mamoulis, “Heterogeneous information network embedding for meta path based proximity,” arXiv: 1701.05291, 2017. ']
985	24	[224]	uang and Mamoulis [59] propose a meta path similarity preserving heterogeneous information network embedding algorithm. // To model a particular relationship, a meta path ==[60]== is a sequence of object types with edge types in between. // They develop a fast dynamic programming approach to calculate the truncated meta path based proximities, whose time complexity is linear to the size of the network.	b	A Survey on Network Embedding	[60]	['[60]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu, “Pathsim: Meta pathbased top-k similarity search in heterogeneous information networks,” Proc. VLDB Endowment, vol. 4, no. 11, pp. 992–1003, 2011. ']
986	24	[1]	They develop a fast dynamic programming approach to calculate the truncated meta path based proximities, whose time complexity is linear to the size of the network. // They adopt a similar strategy as LINE ==[10]== to preserve the proximity in the low dimensional space. // Xu et al. [61] propose a network embedding method for coupled heterogeneous network. The coupled heterogeneous network consists of two different but related homogeneous networks.	b	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
987	24	[387]	They adopt a similar strategy as LINE [10] to preserve the proximity in the low dimensional space. // Xu et al. ==[61]== propose a network embedding method for coupled heterogeneous network. The coupled heterogeneous network consists of two different but related homogeneous networks. // The coupled heterogeneous network consists of two different but related homogeneous networks.	b	A Survey on Network Embedding	[61]	['[61]  L. Xu, X. Wei, J. Cao, and P. S. Yu, “Embedding of embedding (eoe): Joint embedding for coupled heterogeneous networks,” in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 741– 749. ']
988	24	[388]	Information Diffusion. // Information diffusion ==[62]== is an ubiquitous phenomenon on the web, especially in social networks. // Many real applications, such as marketing, public opinion formation, epidemics, are related to information diffusion.	b	A Survey on Network Embedding	[62]	['[62]  A. Guille, H. Hacid, C. Favre, and D. A. Zighed, “Information diffusion in online social networks: A survey,” ACM SIGMOD Record, vol. 42, no. 2, pp. 17–28, 2013. ']
989	24	[297]	Most of the previous studies on information diffusion are conducted in original network spaces. // Recently, Simon et al. ==[63]== propose a social network embedding algorithm for predicting information diffusion. // The basic idea is to map the observed information diffusion process into a heat diffusion process modeled by a diffusion kernel in the continuous space.	b	A Survey on Network Embedding	[63]	['[63]  S. Bourigault, C. Lagnier, S. Lamprier, L. Denoyer, and P. Gallinari, “Learning social network embeddings for predicting information diffusion,” in Proc. 7th ACM Int. Conf. Web search Data Mining, 2014, pp. 393–402. ']
990	24	[371]	By minimizing the Eq. (18) and reformulating it as a ranking problem, the optimal representations U of nodes can be obtained. // The cascade prediction problem here is defined as predicting the increment of cascade size after a given time interval ==[18]==. // Li et al. [18] argue that the previous work on cascade prediction all depends on the bag of hand-crafting features to represent the cascade and network structures. Instead, they present an end-to-end deep learning model to solve this problem using the idea of network embedding, as illustrated in Fig. 12.	b	A Survey on Network Embedding	[18]	['[18]  C. Li, J. Ma, X. Guo, and Q. Mei, “Deepcas: An end-to-end predictor of information cascades,” in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 577–586. ']
991	24	[371]	The cascade prediction problem here is defined as predicting the increment of cascade size after a given time interval [18]. // Li et al. ==[18]== argue that the previous work on cascade prediction all depends on the bag of hand-crafting features to represent the cascade and network structures. // Instead, they present an end-to-end deep learning model to solve this problem using the idea of network embedding, as illustrated in Fig. 12.	b	A Survey on Network Embedding	[18]	['[18]  C. Li, J. Ma, X. Guo, and Q. Mei, “Deepcas: An end-to-end predictor of information cascades,” in Proc. 26th Int. Conf. World Wide Web, 2017, pp. 577–586. ']
992	24	[6]	Instead, they present an end-to-end deep learning model to solve this problem using the idea of network embedding, as illustrated in Fig. // Similar to DeepWalk ==[3]==, they perform a random walk over a cascade graph to sample a set of paths. // Then the Gated Recurrent Unite (GRU) [64], a specific type of recurrent neural network [65], is applied to these paths and learn the embeddings for these paths. The attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph.	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
993	24	[389]	Similar to DeepWalk [3], they perform a random walk over a cascade graph to sample a set of paths. // Then the Gated Recurrent Unite (GRU) ==[64]==, a specific type of recurrent neural network [65], is applied to these paths and learn the embeddings for these paths. // The attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph.	b	A Survey on Network Embedding	[64]	['[64]  S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997. ']
994	24	[390]	Similar to DeepWalk [3], they perform a random walk over a cascade graph to sample a set of paths. // Then the Gated Recurrent Unite (GRU) [64], a specific type of recurrent neural network ==[65]==, is applied to these paths and learn the embeddings for these paths. // The attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph.	b	A Survey on Network Embedding	[65]	['[65]  T. Mikolov, M. Karafi at, L. Burget, J. Cernocky, and S. Khudanpur,   “Recurrent neural network based language model,” Interspeech, vol. 2, 2010, Art. no. 3. ']
995	24	[391]	The attention mechanism is then used to assemble these embeddings to learn the representation of this cascade graph. // Once the representation of this cascade is known, a multi-layer perceptron ==[66]== can be adopted to output the final predicted size of this cascade. // The whole procedure is able to learn the representation of cascade graph in an end-to-end manner.	b	A Survey on Network Embedding	[66]	['[66]  D. W. Ruck, S. K. Rogers, M. Kabrisky, M. E. Oxley, and B. W. Suter, “The multilayer perceptron as an approximation to a bayes optimal discriminant function,” IEEE Trans. Neural Netw., vol. 1, no. 4, pp. 296–298, Dec. 1990. ']
996	24	[392]	Anomaly Detection. // Anomaly detection has been widely investigated in previous work ==[67]==. // Anomaly detection in networks aims to infer the structural inconsistencies, which means the anomalous nodes that connect to various diverse influential communities [21], [68], such as the red node in Fig. 13.	b	A Survey on Network Embedding	[67]	['[67]  L. Akoglu, H. Tong, and D. Koutra, “Graph based anomaly detection and description: A survey,” Data Mining Knowl. Discovery, vol. 29, no. 3, pp. 626–688, 2015. ']
997	24	[374]	Anomaly detection has been widely investigated in previous work [67]. // Anomaly detection in networks aims to infer the structural inconsistencies, which means the anomalous nodes that connect to various diverse influential communities ==[21]==, [68], such as the red node in Fig. 13. // Hu et al. [21] propose a network embedding based method for anomaly detection.	b	A Survey on Network Embedding	[21]	['[21]  R. Hu, C. C. Aggarwal, S. Ma, and J. Huai, “An embedding approach to anomaly detection,” in Proc. IEEE 32nd Int. Conf. Data Eng., 2016, pp. 385–396. ']
998	24	[393]	Anomaly detection has been widely investigated in previous work [67]. // Anomaly detection in networks aims to infer the structural inconsistencies, which means the anomalous nodes that connect to various diverse influential communities [21], ==[68]==, such as the red node in Fig. 13. // Hu et al. [21] propose a network embedding based method for anomaly detection.	b	A Survey on Network Embedding	[68]	['[68]  R. S. Burt, “Structural holes and good ideas,” Amer. J. Sociology, vol. 110, no. 2, pp. 349–399, 2004. ']
999	24	[374]	Anomaly detection in networks aims to infer the structural inconsistencies, which means the anomalous nodes that connect to various diverse influential communities [21], [68], such as the red node in Fig. 13. // Hu et al. ==[21]== propose a network embedding based method for anomaly detection. // In particular, in the proposed model, the kth element uki in the embedding ui of node i represents the correlation between node i and community k.	b	A Survey on Network Embedding	[21]	['[21]  R. Hu, C. C. Aggarwal, S. Ma, and J. Huai, “An embedding approach to anomaly detection,” in Proc. IEEE 32nd Int. Conf. Data Eng., 2016, pp. 385–396. ']
1000	24	[375]	The goal of network alignment is to establish the correspondence between the nodes from two networks. // Man et al. ==[22]== propose a network embedding algorithm to predict the anchor links across social networks. // The same users who are shared by different social networks naturally form the anchor links, and these links bridge the different networks.	b	A Survey on Network Embedding	[22]	['[22]  T. Man, H. Shen, S. Liu, X. Jin, and X. Cheng, “Predict anchor links across social networks via an embedding approach,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 1823–1829. ']
1001	24	[375]	As illustrated in Fig. 14, the anchor link prediction problem is, given source network Gs and target network Gt and a set of observed anchor links T, to identify the hidden anchor links across Gs and Gt. // First, Man et al. ==[22]== extend the original sparse networks Gs and Gt to the denser networks. // The basic idea is that given a pair of users with anchor links, if they have a connection in one network, so do their counterparts in the other network [69], in this way, more links will be added to the original networks.	b	A Survey on Network Embedding	[22]	['[22]  T. Man, H. Shen, S. Liu, X. Jin, and X. Cheng, “Predict anchor links across social networks via an embedding approach,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 1823–1829. ']
1002	24	[394]	First, Man et al. [22] extend the original sparse networks Gs and Gt to the denser networks. // The basic idea is that given a pair of users with anchor links, if they have a connection in one network, so do their counterparts in the other network ==[69]==, in this way, more links will be added to the original networks. // For a pair of nodes i and j whose representations are ui and uj, respectively, by combining the negative sampling strategy, they use the following function to preserve the structures of Gs and Gt in a vector space: log sðuTi ujÞ þXKk¼1 Evk/PnðvÞ½log ð1  sðuTi ukÞÞ; (20) where sðxÞ ¼ 1=ð1 þ expðxÞÞ.	b	A Survey on Network Embedding	[69]	['[69]  M. Bayati, M. Gerritsen, D. F. Gleich, A. Saberi, and Y. Wang, “Algorithms for large, sparse network alignment problems,” in Proc. 9th IEEE Int. Conf. Data Mining, 2009, pp. 705–710. ']
1003	24	[391]	The loss function is defined as: kfðui; uÞ  ujkF : // The mapping function can be linear or non-linear via MultiLayer Perceptron (MLP) ==[66]==. // By optimizing Eq. (20) and Eq. (21) simultaneously, the representations that can preserve the network structure and respect the observed anchor links can be learned.	b	A Survey on Network Embedding	[66]	['[66]  D. W. Ruck, S. K. Rogers, M. Kabrisky, M. E. Oxley, and B. W. Suter, “The multilayer perceptron as an approximation to a bayes optimal discriminant function,” IEEE Trans. Neural Netw., vol. 1, no. 4, pp. 296–298, Dec. 1990. ']
1004	24	[33]	One instance of this data set can be found at http:socialcomputing.asu.edu/datasets/BlogCatalog3. // FLICKR ==[70]==. This is a network of the contacts between users of the photo sharing websites Flickr. // One instance of the network can be downloaded at http:socialcomputing.asu.edu/datasets/Flickr.	b	A Survey on Network Embedding	[70]	['[70]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2009, pp. 817–826. ']
1005	24	[56]	One instance of the network can be downloaded at http:socialcomputing.asu.edu/datasets/Flickr. // YOUTUBE ==[71]==. This is a network between users of the popular video sharing website, Youtube. // One instance of the network can be found at http:socialcomputing.asu.edu/datasets/YouTube2.	b	A Survey on Network Embedding	[71]	['[71]  L. tang and H. Liu, “Scalable learning of collective behavior based on sparse social dimensions,” in Proc. 18th ACM Conf. Inf. Knowl. Manage., 2009, pp. 1107–1116. ']
1006	24	[263]	One instance of the network can be found at http:socialcomputing.asu.edu/datasets/YouTube2. // Twitter ==[72]==. This is a network between users on a social news website Twitter. // One instance of the network can be downloaded at http:socialcomputing.asu.edu/datasets/Twitter.	b	A Survey on Network Embedding	[72]	['[72]  M. De Choudhury, Y.-R. Lin, H. Sundaram, K. S. Candan, L. Xie, A. Kelliher et al., “How does the data sampling strategy impact the discovery of information diffusion in social media?” ICWSM, vol. 10, pp. 34–41, 2010. ']
1007	24	[159]	Citation Networks. // DBLP ==[73]==. This network represents the citation relationships between authors and papers. // One instance of the data set can be found at http:arnetminer.org/citation.	b	A Survey on Network Embedding	[73]	['[73]  J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, “Arnetminer: extraction and mining of academic social networks,” in Proc. 14th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2008, pp. 990–998. ']
1008	24	[395]	One instance of the data set can be found at http:arnetminer.org/citation. // Cora ==[74]==. This network represents the citation relationships between scientific publications. // Besides the link information, each publication is also associated with a word vector indicating the absence/presence of the corresponding words from the dictionary.	b	A Survey on Network Embedding	[74]	['[74]  A. K. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating the construction of internet portals with machine learning,” Inf. Retrieval, vol. 3, no. 2, pp. 127–163, 2000. ']
1009	24	[395]	One instance of the data set can be found at https:linqs.soe.ucsc.edu/node/236. // Citeseer==[74]==. This network, similar to Cora, also consists of scientific publications and their citation relationships. // One instance of the data set can be downloaded at https:linqs.soe.ucsc.edu/node/236.	b	A Survey on Network Embedding	[74]	['[74]  A. K. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating the construction of internet portals with machine learning,” Inf. Retrieval, vol. 3, no. 2, pp. 127–163, 2000. ']
1010	24	[84]	One instance of the data set can be downloadedat https:linqs.soe.ucsc.edu/node/236. // ArXiv ==[75]==, [76]. This is the collaboration network constructed from the ArXiv website. // One instance of the data set can be found at http:snap.stanford.edu/data/ca-AstroPh.html.	b	A Survey on Network Embedding	[75]	['[75]  J. Leskovec, J. Kleinberg, and C. Faloutsos, “Graph evolution: Densification and shrinking diameters,” ACM Trans. Knowl. Discovery Data, vol. 1, no. 1, 2007, Art. no. 2. ']
1011	24	[184]	One instance of the data set can be downloadedat https:linqs.soe.ucsc.edu/node/236. // ArXiv [75], ==[76]==. This is the collaboration network constructed from the ArXiv website. // One instance of the data set can be found at http:snap.stanford.edu/data/ca-AstroPh.html.	b	A Survey on Network Embedding	[76]	['[76]  J. Leskovec and A. Krevl, “Snap datasets: Stanford large network dataset collection (2014),” 2016. ']
1012	24	[6]	Given some nodes with known labels in a network, the node classification problem is to classify the rest nodes into different classes. // Node classification is one of most primary applications for network embedding ==[3]==, [10]. // Essentially, node classification based on network embedding for can be divided into three steps.	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
1013	24	[1]	Given some nodes with known labels in a network, the node classification problem is to classify the rest nodes into different classes. // Node classification is one of most primary applications for network embedding [3], ==[10]==. // Essentially, node classification based on network embedding for can be divided into three steps.	b	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
1014	24	[61]	Then, the nodes with known labels are used as the training set. // Last, a classifier, such as Liblinear ==[79]==, is learned from the training set. // Using the trained classifier, we can infer the labels of the rest nodes.	b	A Survey on Network Embedding	[79]	['[79]  R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “Liblinear: A library for large linear classification,” J. Mach. Learn. Res., vol. 9, no. Aug, pp. 1871–1874, 2008. ']
1015	24	[33]	Using the trained classifier,we can infer the labels of the rest nodes. // The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 ==[70]==. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE [71]), citation networks (DBLP [73], Cora [74], and Citeseer [74]), language networks (Wikipedia [77]), and biological networks (PPI [78]).	b	A Survey on Network Embedding	[70]	['[70]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2009, pp. 817–826. ']
1016	24	[33]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG ==[70]==, FLICKR ==[70]==, and YOUTUBE [71]), citation networks (DBLP [73], Cora [74], and Citeseer [74]), language networks (Wikipedia [77]), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[70]	['[70]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2009, pp. 817–826. ']
1017	24	[33]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG ==[70]==, FLICKR ==[70]==, and YOUTUBE [71]), citation networks (DBLP [73], Cora [74], and Citeseer [74]), language networks (Wikipedia [77]), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[70]	['[70]  L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2009, pp. 817–826. ']
1018	24	[56]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE ==[71]==), citation networks (DBLP [73], Cora [74], and Citeseer [74]), language networks (Wikipedia [77]), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[71]	['[71]  L. tang and H. Liu, “Scalable learning of collective behavior based on sparse social dimensions,” in Proc. 18th ACM Conf. Inf. Knowl. Manage., 2009, pp. 1107–1116. ']
1019	24	[159]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE [71]), citation networks (DBLP ==[73]==, Cora [74], and Citeseer [74]), language networks (Wikipedia [77]), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[73]	['[73]  J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, “Arnetminer: extraction and mining of academic social networks,” in Proc. 14th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2008, pp. 990–998. ']
1020	24	[395]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE [71]), citation networks (DBLP [73], Cora ==[74]==, and Citeseer ==[74]==), language networks (Wikipedia [77]), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[74]	['[74]  A. K. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating the construction of internet portals with machine learning,” Inf. Retrieval, vol. 3, no. 2, pp. 127–163, 2000. ']
1021	24	[395]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE [71]), citation networks (DBLP [73], Cora ==[74]==, and Citeseer ==[74]==), language networks (Wikipedia [77]), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[74]	['[74]  A. K. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating the construction of internet portals with machine learning,” Inf. Retrieval, vol. 3, no. 2, pp. 127–163, 2000. ']
1022	24	[182]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE [71]), citation networks (DBLP [73], Cora [74], and Citeseer [74]), language networks (Wikipedia ==[77]==), and biological networks (PPI [78]). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[77]	['[77]  M. Mahoney, “Large text compression benchmark,” 2011, http:// www. mattmahoney.net/text/text.html ']
1023	24	[86]	The popularly used evaluation metrics for multi-label classification problem include Micro-F1 and Macro-F1 [70]. // The multi-label classification application has been successfully tested on four categories of data sets, namely social networks (BLOGCATALOG [70], FLICKR [70], and YOUTUBE [71]), citation networks (DBLP [73], Cora [74], and Citeseer [74]), language networks (Wikipedia [77]), and biological networks (PPI ==[78]==). // Specifically, a social network usually is a communication network among users on online platforms.	b	A Survey on Network Embedding	[78]	['[78]  B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. B€ahler, V. Wood, et al., “The biogrid interaction database: 2008 update,” Nucleic Acids Res., vol. 36, no. suppl_1, pp. D637–D640, 2007. ']
1024	24	[6]	Specifically, a social network usually is a communication network among users on online platforms. // DeepWalk ==[3]==, GraRep [34], SDNE [6], node2vec [25], and LANE [54] conduct classification on BLOGCATALOG to evaluate the performance. // Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54].	b	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
1025	24	[26]	Specifically, a social network usually is a communication network among users on online platforms. // DeepWalk [3], GraRep ==[34]==, SDNE [6], node2vec [25], and LANE [54] conduct classification on BLOGCATALOG to evaluate the performance. // Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54].	b	A Survey on Network Embedding	[34]	['[34]  S. Cao, W. Lu, and Q. Xu, “Grarep: Learning graph representations with global structural information,” in Proc. 24th ACM Int. Conf. Inf. Knowl. Manage, 2015, pp. 891–900. ']
1026	24	[19]	Specifically, a social network usually is a communication network among users on online platforms. // DeepWalk [3], GraRep [34], SDNE ==[6]==, node2vec [25], and LANE [54] conduct classification on BLOGCATALOG to evaluate the performance. // Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54].	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1027	24	[37]	Specifically, a social network usually is a communication network among users on online platforms. // DeepWalk [3], GraRep [34], SDNE [6], node2vec ==[25]==, and LANE [54] conduct classification on BLOGCATALOG to evaluate the performance. // Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54].	b	A Survey on Network Embedding	[25]	['[25]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1028	24	[24]	Specifically, a social network usually is a communication network among users on online platforms. // DeepWalk [3], GraRep [34], SDNE [6], node2vec [25], and LANE ==[54]== conduct classification on BLOGCATALOG to evaluate the performance. // Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54].	b	A Survey on Network Embedding	[54]	['[54]  X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 731–739. 850 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 31, NO. 5, MAY 2019 ']
1029	24	[6]	Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54]. // Some studies ==[3]==, [6], [10] apply their algorithms to the Youtube network, which also achieves promising classification results. // A citation network usually represents the citation relationships between authors or between papers.	h+	A Survey on Network Embedding	[3]	['[3]  B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning of social representations,” in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2014, pp. 701–710. ']
1030	24	[19]	Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54]. // Some studies [3], ==[6]==, [10] apply their algorithms to the Youtube network, which also achieves promising classification results. // A citation network usually represents the citation relationships between authors or between papers.	h+	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1031	24	[1]	Also, the classification performance on FLICKR has been assessed in [3], [6], [10], [54]. // Some studies [3], [6], ==[10]== apply their algorithms to the Youtube network, which also achieves promising classification results. // A citation network usually represents the citation relationships between authors or between papers.	h+	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
1032	24	[1]	A citation network usually represents the citation relationships between authors or between papers. // For example, ==[10]==, [53] use the DBLP network to test the classification performance. // Cora is used in [14], [15]. Citeseer is used in [14], [15], [53].	b	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
1033	24	[41]	A citation network usually represents the citation relationships between authors or between papers. // For example, [10], ==[53]== use the DBLP network to test the classification performance. // Cora is used in [14], [15]. Citeseer is used in [14], [15], [53].	b	A Survey on Network Embedding	[53]	['[53]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” Netw., vol. 11, no. 9, 2016, Art. no. 12. ']
1034	24	[396]	The Protein-Protein Interactions (PPI) is used in [25]. // Based on NUSWIDE ==[80]==, a heterogeneous network extracted from Flickr, Chang et al. [16] validated the superior classification performance of network embedding on heterogeneous networks. // To summarize, network embedding algorithms havebeen widely used on various networks and have been well demonstrated their effectiveness on node classification.	h+	A Survey on Network Embedding	[80]	['[80]  T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “Nuswide: A real-world web image database from national university of singapore,” in Proc. ACM Int. Conf. Image Video Retrieval, 2009, Art. no. 48. ']
1035	24	[109]	The Protein-Protein Interactions (PPI) is used in [25]. // Based on NUSWIDE [80], a heterogeneous network extracted from Flickr, Chang et al. ==[16]== validated the superior classification performance of network embedding on heterogeneous networks. // To summarize, network embedding algorithms havebeen widely used on various networks and have been well demonstrated their effectiveness on node classification.	h+	A Survey on Network Embedding	[16]	['[16]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang, “Heterogeneous network embedding via deep architectures,” in Proc. 21th ACM SIGKDD Int. Conf. Knowl Discovery Data Mining, 2015, pp. 119–128. ']
1036	24	[70]	Link Prediction. // Link prediction, as one of the most fundamental problems on network analysis, has received a considerable amount of attention ==[7]==, [82]. // It aims to estimate the likelihood of the existence of an edge between two nodes based on observed network structure [83].	b	A Survey on Network Embedding	[7]	['[7]  D. Liben-Nowell and J. Kleinberg, “The link-prediction problem for social networks,” J. Assoc. Inf. Sci. Technol., vol. 58, no. 7, pp. 1019–1031, 2007. ']
1037	24	[12]	Link Prediction. // Link prediction, as one of the most fundamental problems on network analysis, has received a considerable amount of attention [7], ==[82]==. // It aims to estimate the likelihood of the existence of an edge between two nodes based on observed network structure [83].	b	A Survey on Network Embedding	[82]	['[82]  L. Lu and T. Zhou, “Link prediction in complex networks: A € survey,” Physica A: Statist. Mech. Appl., vol. 390, no. 6, pp. 1150– 1170, 2011. ']
1038	24	[397]	Link prediction, as one of the most fundamental problems on network analysis, has received a considerable amount of attention [7], [82]. // It aims to estimate the likelihood of the existence of an edge between two nodes based on observed network structure ==[83]==. // Since network embedding algorithms are able to learn the vector based features for each node, the similarity between nodes can be easily estimated, for example, by the inner product or the cosine similarity.	b	A Survey on Network Embedding	[83]	['[83]  L. Getoor and C. P. Diehl, “Link mining: a survey,” ACM SIGKDD Explorations Newsletter, vol. 7, no. 2, pp. 3–12, 2005. ']
1039	24	[84]	Generally, precision @k and Mean Average Precision (MAP) are used to evaluate the link prediction performance [6]. // The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV ==[75]==, [76] and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter [72], Facebook [76], Epinions , and Slashdot), and biological networks (PPI [78]). // Specifically, [6] and [25] test the effectiveness on ARXIV.	b	A Survey on Network Embedding	[75]	['[75]  J. Leskovec, J. Kleinberg, and C. Faloutsos, “Graph evolution: Densification and shrinking diameters,” ACM Trans. Knowl. Discovery Data, vol. 1, no. 1, 2007, Art. no. 2. ']
1040	24	[184]	Generally, precision @k and Mean Average Precision (MAP) are used to evaluate the link prediction performance [6]. // The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV [75], ==[76]== and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter [72], Facebook ==[76]==, Epinions , and Slashdot), and biological networks (PPI [78]). // Specifically, [6] and [25] test the effectiveness on ARXIV.	b	A Survey on Network Embedding	[76]	['[76]  J. Leskovec and A. Krevl, “Snap datasets: Stanford large network dataset collection (2014),” 2016. ']
1041	24	[263]	Generally, precision @k and Mean Average Precision (MAP) are used to evaluate the link prediction performance [6]. // The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV [75], [76] and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter ==[72]==, Facebook [76], Epinions , and Slashdot), and biological networks (PPI [78]). // Specifically, [6] and [25] test the effectiveness on ARXIV.	b	A Survey on Network Embedding	[72]	['[72]  M. De Choudhury, Y.-R. Lin, H. Sundaram, K. S. Candan, L. Xie, A. Kelliher et al., “How does the data sampling strategy impact the discovery of information diffusion in social media?” ICWSM, vol. 10, pp. 34–41, 2010. ']
1042	24	[184]	Generally, precision @k and Mean Average Precision (MAP) are used to evaluate the link prediction performance [6]. // The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV [75], ==[76]== and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter [72], Facebook ==[76]==, Epinions , and Slashdot), and biological networks (PPI [78]). // Specifically, [6] and [25] test the effectiveness on ARXIV.	b	A Survey on Network Embedding	[76]	['[76]  J. Leskovec and A. Krevl, “Snap datasets: Stanford large network dataset collection (2014),” 2016. ']
1043	24	[86]	Generally, precision @k and Mean Average Precision (MAP) are used to evaluate the link prediction performance [6]. // The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV [75], [76] and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter [72], Facebook [76], Epinions , and Slashdot), and biological networks (PPI ==[78]==). // Specifically, [6] and [25] test the effectiveness on ARXIV.	b	A Survey on Network Embedding	[78]	['[78]  B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. B€ahler, V. Wood, et al., “The biogrid interaction database: 2008 update,” Nucleic Acids Res., vol. 36, no. suppl_1, pp. D637–D640, 2007. ']
1044	24	[19]	The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV [75], [76] and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter [72], Facebook [76], Epinions , and Slashdot), and biological networks (PPI [78]). // Specifically, ==[6]== and [25] test the effectiveness on ARXIV. // HOPE [8] applies network embedding to link prediction on two directed networks SNTwitter, which is a subnetwork of Twitter6 , and SN-TWeibo, which is a subnetwork of the social network in Tencent Weibo7 .	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1045	24	[37]	The popularly used real networks for the link prediction task can be divided into three categories: citation networks (ARXIV [75], [76] and DBLP1 ), social networks (SN-TWeibo2 , SN-Twitter [72], Facebook [76], Epinions , and Slashdot), and biological networks (PPI [78]). // Specifically, [6] and ==[25]== test the effectiveness on ARXIV. // HOPE [8] applies network embedding to link prediction on two directed networks SNTwitter, which is a subnetwork of Twitter6 , and SN-TWeibo, which is a subnetwork of the social network in Tencent Weibo7 .	b	A Survey on Network Embedding	[25]	['[25]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1046	24	[48]	Specifically, [6] and [25] test the effectiveness on ARXIV. // HOPE ==[8]== applies network embedding to link prediction on two directed networks SNTwitter, which is a subnetwork of Twitter6 , and SN-TWeibo, which is a subnetwork of the social network in Tencent Weibo. // Node2vec [25] tests the performance of link prediction on a social network Facebook and a biological network PPI.	b	A Survey on Network Embedding	[8]	['[8]  M. Ou, P. Cui, J. Pei, Z. Zhang, and W. Zhu, “Asymmetric transitivity preserving graph embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 672–681. ']
1047	24	[37]	HOPE [8] applies network embedding to link prediction on two directed networks SNTwitter, which is a subnetwork of Twitter6 , and SN-TWeibo, which is a subnetwork of the social network in Tencent Weibo. // Node2vec ==[25]== tests the performance of link prediction on a social network Facebook and a biological network PPI. // EOE [61] uses DBLP to demonstrate the effectiveness on citation networks. Based on two social networks, Epinions and Slashdot, SiNE [13] shows the superior performance of signed network embedding on link prediction.	b	A Survey on Network Embedding	[25]	['[25]  A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1048	24	[387]	Node2vec [25] tests the performance of link prediction on a social network Facebook and a biological network PPI. // EOE ==[61]== uses DBLP to demonstrate the effectiveness on citation networks. Based on two social networks, Epinions and Slashdot, SiNE [13] shows the superior performance of signed network embedding on link prediction. // To sum up, network embedding is able to capture inherent network structures, and thus naturally it is suitable for link prediction applications.	h+	A Survey on Network Embedding	[61]	['[61]  L. Xu, X. Wei, J. Cao, and P. S. Yu, “Embedding of embedding (eoe): Joint embedding for coupled heterogeneous networks,” in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 741– 749. ']
1049	24	[118]	Node2vec [25] tests the performance of link prediction on a social network Facebook and a biological network PPI. // EOE [61] uses DBLP to demonstrate the effectiveness on citation networks. Based on two social networks, Epinions and Slashdot, SiNE ==[13]== shows the superior performance of signed network embedding on link prediction. // To sum up, network embedding is able to capture inherent network structures, and thus naturally it is suitable for link prediction applications.	h+	A Survey on Network Embedding	[13]	['[13]  S. Wang, J. Tang, C. Aggarwal, Y. Chang, and H. Liu, “Signed network embedding in social media,” in Proc. SIAM Int. Conf. Data Mining, 2017, pp. 327–335. ']
1050	24	[398]	Many evaluation criteria have been proposed for clustering evaluation. // Accuracy (AC) and normalized mutual information (NMI) ==[85]== are frequently used to assess the clustering performance on graphs and networks. // The node clustering performance is tested on three types of networks: social networks (e.g., Facebook [86] and YELP [59]), citation networks (e.g., DBLP [60]), and document networks (e.g., 20-NewsGroup [87]).	b	A Survey on Network Embedding	[85]	['[85]  D. Cai, X. He, J. Han, and T. S. Huang, “Graph regularized nonnegative matrix factorization for data representation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1548–1560, Aug. 2011. ']
1051	24	[83]	Accuracy (AC) and normalized mutual information (NMI) [85] are frequently used to assess the clustering performance on graphs and networks. // The node clustering performance is tested on three types of networks: social networks (e.g., Facebook ==[86]== and YELP [59]), citation networks (e.g., DBLP [60]), and document networks (e.g., 20-NewsGroup [87]). // In particular, [16] extracts a social network from a social blogging site. It uses the TF-IDF features extracted from the blogs as the features of blog users and the “following” behaviors to construct the linkages.	b	A Survey on Network Embedding	[86]	['[86]  A. L. Traud, P. J. Mucha, and M. A. Porter, “Social structure of facebook networks,” Physica A: Statist. Mech. Appl., vol. 391, no. 16, pp. 4165–4180, 2012. ']
1052	24	[110]	Accuracy (AC) and normalized mutual information (NMI) [85] are frequently used to assess the clustering performance on graphs and networks. // The node clustering performance is tested on three types of networks: social networks (e.g., Facebook [86] and YELP ==[59]==), citation networks (e.g., DBLP [60]), and document networks (e.g., 20-NewsGroup [87]). // In particular, [16] extracts a social network from a social blogging site. It uses the TF-IDF features extracted from the blogs as the features of blog users and the “following” behaviors to construct the linkages.	b	A Survey on Network Embedding	[59]	['[59]  Z. Huang and N. Mamoulis, “Heterogeneous information network embedding for meta path based proximity,” arXiv: 1701.05291, 2017. ']
1053	24	[224]	Accuracy (AC) and normalized mutual information (NMI) [85] are frequently used to assess the clustering performance on graphs and networks. // The node clustering performance is tested on three types of networks: social networks (e.g., Facebook [86] and YELP [59]), citation networks (e.g., DBLP ==[60]==), and document networks (e.g., 20-NewsGroup [87]). // In particular, [16] extracts a social network from a social blogging site. It uses the TF-IDF features extracted from the blogs as the features of blog users and the “following” behaviors to construct the linkages.	b	A Survey on Network Embedding	[60]	['[60]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu, “Pathsim: Meta pathbased top-k similarity search in heterogeneous information networks,” Proc. VLDB Endowment, vol. 4, no. 11, pp. 992–1003, 2011. ']
1054	24	[176]	Accuracy (AC) and normalized mutual information (NMI) [85] are frequently used to assess the clustering performance on graphs and networks. // The node clustering performance is tested on three types of networks: social networks (e.g., Facebook [86] and YELP [59]), citation networks (e.g., DBLP [60]), and document networks (e.g., 20-NewsGroup ==[87]==). // In particular, [16] extracts a social network from a social blogging site. It uses the TF-IDF features extracted from the blogs as the features of blog users and the “following” behaviors to construct the linkages.	b	A Survey on Network Embedding	[87]	['[87]  F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu, “Learning deep representations for graph clustering” in Proc. AAAI, 2014, pp. 1293–1299. ']
1055	24	[109]	The node clustering performance is tested on three types of networks: social networks (e.g., Facebook [86] and YELP [59]), citation networks (e.g., DBLP [60]), and document networks (e.g., 20-NewsGroup [87]). // In particular, ==[16]== extracts a social network from a social blogging site. It uses the TF-IDF features extracted from the blogs as the features of blog users and the “following” behaviors to construct the linkages. // It usesthe TF-IDF features extracted from the blogs as the features of blog users and the “following” behaviors to construct the linkages.	b	A Survey on Network Embedding	[16]	['[16]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang, “Heterogeneous network embedding via deep architectures,” in Proc. 21th ACM SIGKDD Int. Conf. Knowl Discovery Data Mining, 2015, pp. 119–128. ']
1056	24	[30]	It successfully applies network embedding to thenode clustering task. // ==[4]== uses the Facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering. // [59] is applied to more social networks including MOVIE, a network extracted from YAGO [88] that contains knowledge about movies, YELP, a network extracted from YELP that is about reviews given to restaurants, and GAME, extracted from Freebase [89] that is related to video games.	b	A Survey on Network Embedding	[4]	['[4]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang, “Community preserving network embedding,” in Proc. 31th AAAI Conf. Artif. Intell., AAAI Press, 2017, pp. 203–209. ']
1057	24	[110]	[4] uses the Facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering. // ==[59]== is applied to more social networks including MOVIE, a network extracted from YAGO [88] that contains knowledge about movies, YELP, a network extracted from YELP that is about reviews given to restaurants, and GAME, extracted from Freebase [89] that is related to video games. // [26] tests the node clustering performance on a document network, 20-NewsGroup network, which consists of documents.	b	A Survey on Network Embedding	[59]	['[59]  Z. Huang and N. Mamoulis, “Heterogeneous information network embedding for meta path based proximity,” arXiv: 1701.05291, 2017. ']
1058	24	[399]	[4] uses the Facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering. // [59] is applied to more social networks including MOVIE, a network extracted from YAGO ==[88]== that contains knowledge about movies, YELP, a network extracted from YELP that is about reviews given to restaurants, and GAME, extracted from Freebase [89] that is related to video games. // [26] tests the node clustering performance on a document network, 20-NewsGroup network, which consists of documents.	b	A Survey on Network Embedding	[88]	['[88]  Z. Huang, Y. Zheng, R. Cheng, Y. Sun, N. Mamoulis, and X. Li, “Meta structure: Computing relevance in large heterogeneous information networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1595–1604. ']
1059	24	[400]	[4] uses the Facebook social network to demonstrate the effectiveness of community preserving network embedding on node clustering. // [59] is applied to more social networks including MOVIE, a network extracted from YAGO [88] that contains knowledge about movies, YELP, a network extracted from YELP that is about reviews given to restaurants, and GAME, extracted from Freebase ==[89]== that is related to video games. // [26] tests the node clustering performance on a document network, 20-NewsGroup network, which consists of documents.	b	A Survey on Network Embedding	[89]	['[89]  K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, “Freebase: A collaboratively created graph database for structuring human knowledge,” in Proc. ACM SIGMOD Int. Conf. Manage. Data, 2008, pp. 1247–1250. ']
1060	24	[9]	[59] is applied to more social networks including MOVIE, a network extracted from YAGO [88] that contains knowledge about movies, YELP, a network extracted from YELP that is about reviews given to restaurants, and GAME, extracted from Freebase [89] that is related to video games. // ==[26]== tests the node clustering performance on a document network, 20-NewsGroup network, which consists of documents. // The node clustering performance on citation networks is tested [59] by clustering authors in DBLP.	b	A Survey on Network Embedding	[26]	['[26]  S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 1145–1152. ']
1061	24	[19]	More often than not, the quality of network visualizationby different network embedding algorithms is evaluated visually. // Fig.15 is an example by SDNE ==[6]== where SDNE is applied to 20-NewsGroup. // In Fig. 15, each document is mapped into a two dimensional space as a point, and different colors on the points represent the labels.	b	A Survey on Network Embedding	[6]	['[6]  D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1225–1234. ']
1062	24	[1]	As can be seen, network embedding preserves the intrinsic structure of the network, where similar nodes are closer to each other than dissimilar nodes in the low-dimensional space. // Also, LINE ==[10]==, GraRep [34], and EOE [61] are applied to a citation network DBLP and generate meaningful layout of the network. // Pan et al. [53] show the visualization of another citation network Citeseer-M10 [91] consisting of scientific publications from ten distinct research areas.	b	A Survey on Network Embedding	[10]	['[10]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in Proc. 24th Int. Conf. World Wide Web, 2015, pp. 1067–1077. ']
1063	24	[26]	As can be seen, network embedding preserves the intrinsic structure of the network, where similar nodes are closer to each other than dissimilar nodes in the low-dimensional space. // Also, LINE [10], GraRep ==[34]==, and EOE [61] are applied to a citation network DBLP and generate meaningful layout of the network. // Pan et al. [53] show the visualization of another citation network Citeseer-M10 [91] consisting of scientific publications from ten distinct research areas.	b	A Survey on Network Embedding	[34]	['[34]  S. Cao, W. Lu, and Q. Xu, “Grarep: Learning graph representations with global structural information,” in Proc. 24th ACM Int. Conf. Inf. Knowl. Manage, 2015, pp. 891–900. ']
1064	24	[387]	As can be seen, network embedding preserves the intrinsic structure of the network, where similar nodes are closer to each other than dissimilar nodes in the low-dimensional space. // Also, LINE [10], GraRep [34], and EOE ==[61]== are applied to a citation network DBLP and generate meaningful layout of the network. // Pan et al. [53] show the visualization of another citation network Citeseer-M10 [91] consisting of scientific publications from ten distinct research areas.	b	A Survey on Network Embedding	[61]	['[61]  L. Xu, X. Wei, J. Cao, and P. S. Yu, “Embedding of embedding (eoe): Joint embedding for coupled heterogeneous networks,” in Proc. 10th ACM Int. Conf. Web Search Data Mining, 2017, pp. 741– 749. ']
1065	24	[41]	Also, LINE [10], GraRep [34], and EOE [61] are applied to a citation network DBLP and generate meaningful layout of the network. // Pan et al. ==[53]== show the visualization of another citation network Citeseer-M10 [91] consisting of scientific publications from ten distinct research areas. // Open Source Software.	b	A Survey on Network Embedding	[53]	['[53]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang, “Tri-party deep network representation,” Netw., vol. 11, no. 9, 2016, Art. no. 12. ']
1066	24	[401]	Also, LINE [10], GraRep [34], and EOE [61] are applied to a citation network DBLP and generate meaningful layout of the network. // Pan et al. [53] show the visualization of another citation network Citeseer-M10 ==[91]== consisting of scientific publications from ten distinct research areas. // Open Source Software.	b	A Survey on Network Embedding	[91]	['[91]  K. W. Lim and W. Buntine, “Bibliographic analysis with the citation network topic model,” arXiv:1609.06826, 2016. ']
1067	24	[399]	Besides, in a heterogeneous information network, to measure the relevance of two objects, the meta path, a sequence of object types with edge types in between, has been widely used. // However, meta structure ==[88]==, which is essentially a directed acyclic graph of object and edge types, provides a higher-order structure constraint. // This suggests a huge potential direction for improving heterogeneous information network embedding.	b	A Survey on Network Embedding	[88]	['[88]  Z. Huang, Y. Zheng, R. Cheng, Y. Sun, N. Mamoulis, and X. Li, “Meta structure: Computing relevance in large heterogeneous information networks,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, 2016, pp. 1595–1604. ']
1068	24	[402]	In general, the principle of network embedding can be extended to other target spaces. // For example, recently some studies ==[96]== assume that the underlying structure of a network is in the hyperbolic space. // Under this assumption, heterogeneous degree distributions and strong clustering emerge naturally, as they are the simple reflections of the negative curvature and metric property of the underlying hyperbolic geometry.	b	A Survey on Network Embedding	[96]	['[96]  D. Krioukov, F. Papadopoulos, M. Kitsak, A. Vahdat, and M. Boguna, “Hyperbolic geometry of complex networks,”   Phys. Rev. E, vol. 82, no. 3, 2010, Art. no. 036106.']
1069	105	[26, 37, 6, 1, 19]	Introduction. // Low-dimensional vector embeddings of nodes in large graphs have proved extremely useful as feature inputs for a wide variety of prediction and graph analysis tasks ==[5, 11, 28, 35, 36]==. // The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high-dimensional information about a node’s neighborhood into a dense vector embedding.	b	Inductive Representation Learning on Large Graphs	[5, 11, 28, 35, 36]	['[5]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In KDD, 2015. ', '[11]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In KDD, 2016. ', '[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ', '[35]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, 2015. ', '[36]  D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In KDD, 2016. ']
1070	105	[37, 6, 1]	The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high-dimensional information about a node’s neighborhood into a dense vector embedding. // These node embeddings can then be fed to downstream machine learning systems and aid in tasks such as node classification, clustering, and link prediction ==[11, 28, 35]==. // However, previous works have focused on embedding nodes from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)graphs.	h-	Inductive Representation Learning on Large Graphs	[11, 28, 35]	['[11]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In KDD, 2016. ', '[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ', '[35]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, 2015. ']
1071	105	[26, 37, 290, 6, 1, 19, 30, 403]	Most existing approaches to generating node embeddings are inherently transductive. // The majority of these approaches directly optimize the embeddings for each node using matrix-factorization-based objectives, and do not naturally generalize to unseen data, since they make predictions on nodes in a single, fixed graph ==[5, 11, 23, 28, 35, 36, 37, 39]==. // These approaches can be modified to operate in an inductive setting (e.g., [28]), but these modifications tend to be computationally expensive, requiring additional rounds of gradient descent before new predictions can be made.	h-	Inductive Representation Learning on Large Graphs	[5, 11, 23, 28, 35, 36, 37, 39]	['[5]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In KDD, 2015. ', '[11]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In KDD, 2016. ', '[23]  A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. In NIPS, 2001. ', '[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ', '[35]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, 2015. ', '[36]  D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In KDD, 2016. ', '[37]  X. Wang, P. Cui, J. Wang, J. Pei, W. Zhu, and S. Yang. Community preserving network embedding. In AAAI, 2017. ', '[39]  L. Xu, X. Wei, J. Cao, and P. S. Yu. Embedding identity and interest for social networks. In WWW, 2017. ']
1072	105	[6]	The majority of these approaches directly optimize the embeddings for each node using matrix-factorization-based objectives, and do not naturally generalize to unseen data, since they make predictions on nodes in a single, fixed graph [5, 11, 23, 28, 35, 36, 37, 39]. // These approaches can be modified to operate in an inductive setting (e.g., ==[28]==), but these modifications tend to be computationally expensive, requiring additional rounds of gradient descent before new predictions can be made. // There are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology [17].	h-	Inductive Representation Learning on Large Graphs	[28]	['[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ']
1073	105	[266]	These approaches can be modified to operate in an inductive setting (e.g., [28]), but these modifications tend to be computationally expensive, requiring additional rounds of gradient descent before new predictions can be made. // There are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology ==[17]==. // So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs [17, 18].	b	Inductive Representation Learning on Large Graphs	[17]	['[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ']
1074	105	[266, 404]	There are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology [17]. // So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs ==[17, 18]==. // In this work we both extend GCNs to the task of inductive unsupervised learning and propose a framework that generalizes the GCN approach to use trainable aggregation functions (beyond simple convolutions).	h-	Inductive Representation Learning on Large Graphs	[17, 18]	['[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ', '[18]  T. N. Kipf and M. Welling. Variational graph auto-encoders. In NIPS Workshop on Bayesian Deep Learning, 2016. ']
1075	105	[6]	We use two evolving document graphs based on citation data and Reddit post data (predicting paper and post categories, respectively), and a multigraph generalization experiment based on a dataset of protein-protein interactions (predicting protein functions). // Using these benchmarks, we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification F1-scores by an average of 51% compared to using node features alone and GraphSAGE consistently outperforms a strong, transductive baseline ==[28]==, despite this baseline taking ∼100× longer to run on unseen nodes. // We also show that the new aggregator architectures we propose provide significant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks [17].	h-e	Inductive Representation Learning on Large Graphs	[28]	['[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ']
1076	105	[266]	Using these benchmarks, we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification F1-scores by an average of 51% compared to using node features alone and GraphSAGE consistently outperforms a strong, transductive baseline [28], despite this baseline taking ∼100× longer to run on unseen nodes. // We also show that the new aggregator architectures we propose provide significant gains (7.4% on average) compared to an aggregator inspired by graph convolutional networks ==[17]==. // Lastly, we probe the expressive capability of our approach and show, through theoretical analysis, that GraphSAGE is capable of learning structural information about a node’s role in a graph, despite the fact that it is inherently based on features (Section 5).	h-e	Inductive Representation Learning on Large Graphs	[17]	['[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ']
1077	105	[26, 37, 6, 1, 19]	Factorization-based embedding approaches. // There are a number of recent node embedding approaches that learn low-dimensional embeddings using random walk statistics and matrix factorization-based learning objectives ==[5, 11, 28, 35, 36]==. // These methods also bear close relationships to more classic approaches to spectral clustering [23], multi-dimensional scaling [19], as well as the PageRank algorithm [25].	b	Inductive Representation Learning on Large Graphs	[5, 11, 28, 35, 36]	['[5]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In KDD, 2015. ', '[11]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In KDD, 2016. ', '[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ', '[35]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, 2015. ', '[36]  D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In KDD, 2016. ']
1078	105	[290]	There are a number of recent node embedding approaches that learn low-dimensional embeddings using random walk statistics and matrix factorization-based learning objectives [5, 11, 28, 35, 36]. // These methods also bear close relationships to more classic approaches to spectral clustering ==[23]==, multi-dimensional scaling [19], as well as the PageRank algorithm [25]. // In addition, for many of these approaches (e.g., [11, 28, 35, 36]) the objective function is invariant to orthogonal transformations of the embeddings, which means that the embedding space does not naturally generalize between graphs and can drift during re-training.	b	Inductive Representation Learning on Large Graphs	[23]	['[23]  A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. In NIPS, 2001. ']
1079	105	[405]	There are a number of recent node embedding approaches that learn low-dimensional embeddings using random walk statistics and matrix factorization-based learning objectives [5, 11, 28, 35, 36]. // These methods also bear close relationships to more classic approaches to spectral clustering [23], multi-dimensional scaling ==[19]==, as well as the PageRank algorithm [25]. // In addition, for many of these approaches (e.g., [11, 28, 35, 36]) the objective function is invariant to orthogonal transformations of the embeddings, which means that the embedding space does not naturally generalize between graphs and can drift during re-training.	b	Inductive Representation Learning on Large Graphs	[19]	['[19]  J. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964. ']
1080	105	[158]	There are a number of recent node embedding approaches that learn low-dimensional embeddings using random walk statistics and matrix factorization-based learning objectives [5, 11, 28, 35, 36]. // These methods also bear close relationships to more classic approaches to spectral clustering [23], multi-dimensional scaling [19], as well as the PageRank algorithm ==[25]==. // In addition, for many of these approaches (e.g., [11, 28, 35, 36]) the objective function is invariant to orthogonal transformations of the embeddings, which means that the embedding space does not naturally generalize between graphs and can drift during re-training.	b	Inductive Representation Learning on Large Graphs	[25]	['[25]  L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999. ']
1081	105	[37, 6, 1, 19]	These methods also bear close relationships to more classic approaches to spectral clustering [23], multi-dimensional scaling [19], as well as the PageRank algorithm [25]. // In addition, for many of these approaches (e.g., ==[11, 28, 35, 36]==) the objective function is invariant to orthogonal transformations of the embeddings, which means that the embedding space does not naturally generalize between graphs and can drift during re-training. // One notable exception to this trend is the Planetoid-I algorithm introduced by Yang et al. [40], which is an inductive, embeddingbased approach to semi-supervised learning.	h-	Inductive Representation Learning on Large Graphs	[11, 28, 35, 36]	['[11]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In KDD, 2016. ', '[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ', '[35]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, 2015. ', '[36]  D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In KDD, 2016. ']
1082	105	[66]	In addition, for many of these approaches (e.g., [11, 28, 35, 36]) the objective function is invariant to orthogonal transformations of the embeddings, which means that the embedding space does not naturally generalize between graphs and can drift during re-training. // One notable exception to this trend is the Planetoid-I algorithm introduced by Yang et al. ==[40]==, which is an inductive, embeddingbased approach to semi-supervised learning. // However, Planetoid-I does not use any graph structural information during inference; instead, it uses the graph structure as a form of regularization during training.	b	Inductive Representation Learning on Large Graphs	[40]	['[40]  Z. Yang, W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In ICML, 2016. ']
1083	105	[220]	Beyond node embedding approaches, there is a rich literature on supervised learning over graph-structured data. // This includes a wide variety of kernel-based approaches, where feature vectors for graphs are derived from various graph kernels (see ==[32]== and references therein). // There are also a number of recent neural network approaches to supervised learning over graph structures [7, 10, 21, 31]. Our approach is conceptually inspired by a number of these algorithms.	b	Inductive Representation Learning on Large Graphs	[32]	['[32]  N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeilerlehman graph kernels. Journal of Machine Learning Research, 12:2539–2561, 2011. ']
1084	105	[406, 407, 175, 408]	This includes a wide variety of kernel-based approaches, where feature vectors for graphs are derived from various graph kernels (see [32] and references therein). // There are also a number of recent neural network approaches to supervised learning over graph structures ==[7, 10, 21, 31]==. // However, whereas these previous approaches attempt to classify entire graphs (or subgraphs), the focus of this work is generating useful representations for individual nodes.	h-	Inductive Representation Learning on Large Graphs	[7, 10, 21, 31]	['[7]  H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In ICML, 2016. ', '[10]  M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks, volume 2, pages 729–734, 2005. ', '[21]  Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In ICLR, 2015. ', '[31]  F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009. ']
1085	105	[409, 410, 411, 266, 381]	Graph convolutional networks. // In recent years, several convolutional neural network architectures for learning over graphs have been proposed (e.g., ==[4, 9, 8, 17, 24]==). // The majority of these methods do not scale to large graphs or are designed for whole-graph classification (or both) [4, 9, 8, 24].	h-	Inductive Representation Learning on Large Graphs	[4, 9, 8, 17, 24]	['[4]  J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. ', '[9]  D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In NIPS, 2015. ', '[8]  M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS, 2016. ', '[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ', '[24]  M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML, 2016. 10 ']
1086	105	[409, 410, 411, 381]	In recent years, several convolutional neural network architectures for learning over graphs have been proposed (e.g., [4, 9, 8, 17, 24]). // The majority of these methods do not scale to large graphs or are designed for whole-graph classification (or both) ==[4, 9, 8, 24]==. // However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. [17, 18].	h-	Inductive Representation Learning on Large Graphs	[4, 9, 8, 24]	['[4]  J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014. ', '[9]  D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. In NIPS, 2015. ', '[8]  M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS, 2016. ', '[24]  M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML, 2016. 10 ']
1087	105	[266, 404]	The majority of these methods do not scale to large graphs or are designed for whole-graph classification (or both) [4, 9, 8, 24]. // However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. ==[17, 18]==. // The original GCN algorithm [17] is designed for semi-supervised learning in a transductive setting, and the exact algorithm requires that the full graph Laplacian is known during training. A simple variant of our algorithm can be viewed as an extension of the GCN framework to the inductive setting, a point which we revisit in Section 3.3.	b	Inductive Representation Learning on Large Graphs	[17, 18]	['[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ', '[18]  T. N. Kipf and M. Welling. Variational graph auto-encoders. In NIPS Workshop on Bayesian Deep Learning, 2016. ']
1088	105	[266]	However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. [17, 18]. // The original GCN algorithm ==[17]== is designed for semi-supervised learning in a transductive setting, and the exact algorithm requires that the full graph Laplacian is known during training. // A simple variant of our algorithm can be viewed as an extension of the GCN framework to the inductive setting, a point which we revisit in Section 3.3.	b	Inductive Representation Learning on Large Graphs	[17]	['[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ']
1089	105	[389]	LSTM aggregator. // We also examined a more complex aggregator based on an LSTM architecture ==[14]==. // Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not inherently symmetric (i.e., they are not permutation invariant), since they process their inputs in a sequential manner.	h-	Inductive Representation Learning on Large Graphs	[14]	['[14]  S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735– 1780, 1997. ']
1090	105	[6]	Experimental set-up. // To contextualize the empirical results on our inductive benchmarks, we compare against four baselines: a random classifer, a logistic regression feature-based classifier (that ignores graph structure), the DeepWalk algorithm ==[28]== as a representative factorization-based approach, and a concatenation of the raw features and DeepWalk embeddings. // We also compare four variants of GraphSAGE that use the different aggregator functions (Section 3.3).	b	Inductive Representation Learning on Large Graphs	[28]	['[28]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, 2014. ']
1091	105	[266]	We also compare four variants of GraphSAGE that use the different aggregator functions (Section 3.3). // Since, the “convolutional” variant of GraphSAGE is an extended, inductive version of Kipf et al’s semi-supervised GCN ==[17]==, we term this variant GraphSAGE-GCN. // We test unsupervised variants of GraphSAGE trained according to the loss in Equation (1), as well as supervised variants that are trained directly on classification cross-entropy loss.	ho	Inductive Representation Learning on Large Graphs	[17]	['[17]  T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2016. ']
1092	105	[412]	In the multi-graph setting, we cannot apply DeepWalk, since the embedding spaces generated by running the DeepWalk algorithm on different disjoint graphs can be arbitrarily rotated with respect to each other (Appendix ). // All models were implemented in TensorFlow ==[1]== with the Adam optimizer [16] (except DeepWalk, which performed better with the vanilla gradient descent optimizer). // We designed our experiments with the goals of (i) verifying the improvement of GraphSAGE over the baseline approaches (i.e., raw features and DeepWalk) and (ii) providing a rigorous comparison of the different GraphSAGE aggregator architectures.	hoe	Inductive Representation Learning on Large Graphs	[1]	['[1]  M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint , 2016. ']
1093	105	[413]	In the multi-graph setting, we cannot apply DeepWalk, since the embedding spaces generated by running the DeepWalk algorithm on different disjoint graphs can be arbitrarily rotated with respect to each other (Appendix ). // All models were implemented in TensorFlow [1] with the Adam optimizer ==[16]== (except DeepWalk, which performed better with the vanilla gradient descent optimizer). // We designed our experiments with the goals of (i) verifying the improvement of GraphSAGE over the baseline approaches (i.e., raw features and DeepWalk) and (ii) providing a rigorous comparison of the different GraphSAGE aggregator architectures.	hoe	Inductive Representation Learning on Large Graphs	[16]	['[16]  D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. ']
1094	584	[159]	The fourth point makes our prediction more positive. It turns out to be a paper called SEISMIC published in KDD’15. // Open academic datasets (such as AMiner ==[20]== and Microsoft Academic Graph [18]) make hundreds of millions of successful examples (i.e., published papers) available, and the above clues we assess the success are lying beneath the huge data. // In this work, we aim at proposing a data mining approach to predict the success of a plan towards a goal.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[20]	['[20]  Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 990–998. ']
1095	584	[414]	The fourth point makes our prediction more positive. It turns out to be a paper called SEISMIC published in KDD’15. // Open academic datasets (such as AMiner [20] and Microsoft Academic Graph ==[18]==) make hundreds of millions of successful examples (i.e., published papers) available, and the above clues we assess the success are lying beneath the huge data. // In this work, we aim at proposing a data mining approach to predict the success of a plan towards a goal.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[18]	['[18]  Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-june Paul Hsu, and Kuansan Wang. 2015. An overview of microsoft academic service (mas) and applications. In Proceedings of the 24th international conference on world wide web (WWW). ACM, 243–246. ']
1096	584	[111, 415, 37, 6, 1, 416]	Existing approaches formulate the behavior data as an information network and the papers or components or venues as nodes.  // Then network embedding methods have been widely applied for learning the node’s feature vectors by preserving the pair-wise node proximity, neighborhood, or global structure ==[4, 7, 8, 14, 19, 21]==. // Unfortunately, the outcomes of behavior, or say, the underlying patterns of the components forming an (un-)successful plan towards the goal, were not preserved in the embeddings, and thus not effective for predicting the success (as shown in the experiments).	h-	TUBE: Embedding Behavior Outcomes for Predicting Success	[4, 7, 8, 14, 19, 21]	['[4]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (KDD). ACM, 135–144. ', '[7]  Ming Gao, Leihui Chen, Xiangnan He, and Aoying Zhou. 2018. BiNE: Bipartite Network Embedding.. In Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 715–724. ', '[8]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 855–864. ', '[14]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 701–710. ', '[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web (WWW). International World Wide Web Conferences Steering Committee, 1067–1077. ', '[21]  Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference on World Wide Web (WWW). International World Wide Web Conferences Steering Committee, 539–548. ']
1097	584	[36]	We assume that the positive examples (i.e., paper records in the datasets) have an effectiveness of 1 (and thus a distance ε of 0). // We adopt the negative sampling strategy, which has widely been applied to word embeddings ==[13]==, to generate negative examples and assume they have an effectiveness of a small value. // Figure 1 presents a 2-D visualization of ten behavior goals (i.e., conferences) and two successful behavior plans (a KDD’15 paper and a NIPS’11 paper) losing some dimensions in the embedding results.	h+	TUBE: Embedding Behavior Outcomes for Predicting Success	[13]	['[13]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (NIPS). 3111–3119. ']
1098	584	[1]	We compare our TUBE method against the state-of-the-art network embedding methods, and a very recent method for success prediction. // LINE ==[19]==: It preserves both local and global structure of the network by conducting edge sampling. // node2vec [8]: This method uses biased random walks to capture the homophily and structural equivalence properties of network.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[19]	['[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web (WWW). International World Wide Web Conferences Steering Committee, 1067–1077. ']
1099	584	[37]	LINE [19]: It preserves both local and global structure of the network by conducting edge sampling. // node2vec ==[8]==: This method uses biased random walks to capture the homophily and structural equivalence properties of network. // We also considered the DeepWalk [14] model here. Since DeepWalk can be seen as a special case of node2vec that uses truncated uniform random walks, we only report the better performance among them in experiments.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 855–864. ']
1100	584	[6]	node2vec [8]: This method uses biased random walks to capture the homophily and structural equivalence properties of network. // We also considered the DeepWalk ==[14]== model here. // Since DeepWalk can be seen as a special case of node2vec that uses truncated uniform random walks, we only report the better performance among them in experiments.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[14]	['[14]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 701–710. ']
1101	584	[416]	We also considered the DeepWalk [14] model here. Since DeepWalk can be seen as a special case of node2vec that uses truncated uniform random walks, we only report the better performance among them in experiments. // VERSE ==[21]==: It is able to preserves the distributions of a selected vertex-to-vertex similarity measure in homogeneous network such as Personalized PageRank, SimRank and etc. // BiNE [7]: This method aims at learning the representations of vertices in a bipartite network. It conducts biased random walks to preserve the long-tail distribution of vertices.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[21]	['[21]  Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference on World Wide Web (WWW). International World Wide Web Conferences Steering Committee, 539–548. ']
1102	584	[415]	VERSE [21]: It is able to preserves the distributions of a selected vertex-to-vertex similarity measure in homogeneous network such as Personalized PageRank, SimRank and etc. // BiNE ==[7]==: This method aims at learning the representations of vertices in a bipartite network. It conducts biased random walks to preserve the long-tail distribution of vertices. // Metapath2vec [4]: It is the state-of-the-art method for heterogeneous network embedding.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[7]	['[7]  Ming Gao, Leihui Chen, Xiangnan He, and Aoying Zhou. 2018. BiNE: Bipartite Network Embedding.. In Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 715–724. ']
1103	584	[111]	BiNE [7]: This method aims at learning the representations of vertices in a bipartite network. It conducts biased random walks to preserve the long-tail distribution of vertices. // Metapath2vec ==[4]==: It is the state-of-the-art method for heterogeneous network embedding. It samples meta path-based random walks for learning. // We use the advanced version Metapath2vec++ which also conducts heterogeneous negative sampling in network.	h+	TUBE: Embedding Behavior Outcomes for Predicting Success	[4]	['[4]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (KDD). ACM, 135–144. ']
1104	584	[417]	We use the advanced version Metapath2vec++ which also conducts heterogeneous negative sampling in network. // LearnSuc ==[24]==: It is a recent work that formulates behavior as a multi-type itemset instead of a node in networks, and learns item embeddings collectively for success prediction. // Parameter settings.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[24]	['[24]  Daheng Wang, Meng Jiang, Qingkai Zeng, Zachary Eberhart, and Nitesh V Chawla. 2018. Multi-type itemset embedding for learning behavior success. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). ACM, 2397–2406. ']
1105	584	[417]	// Harmonic Mean of Ranks (HMR) ==[24]==: This metric is to see whether the method ranks the true venue at the top. A smaller value of HMR indicates better performance. //	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[24]	['[24]  Daheng Wang, Meng Jiang, Qingkai Zeng, Zachary Eberhart, and Nitesh V Chawla. 2018. Multi-type itemset embedding for learning behavior success. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). ACM, 2397–2406. ']
1106	584	[1]	Learning representations of network data, or network embedding, aims at learning the low-dimensional vector representations of nodes in network while preserving the pair-wise proximities [1, 9–11]. // LINE ==[19]== first introduced the notion of 1st and 2nd order proximity to preserve both local and global structure of the network by conducting edge sampling. // DeepWalk [14] used truncated uniform random walks to explore the neighborhood of a node and expected nodes with higher proximity yield similar representations.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[19]	['[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th international conference on world wide web (WWW). International World Wide Web Conferences Steering Committee, 1067–1077. ']
1107	584	[6]	LINE [19] first introduced the notion of 1st and 2nd order proximity to preserve both local and global structure of the network by conducting edge sampling. // DeepWalk ==[14]== used truncated uniform random walks to explore the neighborhood of a node and expected nodes with higher proximity yield similar representations. // node2vec [8] extended it to use biased random walks to capture the homophily and structural equivalence properties of network.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[14]	['[14]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 701–710. ']
1108	584	[37]	DeepWalk [14] used truncated uniform random walks to explore the neighborhood of a node and expected nodes with higher proximity yield similar representations. // node2vec ==[8]== extended it to use biased random walks to capture the homophily and structural equivalence properties of network. // VERSE [21] was designed to preserve the distributions of a selected pair-wise similarity measure in network such as Personalized PageRank or SimRank.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[8]	['[8]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 855–864. ']
1109	584	[416]	node2vec [8] extended it to use biased random walks to capture the homophily and structural equivalence properties of network. // VERSE ==[21]== was designed to preserve the distributions of a selected pair-wise similarity measure in network such as Personalized PageRank or SimRank. // Besides methods focusing on homogeneous networks, BiNE [7] was able to learn the representations of vertices in a bipartite network by conducting biased random walks to preserve the long-tail distribution of vertices.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[21]	['[21]  Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference on World Wide Web (WWW). International World Wide Web Conferences Steering Committee, 539–548. ']
1110	584	[415]	VERSE [21] was designed to preserve the distributions of a selected pair-wise similarity measure in network such as Personalized PageRank or SimRank. // Besides methods focusing on homogeneous networks, BiNE ==[7]== was able to learn the representations of vertices in a bipartite network by conducting biased random walks to preserve the long-tail distribution of vertices. // For heterogeneous network embedding, Metapath2vec [4] was based on meta-path-based random walks and the heterogeneous Research Track Paper KDD ’19, August 4–8, 2019, Anchorage, AK, USA 1689 Skip-gram model.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[7]	['[7]  Ming Gao, Leihui Chen, Xiangnan He, and Aoying Zhou. 2018. BiNE: Bipartite Network Embedding.. In Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 715–724. ']
1111	584	[111]	Besides methods focusing on homogeneous networks, BiNE [7] was able to learn the representations of vertices in a bipartite network by conducting biased random walks to preserve the long-tail distribution of vertices. // For heterogeneous network embedding, Metapath2vec ==[4]== was based on meta-path-based random walks and the heterogeneous Skip-gram model. // here is another line of methods that utilized deep models [6, 22, 23, 26].	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[4]	['[4]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec: Scalable representation learning for heterogeneous networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining (KDD). ACM, 135–144. ']
1112	584	[418, 112, 19, 44]	For heterogeneous network embedding, Metapath2vec [4] was based on meta-path-based random walks and the heterogeneous Skip-gram model. // There is another line of methods that utilized deep models ==[6, 22, 23, 26]==. // However, none of these existing methods has an effective formulation of behavior and does not preserve the outcome information of behaviors.	h-	TUBE: Embedding Behavior Outcomes for Predicting Success	[6, 22, 23, 26]	['[6]  Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. 2018. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). ACM, 1416–1424. ', '[22]  Ke Tu, Peng Cui, Xiao Wang, Fei Wang, and Wenwu Zhu. 2018. Structural deep embedding for hyper-networks. In Thirty-Second AAAI Conference on Artificial Intelligence (AAAI). ', '[23]  Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining (KDD). ACM, 1225–1234. ', '[26]  Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. Graphgan: Graph representation learning with generative adversarial nets. In Thirty-Second AAAI Conference on Artificial Intelligence (AAAI). ']
1113	584	[417]	However, none of these existing methods has an effective formulation of behavior and does not preserve the outcome information of behaviors. // A recent work LearnSuc ==[24]== formulated behavior as a multi-type itemset structure instead of nodes in network and preserved the behavioral success property. // But this model does not have an explicit definition of behavior outcomes.	h-	TUBE: Embedding Behavior Outcomes for Predicting Success	[24]	['[24]  Daheng Wang, Meng Jiang, Qingkai Zeng, Zachary Eberhart, and Nitesh V Chawla. 2018. Multi-type itemset embedding for learning behavior success. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). ACM, 2397–2406. ']
1114	584	[189, 419, 420, 421]	Quantifying success. // There exists a wide line of research on quantifying success in various fields and areas ==[12, 16, 17, 27]==. // Wang et al. [25] proposed a model for predicting long-term scientific impact by collapsing the citation histories of papers from different journals and disciplines into a single curve to model the citation dynamics of individual papers.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[12, 16, 17, 27]	['[12]  Meng Jiang, Christos Faloutsos, and Jiawei Han. 2016. Catchtartan: Representing and summarizing dynamic multicontextual behaviors. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD). ACM, 945–954. ', '[16]  Vedran Sekara, Pierre Deville, Sebastian E Ahnert, Albert-László Barabási, Roberta Sinatra, and Sune Lehmann. 2018. The chaperone effect in scientific publishing. Proceedings of the National Academy of Sciences (PNAS) 115, 50 (2018), 12603– 12607. ', '[17]  Roberta Sinatra, Dashun Wang, Pierre Deville, Chaoming Song, and Albert-László Barabási. 2016. Quantifying the evolution of individual scientific impact. Science 354, 6312 (2016), 5239. ', '[27]  Burcu Yucesoy and Albert-László Barabási. 2016. Untangling performance from success. EPJ Data Science 5, 1 (2016), 17. ']
1115	584	[422]	There exists a wide line of research on quantifying success in various fields and areas [12, 16, 17, 27]. // Wang et al. ==[25]== proposed a model for predicting long-term scientific impact by collapsing the citation histories of papers from different journals and disciplines into a single curve to model the citation dynamics of individual papers. // To predict the success in art, Fraiberger et al. [5] used a Markov model to predict the career trajectory of individual artists and documents the strong path and history dependence of valuation in art.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[25]	['[25]  Dashun Wang, Chaoming Song, and Albert-László Barabási. 2013. Quantifying long-term scientific impact. Science 342, 6154 (2013), 127–132. ']
1116	584	[423]	Wang et al. [25] proposed a model for predicting long-term scientific impact by collapsing the citation histories of papers from different journals and disciplines into a single curve to model the citation dynamics of individual papers. // To predict the success in art, Fraiberger et al. ==[5]== used a Markov model to predict the career trajectory of individual artists and documents the strong path and history dependence of valuation in art. // Yucesoy et al. [27] proposed a model aiming at predicting which books will become bestsellers.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[5]	['[5]  Samuel P Fraiberger, Roberta Sinatra, Magnus Resch, Christoph Riedl, and AlbertLászló Barabási. 2018. Quantifying reputation and success in art. Science 362, 6416 (2018), 825–829. ']
1117	584	[421]	To predict the success in art, Fraiberger et al. [5] used a Markov model to predict the career trajectory of individual artists and documents the strong path and history dependence of valuation in art. // Yucesoy et al. ==[27]== proposed a model aiming at predicting which books will become bestsellers. // And, Deville et al. [3] studied quantifying the career choices such as changing institutions affecting scientific outcomes.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[27]	['[27]  Burcu Yucesoy and Albert-László Barabási. 2016. Untangling performance from success. EPJ Data Science 5, 1 (2016), 17. ']
1118	584	[424]	Yucesoy et al. [27] proposed a model aiming at predicting which books will become bestsellers. // And, Deville et al. ==[3]== studied quantifying the career choices such as changing institutions affecting scientific outcomes. // However, the definition of success, or outcome, in previous studies varies greatly from paper citations number to art value.	b	TUBE: Embedding Behavior Outcomes for Predicting Success	[3]	['[3]  Pierre Deville, Dashun Wang, Roberta Sinatra, Chaoming Song, Vincent D Blondel, and Albert-László Barabási. 2014. Career on the move: Geography, stratification, and scientific impact. Scientific reports 4 (2014), 4770. ']
1119	415	[425, 426]	It has been widely used in many applications such as recommender systems, search engines, question answering systems and so on. // For example, in search engines, queries and webpages form a bipartite network, where the edges can indicate users’ click behaviors that provide valuable relevance signal ==[1, 2]==; in another application of recommender systems, users and items form a bipartite network, where the edges can encode users’ rating behaviors that contain rich collaborative filtering patterns [3]. // To perform predictive analytics on network data, it is crucial to first obtain the representations (i.e., feature vectors) for vertices.	b	BiNE: Bipartite Network Embedding	[1, 2]	['[1]  Hongbo Deng, Michael R. Lyu, and Irwin King. A generalized co-hits algorithm and its application to bipartite graphs. In KDD, pages 239–248, 2009. ', '[2]  Shan Jiang, Yuening Hu, Changsung Kang, Tim Daly Jr., Dawei Yin, Yi Chang, and ChengXiang Zhai. Learning query and document relevance from a web-scale click graph. In SIGIR, pages 185–194, 2016. ']
1120	415	[427]	It has been widely used in many applications such as recommender systems, search engines, question answering systems and so on. // For example, in search engines, queries and webpages form a bipartite network, where the edges can indicate users’ click behaviors that provide valuable relevance signal [1, 2]; in another application of recommender systems, users and items form a bipartite network, where the edges can encode users’ rating behaviors that contain rich collaborative filtering patterns ==[3]==. // To perform predictive analytics on network data, it is crucial to first obtain the representations (i.e., feature vectors) for vertices.	b	BiNE: Bipartite Network Embedding	[3]	['[3]  Xiangnan He, Ming Gao, Min-Yen Kan, and Dingxian Wang. Birank: Towards ranking on bipartite graphs. TKDE, 29(1):57–71, 2017. ']
1121	415	[6]	To date, existing works have primarily focused on embedding homogeneous networks where vertices are of the same type [4, 8– 10]. // Following the pioneering work of DeepWalk ==[8]==, these methods typically apply a two-step solution: first performing random walks on the network to obtain a “corpus” of vertices, and then applying word embedding methods such as word2vec [11] to obtain the embeddings for vertices. //  Despite effectiveness and prevalence, we argue that these methods can be suboptimal for embedding bipartite networks due to two primary reasons.	h-	BiNE: Bipartite Network Embedding	[8]	['[8]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In KDD, pages 701–710, 2014. ']
1122	415	[36]	To date, existing works have primarily focused on embedding homogeneous networks where vertices are of the same type [4, 8– 10]. // Following the pioneering work of DeepWalk [8], these methods typically apply a two-step solution: first performing random walks on the network to obtain a “corpus” of vertices, and then applying word embedding methods such as word2vec ==[11]== to obtain the embeddings for vertices. //  Despite effectiveness and prevalence, we argue that these methods can be suboptimal for embedding bipartite networks due to two primary reasons.	b	BiNE: Bipartite Network Embedding	[11]	['[11]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
1123	415	[428]	Although edges exist between vertices of different types only, there are essentially implicit relations between vertices of the same type. // For example, in the useritem bipartite network built for recommendation, there exists an implicit relation between users which can indicate their preference in consuming the same item; and importantly, it is recently reported that modeling such implicit relations can improve the recommendation performance ==[12]==. // However, existing network embedding methods modeled the explicit relation (i.e., observed edges) only and ignored the underlying implicit relations.	h-	BiNE: Bipartite Network Embedding	[12]	['[12]  Lu Yu, Chuxu Zhang, Shichao Pei, Guolei Sun, and Xiangliang Zhang and. Walkranker: A unified pairwise ranking model with multiple relations for item recommendation. In AAAI, 2018. ']
1124	415	[111]	However, existing network embedding methods modeled the explicit relation (i.e., observed edges) only and ignored the underlying implicit relations. // While a recent work by Dong et al. ==[14]== proposed metapath2vec++ for embedding heterogeneous networks which can also be applied to bipartite networks, we argue that a key limitation is that it treats the explicit and implicit relations as contributing equally to the learning. // In real-world bipartite networks, the explicit and implicit relations typically carry different semantics.	h-	BiNE: Bipartite Network Embedding	[14]	['[14]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In KDD, pages 135–144. ACM, 2017. ']
1125	415	[192]	To our knowledge, none of the existing works has paid special attention to embed bipartite networks. // This can be evidenced by existing recommendation works ==[15]== that usually assign varying weights on different sources of information to allow a flexible tuning on the learning process. // In this work, we focus on the problem of learning vertex representations for bipartite networks.	b	BiNE: Bipartite Network Embedding	[15]	['[15]  Meng Jiang, Peng Cui, Nicholas Jing Yuan, Xing Xie, and Shiqiang Yang. Little is much: Bridging cross-platform behaviors through overlapped crowds. In AAAI, pages 13–19, 2016. ']
1126	415	[429]	Our work is related to vertex representation learning methods on homogeneous networks, which can be categorized into two types: matrix factorization (MF)-based and neural network-based methods. // MF-based methods are either linear ==[16]== or nonlinear [17] in learning vertex embeddings. // The former employs the linear transformations to embed network vertices into a low dimensional embedding space, such as singular value decomposition (SVD) and multiple dimensional scaling (MDS) [16].	b	BiNE: Bipartite Network Embedding	[16]	['[16]  Trevor F Cox and Michael AA Cox. Multidimensional scaling. CRC press, 2000. ']
1127	415	[430]	Our work is related to vertex representation learning methods on homogeneous networks, which can be categorized into two types: matrix factorization (MF)-based and neural network-based methods. // MF-based methods are either linear [16] or nonlinear ==[17]== in learning vertex embeddings. // The former employs the linear transformations to embed network vertices into a low dimensional embedding space, such as singular value decomposition (SVD) and multiple dimensional scaling (MDS) [16].	b	BiNE: Bipartite Network Embedding	[17]	['[17]  Angelia Nedic and Asuman E. Ozdaglar. A geometric framework for nonconvex optimization duality using augmented lagrangian functions. J. Global Optimization, 40(4):545–573, 2008. ']
1128	415	[429]	MF-based methods are either linear [16] or nonlinear [17] in learning vertex embeddings. // The former employs the linear transformations to embed network vertices into a low dimensional embedding space, such as singular value decomposition (SVD) and multiple dimensional scaling (MDS) ==[16]==. // However, the latter maps network vertices into a low dimensional latent space by utilizing the nonlinear transformations, e.g., kernel PCA, spectral embedding , marginal fisher analysis (MFA), and manifold learning approaches include LLE and ISOMAP [17].	b	BiNE: Bipartite Network Embedding	[16]	['[16]  Trevor F Cox and Michael AA Cox. Multidimensional scaling. CRC press, 2000. ']
1129	415	[430]	The former employs the linear transformations to embed network vertices into a low dimensional embedding space, such as singular value decomposition (SVD) and multiple dimensional scaling (MDS) [16]. // However, the latter maps network vertices into a low dimensional latent space by utilizing the nonlinear transformations, e.g., kernel PCA, spectral embedding , marginal fisher analysis (MFA), and manifold learning approaches include LLE and ISOMAP ==[17]==. // Generally speaking, MF-based methods have two main drawbacks: (1) they are usually computationally expensive due to the eigen-decomposition operations on data matrices, making them difficult to handle large-scale networks [18, 19]; (2) their performance are rather sensitive to the predefined proximity measures for calculating the affinity matrix.	h-	BiNE: Bipartite Network Embedding	[17]	['[17]  Angelia Nedic and Asuman E. Ozdaglar. A geometric framework for nonconvex optimization duality using augmented lagrangian functions. J. Global Optimization, 40(4):545–573, 2008. ']
1130	415	[6]	Neural network-based methods are the state-of-art vertex representation learning techniques. // The pioneer work DeepWalk ==[8]== and Node2vec [4] extend the idea of Skip-gram [11] to model homogeneous network, which is convert to a corpus of vertex sequences by performing truncated random walks. // However, they may not be effective to preserve both explicit and implicit relations of the network.	h-	BiNE: Bipartite Network Embedding	[8]	['[8]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In KDD, pages 701–710, 2014. ']
1131	415	[37]	Neural network-based methods are the state-of-art vertex representation learning techniques. // The pioneer work DeepWalk [8] and Node2vec ==[4]== extend the idea of Skip-gram [11] to model homogeneous network, which is convert to a corpus of vertex sequences by performing truncated random walks. // However, they may not be effective to preserve both explicit and implicit relations of the network.	h-	BiNE: Bipartite Network Embedding	[4]	['[4]  Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pages 855–864, 2016. ']
1132	415	[36]	Neural network-based methods are the state-of-art vertex representation learning techniques. // The pioneer work DeepWalk [8] and Node2vec [4] extend the idea of Skip-gram ==[11]== to model homogeneous network, which is convert to a corpus of vertex sequences by performing truncated random walks. // However, they may not be effective to preserve both explicit and implicit relations of the network.	b	BiNE: Bipartite Network Embedding	[11]	['[11]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
1133	415	[1]	There are some follow-up works exploiting both 1st-order and 2nd-order proximities between vertices to embed homogeneous networks. // Specifically, LINE ==[20]== learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information [24], textual content [25], user profiles [9], location information [26], among others.	b	BiNE: Bipartite Network Embedding	[20]	['[20]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale information network embedding. In WWW, pages 1067–1077, 2015. ']
1134	415	[19]	There are some follow-up works exploiting both 1st-order and 2nd-order proximities between vertices to embed homogeneous networks. // Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE ==[21]== incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information [24], textual content [25], user profiles [9], location information [26], among others.	b	BiNE: Bipartite Network Embedding	[21]	['[21]  Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In KDD, pages 1225–1234, 2016. ']
1135	415	[26]	There are some follow-up works exploiting both 1st-order and 2nd-order proximities between vertices to embed homogeneous networks. // Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep ==[22]== further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information [24], textual content [25], user profiles [9], location information [26], among others.	b	BiNE: Bipartite Network Embedding	[22]	['[22]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information. In CIKM, pages 891–900, 2015. ']
1136	415	[103, 29]	Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels ==[10, 23]==, community information [24], textual content [25], user profiles [9], location information [26], among others. // It is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks, for which there is only one type of vertices.	h-	BiNE: Bipartite Network Embedding	[10, 23]	['[10]  Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. Attributed network embedding for learning in a dynamic environment. In CIKM, pages 387–396, 2017. ', '[23]  Xiao Huang, Jundong Li, and Xia Hu. Label informed attributed network embedding. In WSDM, pages 550–558, 2017. ']
1137	415	[39]	Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information ==[24]==, textual content [25], user profiles [9], location information [26], among others. // It is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks, for which there is only one type of vertices.	h-	BiNE: Bipartite Network Embedding	[24]	['[24]  Jifan Chen, Qi Zhang, and Xuanjing Huang. Incorporate group information to enhance network embedding. In CIKM, pages 1901–1904, 2016. ']
1138	415	[431]	Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information [24], textual content ==[25]==, user profiles [9], location information [26], among others. // It is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks, for which there is only one type of vertices.	h-	BiNE: Bipartite Network Embedding	[25]	['[25]  Chuan-Ju Wang, Ting-Hsiang Wang, Hsiu-Wei Yang, Bo-Sin Chang, and MingFeng Tsai. Ice: Item concept embedding via textual information. In SIGIR, pages 85–94, 2017. ']
1139	415	[432]	Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information [24], textual content [25], user profiles ==[9]==, location information [26], among others. // It is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks, for which there is only one type of vertices.	h-	BiNE: Bipartite Network Embedding	[9]	['[9]  Lizi Liao, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua. Attributed social network embedding. TKDE, 2018. ']
1140	415	[15]	Specifically, LINE [20] learns two separated embeddings for 1st-order and 2nd-order relations; SDNE [21] incorporates both 1st-order and 2nd-order proximities to preserve the network structure; and GraRep [22] further extends the method to capture higher-order proximities. // Besides capturing high-order proximities, there are several proposals to incorporate side information into vertex embedding learning, such as vertex labels [10, 23], community information [24], textual content [25], user profiles [9], location information ==[26]==, among others. // It is worth pointing out that the above mentioned methods are designed for embedding homogeneous networks, for which there is only one type of vertices.	h-	BiNE: Bipartite Network Embedding	[26]	['[26]  Min Xie, Hongzhi Yin, Hao Wang, Fanjiang Xu, Weitong Chen, and Sen Wang. Learning graph-based POI embedding for location-based recommendation. In CIKM, pages 15–24, 2016. ']
1141	415	[111]	Thus, these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network. // Metapath2vec++ ==[14]==, HNE [27] and EOE [28] are representative vertex embedding methods for heterogeneous networks. //  Although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks, they are not tailored for learning on bipartite networks.	h-	BiNE: Bipartite Network Embedding	[14]	['[14]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In KDD, pages 135–144. ACM, 2017. ']
1142	415	[109]	Thus, these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network. // Metapath2vec++ [14], HNE ==[27]== and EOE [28] are representative vertex embedding methods for heterogeneous networks. //  Although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks, they are not tailored for learning on bipartite networks.	h-	BiNE: Bipartite Network Embedding	[27]	['[27]  Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C. Aggarwal, and Thomas S. Huang. Heterogeneous network embedding via deep architectures. In KDD, pages 119–128, 2015. ']
1143	415	[387]	Thus, these homogeneous network embedding methods might be suboptimal for learning vertex representations for a bipartite network. // Metapath2vec++ [14], HNE [27] and EOE ==[28]== are representative vertex embedding methods for heterogeneous networks. //  Although they can be applied to bipartite network which can be seen as a special type of heterogeneous networks, they are not tailored for learning on bipartite networks.	h-	BiNE: Bipartite Network Embedding	[28]	['[28]  Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip S. Yu. Embedding of embedding (EOE): joint embedding for coupled heterogeneous networks. In WSDM, pages 741–749, 2017. ']
1144	415	[215]	As a ubiquitous data structure, bipartite networks have been mined for many applications, among which vertex ranking is an active research problem. // For example, HITS ==[29]== learns to rank vertices by capturing some semantic relations within a bipartite network. // Co-HITS [1] incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network.	b	BiNE: Bipartite Network Embedding	[29]	['[29]  Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. In ACM-SIAM, pages 668–677, 1998. ']
1145	415	[425]	For example, HITS [29] learns to rank vertices by capturing some semantic relations within a bipartite network. // Co-HITS ==[1]== incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network. // BiRank [3] ranks vertices by taking into account both the network structure and prior knowledge.	b	BiNE: Bipartite Network Embedding	[1]	['[1]  Hongbo Deng, Michael R. Lyu, and Irwin King. A generalized co-hits algorithm and its application to bipartite graphs. In KDD, pages 239–248, 2009. ']
1146	415	[427]	Co-HITS [1] incorporates content information of vertices and the constraints on relevance into vertex ranking of bipartite network. // BiRank ==[3]== ranks vertices by taking into account both the network structure and prior knowledge. // Distributed vertex representation is an alternative way to leverage signals from bipartite network.	b	BiNE: Bipartite Network Embedding	[3]	['[3]  Xiangnan He, Ming Gao, Min-Yen Kan, and Dingxian Wang. Birank: Towards ranking on bipartite graphs. TKDE, 29(1):57–71, 2017. ']
1147	415	[433]	And a typical implementation of LFM is based on matrix factorization [30– 32]. // Recent advances utilize deep learning methods to learn vertex embeddings on the user-item network for recommendation ==[33]==. // It is worth pointing out that these methods are tailored for the recommendation task, rather than for learning informative vertex embeddings.	h-	BiNE: Bipartite Network Embedding	[33]	['[33]  Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In WWW, pages 173–182, 2017. ']
1148	415	[428, 192]	It is worth pointing out that these methods are tailored for the recommendation task, rather than for learning informative vertex embeddings. // Moreover, they model the explicit relations in bipartite network only, which can be improved by incorporating implicit relations as shown in ==[12, 15]==. // PROBLEM FORMULATION	b	BiNE: Bipartite Network Embedding	[12, 15]	['[12]  Lu Yu, Chuxu Zhang, Shichao Pei, Guolei Sun, and Xiangliang Zhang and. Walkranker: A unified pairwise ranking model with multiple relations for item recommendation. In AAAI, 2018. ', '[15]  Meng Jiang, Peng Cui, Nicholas Jing Yuan, Xing Xie, and Shiqiang Yang. Little is much: Bridging cross-platform behaviors through overlapped crowds. In AAAI, pages 13–19, 2016. ']
1149	415	[428, 192]	Modeling Implicit Relations. // As illustrated in existing recommedation works ==[12, 15]==, both explicit and implicit relations are helpful to reveal different semantic in bipartite networks. // To be comprehensive, it is crucial to also account for the implicit relation between two vertices of the same type, even though they are not explicitly connected.	b	BiNE: Bipartite Network Embedding	[12, 15]	['[12]  Lu Yu, Chuxu Zhang, Shichao Pei, Guolei Sun, and Xiangliang Zhang and. Walkranker: A unified pairwise ranking model with multiple relations for item recommendation. In AAAI, 2018. ', '[15]  Meng Jiang, Peng Cui, Nicholas Jing Yuan, Xing Xie, and Shiqiang Yang. Little is much: Bridging cross-platform behaviors through overlapped crowds. In AAAI, pages 13–19, 2016. ']
1150	415	[37, 6]	Constructing Corpus of Vertex Sequences. // It is a common way to convert a network into a corpus of vertex sequences by performing random walks on the network, which has been used in some homogeneous network embedding methods ==[4, 8]==. // However, directly performing random walks on a bipartite network could fail, since there is no stationary distribution of random walks on bipartite networks due to the periodicity issue [34].	h-	BiNE: Bipartite Network Embedding	[4, 8]	['[4]  Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pages 855–864, 2016. ', '[8]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In KDD, pages 701–710, 2014. ']
1151	415	[434]	It is a common way to convert a network into a corpus of vertex sequences by performing random walks on the network, which has been used in some homogeneous network embedding methods [4, 8]. // However, directly performing random walks on a bipartite network could fail, since there is no stationary distribution of random walks on bipartite networks due to the periodicity issue ==[34]==. // To address this issue, we consider performing random walks on two homogeneous networks that contain the 2nd-order proximity between vertices of the same type.	ho	BiNE: Bipartite Network Embedding	[34]	['[34]  Taher Alzahrani, Kathy J. Horadam, and Serdar Boztas. Community detection in bipartite networks using random walks. In CompleNet, pages 157–165, 2014. ']
1152	415	[425]	To address this issue, we consider performing random walks on two homogeneous networks that contain the 2nd-order proximity between vertices of the same type. // Following the idea of Co-HITS ==[1]==, we define the 2nd-order proximity between two vertices as: where wij is the weight of edge eij . // Hence, we can use the |U | × |U | matrix WU = [w U ij ] and the |V | × |V | matrix WV = [w V ij] to represent the two induced homogeneous networks, respectively.	hoe	BiNE: Bipartite Network Embedding	[1]	['[1]  Hongbo Deng, Michael R. Lyu, and Irwin King. A generalized co-hits algorithm and its application to bipartite graphs. In KDD, pages 239–248, 2009. ']
1153	415	[111]	We assign a probability to stop a random walk in each step. // In contrast to DeepWalk and other work ==[14]== that apply a fixed length on the random walk, we allow the generated vertex sequences have a variable length, in order to have a close analogy to the variable-length sentences in natural languages. // Generally speaking, the above generation process follows the principle of “rich gets richer”, which is a physical phenomena existing in many real networks, i.e., the vertex connectivities follow a scalefree power-law distribution [35].	ro	BiNE: Bipartite Network Embedding	[14]	['[14]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In KDD, pages 135–144. ACM, 2017. ']
1154	415	[36]	Nevertheless, optimizing the objectives is non-trivial, since each evaluation of the softmax function needs to traverse all vertices of a side, which is very time-costing. // To reduce the learning complexity, we employ the idea of negative sampling ==[11]==. // Negative Sampling.	ho	BiNE: Bipartite Network Embedding	[11]	['[11]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
1155	415	[435]	Negative Sampling. // The idea of negative sampling is to approximate the costly denominator term of softmax with some sampled negative instances ==[36]==. // Then the learning can be performed by optimizing a point-wise classification loss.	b	BiNE: Bipartite Network Embedding	[36]	['[36]  Hongzhi Yin, Lei Zou, Quoc Viet Hung Nguyen, Zi Huang, and Xiaofang Zhou. Joint event-partner recommendation in event-based social networks. In ICDE, 2018. ']
1156	415	[36]	For a center vertex ui, high-quality negatives should be the vertices that are dissimilar from ui. // Towards this goal, some heuristics have been applied, such as sampling from popularity-biased non-uniform distribution ==[11]==. // Here we propose a more grounded sampling method that caters the network data.	ro	BiNE: Bipartite Network Embedding	[11]	['[11]  Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013. ']
1157	415	[436]	Here we propose a more grounded sampling method that caters the network data. // First we employ locality sensitive hashing (LSH) ==[37]== to block vertices after shingling each vertex by its ws-hop neighbors with respect to the topological structure in the input bipartite network. // Given a center vertex, we then randomly choose the negative samples from the buckets that are different from the bucket contained the center vertex.	ho	BiNE: Bipartite Network Embedding	[37]	['[37]  Hongya Wang, Jiao Cao, LihChyun Shu, and Davood Rafiei. Locality sensitive hashing revisited: filling the gap between theory and algorithm analysis. In CIKM, pages 1969–1978, 2013. ']
1158	415	[436]	Given a center vertex, we then randomly choose the negative samples from the buckets that are different from the bucket contained the center vertex.  // Through this way, we can obtain high-quality and diverse negative samples, since LSH can guarantee that dissimilar vertices are located in different buckets in a probabilistic way ==[37]==. // Let N ns S(ui) denote the ns negative samples for a center vertex ui in sequence S ∈ DU , we can then approximate the conditional probability	ho	BiNE: Bipartite Network Embedding	[37]	['[37]  Hongya Wang, Jiao Cao, LihChyun Shu, and Davood Rafiei. Locality sensitive hashing revisited: filling the gap between theory and algorithm analysis. In CIKM, pages 1969–1978, 2013. ']
1159	415	[6]	For each method, we use the released implementations of the authors for our experiments. // DeepWalk ==[8]==: As a homogeneous network embedding method, DeepWalk performs uniform random walks to get a corpus of vertex sequences. // Then the word2vec is applied on the corpus to learn vertex embeddings.	b	BiNE: Bipartite Network Embedding	[8]	['[8]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In KDD, pages 701–710, 2014. ']
1160	415	[1]	Then the word2vec is applied on the corpus to learn vertex embeddings. // LINE ==[20]==: This approach optimizes both the 1st-order and 2nd-order proximities in a homogeneous network. // We use the LINE(1st+2nd) method which has shown the best results in their paper.	b	BiNE: Bipartite Network Embedding	[20]	['[20]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale information network embedding. In WWW, pages 1067–1077, 2015. ']
1161	415	[37]	We use the LINE(1st+2nd) method which has shown the best results in their paper. // Node2vec ==[4]==: This method extends DeepWalk by performing biased random walks to generate the corpus of vertex sequences. // The hyper-parameters p and q are set to 0.5 which has empirically shown good results.	b	BiNE: Bipartite Network Embedding	[4]	['[4]  Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pages 855–864, 2016. ']
1162	415	[111]	The hyper-parameters p and q are set to 0.5 which has empirically shown good results. // Metapath2vec++ ==[14]==: This is the state-of-the-art method for embedding heterogeneous networks. // The meta-path scheme chosen in our experiments are “IUI” (item-user-item) and “IUI”+“UIU” (user-item-user), and we only report the best result between them.	b	BiNE: Bipartite Network Embedding	[14]	['[14]  Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In KDD, pages 135–144. ACM, 2017. ']
1163	415	[437]	We compare with several competitive methods that are designed for the top-K item recommendation task. // BPR ==[31]==:This method optimizes the matrix factorization (MF) model with a pairwise ranking-aware objective. //  This method has been widely used in recommendation literature as a highly competitive baseline [33].	b	BiNE: Bipartite Network Embedding	[31]	['[31]  Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. BPR: bayesian personalized ranking from implicit feedback. In UAI, pages 452– 461, 2009. ']
1164	415	[433]	BPR [31]:This method optimizes the matrix factorization (MF) model with a pairwise ranking-aware objective. // This method has been widely used in recommendation literature as a highly competitive baseline ==[33]==. // RankALS [39]: This method also optimizes the MF model for the ranking task, by towards a different pairwise regressionbased loss.	b	BiNE: Bipartite Network Embedding	[33]	['[33]  Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering. In WWW, pages 173–182, 2017. ']
1165	415	[438]	This method has been widely used in recommendation literature as a highly competitive baseline [33]. // RankALS ==[39]==: This method also optimizes the MF model for the ranking task, by towards a different pairwise regressionbased loss. // FISMauc [40]: Distinct to MF, factored item similarity model (FISM) is an item-based collaborative filtering method. We employ the AUC-based objective to optimize FISM for the top-K task.	b	BiNE: Bipartite Network Embedding	[39]	['[39]  Gábor Takács and Domonkos Tikk. Alternating least squares for personalized ranking. In RecSys, pages 83–90, 2012. ']
1166	415	[439]	RankALS [39]: This method also optimizes the MF model for the ranking task, by towards a different pairwise regressionbased loss. // FISMauc ==[40]==: Distinct to MF, factored item similarity model (FISM) is an item-based collaborative filtering method. // We employ the AUC-based objective to optimize FISM for the top-K task.	b	BiNE: Bipartite Network Embedding	[40]	['[40]  Santosh Kabbur, Xia Ning, and George Karypis. FISM: factored item similarity models for top-n recommender systems. In KDD, pages 659–667, 2013. ']
1167	415	[440]	This is due to the factors that. // One index proposed in ==[38]== only emphasizes one kind of network topological structure, rather than the global structure. // The neural network-based methods predict the links in a data-dependent supervised manner, which is more advantageous.	h-	BiNE: Bipartite Network Embedding	[38]	['[38]  Shuang Xia, Bing Tian Dai, Ee-Peng Lim, Yong Zhang, and Chunxiao Xing. Link prediction for bipartite social networks: The role of structural holes. In ASONAM, pages 153–157, 2012. ']
1168	54	[441]	Network (or graph) is a group of interconnected nodes and contains a wealth of information on the relationships between every pair of nodes. // The analysis of graph is required in almost every field, for instance, online social network ==[4]==, biological research [23], credit rating [9] and so on. // Birds of a feather flock together and people always have certain characteristics in common with their friends surrounded by.	b	Enhancing the Network Embedding Quality with Structural Similarity	[4]	['[4]  Linton C Freeman. 2000. Visualizing social networks. Journal of social structure 1, 1 (2000), 4. ']
1169	54	[442]	Network (or graph) is a group of interconnected nodes and contains a wealth of information on the relationships between every pair of nodes. // The analysis of graph is required in almost every field, for instance, online social network [4], biological research ==[23]==, credit rating [9] and so on. // Birds of a feather flock together and people always have certain characteristics in common with their friends surrounded by.	b	Enhancing the Network Embedding Quality with Structural Similarity	[23]	['[23]  Athanasios Theocharidis, Stjin Van Dongen, Anton J Enright, and Tom C Freeman. 2009. Network visualization and analysis of gene expression data using BioLayout Express3D. Nature protocols 4, 10 (2009), 1535–1550. ']
1170	54	[443]	Network (or graph) is a group of interconnected nodes and contains a wealth of information on the relationships between every pair of nodes. // The analysis of graph is required in almost every field, for instance, online social network [4], biological research [23], credit rating ==[9]== and so on. // Birds of a feather flock together and people always have certain characteristics in common with their friends surrounded by.	b	Enhancing the Network Embedding Quality with Structural Similarity	[9]	['[9]  Shian-Chang Huang. 2009. Integrating nonlinear graph based dimensionality reduction schemes with SVMs for credit rating forecasting. Expert Systems with Applications 36, 4 (2009), 7515–7518. ']
1171	54	[3]	Unsupervised network embedding algorithms try to preserve the local relationships of each node in the graph. // IsoMap ==[22]==, LLE [17] and Laplacian eigenmaps [1] are three classic dimensionality reduction and data representation algorithms. // Finding the k nearest neighbors is the key step of these three algorithms.	b	Enhancing the Network Embedding Quality with Structural Similarity	[22]	['[22]  Joshua B Tenenbaum, Vin De Silva, and John C Langford. 2000. A global geometric framework for nonlinear dimensionality reduction. science 290, 5500 (2000), 2319– 2323. ']
1172	54	[4]	Unsupervised network embedding algorithms try to preserve the local relationships of each node in the graph. // IsoMap [22], LLE ==[17]== and Laplacian eigenmaps [1] are three classic dimensionality reduction and data representation algorithms. // Finding the k nearest neighbors is the key step of these three algorithms.	b	Enhancing the Network Embedding Quality with Structural Similarity	[17]	['[17]  Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science 290, 5500 (2000), 2323–2326. ']
1173	54	[444]	Unsupervised network embedding algorithms try to preserve the local relationships of each node in the graph. // IsoMap [22], LLE [17] and Laplacian eigenmaps ==[1]== are three classic dimensionality reduction and data representation algorithms. // Finding the k nearest neighbors is the key step of these three algorithms.	b	Enhancing the Network Embedding Quality with Structural Similarity	[1]	['[1]  Mikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation 15, 6 (2003), 1373–1396. ']
1174	54	[6]	Classic algorithms can hardly tackle real networks due to the huge computational complexity of eigen-decomposition. // DeepWalk ==[15]== first introduces deep learning techniques word2vec to the network embedding task. // The authors pioneered the similarity between random walks and natural language.	b	Enhancing the Network Embedding Quality with Structural Similarity	[15]	['[15]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701–710. ']
1175	54	[26, 37, 1, 20]	The authors pioneered the similarity between random walks and natural language. // Later, more work ==[2, 7, 19, 28]== is presented to improve DeepWalk by extending the definition of neighborhood and capturing neighborhood information from different levels of scope, namely first-order proximity, second-order proximity and higher-order proximity. // Using random walks to capture the local structure is truly a neat idea, which makes it possible to build representations of big networks.	h+	Enhancing the Network Embedding Quality with Structural Similarity	[2, 7, 19, 28]	['[2]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 891–900. ', '[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ', '[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web. ACM, 1067–1077. ', '[28]  Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. 2016. Homophily, Structure, and Content Augmented Network Representation Learning. In Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 609–618.']
1176	54	[37]	The strategy focuses on neighbors.  // On the other hand, Figure 1b shows another strategy, that is dividing nodes by their structural similarity, namely structural equivalence defined in ==[7]==. // There are three groups altogether, core nodes, peripheral nodes and hub nodes under this criteria for network partition. These different starting points induce different definitions of similar nodes: (1) Densely connected nodes or (2) Nodes with similar network positions.	b	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1177	54	[445]	The most sensible approach is to make a balance between these two strategies. // Structural equivalence is being discussed not only in social role mining task ==[3]==, but in recent network embedding algorithms as well [7]. // We support the motivation that both kinds of similar nodes should be taken into consideration.	b	Enhancing the Network Embedding Quality with Structural Similarity	[3]	['[3]  Sarvenaz Choobdar, Pedro Ribeiro, Srinivasan Parthasarathy, and Fernando Silva. 2015. Dynamic inference of social roles in information cascades. Data Mining and Knowledge Discovery 29, 5 (2015), 1152–1177. ']
1178	54	[37]	The most sensible approach is to make a balance between these two strategies. // Structural equivalence is being discussed not only in social role mining task [3], but in recent network embedding algorithms as well ==[7]==. // We support the motivation that both kinds of similar nodes should be taken into consideration.	b	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1179	54	[56, 55]	We refer the reader to [6] for more comprehensive details. // Traditional methods ==[20, 21]== mainly focus on dimension reduction. // By the techniques of matrix factorization, traditional methods project the adjacency matrix to a low-dimension space.	b	Enhancing the Network Embedding Quality with Structural Similarity	[20, 21]	['[20]  Lei Tang and Huan Liu. 2009. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management. ACM, 1107–1116. ', '[21]  Lei Tang and Huan Liu. 2011. Leveraging social media networks for classification. Data Mining and Knowledge Discovery 23, 3 (2011), 447–478. ']
1180	54	[6]	The difference between the stateof-art neural network based algorithms lies in the neighborhood sampling strategies. // DeepWalk ==[15]== uses depth-first search in order to sample the neighborhood of the target node. The depth is set to 2 by default. // GraRep [2] also uses DFS but the depth is larger	b	Enhancing the Network Embedding Quality with Structural Similarity	[15]	['[15]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701–710. ']
1181	54	[26]	DeepWalk [15] uses depth-first search in order to sample the neighborhood of the target node. The depth is set to 2 by default. // GraRep ==[2]== also uses DFS but the depth is larger. // LINE [19] uses breadth-first search as well as depth-first search. The number of step are all constrained below two.	b	Enhancing the Network Embedding Quality with Structural Similarity	[2]	['[2]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 891–900. ']
1182	54	[1]	GraRep [2] also uses DFS but the depth is larger. // LINE ==[19]== uses breadth-first search as well as depth-first search. The number of step are all constrained below two. // The number of step are all constrained below two.	b	Enhancing the Network Embedding Quality with Structural Similarity	[19]	['[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web. ACM, 1067–1077. ']
1183	54	[37]	The number of step are all constrained below two. // node2vec ==[7]== also uses both two kinds of search methods and finds the balance point by semi-supervised learning. // It selects the optimal balance point with grid search method, which is time-consuming.	h-	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1184	54	[19]	Obviously, none of these algorithms take structural similarity into consideration. // SDNE ==[24]== is not based on word2vec framework but deep autoencoder instead. // It has the same goal as LINE does, considering both first-order proximity and second-order proximity.	b	Enhancing the Network Embedding Quality with Structural Similarity	[24]	['[24]  Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1225–1234. ']
1185	54	[26]	. It has the same goal as LINE does, considering both first-order proximity and second-order proximity. // GraRep ==[2]== and node2vec [7] discuss local structure and structural equivalence. // However, it is questionable whether the structural similarity is actually used during the learning process.	h-	Enhancing the Network Embedding Quality with Structural Similarity	[2]	['[2]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 891–900. ']
1186	54	[37]	. It has the same goal as LINE does, considering both first-order proximity and second-order proximity. // GraRep [2] and node2vec ==[7]== discuss local structure and structural equivalence. // However, it is questionable whether the structural similarity is actually used during the learning process.	h-	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1187	54	[446]	Computing similarities between structured objects (network) is a hot topic in recent years. // Methods are mainly based on graphlets, subtree patterns and random walks ==[25]==. //  In the network embedding task, we focus on nodes rather than the whole network structure, which means that we need a metric describing the structural characteristics of a single node.	b	Enhancing the Network Embedding Quality with Structural Similarity	[25]	['[25]  Pinar Yanardag and S.V.N. Vishwanathan. 2015. Deep Graph Kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’15). ACM, New York, NY, USA, 1365–1374. https://doi.org/10.1145/2783258.2783417 ']
1188	54	[447]	In the network embedding task, we focus on nodes rather than the whole network structure, which means that we need a metric describing the structural characteristics of a single node. // This topic has been given much consideration in the real fields such as biology and social science ==[26]==. // We first introduce graphlets and graphlet-based network distance measures.	b	Enhancing the Network Embedding Quality with Structural Similarity	[26]	['[26]  Ömer Nebil Yaveroğlu, Noël Malod-Dognin, Darren Davis, Zoran Levnajic, Vuk Janjic, Rasa Karapandza, Aleksandar Stojmirovic, and Nataša Pržulj. 2014. Revealing the hidden language of complex networks. Scientific reports 4 (2014), 4547. ']
1189	54	[448]	There are many graphlet counting algorithms that can provide precise results on small graph and approximate results on big graph with a quick speed. // In this paper, we use orca ==[8]== to help us calculate GDV of each node. // The code is available on the authors’ website.	ho	Enhancing the Network Embedding Quality with Structural Similarity	[8]	['[8]  Tomaž Hočevar and Janez Demšar. 2014. A combinatorial approach to graphlet counting. Bioinformatics 30, 4 (2014), 559–565. ']
1190	54	[449]	Too many features, however, will result in over-fitting and not all orbits contribute equally to a certain task. // Therefore, we evaluate the importance of different orbits on the node classification task ==[5]== with the help of Random Forest [14]. // Random Forest consists of a number of decision trees.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[5]	['[5]  Robin Genuer, Jean-Michel Poggi, and Christine Tuleau-Malot. 2010. Variable selection using random forests. Pattern Recognition Letters 31, 14 (2010), 2225– 2236. ']
1191	54	[450]	Too many features, however, will result in over-fitting and not all orbits contribute equally to a certain task. // Therefore, we evaluate the importance of different orbits on the node classification task [5] with the help of Random Forest ==[14]==. // Random Forest consists of a number of decision trees.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[14]	['[14]  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830. ']
1192	54	[450]	Note that features with more categories and with less correlated features are considered to be more important when using the impurity based ranking. // We use random forest implemented by sklearn ==[14]== and calculate the orbit importance Io as stated before. The experiments are repeated on several datasets. // The experiments are repeated on several datasets.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[14]	['[14]  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830. ']
1193	54	[6]	EXPERIMENTS. // In this section we first visualize a small network using DeepWalk ==[15]==, node2vec [7] and our proposed algorithm separately, which demonstrates the arguments in Section 3 visually. // Moreover, we show the performance of different network embedding algorithms on multi-label classification task.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[15]	['[15]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701–710. ']
1194	54	[37]	EXPERIMENTS. // In this section we first visualize a small network using DeepWalk [15], node2vec ==[7]== and our proposed algorithm separately, which demonstrates the arguments in Section 3 visually. // Moreover, we show the performance of different network embedding algorithms on multi-label classification task.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1195	54	[6]	Nodes in this network have various structural positions, making it easier to analyze different sampling strategies thoroughly. // We compare our algorithm with DeepWalk ==[15]== and node2vec [7]. // Both of them are classic network embedding algorithm with random-walk based sampling strategy.	b	Enhancing the Network Embedding Quality with Structural Similarity	[15]	['[15]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701–710. ']
1196	54	[37]	Nodes in this network have various structural positions, making it easier to analyze different sampling strategies thoroughly. // We compare our algorithm with DeepWalk [15] and node2vec ==[7]==. // Both of them are classic network embedding algorithm with random-walk based sampling strategy.	b	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1197	54	[180]	They all exist a mix of homophily and structural equivalences [7]. // BlogCatalog ==[27]== is the social blog directory which manages the bloggers and their blogs. // The network depicts the contact between users and the user labels represent their interests.	b	Enhancing the Network Embedding Quality with Structural Similarity	[27]	['[27]  R. Zafarani and H. Liu. 2009. Social Computing Data Repository at ASU. (2009). http://socialcomputing.asu.edu ']
1198	54	[451]	The network contains 10,312 nodes, 333,983 edges and 39 labels. // Protein-Protein Interactions ==[18]== is a subgraph of the PPI network for Homo Sapiens. // Labels stand for the protein biological states.	b	Enhancing the Network Embedding Quality with Structural Similarity	[18]	['[18]  Chris Stark, Bobby-Joe Breitkreutz, Teresa Reguly, Lorrie Boucher, Ashton Breitkreutz, and Mike Tyers. 2006. BioGRID: a general repository for interaction datasets. Nucleic acids research 34, suppl 1 (2006), D535–D539. ']
1199	54	[182]	The network contains 3,890 nodes, 76,584 edges and 50 labels. // POS ==[11]== is a co-occurrence network of words appearing in the first million bytes of the Wikipedia dump. // A part of speech (POS) is a category of words which have similar grammatical properties.	b	Enhancing the Network Embedding Quality with Structural Similarity	[11]	['[11]  Matt Mahoney. 2009. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text. html (2009). ']
1200	54	[55]	We choose a classic method aiming at dimension reduction and three representative neural network embedding algorithms as baselines. // Spectral Clustering ==[21]== is based on matrix factorization and aims at minimizing Normalized Cut. // We set d = 500 , the same setting in [21].	b	Enhancing the Network Embedding Quality with Structural Similarity	[21]	['[21]  Lei Tang and Huan Liu. 2011. Leveraging social media networks for classification. Data Mining and Knowledge Discovery 23, 3 (2011), 447–478. ']
1201	54	[6]	We set d = 500 , the same setting in [21]. // DeepWalk ==[15]== is the first embedding algorithm that brings the deep learning technology. // It is an unsupervised learning algorithm.	b	Enhancing the Network Embedding Quality with Structural Similarity	[15]	['[15]  Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 701–710. ']
1202	54	[1]	// LINE ==[19]== defines a loss function based on 1-hop and 2-hop relational information. // It learns d/2 dimensions of the node vector by these two parts of information respectively and combines them directly as the final output.	b	Enhancing the Network Embedding Quality with Structural Similarity	[19]	['[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web. ACM, 1067–1077. ']
1203	54	[1]	In a supervised learning task, it finds the weighting of dimensions based on training data and achieves a better performance. // In our experiments, we try the unsupervised mode of LINE ==[19]== and set d = 128, r = 10, l = 80, k = 10, the same setting in ==[19]==. // node2vec [7] simulates biased random walks over the underlying network. It uses parameter p and q to balance the BFS strategy with DFS strategy.	ho	Enhancing the Network Embedding Quality with Structural Similarity	[19]	['[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web. ACM, 1067–1077. ']
1204	54	[1]	In a supervised learning task, it finds the weighting of dimensions based on training data and achieves a better performance. // In our experiments, we try the unsupervised mode of LINE ==[19]== and set d = 128, r = 10, l = 80, k = 10, the same setting in ==[19]==. // node2vec [7] simulates biased random walks over the underlying network. It uses parameter p and q to balance the BFS strategy with DFS strategy.	ho	Enhancing the Network Embedding Quality with Structural Similarity	[19]	['[19]  Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web. ACM, 1067–1077. ']
1205	54	[37]	In our experiments, we try the unsupervised mode of LINE [19] and set d = 128, r = 10, l = 80, k = 10, the same setting in [19]. // node2vec ==[7]== simulates biased random walks over the underlying network. // It uses parameter p and q to balance the BFS strategy with DFS strategy.	b	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1206	54	[37]	node2vec is a semi-supervised algorithm and it need 10% labeled data of the network to decide the value of p and q. // We use the value of p and q as showed in the authors’ paper ==[7]==. // The settings of our algorithm SNS is as follows: in the preprocessing step, K = 5, S = 1, O = 14, R = 9.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1207	54	[448]	In the learning step, we use the same parameters as DeepWalk does and C = 5. // In the pre-processing step, we use orca ==[8]==, a very efficient algorithm for graphlet enumeration. // As reported in the paper [8], on a desktop computer (Intel Core 2, 2.67 GHz), orca only takes 2.5s to count the four-node graphlets of a network with 25,368 nodes and 75,004 edges.	h+e	Enhancing the Network Embedding Quality with Structural Similarity	[8]	['[8]  Tomaž Hočevar and Janez Demšar. 2014. A combinatorial approach to graphlet counting. Bioinformatics 30, 4 (2014), 559–565. ']
1208	54	[448]	In the pre-processing step, we use orca [8], a very efficient algorithm for graphlet enumeration. // As reported in the paper ==[8]==, on a desktop computer (Intel Core 2, 2.67 GHz), orca only takes 2.5s to count the four-node graphlets of a network with 25,368 nodes and 75,004 edges. // SNS spends more time than DeepWalk does, as more parameters are involved in the learning process.	h+e	Enhancing the Network Embedding Quality with Structural Similarity	[8]	['[8]  Tomaž Hočevar and Janez Demšar. 2014. A combinatorial approach to graphlet counting. Bioinformatics 30, 4 (2014), 559–565. ']
1209	54	[37]	Multi-label Classification. // This task uses the exact same datasets and experimental procedure as presented in ==[7]==. // The node vectors are the input to a one-vsrest logistic regression implemented by sklearn [14].	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[7]	['[7]  Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 855–864. ']
1210	54	[450]	This task uses the exact same datasets and experimental procedure as presented in [7]. // The node vectors are the input to a one-vsrest logistic regression implemented by sklearn ==[14]==.  // We sample a portion of the labeled nodes as training data and use the rest nodes as test data.	hoe	Enhancing the Network Embedding Quality with Structural Similarity	[14]	['[14]  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830. ']
1211	54	[26, 52]	Utilizing global information of the network is a difficult field and we have to acknowledge that the computational cost of SNS is huge when S is a large value. // Some related work ==[2, 16]== suffers from the same drawback. // A possible way to improve the performance might be designing heuristics and limiting the search scope.	h-	Enhancing the Network Embedding Quality with Structural Similarity	[2, 16]	['[2]  Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 891–900. ', '[16]  Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. 2017. struc2vec: Learning Node Representations from Structural Identity. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 385–394. ']
1212	585	[452, 453, 454]	Here we address the problem of structure learning on graphs by developing GraphWave. // Building upon techniques from graph signal processing ==[5, 15, 30]==, our approach learns a multidimensional structural embedding for each node based on the diffusion of a spectral graph wavelet centered at the node. //  Intuitively, each node propagates a unit of energy over the graph and characterizes its neighboring topology based on the response of the network to this probe.	ho	Learning Structural Node Embeddings via Diffusion Wavelets	[5, 15, 30]	['[5]  Ronald Coifman et al. 2006. Diffusion maps. Applied and Computational Harmonic Analysis 21, 1 (2006), 5–30. ', '[15]  David Hammond et al. 2011. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis 30, 2 (2011), 129–150. ', '[30]  David Shuman et al. 2013. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine 30, 3 (2013), 83–98. ']
1213	585	[455]	This way, the structural information is contained in how the diffusion spreads over the network rather than where it spreads. // In order to provide vector-valued embeddings, we embed these wavelet distributions using the empirical characteristic function ==[23]==. // The advantage of empirical characteristic functions is that they capture all the moments (including higher-order moments) of a given distribution.	ho	Learning Structural Node Embeddings via Diffusion Wavelets	[23]	['[23]  Eugene Lukacs. 1970. Characteristic functions. (1970). ']
1214	585	[456, 169]	These methods generate an exhaustive listing of each node’s local topological properties (e.g., node degree, number of triangles it participates in, number of k-cliques, its PageRank score) before computing node similarities based on such heuristic representations. // A notable example of such approaches is RolX ==[11, 16]==, a matrix-factorization based method which aims to recover a softclustering of nodes into a predetermined number of K distinct roles using recursive feature extraction [17]. // Similarly, struc2vec [27] uses a heuristic to construct a multilayered graph based on topological metrics and simulates random walks on the graph to capture structural information. In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[11, 16]	['[11]  Sean Gilpin, Tina Eliassi-Rad, and Ian Davidson. 2013. Guided learning for role discovery: framework, algorithms, and applications. In KDD. 113–121. ', '[16]  Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. 2012. RolX: structural role extraction & mining in large graphs. In KDD. 1231–1239. ']
1215	585	[171]	These methods generate an exhaustive listing of each node’s local topological properties (e.g., node degree, number of triangles it participates in, number of k-cliques, its PageRank score) before computing node similarities based on such heuristic representations. // A notable example of such approaches is RolX [11, 16], a matrix-factorization based method which aims to recover a softclustering of nodes into a predetermined number of K distinct roles using recursive feature extraction ==[17]==. // Similarly, struc2vec [27] uses a heuristic to construct a multilayered graph based on topological metrics and simulates random walks on the graph to capture structural information. In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[17]	['[17]  Keith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi-Rad, Hanghang Tong, and Christos Faloutsos. 2011. It’s who you know: graph mining using recursive structural features. In KDD. 663–671. ']
1216	585	[52]	A notable example of such approaches is RolX [11, 16], a matrix-factorization based method which aims to recover a softclustering of nodes into a predetermined number of K distinct roles using recursive feature extraction [17]. // Similarly, struc2vec ==[27]== uses a heuristic to construct a multilayered graph based on topological metrics and simulates random walks on the graph to capture structural information. // In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters.	h-	Learning Structural Node Embeddings via Diffusion Wavelets	[27]	['[27]  Leonardo Ribeiro, Pedro Saverese, and Daniel Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In KDD. 385–394. ']
1217	585	[406]	In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters. // Recent neural representation learning methods (structure2vec ==[6]==, neural fingerprints [8], graph convolutional networks (GCNs) [13,20], message passing networks [10], etc.) are a related line of research. // However, these graph embedding methods do not apply in our setting, since they solve a (supervised) graph classification task and/or embed entire graphs while we embed individual nodes.	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[6]	['[6]  Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative embeddings of latent variable models for structured data. In ICML. 2702–2711. ']
1218	585	[410]	In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters. // Recent neural representation learning methods (structure2vec [6], neural fingerprints ==[8]==, graph convolutional networks (GCNs) [13,20], message passing networks [10], etc.) are a related line of research. // However, these graph embedding methods do not apply in our setting, since they solve a (supervised) graph classification task and/or embed entire graphs while we embed individual nodes.	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[8]	['[8]  David K Duvenaud et al. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS. 2224–2232. ']
1219	585	[105, 266]	In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters. // Recent neural representation learning methods (structure2vec [6], neural fingerprints [8], graph convolutional networks (GCNs) ==[13,20]==, message passing networks [10], etc.) are a related line of research. // However, these graph embedding methods do not apply in our setting, since they solve a (supervised) graph classification task and/or embed entire graphs while we embed individual nodes.	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[13, 20]	['[13]  William Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NIPS. ', '[20]  Thomas Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. ICLR (2017). ']
1220	585	[457]	In contrast, our approach does not rely on heuristics (we mathematically prove its efficacy) and does not require explicit manual feature engineering or hand-tuning of parameters. // Recent neural representation learning methods (structure2vec [6], neural fingerprints [8], graph convolutional networks (GCNs) [13,20], message passing networks ==[10]==, etc.) are a related line of research. // However, these graph embedding methods do not apply in our setting, since they solve a (supervised) graph classification task and/or embed entire graphs while we embed individual nodes.	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[10]	['[10]  Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. ICML (2017). ']
1221	585	[452]	However, these graph embedding methods do not apply in our setting, since they solve a (supervised) graph classification task and/or embed entire graphs while we embed individual nodes. // Another line of related work are graph diffusion kernels ==[5]== which have been utilized for various graph modeling purposes [3, 22, 29, 34]. // However, to the best of our knowledge, our paper is the first to apply graph diffusion kernels for determining structural roles in graphs.	ho	Learning Structural Node Embeddings via Diffusion Wavelets	[5]	['[5]  Ronald Coifman et al. 2006. Diffusion maps. Applied and Computational Harmonic Analysis 21, 1 (2006), 5–30. ']
1222	585	[458, 459, 460, 461]	However, these graph embedding methods do not apply in our setting, since they solve a (supervised) graph classification task and/or embed entire graphs while we embed individual nodes. // Another line of related work are graph diffusion kernels [5] which have been utilized for various graph modeling purposes ==[3, 22, 29, 34]==. // However, to the best of our knowledge, our paper is the first to apply graph diffusion kernels for determining structural roles in graphs.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[3, 22, 29, 34]	['[3]  Fan Chung. 2007. The heat kernel as the PageRank of a graph. PNAS 104, 50 (2007), 19735–19740. ', '[22]  Risi Kondor and John Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In ICML, Vol. 2. 315–322. ', '[29]  Raif Rustamov and Leonidas Guibas. 2013. Wavelets on graphs via deep learning. In NIPS. 998–1006. ', '[34]  Nicolas Tremblay et al. 2014. Graph wavelets for multiscale community mining. IEEE TSP 62, 20 (2014), 5227–5239. ']
1223	585	[462, 463, 464]	However, to the best of our knowledge, our paper is the first to apply graph diffusion kernels for determining structural roles in graphs. // Kernels have been shown to efficiently capture geometrical properties and have been successfully used for shape detection in the image processing community ==[1, 25, 33]==. // However, in contrast to shape-matching problems, GraphWave considers these kernels as probability distributions over real-world graphs.	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[1, 25, 33]	['[1]  Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers. 2011. The wave kernel signature: A quantum mechanical approach to shape analysis. In ICCV Computer Vision Workshop. 1626–1633. ', '[25]  Maks Ovsjanikov et al. 2010. One point isometric matching with the heat kernel. In Computer Graphics Forum, Vol. 29. 1555–1564. ', '[33]  Jian Sun et al. 2009. A Concise and Provably Informative Multi-Scale Signature Based on Heat Diffusion. In Computer Graphics Forum, Vol. 28. 1383–1392. ']
1224	585	[453]	we consider the problem of learning, for every node ai , a structural embedding representing ai’s position in a continuous multidimensional space of structural roles. // We frame this as an unsupervised learning problem based on spectral graph wavelets ==[15]== and develop an approach called GraphWave that provides mathematical guarantees on the optimality of learned structural embeddings. //  Spectral graph wavelets.	ho	Learning Structural Node Embeddings via Diffusion Wavelets	[15]	['[15]  David Hammond et al. 2011. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis 30, 2 (2011), 129–150. ']
1225	585	[453, 454]	Spectral graph wavelets. // In this section, we provide background on the spectral graph waveletbased model ==[15, 30]== that we will use in the rest of the paper. // Let U be the eigenvector decomposition of the unnormalized graph Laplacian L = D − A = UΛU T and let λ1 < λ2 ≤ · · · ≤ λN (Λ = Diag(λ1, . . . , λN )) denote the eigenvalues of L.	ho	Learning Structural Node Embeddings via Diffusion Wavelets	[15, 30]	['[15]  David Hammond et al. 2011. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis 30, 2 (2011), 129–150. ', '[30]  David Shuman et al. 2013. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine 30, 3 (2013), 83–98. ']
1226	585	[465]	Let дs be a filter kernel with scaling parameter s. // In this paper, we use the heat kernel дs (λ) = e −λs , but our results apply to any scaling wavelet ==[31]==. // For now, we assume thats is given; we develop a method for selecting an appropriate value of s in Section 4.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[31]	['[31]  David Shuman et al. 2016. Vertex-frequency analysis on graphs. Applied and Computational Harmonic Analysis 40, 2 (2016), 260–291. ']
1227	585	[453, 454]	For now, we assume thats is given; we develop a method for selecting an appropriate value of s in Section 4. // Graph signal processing ==[15, 30]== defines the spectral graph wavelet associated with дs as the signal resulting from the modulation in the spectral domain of a Dirac signal centered around node a. // The spectral graph wavelet Ψa is given by an N-dimensional vector: Ψa = U Diag(дs (λ1), . . . ,дs (λN ))U T δa, (1) where δa = 1(a) is the one-hot vector for node a.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[15, 30]	['[15]  David Hammond et al. 2011. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis 30, 2 (2011), 129–150. ', '[30]  David Shuman et al. 2013. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine 30, 3 (2013), 83–98. ']
1228	585	[454]	The m-th wavelet coefficient of this column vector is thus given by Ψma = PN l=1 дs (λl )UmlUal . // In spectral graph wavelets, the kernel дs modulates the eigenspectrum such that the resulting signal is typically localized on the graph and in the spectral domain ==[30]==. // Spectral graph wavelets are based on an analogy between temporal frequencies of a signal and the Laplacian’s eigenvalues.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[30]	['[30]  David Shuman et al. 2013. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine 30, 3 (2013), 83–98. ']
1229	585	[52]	Baseline methods. // We evaluate the performance of GraphWave1 against two state-of-the-art baselines for learning structural embeddings: struc2vec ==[27]==, a method which discovers structural embeddings at different scales through a sequence of walks on a multilayered graph, and RolX [16], a method based on non-negative matrix factorization of a node-feature matrix (number of neighbors, triangles, etc.) that describes each node based on this given set of latent features. // While in [16], the authors develop a method for automatically selecting the number of roles in RolX, we use RolX as an oracle estimator, providing it with the correct number of classes.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[27]	['[27]  Leonardo Ribeiro, Pedro Saverese, and Daniel Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In KDD. 385–394. ']
1230	585	[169]	Baseline methods. // We evaluate the performance of GraphWave1 against two state-of-the-art baselines for learning structural embeddings: struc2vec [27], a method which discovers structural embeddings at different scales through a sequence of walks on a multilayered graph, and RolX ==[16]==, a method based on non-negative matrix factorization of a node-feature matrix (number of neighbors, triangles, etc.) that describes each node based on this given set of latent features. // While in [16], the authors develop a method for automatically selecting the number of roles in RolX, we use RolX as an oracle estimator, providing it with the correct number of classes.	b	Learning Structural Node Embeddings via Diffusion Wavelets	[16]	['[16]  Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. 2012. RolX: structural role extraction & mining in large graphs. In KDD. 1231–1239. ']
1231	585	[169]	We evaluate the performance of GraphWave1 against two state-of-the-art baselines for learning structural embeddings: struc2vec [27], a method which discovers structural embeddings at different scales through a sequence of walks on a multilayered graph, and RolX [16], a method based on non-negative matrix factorization of a node-feature matrix (number of neighbors, triangles, etc.) that describes each node based on this given set of latent features. // While in ==[16]==, the authors develop a method for automatically selecting the number of roles in RolX, we use RolX as an oracle estimator, providing it with the correct number of classes. // We note that GraphWave and struc2vec learn embeddings on a continuous spectrum instead of into discrete classes (and thus they do not require this parameter).	hoe	Learning Structural Node Embeddings via Diffusion Wavelets	[16]	['[16]  Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong, Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. 2012. RolX: structural role extraction & mining in large graphs. In KDD. 1231–1239. ']
1232	585	[406]	For all baselines, we use the default parameter values in the available solvers, and for GraphWave, we use the multiscale version (Section 4), set d = 50 and use evenly spaced sampling points ti in range [0, 100]. // We again note that graph embedding methods (structure2vec ==[6]==, neural fingerprints [8], GCNs [13, 20], etc.) do not apply in these settings, since they embed entire graphs while we embed individual nodes. //  Barbell graph	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[6]	['[6]  Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative embeddings of latent variable models for structured data. In ICML. 2702–2711. ']
1233	585	[410]	For all baselines, we use the default parameter values in the available solvers, and for GraphWave, we use the multiscale version (Section 4), set d = 50 and use evenly spaced sampling points ti in range [0, 100]. // We again note that graph embedding methods (structure2vec [6], neural fingerprints ==[8]==, GCNs [13, 20], etc.) do not apply in these settings, since they embed entire graphs while we embed individual nodes. //  Barbell graph	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[8]	['[8]  David K Duvenaud et al. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS. 2224–2232. ']
1234	585	[105, 266]	For all baselines, we use the default parameter values in the available solvers, and for GraphWave, we use the multiscale version (Section 4), set d = 50 and use evenly spaced sampling points ti in range [0, 100]. // We again note that graph embedding methods (structure2vec [6], neural fingerprints [8], GCNs ==[13, 20]==, etc.) do not apply in these settings, since they embed entire graphs while we embed individual nodes. //  Barbell graph	ro	Learning Structural Node Embeddings via Diffusion Wavelets	[13, 20]	['[13]  William Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NIPS. ', '[20]  Thomas Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. ICLR (2017). ']
1235	585	[52]	Mirrored Karate network. // We first consider a mirrored Karate network, using the same experimental setup as in ==[27]==. // The mirrored Karate network is created by taking two copies of Zachary’s karate network and adding a given number of random edges, each edge connecting mirrored nodes from different copies representing the same individual in the karate club (i.e., mirrored edges).	ho	Learning Structural Node Embeddings via Diffusion Wavelets	[27]	['[27]  Leonardo Ribeiro, Pedro Saverese, and Daniel Figueiredo. 2017. struc2vec: Learning node representations from structural identity. In KDD. 385–394. ']
1236	585	[466]	Data and setup. // Nodes represent Enron employees and edges correspond to email communication between the employees ==[21]==. // An employee has one of seven functions in the company (e.g., CEO, president, manager).	b	Learning Structural Node Embeddings via Diffusion Wavelets	[21]	['[21]  Bryan Klimt and Yiming Yang. 2004. Introducing the Enron Corpus.. In CEAS. ']
1237	19	[109]	Therefore, mining the information in the network is very important. // One of the fundamental problems is how to learn useful network representations ==[5]==. // An effective way is to embed networks into a low-dimensional space, i.e. learn vector representations for each vertex, with the goal of reconstructing the network in the learned embedding space.	b	Structural Deep Network Embedding	[5]	['[5]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 119–128. ACM, 2015. ']
1238	19	[467]	An effective way is to embed networks into a low-dimensional space, i.e. learn vector representations for each vertex, with the goal of reconstructing the network in the learned embedding space. // As a result, mining information in networks, such as information retrieval ==[34]==, classification [15], and clustering [20], can be directly conducted in the low-dimensional space. // Learning network representations faces the following great challenges.	b	Structural Deep Network Embedding	[34]	['[34]  Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Advances in neural information processing systems, pages 1753–1760, 2009. ']
1239	19	[149]	An effective way is to embed networks into a low-dimensional space, i.e. learn vector representations for each vertex, with the goal of reconstructing the network in the learned embedding space. // As a result, mining information in networks, such as information retrieval [34], classification ==[15]==, and clustering [20], can be directly conducted in the low-dimensional space. // Learning network representations faces the following great challenges.	b	Structural Deep Network Embedding	[15]	['[15]  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. ']
1240	19	[290]	An effective way is to embed networks into a low-dimensional space, i.e. learn vector representations for each vertex, with the goal of reconstructing the network in the learned embedding space. // As a result, mining information in networks, such as information retrieval [34], classification [15], and clustering ==[20]==, can be directly conducted in the low-dimensional space. // Learning network representations faces the following great challenges.	b	Structural Deep Network Embedding	[20]	['[20]  A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849–856, 2002. ']
1241	19	[468]	Learning network representations faces the following great challenges. // High non-linearity: As ==[19]== stated, the underlying structure of the network is highly non-linear. // Therefore, how to design a model to capture the highly non-linear structure is rather difficult.	ho	Structural Deep Network Embedding	[19]	['[19]  D. Luo, F. Nie, H. Huang, and C. H. Ding. Cauchy graph embedding. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 553–560, 2011. ']
1242	19	[293]	To support applications analyzing networks, network embedding is required to preserve the network structure. // However, the underlying structure of the network is very complex ==[24]==. // The similarity of vertexes is dependent on both the local and global network structure.	ho	Structural Deep Network Embedding	[24]	['[24]  B. Shaw and T. Jebara. Structure preserving embedding. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 937–944. ACM, 2009. ']
1243	19	[6]	Therefore, how to simultaneously preserve the local and global structure is a tough problem. // Sparsity: Many real-world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance ==[21]==. // In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) [1] and Line [26].	ho	Structural Deep Network Embedding	[21]	['[21]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1244	19	[3]	Sparsity: Many real-world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance [21]. // In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP ==[29]==, Laplacian Eigenmaps (LE) [1] and Line [26]. // However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure [30].	h-	Structural Deep Network Embedding	[29]	['[29]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
1245	19	[444]	Sparsity: Many real-world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance [21]. // In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) ==[1]== and Line [26]. // However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure [30].	h-	Structural Deep Network Embedding	[1]	['[1]  M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373–1396, 2003. ']
1246	19	[1]	Sparsity: Many real-world networks are often so sparse that only utilizing the very limited observed links is not enough to reach a satisfactory performance [21]. // In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) [1] and Line ==[26]==. // However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure [30].	h-	Structural Deep Network Embedding	[26]	['[26]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1247	19	[469]	In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) [1] and Line [26]. // However, due to the limited representation ability of shallow models ==[2]==, it is difficult for them to capture the highly nonlinear network structure [30]. // Although some methods adopt kernel techniques [32], as [36] stated, kernel methods are also shallow models and cannot capture the highly non-linear structure well.	h-	Structural Deep Network Embedding	[2]	['[2]  Y. Bengio. Learning deep architectures for ai. Foundations and trendsR in Machine Learning, 2(1):1–127, 2009. ']
1248	19	[176]	In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) [1] and Line [26]. // However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure ==[30]==. // Although some methods adopt kernel techniques [32], as [36] stated, kernel methods are also shallow models and cannot capture the highly non-linear structure well.	ho	Structural Deep Network Embedding	[30]	['[30]  F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1293–1299, 2014. ']
1249	19	[220]	However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure [30]. // Although some methods adopt kernel techniques ==[32]==, as [36] stated, kernel methods are also shallow models and cannot capture the highly non-linear structure well. // In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks.	h-	Structural Deep Network Embedding	[32]	['[32]  S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. The Journal of Machine Learning Research, 11:1201–1242, 2010. ']
1250	19	[470]	However, due to the limited representation ability of shallow models [2], it is difficult for them to capture the highly nonlinear network structure [30]. // Although some methods adopt kernel techniques [32], as ==[36]== stated, kernel methods are also shallow models and cannot capture the highly non-linear structure well. // In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks.	ho	Structural Deep Network Embedding	[36]	['[36]  J. Zhuang, I. W. Tsang, and S. Hoi. Two-layer multiple kernel learning. In International conference on artificial intelligence and statistics, pages 909–917, 2011.']
1251	19	[469]	In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks. // This is motivated by the recent success of deep learning, which has been demonstrated to have a powerful representation ability to learn complex structures of the data ==[2]== and has achieved substantial success in dealing with images [15], text [25] and audio [10] data. // In particular, in our proposed model we design a multilayer architecture which consists of multiple non-linear functions.	h+	Structural Deep Network Embedding	[2]	['[2]  Y. Bengio. Learning deep architectures for ai. Foundations and trendsR in Machine Learning, 2(1):1–127, 2009. ']
1252	19	[149]	In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks. // This is motivated by the recent success of deep learning, which has been demonstrated to have a powerful representation ability to learn complex structures of the data [2] and has achieved substantial success in dealing with images ==[15]==, text [25] and audio [10] data. // In particular, in our proposed model we design a multilayer architecture which consists of multiple non-linear functions.	h+	Structural Deep Network Embedding	[15]	['[15]  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. ']
1253	19	[471]	In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks. // This is motivated by the recent success of deep learning, which has been demonstrated to have a powerful representation ability to learn complex structures of the data [2] and has achieved substantial success in dealing with images [15], text ==[25]== and audio [10] data. // In particular, in our proposed model we design a multilayer architecture which consists of multiple non-linear functions.	h+	Structural Deep Network Embedding	[25]	['[25]  R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer, 2013. ']
1254	19	[472]	In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks. // This is motivated by the recent success of deep learning, which has been demonstrated to have a powerful representation ability to learn complex structures of the data [2] and has achieved substantial success in dealing with images [15], text [25] and audio ==[10]== data. // In particular, in our proposed model we design a multilayer architecture which consists of multiple non-linear functions.	h+	Structural Deep Network Embedding	[10]	['[10]  G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97, 2012. ']
1255	19	[1]	The composition of multiple layers of non-linear functions can map the data into a highly non-linear latent space, thereby being able to capture the highly non-linear network structure. // In order to address the structure-preserving and sparsity problems in the deep model, we further propose to exploit the first-order and second-order proximity ==[26]== jointly into the learning process. // The first-order proximity is the local pairwise similarity only between the vertexes linked by edges, which characterizes the local network structure.	ho	Structural Deep Network Embedding	[26]	['[26]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1256	19	[124, 473]	Deep Neural Network. // Representation learning has long been an important problem of machine learning and many works aim at learning representations for samples ==[3, 35]==. // Recent advances in deep neural networks have witnessed that they have powerful representations abilities [12] and can generate very useful representations for many types of data.	b	Structural Deep Network Embedding	[3, 35]	['[3]  Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798–1828, 2013. ', '[35]  C. Xu, D. Tao, and C. Xu. Multi-view intact space learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 37(12):2531–2544, 2015. ']
1257	19	[43]	Representation learning has long been an important problem of machine learning and many works aim at learning representations for samples [3, 35]. // Recent advances in deep neural networks have witnessed that they have powerful representations abilities ==[12]== and can generate very useful representations for many types of data. // For example, [15] proposed a seven-layer convolutional neural network to generate image representations for classification.	h+	Structural Deep Network Embedding	[12]	['[12]  G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006. ']
1258	19	[149]	Recent advances in deep neural networks have witnessed that they have powerful representations abilities [12] and can generate very useful representations for many types of data. // For example, ==[15]== proposed a seven-layer convolutional neural network to generate image representations for classification. // [33] proposed a multimodal deep model to learn image-text unified representations to achieve cross-modality retrieval task.	b	Structural Deep Network Embedding	[15]	['[15]  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. ']
1259	19	[474]	For example, [15] proposed a seven-layer convolutional neural network to generate image representations for classification. // ==[33]== proposed a multimodal deep model to learn image-text unified representations to achieve cross-modality retrieval task. // However, to the best of our knowledge, there have been few deep learning works handling networks, especially learning network representations.	b	Structural Deep Network Embedding	[33]	['[33]  D. Wang, P. Cui, M. Ou, and W. Zhu. Deep multimodal hashing with orthogonal regularization. In Proceedings of the 24th International Conference on Artificial Intelligence, pages 2291–2297. AAAI Press, 2015. ']
1260	19	[475]	However, to the best of our knowledge, there have been few deep learning works handling networks, especially learning network representations. // In ==[9]==, Restricted Boltzmann Machines were adopted to do collaborative filtering. // [30] adopted deep autoencoder to do graph clustering.	b	Structural Deep Network Embedding	[9]	['[9]  K. Georgiev and P. Nakov. A non-iid framework for collaborative filtering with restricted boltzmann machines. In ICML-13, pages 1148–1156, 2013. ']
1261	19	[176]	In [9], Restricted Boltzmann Machines were adopted to do collaborative filtering. // ==[30]== adopted deep autoencoder to do graph clustering. // [5] proposed a heterogeneous deep model to do heterogeneous data embedding.	b	Structural Deep Network Embedding	[30]	['[30]  F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1293–1299, 2014. ']
1262	19	[109]	[30] adopted deep autoencoder to do graph clustering. // ==[5]== proposed a heterogeneous deep model to do heterogeneous data embedding. // We differ from these works in two aspects.	ro	Structural Deep Network Embedding	[5]	['[5]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 119–128. ACM, 2015. ']
1263	19	[4]	Network Embedding. // Some earlier works like Local Linear Embedding (LLE) ==[22]==, IsoMAP [29] first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations. // More recently, [26] designed two loss functions attempting to capture the local and global network structure respectively.	b	Structural Deep Network Embedding	[22]	['[22]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ']
1264	19	[3]	Network Embedding. // Some earlier works like Local Linear Embedding (LLE) [22], IsoMAP ==[29]== first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations. // More recently, [26] designed two loss functions attempting to capture the local and global network structure respectively.	b	Structural Deep Network Embedding	[29]	['[29]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
1265	19	[1]	Some earlier works like Local Linear Embedding (LLE) [22], IsoMAP [29] first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations. // More recently, ==[26]== designed two loss functions attempting to capture the local and global network structure respectively. // Furthermore, [4] extended the work to utilize high-order information.	b	Structural Deep Network Embedding	[26]	['[26]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1266	19	[26]	More recently, [26] designed two loss functions attempting to capture the local and global network structure respectively. // Furthermore, ==[4]== extended the work to utilize high-order information. // Despite the success of these network embedding approaches, they all adopt shallow models.	h-	Structural Deep Network Embedding	[4]	['[4]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891–900. ACM, 2015. ']
1267	19	[6]	Obviously, it is sub-optimal than simultaneously modeling them in a unified architecture to capture both the local and global network structure. // DeepWalk ==[21]== combined random walk and skip-gram to learn network representations. // Although empirically effective, it lacks a clear objective function to articulate how to preserve the network structure.	h-	Structural Deep Network Embedding	[21]	['[21]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1268	19	[476, 477]	Intuitively, the second-order proximity assumes that if two vertexes share many common neighbors, they tend to be similar. // Such an assumption has been proved reasonable in many fields ==[6, 14]==. // For example, in linguistics words will be similar if they are always surrounded by similar contexts [6].	ho	Structural Deep Network Embedding	[6, 14]	['[6]  N. S. Dash. Context and contextual word meaning. SKASE Journal of Theoretical Linguistics, 5(2):21–31, 2008. ', '[14]  E. M. Jin, M. Girvan, and M. E. Newman. Structure of growing social networks. Physical review E, 64(4):046132, 2001. ']
1269	19	[476]	Such an assumption has been proved reasonable in many fields [6, 14]. // For example, in linguistics words will be similar if they are always surrounded by similar contexts ==[6]==. // People will be friends if they have many common friends [14].	ho	Structural Deep Network Embedding	[6]	['[6]  N. S. Dash. Context and contextual word meaning. SKASE Journal of Theoretical Linguistics, 5(2):21–31, 2008. ']
1270	19	[477]	For example, in linguistics words will be similar if they are always surrounded by similar contexts [6]. // People will be friends if they have many common friends ==[14]==. // The second-order proximity has been demonstrated to be a good metric to define the similarity of a pair of vertexes, even if they are not linked by an edge [17], and thus can highly enrich the relationship of vertexes.	ho	Structural Deep Network Embedding	[14]	['[14]  E. M. Jin, M. Girvan, and M. E. Newman. Structure of growing social networks. Physical review E, 64(4):046132, 2001. ']
1271	19	[70]	People will be friends if they have many common friends [14]. // The second-order proximity has been demonstrated to be a good metric to define the similarity of a pair of vertexes, even if they are not linked by an edge ==[17]==, and thus can highly enrich the relationship of vertexes. // Therefore, by introducing the second-order proximity, it is able to characterize the global network structure and alleviate the sparsity problem.	h+	Structural Deep Network Embedding	[17]	['[17]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
1272	19	[46]	Therefore, si describes the neighborhood structure of the vertex vi and S provides the information of the neighborhood structure of each vertex. // With S, we extend the traditional deep autoencoder ==[23]== to preserve the second-order proximity. // For the consideration of being self-contained, we briefly review the key idea of deep autoencoder.	ho	Structural Deep Network Embedding	[23]	['[23]  R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969–978, 2009. ']
1273	19	[478]	Note that due to the high nonlinearity of the model, it suffers from many local optimal in the parameter space. // Therefore, in order to find a good region of parameter space, we use Deep Belief Network to pretrain the parameters at first ==[11]==, which has been demonstrated as an essential initialization of parameters for deep learning in literature [7]. //  The full algorithm is presented in Alg. 1.	ho	Structural Deep Network Embedding	[11]	['[11]  G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006. ']
1274	19	[148]	Note that due to the high nonlinearity of the model, it suffers from many local optimal in the parameter space. // Therefore, in order to find a good region of parameter space, we use Deep Belief Network to pretrain the parameters at first [11], which has been demonstrated as an essential initialization of parameters for deep learning in literature ==[7]==. //  The full algorithm is presented in Alg. 1.	ho	Structural Deep Network Embedding	[7]	['[7]  D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010. ']
1275	19	[33]	The detailed descriptions are listed as follows. // BLOGCATALOG ==[27]==, FLICKR ==[27]== and YOUTUBE [28]: They are social network of online users. // Each user is labelled by at least one category.	b	Structural Deep Network Embedding	[27]	['[27]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2009. ']
1276	19	[33]	The detailed descriptions are listed as follows. // BLOGCATALOG ==[27]==, FLICKR ==[27]== and YOUTUBE [28]: They are social network of online users. // Each user is labelled by at least one category.	b	Structural Deep Network Embedding	[27]	['[27]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2009. ']
1277	19	[56]	The detailed descriptions are listed as follows. // BLOGCATALOG [27], FLICKR [27] and YOUTUBE ==[28]==: They are social network of online users. // Each user is labelled by at least one category.	b	Structural Deep Network Embedding	[28]	['[28]  L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1107–1116. ACM, 2009. ']
1278	19	[84]	Therefore, they can be evaluated on the multi-label classification task. // ARXIV GR-QC ==[16]==: It is a paper collaboration network which covers papers in the field of General Relativity and Quantum Cosmology from arXiv. // In this network, the vertex represents an author and the edge indicates that the authors have coauthored a scientific paper in arXiv.	b	Structural Deep Network Embedding	[16]	['[16]  J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densification and shrinking diameters. ACM Transactions on Knowledge Discovery from Data (TKDD), 1(1):2, 2007. ']
1279	19	[6]	Common Neighbor directly predicts the links over the networks, which has been demonstrated to be an effective method to perform link prediction [17]. // DeepWalk ==[21]==: It adopts random walk and skip-gram model to generate network representations. // LINE [26]: It defines loss functions to preserve the first-order or second-order proximity separately. After optimizing the loss functions, it concatenates these representations.	b	Structural Deep Network Embedding	[21]	['[21]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1280	19	[1]	DeepWalk [21]: It adopts random walk and skip-gram model to generate network representations. // LINE ==[26]==: It defines loss functions to preserve the first-order or second-order proximity separately. // After optimizing the loss functions, it concatenates these representations.	b	Structural Deep Network Embedding	[26]	['[26]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1281	19	[26]	After optimizing the loss functions, it concatenates these representations. // GraRep ==[4]==: It extends to high-order proximity and uses the SVD to train the model. // It also directly concatenates the representations of first-order and high-order.	b	Structural Deep Network Embedding	[4]	['[4]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891–900. ACM, 2015. ']
1282	19	[444]	It also directly concatenates the representations of first-order and high-order. // Laplacian Eigenmaps (LE) ==[1]==: It generates network representations by factorizing the Laplacian matrix of the adjacency matrix. // It only exploits the first-order proximity to preserve the network structure.	b	Structural Deep Network Embedding	[1]	['[1]  M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):1373–1396, 2003. ']
1283	19	[70]	It only exploits the first-order proximity to preserve the network structure. // Common Neighbor ==[17]==: It only uses the number of common neighbors to measure the similarity between vertexes. // It is used as the baseline only in the task of link prediction.	b	Structural Deep Network Embedding	[17]	['[17]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
1284	19	[479]	Multi-label Classification. // Classification is a so important task among many applications that the related algorithm and theories have been investigated by many works ==[18]==. //  Therefore, we evaluate the effectiveness of different network representations through a multilabel classification task in this experiment.	b	Structural Deep Network Embedding	[18]	['[18]  T. Liu and D. Tao. Classification with noisy labels by importance reweighting. TPAMI, (1):1–1. ']
1285	19	[61]	The representations for the vertexes are generated from the network embedding methods and are used as features to classify each vertex into a set of labels. // Specifically, we adopt the LIBLINEAR package ==[8]== to train the classifiers. // When training the classifier, we randomly sample a portion of the labeled nodes as the training data and the rest as the test.	hoe	Structural Deep Network Embedding	[8]	['[8]  R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874, 2008. ']
1286	19	[70]	Therefore, this task can show the performance of predictability of different network embedding methods. // In addition, we add Common Neighbor in this task because it has been proved as an effective method to do link prediction ==[17]==. // For the first experiment, we randomly hide 15 precentage of existing links (about 4000 links) and use the precision@k as the evaluation metric of predicting the hidden links.	h+	Structural Deep Network Embedding	[17]	['[17]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
1287	19	[78]	Therefore, we visualize the learned representations of the 20-NEWSGROUP network. // We use the low-dimensional network representations learned by different network embedding methods as the input to the visualization tool t-SNE ==[31]==. // As a result, each newsgroup document is mapped as a two-dimensional vector.	ho	Structural Deep Network Embedding	[31]	['[31]  L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008. ']
1288	19	[26]	The visualization figure is shown in Figure 7. // Besides the visualization figure, similar to ==[4]== we use the Kullback-Leibler divergence as a quantitative evaluation metric. //  The lower the KL divergence, the better the performance.	ho	Structural Deep Network Embedding	[4]	['[4]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 891–900. ACM, 2015. ']
1289	382	[480, 481]	// With the explosive growth of data, similarity search is becoming increasingly important for a wide range of large scale applications, including image retrieval ==[9, 21]==, document search [30], and recommendation systems [16, 15, 37]. // Due to the simplicity and efficiency, hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search.	b	Non-transitive Hashing with Latent Similarity Components	[9, 21]	['[9]  P. Cui, S.-W. Liu, W.-W. Zhu, H.-B. Luan, T.-S. Chua, and S.-Q. Yang. Social-sensed image search. ACM Transactions on Information Systems (TOIS), 32(2):8, 2014. ', '[21]  S. Liu, P. Cui, H. Luan, W. Zhu, S. Yang, and Q. Tian. Social-oriented visual image search. Computer Vision and Image Understanding, 118:30–39, 2014. ']
1290	382	[46]	// With the explosive growth of data, similarity search is becoming increasingly important for a wide range of large scale applications, including image retrieval [9, 21], document search ==[30]==, and recommendation systems [16, 15, 37]. // Due to the simplicity and efficiency, hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search.	b	Non-transitive Hashing with Latent Similarity Components	[30]	['[30]  R. Salakhutdinov and G. Hinton. Semantic hashing. RBM, 500(3):500, 2007. ']
1291	382	[482, 483, 484]	// With the explosive growth of data, similarity search is becoming increasingly important for a wide range of large scale applications, including image retrieval [9, 21], document search [30], and recommendation systems ==[16, 15, 37]==. // Due to the simplicity and efficiency, hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search.	b	Non-transitive Hashing with Latent Similarity Components	[16, 15, 37]	['[16]  M. Jiang, P. Cui, F. Wang, W. Zhu, and S. Yang. Scalable recommendation with social contextual information. Knowledge and Data Engineering, IEEE Transactions on, 26(11):2789–2802, 2014. ', '[15]  M. Jiang, P. Cui, F. Wang, Q. Yang, W. Zhu, and S. Yang. Social recommendation across multiple relational domains. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 1422–1431. ACM, 2012. ', '[37]  Z. Wang, W. Zhu, P. Cui, L. Sun, and S. Yang. Social media recommendation. In Social Media Retrieval, pages 23–42. Springer, 2013. ']
1292	382	[485, 486, 487, 488]	Due to the simplicity and efficiency, hashing techniques are one of the best options for indexing large scale datasets and performing approximate nearest neighbor search.  // In particular, recent hashing methods often employ machine learning techniques to leverage supervised information like pairwise constraints to design more efficient hash codes to search semantically similar entities ==[35, 39, 22, 28]==. // The key for the learning based hashing methods is to approximate the semantic similarity between entities in the learned Hamming space.	h+	Non-transitive Hashing with Latent Similarity Components	[35, 39, 22, 28]	['[35]  J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised hashing for large-scale search. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(12):2393–2406, 2012. ', '[39]  H. Xu, J. Wang, Z. Li, G. Zeng, S. Li, and N. Yu. Complementary hashing for approximate nearest neighbor search. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 1631–1638. IEEE, 2011. ', '[22]  W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. Supervised hashing with kernels. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2074–2081, 2012. ', '[28]  M. Ou, P. Cui, F. Wang, J. Wang, W. Zhu, and S. Yang. Comparing apples to oranges: a scalable solution with heterogeneous hashing. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 230–238. ACM, 2013. ']
1293	382	[489, 490]	In literature, most of the supervised hashing techniques use the pairwise relationship links without identifying the true similarity components, and neglect the non-transitive property of the semantic metric. // In general, it is difficult to capture the true non-transitive triangle relationship in a single metric space ==[7, 33]==. // As an example illustrated in Figure 2, we have a non-transitive relationship between three images {A, B, C}.	ho	Non-transitive Hashing with Latent Similarity Components	[7, 33]	['[7]  S. Changpinyo, K. Liu, and F. Sha. Similarity component analysis. In Advances in Neural Information Processing Systems, pages 1511–1519, 2013. ', '[33]  L. van der Maaten and G. Hinton. Visualizing non-metric similarities in multiple maps. Machine learning, 87(1):33–55, 2012. ']
1294	382	[246]	Non-transitive Similarity Learning. // Although most of existing methods ==[6]== assume that similarity are all transitive, non-transitive similarity have been studied in different fields //  In metric learning, researchers realize the limitation of metric space on capturing the nontransitive similarity, and propose some effective non-metric learning algorithms for non-transitive similarity [7, 33].	b	Non-transitive Hashing with Latent Similarity Components	[6]	['[6]  S. Chang, G. Qi, C. C. Aggarwal, J. Zhou, M. Wang, and T. S. Huang. Factorized similarity learning in networks. In 2014 IEEE International Conference on Data Mining, ICDM 2014, Shenzhen, China, December 14-17, 2014, pages 60–69, 2014. ']
1295	382	[489, 490]	Although most of existing methods [6] assume that similarity are all transitive, non-transitive similarity have been studied in different fields //  In metric learning, researchers realize the limitation of metric space on capturing the nontransitive similarity, and propose some effective non-metric learning algorithms for non-transitive similarity ==[7, 33]==. // Similarity Component Analysis (SCA) [7] proposes a probabilistic graphical model to discover latent pairwise similarity components, and aggregating them as the final similarity.	h+	Non-transitive Hashing with Latent Similarity Components	[7, 33]	['[7]  S. Changpinyo, K. Liu, and F. Sha. Similarity component analysis. In Advances in Neural Information Processing Systems, pages 1511–1519, 2013. ', '[33]  L. van der Maaten and G. Hinton. Visualizing non-metric similarities in multiple maps. Machine learning, 87(1):33–55, 2012. ']
1296	382	[489]	In metric learning, researchers realize the limitation of metric space on capturing the nontransitive similarity, and propose some effective non-metric learning algorithms for non-transitive similarity [7, 33]. // Similarity Component Analysis (SCA) ==[7]== proposes a probabilistic graphical model to discover latent pairwise similarity components, and aggregating them as the final similarity. // Note that the similarity components discovered in our work are entity-wise, and each entity is represented by a combination of similarity components.	b	Non-transitive Hashing with Latent Similarity Components	[7]	['[7]  S. Changpinyo, K. Liu, and F. Sha. Similarity component analysis. In Advances in Neural Information Processing Systems, pages 1511–1519, 2013. ']
1297	382	[490]	Note that the similarity components discovered in our work are entity-wise, and each entity is represented by a combination of similarity components. // Multiple maps t-SNE ==[33]== aims to represent non-transitive similarity and central objects in two-dimensional visualizations. // In social networks, many works [32, 10, 13, 2, 1] focus on extracting the multiple types of relationship or finding the overlapping communities.	b	Non-transitive Hashing with Latent Similarity Components	[33]	['[33]  L. van der Maaten and G. Hinton. Visualizing non-metric similarities in multiple maps. Machine learning, 87(1):33–55, 2012. ']
1298	382	[491, 492, 493, 494, 495]	Multiple maps t-SNE [33] aims to represent non-transitive similarity and central objects in two-dimensional visualizations. // In social networks, many works ==[32, 10, 13, 2, 1]== focus on extracting the multiple types of relationship or finding the overlapping communities. // Mixed membership stochastic blockmodels [2] discovers overlapping communities in networks, and represents vertices in networks with mixed membership to communities.	b	Non-transitive Hashing with Latent Similarity Components	[32, 10, 13, 2, 1]	['[32]  M. Szell, R. Lambiotte, and S. Thurner. Multirelational organization of large-scale social networks in an online world. Proceedings of the National Academy of Sciences, 107(31):13636–13641, 2010. ', '[10]  S. E. Fienberg, M. M. Meyer, and S. S. Wasserman. Statistical analysis of multiple sociometric relations. Journal of the american Statistical association, 80(389):51–67, 1985. ', '[13]  J. Hopcroft, T. Lou, and J. Tang. Who will follow you back?: reciprocal relationship prediction. In Proceedings of the 20th ACM international conference on Information and knowledge management, pages 1137–1146. ACM, 2011. ', '[2]  E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing. Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9(1981-2014):3, 2008. ', '[1]  I. Abraham, S. Chechik, D. Kempe, and A. Slivkins. Low-distortion inference of latent similarities from a multiplex social network. In SODA, pages 1853–1872. SIAM, 2013. ']
1299	382	[494]	In social networks, many works [32, 10, 13, 2, 1] focus on extracting the multiple types of relationship or finding the overlapping communities. // Mixed membership stochastic blockmodels ==[2]== discovers overlapping communities in networks, and represents vertices in networks with mixed membership to communities. // However, these methods cannot work in the scenario of hashing.	h-	Non-transitive Hashing with Latent Similarity Components	[2]	['[2]  E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing. Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9(1981-2014):3, 2008. ']
1300	382	[496, 497]	With the rapid increase of the data volume, hashing is proposed to solve the efficiency problem on approximate nearest neighbors search in large-scale high-dimensional data. // Locality Sensitive Hashing (i.e. LSH) methods ==[14, 4]== are first proposed, which generate hash codes with random projections or permutations. // Moreover, Mu et. al. [27] apply LSH to non-metric similarity.	b	Non-transitive Hashing with Latent Similarity Components	[14, 4]	['[14]  P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613, 1998. ', '[4]  A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher. Min-wise independent permutations. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 327–336, 1998. ']
1301	382	[498]	Locality Sensitive Hashing (i.e. LSH) methods [14, 4] are first proposed, which generate hash codes with random projections or permutations. // Moreover, Mu et. al. ==[27]== apply LSH to non-metric similarity. // While locality sensitive hashing is independent with data, and has to generate long hash code to achieve high accuracy, Spectral hashing [38] is proposed to learn hash functions based on the data distribution, and achieves much compact hash code and higher accuracy.	h-	Non-transitive Hashing with Latent Similarity Components	[27]	['[27]  Y. Mu and S. Yan. Non-metric locality-sensitive hashing. In AAAI, 2010. ']
1302	382	[467]	Moreover, Mu et. al. [27] apply LSH to non-metric similarity. // While locality sensitive hashing is independent with data, and has to generate long hash code to achieve high accuracy, Spectral hashing ==[38]== is proposed to learn hash functions based on the data distribution, and achieves much compact hash code and higher accuracy. // Some more unsupervised hashing methods [23, 12, 17, 11] based on data distributions are proposed later.	h+	Non-transitive Hashing with Latent Similarity Components	[38]	['[38]  Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In Advances in Neural Information Processing Systems, pages 1753–1760, 2008. ']
1303	382	[499, 500, 501, 502]	While locality sensitive hashing is independent with data, and has to generate long hash code to achieve high accuracy, Spectral hashing [38] is proposed to learn hash functions based on the data distribution, and achieves much compact hash code and higher accuracy. // Some more unsupervised hashing methods ==[23, 12, 17, 11]== based on data distributions are proposed later. // For the wellknown semantic gap between low-level features and semantic concepts, the performance of unsupervised hashing suffers bottleneck.	b	Non-transitive Hashing with Latent Similarity Components	[23, 12, 17, 11]	['[23]  W. Liu, J. Wang, S. Kumar, and S.-F. Chang. Hashing with graphs. In Proceedings of the 28th International Conference on Machine Learning, pages 1–8, 2011. ', '[12]  Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In IEEE Conference on Computer Vision and Pattern Recognition, pages 817–824, 2011. ', '[17]  W. Kong and W.-J. Li. Isotropic hashing. In Advances in Neural Information Processing Systems, pages 1655–1663, 2012. ', '[11]  Y. Gong, S. Kumar, V. Verma, and S. Lazebnik. Angular quantization-based binary codes for fast similarity search. In Advances in Neural Information Processing Systems, pages 1205–1213, 2012. ']
1304	382	[485, 487, 503, 504, 505]	For the wellknown semantic gap between low-level features and semantic concepts, the performance of unsupervised hashing suffers bottleneck. // Semi-supervised or supervised hashing methods ==[36, 22, 34, 19, 20]== exploit the labeled pairwise simiarity relationship between entities to capture the high-level semantics. // Moreover, some multi-modal hashing methods [5, 18, 41, 31, 40, 29] are proposed to exploit multiple features for hashing to get higher accuracy. However, these hashing methods cannot discover the latent similarity components and capture the non-transitive similarity.	b	Non-transitive Hashing with Latent Similarity Components	[36, 22, 34, 19, 20]	['[36]  J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised hashing for large-scale search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(12):2393–2406, 12 2012. ', '[22]  W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. Supervised hashing with kernels. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2074–2081, 2012. ', '[34]  J. Wang, S. Kumar, and S.-F. Chang. Sequential projection learning for hashing with compact codes. In Proceedings of International Conference on Machine Learning, pages 1127–1134, 2010. ', '[19]  G. Lin, C. Shen, Q. Shi, A. van den Hengel, and D. Suter. Fast supervised hashing with decision trees for high-dimensional data. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1971–1978. IEEE, 2014. ', '[20]  G. Lin, C. Shen, D. Suter, and A. van den Hengel. A general two-step approach to learning-based hashing. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 2552–2559. IEEE, 2013. ']
1305	382	[506, 507, 508, 509, 510, 511]	Semi-supervised or supervised hashing methods [36, 22, 34, 19, 20] exploit the labeled pairwise simiarity relationship between entities to capture the high-level semantics. // Moreover, some multi-modal hashing methods ==[5, 18, 41, 31, 40, 29]== are proposed to exploit multiple features for hashing to get higher accuracy. // However, these hashing methods cannot discover the latent similarity components and capture the non-transitive similarity.	h-	Non-transitive Hashing with Latent Similarity Components	[5, 18, 41, 31, 40, 29]	['[5]  M. Bronstein, A. Bronstein, F. Michel, and N. Paragios. Data fusion through cross-modality metric learning using similarity-sensitive hashing. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3594–3601, 2010. ', '[18]  S. Kumar and R. Udupa. Learning hash functions for cross-view similarity search. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, pages 1360–1365, 2011. ', '[41]  Y. Zhen and D. Yeung. A probabilistic model for multimodal hash function learning. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 940–948, 2012.', '[31]  J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen. Inter-media hashing for large-scale retrieval from heterogeneous data sources. In Proceedings of the 2013 international conference on Management of data, pages 785–796. ACM, 2013. ', '[40]  D. Zhang, F. Wang, and L. Si. Composite hashing with multiple information sources. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 225–234. ACM, 2011. ', '[29]  M. Ou, P. Cui, J. Wang, F. Wang, and W. Zhu. Probabilistic attributed hashing. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015. ']
1306	382	[486, 512, 512]	Another class of hashing methods that is related to our work is the hashing methods using multiple hash tables. // In order to improve the recall of hashing and preserve the precision at the same time, some multi-table hashing methods ==[39, 24]== are proposed, where complementary hashing [39] learns multiple hash tables with boosting methods, and reciprocal hashing [24] learns multiple hash tables by selecting hash functions from a pool of hash functions that learned by many hashing methods. //  Since these methods treat similarity and dissimilarity relationships in the same way, these multitable hashing methods are not designed to identify latent similarity components and cannot capture non-transitive similarity.	h-	Non-transitive Hashing with Latent Similarity Components	[39, 24]	['[39]  H. Xu, J. Wang, Z. Li, G. Zeng, S. Li, and N. Yu. Complementary hashing for approximate nearest neighbor search. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 1631–1638. IEEE, 2011. ', '[24]  X. Liu, J. He, and B. Lang. Reciprocal hash tables for nearest neighbor search. In Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013. ', '[24]  X. Liu, J. He, and B. Lang. Reciprocal hash tables for nearest neighbor search. In Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013. ']
1307	382	[486]	Another class of hashing methods that is related to our work is the hashing methods using multiple hash tables. // In order to improve the recall of hashing and preserve the precision at the same time, some multi-table hashing methods [39, 24] are proposed, where complementary hashing ==[39]== learns multiple hash tables with boosting methods, and reciprocal hashing [24] learns multiple hash tables by selecting hash functions from a pool of hash functions that learned by many hashing methods. //  Since these methods treat similarity and dissimilarity relationships in the same way, these multitable hashing methods are not designed to identify latent similarity components and cannot capture non-transitive similarity.	h-	Non-transitive Hashing with Latent Similarity Components	[39]	['[39]  H. Xu, J. Wang, Z. Li, G. Zeng, S. Li, and N. Yu. Complementary hashing for approximate nearest neighbor search. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 1631–1638. IEEE, 2011. ']
1308	382	[512, 512]	Another class of hashing methods that is related to our work is the hashing methods using multiple hash tables. // In order to improve the recall of hashing and preserve the precision at the same time, some multi-table hashing methods [39, 24] are proposed, where complementary hashing [39] learns multiple hash tables with boosting methods, and reciprocal hashing ==[24]== learns multiple hash tables by selecting hash functions from a pool of hash functions that learned by many hashing methods. //  Since these methods treat similarity and dissimilarity relationships in the same way, these multitable hashing methods are not designed to identify latent similarity components and cannot capture non-transitive similarity.	h-	Non-transitive Hashing with Latent Similarity Components	[24]	['[24]  X. Liu, J. He, and B. Lang. Reciprocal hash tables for nearest neighbor search. In Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013. ', '[24]  X. Liu, J. He, and B. Lang. Reciprocal hash tables for nearest neighbor search. In Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013. ']
1309	382	[513, 514]	Since these methods treat similarity and dissimilarity relationships in the same way, these multitable hashing methods are not designed to identify latent similarity components and cannot capture non-transitive similarity. // Hashing on multi-label data ==[26, 25]== learn different hash tables for different known labels. // Heterogeneous hashing [28] generates a variant of original hash table in each domain to search, in order to capture the specific characteristics of target domain.	b	Non-transitive Hashing with Latent Similarity Components	[26, 25]	['[26]  Y. Mu, X. Chen, T.-S. Chua, and S. Yan. Learning reconfigurable hashing for diverse semantics. In Proceedings of the 1st ACM International Conference on Multimedia Retrieval, page 7. ACM, 2011. ', '[25]  X. Liu, Y. Mu, B. Lang, and S.-F. Chang. Compact hashing for mixed image-keyword query over multi-label images. In Proceedings of the 2nd ACM International Conference on Multimedia Retrieval, page 18. ACM, 2012. ']
1310	382	[488]	Hashing on multi-label data [26, 25] learn different hash tables for different known labels. // Heterogeneous hashing ==[28]== generates a variant of original hash table in each domain to search, in order to capture the specific characteristics of target domain. // The setting of the above two class of hashing methods are different from our setting where similarity components (i.e. label or domain) are latent.	b	Non-transitive Hashing with Latent Similarity Components	[28]	['[28]  M. Ou, P. Cui, F. Wang, J. Wang, W. Zhu, and S. Yang. Comparing apples to oranges: a scalable solution with heterogeneous hashing. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 230–238. ACM, 2013. ']
1311	382	[487]	That is, when HRS is relatively small, reduction by 1 on HRS will save most of the search space. // We select three state-of-art hashing methods, i.e. Kernelbased Supervised Hashing (KSH) ==[22]==, Semi-supervised Hashing (SPLH) [36], Iterative Quantization (ITQ) [12], as baselines. // KSH is a supervised method, SPLH is a semi-supervised method, ITQ is a unsupervised methods. For KSH, we randomly select 300 anchors in NUS-WIDE, and 50 anchors in DBLP and the synthetic dataset.	b	Non-transitive Hashing with Latent Similarity Components	[22]	['[22]  W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. Supervised hashing with kernels. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2074–2081, 2012. ']
1312	382	[485]	That is, when HRS is relatively small, reduction by 1 on HRS will save most of the search space. // We select three state-of-art hashing methods, i.e. Kernelbased Supervised Hashing (KSH) [22], Semi-supervised Hashing (SPLH) ==[36]==, Iterative Quantization (ITQ) [12], as baselines. // KSH is a supervised method, SPLH is a semi-supervised method, ITQ is a unsupervised methods. For KSH, we randomly select 300 anchors in NUS-WIDE, and 50 anchors in DBLP and the synthetic dataset.	b	Non-transitive Hashing with Latent Similarity Components	[36]	['[36]  J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised hashing for large-scale search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(12):2393–2406, 12 2012. ']
1313	382	[500]	That is, when HRS is relatively small, reduction by 1 on HRS will save most of the search space. // We select three state-of-art hashing methods, i.e. Kernelbased Supervised Hashing (KSH) [22], Semi-supervised Hashing (SPLH) [36], Iterative Quantization (ITQ) ==[12]==, as baselines. // KSH is a supervised method, SPLH is a semi-supervised method, ITQ is a unsupervised methods. For KSH, we randomly select 300 anchors in NUS-WIDE, and 50 anchors in DBLP and the synthetic dataset.	b	Non-transitive Hashing with Latent Similarity Components	[12]	['[12]  Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In IEEE Conference on Computer Vision and Pattern Recognition, pages 817–824, 2011. ']
1314	382	[515]	Datasets. // NUS-WIDE ==[8]== is an image dataset crawled from Flickr with about 260, 000 images and 81 concept categories. // A pair of images is regarded similar if they share at least one common concept, otherwise, they are dissimilar.	b	Non-transitive Hashing with Latent Similarity Components	[8]	['[8]  T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. Nus-wide: a real-world web image database from national university of singapore. In Proceedings of the ACM International Conference on Image and Video Retrieval, page 48, 2009. ']
1315	109	[35, 167]	Vectorized data representations frequently arise in many data mining applications. // They are easier to handle since each data can be viewed as a point residing in an Euclidean space ==[24, 30]==. // Thus, similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification [37], clustering [21, 43, 45] and retrieval [10].	ho	Heterogeneous Network Embedding via Deep Architectures	[24, 30]	['[24]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv:1301.3781, 2013. ', '[30]  J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. EMNLP, 12, 2014. ']
1316	109	[68]	They are easier to handle since each data can be viewed as a point residing in an Euclidean space [24, 30]. // Thus, similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification ==[37]==, clustering [21, 43, 45] and retrieval [10]. // As shown in [3], learning good representations is one of the fundamental problems in data mining and Web search, and it often has a stronger impact on performance than designing a more sophisticated model.	b	Heterogeneous Network Embedding via Deep Architectures	[37]	['[37]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
1317	109	[516, 176, 517]	They are easier to handle since each data can be viewed as a point residing in an Euclidean space [24, 30]. // Thus, similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification [37], clustering ==[21, 43, 45]== and retrieval [10]. // As shown in [3], learning good representations is one of the fundamental problems in data mining and Web search, and it often has a stronger impact on performance than designing a more sophisticated model.	b	Heterogeneous Network Embedding via Deep Architectures	[21, 43, 45]	['[21]  C. Liu, K. Zhang, H. Xiong, G. Jiang, and Q. Yang. Temporal skeletonization on sequential data: Patterns, categorization, and visualization. In ACM SIGKDD, pages 1336–1345. ACM, 2014. ', '[43]  F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning deep representations for graph clustering. In AAAI, 2014. ', '[45]  J. Yi, L. Zhang, J. Wang, R. Jin, and A. K. Jain. A single-pass algorithm for efficiently recovering sparse cluster centers of high-dimensional data. In ICML, pages 658–666, 2014. ']
1318	109	[518]	They are easier to handle since each data can be viewed as a point residing in an Euclidean space [24, 30]. // Thus, similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification [37], clustering [21, 43, 45] and retrieval ==[10]==. // As shown in [3], learning good representations is one of the fundamental problems in data mining and Web search, and it often has a stronger impact on performance than designing a more sophisticated model.	b	Heterogeneous Network Embedding via Deep Architectures	[10]	['[10]  Y. Fu, H. Xiong, Y. Ge, Z. Yao, Y. Zheng, and Z.-H. Zhou. Exploiting geographic dependencies for real estate appraisal: A mutual perspective of ranking and clustering. In ACM SIGKDD, pages 1047–1056. ACM, 2014. ']
1319	109	[519]	Thus, similarities between different data points can be directly measured by an appropriate metric to solve traditional tasks such as classification [37], clustering [21, 43, 45] and retrieval [10]. // As shown in ==[3]==, learning good representations is one of the fundamental problems in data mining and Web search, and it often has a stronger impact on performance than designing a more sophisticated model. // Unfortunately, many networked data sources (e.g. Facebook, YouTube, Flickr and Twitter) cannot be naturally represented as vectorized inputs.	ho	Heterogeneous Network Embedding via Deep Architectures	[3]	['[3]  S. Chang, W. Han, X. Liu, N. Xu, P. Khorrami, and T. S. Huang. Multimedia classification. Data Classification: Algorithms and Applications, page 337, 2014. ']
1320	109	[309]	A combination of graphs and relational data is commonly used to represent these social networks and social media data. // Current research has focused on either pre-defined feature vectors ==[42]== or sophisticated graph-based algorithms [48] to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification [37], community detection [44], link prediction [47], social recommendation [35], targeted advertising [22], and so on.	b	Heterogeneous Network Embedding via Deep Architectures	[42]	['[42]  J. Tang and H. Liu. Unsupervised feature selection for linked social media data. In KDD, pages 904–912. ACM, 2012. ']
1321	109	[520]	A combination of graphs and relational data is commonly used to represent these social networks and social media data. // Current research has focused on either pre-defined feature vectors [42] or sophisticated graph-based algorithms ==[48]== to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification [37], community detection [44], link prediction [47], social recommendation [35], targeted advertising [22], and so on.	b	Heterogeneous Network Embedding via Deep Architectures	[48]	['[48]  P. Zhao, J. Han, and Y. Sun. P-rank: a comprehensive structural similarity measure over information networks. In CIKM, pages 553–562. ACM, 2009. ']
1322	109	[68]	Current research has focused on either pre-defined feature vectors [42] or sophisticated graph-based algorithms [48] to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification ==[37]==, community detection [44], link prediction [47], social recommendation [35], targeted advertising [22], and so on. //  The development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links.	b	Heterogeneous Network Embedding via Deep Architectures	[37]	['[37]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
1323	109	[521]	Current research has focused on either pre-defined feature vectors [42] or sophisticated graph-based algorithms [48] to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification [37], community detection ==[44]==, link prediction [47], social recommendation [35], targeted advertising [22], and so on. //  The development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links.	b	Heterogeneous Network Embedding via Deep Architectures	[44]	['[44]  T. Yang, R. Jin, Y. Chi, and S. Zhu. Combining link and content for community detection: a discriminative approach. In KDD, pages 927–936. ACM, 2009. ']
1324	109	[522]	Current research has focused on either pre-defined feature vectors [42] or sophisticated graph-based algorithms [48] to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification [37], community detection [44], link prediction ==[47]==, social recommendation [35], targeted advertising [22], and so on. //  The development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links.	b	Heterogeneous Network Embedding via Deep Architectures	[47]	['[47]  J. Zhang, S. Y. Philip, and Z.-H. Zhou. Meta-path based multi-network collective link prediction. In KDD. ACM, 2014. ']
1325	109	[523]	Current research has focused on either pre-defined feature vectors [42] or sophisticated graph-based algorithms [48] to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification [37], community detection [44], link prediction [47], social recommendation ==[35]==, targeted advertising [22], and so on. //  The development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links.	b	Heterogeneous Network Embedding via Deep Architectures	[35]	['[35]  X. Ren, J. Liu, X. Yu, U. Khandelwal, Q. Gu, L. Wang, and J. Han. Cluscite: Effective citation recommendation by information network-based clustering. In KDD. ACM, 2014. ']
1326	109	[524]	Current research has focused on either pre-defined feature vectors [42] or sophisticated graph-based algorithms [48] to solve the underlying tasks. // A significant amount of research has been accomplished on various topics, such as collective classification [37], community detection [44], link prediction [47], social recommendation [35], targeted advertising ==[22]==, and so on. //  The development of a unified representation for networked data in embedded vector form is of great importance to encode both content and links.	b	Heterogeneous Network Embedding via Deep Architectures	[22]	['[22]  D. Liu, G. Ye, C.-T. Chen, S. Yan, and S.-F. Chang. Hybrid social media network. In ACM MM, pages 659–668. ACM, 2012. ']
1327	109	[525]	Given the varied backgrounds of the users, social media tends to be noisy. // In addition, researchers have noticed that spammers generate more data than legitimate users ==[4]==, which makes network mining even more difficult. // Furthermore, social media data contains diverse and heterogeneous information.	ho	Heterogeneous Network Embedding via Deep Architectures	[4]	['[4]  Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia. Who is tweeting on twitter: human, bot, or cyborg? In ACSAC, pages 21–30. ACM, 2010. ']
1328	109	[488]	As a Google search example of “Malaysia Airlines MH 17” illustrates in Figure 1, relevant results include not only text documents but also images and videos. // Moreover, social media data does not exist in isolation, but in combination with various data types ==[28]==. // These interactions can be formed either explicitly or implicitly with the linkages between them.	ho	Heterogeneous Network Embedding via Deep Architectures	[28]	['[28]  M. Ou, P. Cui, F. Wang, J. Wang, W. Zhu, and S. Yang. Comparing apples to oranges: a scalable solution with heterogeneous hashing. In ACM SIGKDD, pages 230–238. ACM, 2013. ']
1329	109	[526, 521, 10]	HNE maps different heterogeneous objects into a unified latent space so that objects from different spaces can be directly compared. // Unlike to traditional linear embedding models ==[29, 44, 50]==, the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture. // Both Deep Neural Networks (DNN) [1, 14] and Convolutional Neural Networks (CNN) [12, 18] are aggregated to handle vectorized data (e.g., text documents) or tensor-based multimedia objects (e.g., color images and videos).	ro	Heterogeneous Network Embedding via Deep Architectures	[29, 44, 50]	['[29]  A. Paccanaro and G. E. Hinton. Learning distributed representations of concepts using linear relational embedding. TKDE, 2001. ', '[44]  T. Yang, R. Jin, Y. Chi, and S. Zhu. Combining link and content for community detection: a discriminative approach. In KDD, pages 927–936. ACM, 2009. ', '[50]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. In SIGIR, pages 487–494. ACM, 2007.']
1330	109	[347, 43]	Unlike to traditional linear embedding models [29, 44, 50], the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture. // Both Deep Neural Networks (DNN) ==[1, 14]== and Convolutional Neural Networks (CNN) [12, 18] are aggregated to handle vectorized data (e.g., text documents) or tensor-based multimedia objects (e.g., color images and videos). //  Our model coordinates two mutually reinforcing parts by iteratively solving an optimization problem with respect to feature learning and objective minimization.	b	Heterogeneous Network Embedding via Deep Architectures	[1, 14]	['[1]  Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. NIPS, 19:153, 2007. ', '[14]  G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006. ']
1331	109	[527, 149]	Unlike to traditional linear embedding models [29, 44, 50], the proposed method decomposes the feature learning process into multiple nonlinear layers of a deep architecture. // Both Deep Neural Networks (DNN) [1, 14] and Convolutional Neural Networks (CNN) ==[12, 18]== are aggregated to handle vectorized data (e.g., text documents) or tensor-based multimedia objects (e.g., color images and videos). //  Our model coordinates two mutually reinforcing parts by iteratively solving an optimization problem with respect to feature learning and objective minimization.	b	Heterogeneous Network Embedding via Deep Architectures	[12, 18]	['[12]  X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier networks. In JMLR, volume 15, pages 315–323, 2011. ', '[18]  A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 25, 2012. ']
1332	109	[528]	Network Embedding. // A branch of latent feature embeddings is motivated by applications such as collaborative filtering and link prediction in networks that model the relations between entities from latent attributes ==[16]==. // These models often transfer the problem as learning an embedding of the entities, which corresponds algebraically to a matrix factorization problem of observed relationships.	b	Heterogeneous Network Embedding via Deep Architectures	[16]	['[16]  R. Jenatton, N. L. Roux, A. Bordes, and G. R. Obozinski. A latent factor model for highly multi-relational data. In NIPS, pages 3167–3175, 2012. ']
1333	109	[10]	These models often transfer the problem as learning an embedding of the entities, which corresponds algebraically to a matrix factorization problem of observed relationships. // Zhu et. al. ==[50]== proposed a joint factorization approach on both the linkage adjacency matrix and document-term frequency for Web page categorization. // Similar concepts also include [29, 44].	b	Heterogeneous Network Embedding via Deep Architectures	[50]	['[50]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. In SIGIR, pages 487–494. ACM, 2007.']
1334	109	[526, 521]	Zhu et. al. [50] proposed a joint factorization approach on both the linkage adjacency matrix and document-term frequency for Web page categorization.  // Similar concepts also include ==[29, 44]==. // In addition, Shaw et.al. [38] proposed a structure preserving embedding framwork that embeds graphs to a low-dimensional Euclidean space with global topological properties preserved.	b	Heterogeneous Network Embedding via Deep Architectures	[29, 44]	['[29]  A. Paccanaro and G. E. Hinton. Learning distributed representations of concepts using linear relational embedding. TKDE, 2001. ', '[44]  T. Yang, R. Jin, Y. Chi, and S. Zhu. Combining link and content for community detection: a discriminative approach. In KDD, pages 927–936. ACM, 2009. ']
1335	109	[293]	Similar concepts also include [29, 44]. // In addition, Shaw et.al. ==[38]== proposed a structure preserving embedding framwork that embeds graphs to a low-dimensional Euclidean space with global topological properties preserved. // Moreover, DeepWalk [31] learns latent representations of vertices in a network from truncated random walks.	b	Heterogeneous Network Embedding via Deep Architectures	[38]	['[38]  B. Shaw and T. Jebara. Structure preserving embedding. In ICML, pages 937–944. ACM, 2009. ']
1336	109	[6]	In addition, Shaw et.al. [38] proposed a structure preserving embedding framwork that embeds graphs to a low-dimensional Euclidean space with global topological properties preserved. // Moreover, DeepWalk ==[31]== learns latent representations of vertices in a network from truncated random walks. // However, these models focus only on single relations that do not adapt to heterogeneous settings and most of them are hardly to generate to other unseen samples.	h-	Heterogeneous Network Embedding via Deep Architectures	[31]	['[31]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In KDD, pages 701–710. ACM, 2014. ']
1337	109	[529, 526, 282]	However, these models focus only on single relations that do not adapt to heterogeneous settings and most of them are hardly to generate to other unseen samples. // A natural extension of these methods to heterogeneous settings is by stacking multiple relational matrices together, and then applying a conventional tensor factorization ==[26, 29, 39]==. // The disadvantage of such multi-relational embeddings is the inherent sharing of parameters between different terms, which does not scale to large graphs.	h-	Heterogeneous Network Embedding via Deep Architectures	[26, 29, 39]	['[26]  M. Nickel, V. Tresp, and H.-P. Kriegel. A three-way model for collective learning on multi-relational data. In ICML, 2011. ', '[29]  A. Paccanaro and G. E. Hinton. Learning distributed representations of concepts using linear relational embedding. TKDE, 2001. ', '[39]  A. P. Singh and G. J. Gordon. Relational learning via collective matrix factorization. In KDD, pages 650–658. ACM, 2008. ']
1338	109	[530]	The disadvantage of such multi-relational embeddings is the inherent sharing of parameters between different terms, which does not scale to large graphs. // A nonlinear embedding model proposed by Yuan et. al. ==[46]==, used Restricted Boltzmann Machines (RBMs) for crossmodel link analysis. // However, it did not utilize all of the social information from the raw data (required a feature vectorization step), which resulted in suboptimal solutions.	h-	Heterogeneous Network Embedding via Deep Architectures	[46]	['[46]  Z. Yuan, J. Sang, Y. Liu, and C. Xu. Latent feature learning in social media network. In ACM MM, pages 253–262. ACM, 2013. ']
1339	109	[149]	However, it did not utilize all of the social information from the raw data (required a feature vectorization step), which resulted in suboptimal solutions. // Moreover, work in computer vision and speech ==[18]== has shown that layer-wise RBM training is inefficient for large-scale settings when compared to DNNs and CNNs. // Deep Learning.	h-	Heterogeneous Network Embedding via Deep Architectures	[18]	['[18]  A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 25, 2012. ']
1340	109	[531]	Deep Learning. // In recent years, machine learning research has seen a marked switch from hand-crafted ==[49]== features to those that are learned from raw data, mainly due to the success of deep learning. // Deep learning models have become increasingly important in speech recognition, object recognition/detection, and more recently in natural language processing.	b	Heterogeneous Network Embedding via Deep Architectures	[49]	['[49]  J. Zhou, F. Wang, J. Hu, and J. Ye. From micro to macro: data driven phenotyping by densification of longitudinal electronic medical records. In ACM SIGKDD, pages 135–144. ACM, 2014. ']
1341	109	[43]	Unsupervised deep learning, often referred to as “pre-training,” provides robust initialization and regularization with the help of unlabeled data, which is copiously available. // For example, Hinton and Salakhutdinov ==[14]== first employed layer-wise initialization of deep neural networks with the use of RBMs. // A similar approach is weight initialized with autoencoders, as proposed by Bengio et. al.. [1].	b	Heterogeneous Network Embedding via Deep Architectures	[14]	['[14]  G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006. ']
1342	109	[347]	For example, Hinton and Salakhutdinov [14] first employed layer-wise initialization of deep neural networks with the use of RBMs. // A similar approach is weight initialized with autoencoders, as proposed by Bengio et. al.. ==[1]==. // The method of dropout [15] has shown particular promise.	b	Heterogeneous Network Embedding via Deep Architectures	[1]	['[1]  Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. NIPS, 19:153, 2007. ']
1343	109	[67]	A similar approach is weight initialized with autoencoders, as proposed by Bengio et. al.. [1]. // The method of dropout ==[15]== has shown particular promise. // A seven-layer convolutional network developed by Krizhevsky et. al. [18] achieved state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge [36], one of the most challenging tasks in computer vision.	h+	Heterogeneous Network Embedding via Deep Architectures	[15]	['[15]  G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012. ']
1344	109	[149]	The method of dropout [15] has shown particular promise. // A seven-layer convolutional network developed by Krizhevsky et. al. ==[18]== achieved state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge [36], one of the most challenging tasks in computer vision. // Later on, features from one of network’s intermediate layers proved to be a superior feature representation for other vision tasks, such as object detection [11].	h+	Heterogeneous Network Embedding via Deep Architectures	[18]	['[18]  A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 25, 2012. ']
1345	109	[532]	The method of dropout [15] has shown particular promise. // A seven-layer convolutional network developed by Krizhevsky et. al. [18] achieved state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge ==[36]==, one of the most challenging tasks in computer vision. // Later on, features from one of network’s intermediate layers proved to be a superior feature representation for other vision tasks, such as object detection [11].	h+	Heterogeneous Network Embedding via Deep Architectures	[36]	['[36]  O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge, 2014. ']
1346	109	[533]	A seven-layer convolutional network developed by Krizhevsky et. al. [18] achieved state-of-the-art performance on the ImageNet Large Scale Visual Recognition Challenge [36], one of the most challenging tasks in computer vision. // Later on, features from one of network’s intermediate layers proved to be a superior feature representation for other vision tasks, such as object detection ==[11]==. // There has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures.	h+	Heterogeneous Network Embedding via Deep Architectures	[11]	['[11]  R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, pages 580–587. IEEE, 2014. ']
1347	109	[534]	There has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures. // Unsupervised deep learning methods such as auto-encoders ==[25]== and RBMs [40] are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network, and successfully applied to several multimodal tasks. // Alternatively, there has also been effort on learning the joint representation with multiple tasks [6].	h+	Heterogeneous Network Embedding via Deep Architectures	[25]	['[25]  J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In ICML, pages 689–696, 2011. ']
1348	109	[535]	There has been a growing interest in leveraging data from multiple modalities within the context of deep learning architectures. // Unsupervised deep learning methods such as auto-encoders [25] and RBMs ==[40]== are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network, and successfully applied to several multimodal tasks. // Alternatively, there has also been effort on learning the joint representation with multiple tasks [6].	h+	Heterogeneous Network Embedding via Deep Architectures	[40]	['[40]  N. Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, pages 2222–2230, 2012. ']
1349	109	[125]	Unsupervised deep learning methods such as auto-encoders [25] and RBMs [40] are deployed to perform feature learning by joint reconstruction of audio and video with a partially shared network, and successfully applied to several multimodal tasks. // Alternatively, there has also been effort on learning the joint representation with multiple tasks ==[6]==. // For image and text, a particular useful scenario is zero-shot learning of image classification on unseen labels achieved by incorporating the semantically meaningful embedding space to image labels [9, 27].	b	Heterogeneous Network Embedding via Deep Architectures	[6]	['[6]  R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pages 160–167, 2008. ']
1350	109	[536, 537]	Alternatively, there has also been effort on learning the joint representation with multiple tasks [6]. // For image and text, a particular useful scenario is zero-shot learning of image classification on unseen labels achieved by incorporating the semantically meaningful embedding space to image labels ==[9, 27]==. // To the best of our knowledge, there have been few previous attempts to make use of the linkage structure in a network for representation learning.	h+	Heterogeneous Network Embedding via Deep Architectures	[9, 27]	['[9]  A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, pages 2121–2129, 2013. ', '[27]  M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. S. Corrado, and J. Dean. Zero-shot learning by convex combination of semantic embeddings. arXiv:1312.5650, 2013. ']
1351	109	[174]	To the best of our knowledge, there have been few previous attempts to make use of the linkage structure in a network for representation learning. // In ==[19]== a conditional temporary RBM was used to model the dynamics of network links, and the main task was to predict future links with historical data. // The most similar work to ours is that of [46].	b	Heterogeneous Network Embedding via Deep Architectures	[19]	['[19]  X. Li, N. Du, H. Li, K. Li, J. Gao, and A. Zhang. A deep learning approach to link prediction in dynamic networks, 2014. ']
1352	109	[530]	In [19] a conditional temporary RBM was used to model the dynamics of network links, and the main task was to predict future links with historical data. // The most similar work to ours is that of ==[46]==. //  A content RBM was trained on multimedia link data.	b	Heterogeneous Network Embedding via Deep Architectures	[46]	['[46]  Z. Yuan, J. Sang, Y. Liu, and C. Xu. Latent feature learning in social media network. In ACM MM, pages 253–262. ACM, 2013. ']
1353	109	[225]	Heterogeneous Networks. // A heterogeneous network ==[41]== is defined as a network with multiple types of objects and/or multiple types of links. // As a mathematical abstraction, we define an undirected graph G “ pV, Eq, where V “ tv1,...,vnu is a set of vertices and E is a set of edges.	b	Heterogeneous Network Embedding via Deep Architectures	[41]	['[41]  Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu. Integrating meta-path selection with user-guided object clustering in heterogeneous information networks. In KDD. ACM, 2012. ']
1354	109	[309]	Datasets and Experiment Settings We use two publicly available datasets from real-world social sites. // The first one is BlogCatalog which is used in ==[42]== to select features in linked social media data. // The second one is a heterogeneous dataset, which is referred to as NUS-WIDE [5].	hoe	Heterogeneous Network Embedding via Deep Architectures	[42]	['[42]  J. Tang and H. Liu. Unsupervised feature selection for linked social media data. In KDD, pages 904–912. ACM, 2012. ']
1355	109	[515]	We use two publicly available datasets from real-world social sites. The first one is BlogCatalog which is used in [42] to select features in linked social media data. // The second one is a heterogeneous dataset, which is referred to as NUS-WIDE ==[5]==. //  This dataset contains both images and text.	b	Heterogeneous Network Embedding via Deep Architectures	[5]	['[5]  T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. Nus-wide: a real-world web image database from national university of singapore. In ICIVR, page 48, 2009. ']
1356	109	[309]	The detailed descriptions and statistics for both datasets are provided below. // BlogCatalog ==[42]==: It is a social blogging site where registered users are able to categorize their blogs under predefined classes. // Such categorizations are used to define class labels, whereas “following” behaviors are used to construct linkages between users.	b	Heterogeneous Network Embedding via Deep Architectures	[42]	['[42]  J. Tang and H. Liu. Unsupervised feature selection for linked social media data. In KDD, pages 904–912. ACM, 2012. ']
1357	109	[515]	Some detailed statistics are summarized in Table 1. // NUS-WIDE ==[5]==: The dataset was originally collected by the Lab for Media Search in the National University of Singapore in the year 2009. // The dataset includes 269,648 unique images with associate tags from Flickr.	b	Heterogeneous Network Embedding via Deep Architectures	[5]	['[5]  T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. Nus-wide: a real-world web image database from national university of singapore. In ICIVR, page 48, 2009. ']
1358	109	[309]	Link-content: We combine features from the previous two. // LUFS ==[42]==: Unsupervised feature selection framework for linked social media data considering both content and links. // LCMF [50]: A matrix co-factorization method that utilizes both linkage structure and content features.	b	Heterogeneous Network Embedding via Deep Architectures	[42]	['[42]  J. Tang and H. Liu. Unsupervised feature selection for linked social media data. In KDD, pages 904–912. ACM, 2012. ']
1359	109	[10]	LUFS [42]: Unsupervised feature selection framework for linked social media data considering both content and links. // LCMF ==[50]==: A matrix co-factorization method that utilizes both linkage structure and content features. // DT [33]: A transfer learning method is used to bridge semantic distances between image and text by latent embeddings.	b	Heterogeneous Network Embedding via Deep Architectures	[50]	['[50]  S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining content and link for classification using matrix factorization. In SIGIR, pages 487–494. ACM, 2007.']
1360	109	[538]	‚ CCA: The Canonical Correlation Analysis embeds two types of input sources into a common latent space by optimizing with respect to their correlations. // DT ==[33]==: A transfer learning method is used to bridge semantic distances between image and text by latent embeddings. // LHNE: The linear version of HNE solves the optimization function in equation (7).	b	Heterogeneous Network Embedding via Deep Architectures	[33]	['[33]  G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Transfer learning of distance metrics by cross-domain metric sampling across heterogeneous spaces. In SDM, pages 528–539. SIAM, 2012. ']
1361	387	[539, 71]	As a result, the network representation is inevitable in many data mining applications. // On the one hand, many data mining applications are designed for networks, such as community detection ==[24, 9]== and link prediction [12]. // On the other hand, other data mining applications can benefit from the analysis of networks, such as collective classification [16] and dimension reduction [25, 26, 11]. All of above applications rely on the analysis of network interactions or edges.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[24, 9]	['[24]  X. Wei, B. Cao, W. Shao, C.-T. Lu, and P. S. Yu. Community detection with partially observable links and node attributes. In IEEE International Conference on Big Data, 2016. ', '[9]  S. Fortunato. Community detection in graphs. Physics reports, 486(3):75–174, 2010. ']
1362	387	[70]	As a result, the network representation is inevitable in many data mining applications. // On the one hand, many data mining applications are designed for networks, such as community detection [24, 9] and link prediction ==[12]==. // On the other hand, other data mining applications can benefit from the analysis of networks, such as collective classification [16] and dimension reduction [25, 26, 11]. All of above applications rely on the analysis of network interactions or edges.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[12]	['[12]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
1363	387	[68]	On the one hand, many data mining applications are designed for networks, such as community detection [24, 9] and link prediction [12]. // On the other hand, other data mining applications can benefit from the analysis of networks, such as collective classification ==[16]== and dimension reduction [25, 26, 11]. // All of above applications rely on the analysis of network interactions or edges.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[16]	['[16]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
1364	387	[540, 541, 405]	On the one hand, many data mining applications are designed for networks, such as community detection [24, 9] and link prediction [12]. // On the other hand, other data mining applications can benefit from the analysis of networks, such as collective classification [16] and dimension reduction ==[25, 26, 11]==. // All of above applications rely on the analysis of network interactions or edges.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[25, 26, 11]	['[25]  X. Wei, B. Cao, and P. S. Yu. Unsupervised feature selection on networks: A generative view. In AAAI, 2016. ', '[26]  X. Wei, S. Xie, and P. S. Yu. Efficient partial order preserving unsupervised feature selection on networks. In SDM, pages 82–90. SIAM, 2015.', '[11]  J. B. Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964. ']
1365	387	[33, 55, 154, 6, 1]	All of above applications rely on the analysis of network interactions or edges. // Nowadays, network embedding is increasingly employed to assist network analysis as it is effective to learn latent features that encode linkage information ==[19, 20, 1, 13, 17]==. // The basic idea of network embedding is to preserve the network structure by presenting pairs of vertices with edges to be close in the latent space.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[19, 20, 1, 13, 17]	['[19]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 817–826. ACM, 2009. ', '[20]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ', '[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37–48. International World Wide Web Conferences Steering Committee, 2013. ', '[13]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ', '[17]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1366	387	[6, 1]	The latent features are beneficial as they are more expressive than edges and can be directly employed by off-the-shelf machine learning techniques. // Although various network embedding methods ==[13, 17]== have been proposed before, they are designed only for learning representations on a single network. // Furthermore, in the era of big data, different types of related information are often available and can be fused together to form a coupled heterogeneous network, where each type of information can be represented as a separate homogeneous network.	h-	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[13, 17]	['[13]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ', '[17]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1367	387	[55]	And we compare the proposed EOE against the following three latent feature learning methods. // Spectral clustering (SC) ==[20]==: This method proposes to use spectral clustering to learn latent features. // Specifically, the top d eigenvectors of the normalized Laplacian matrix are used as the feature vectors.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[20]	['[20]  L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge Discovery, 23(3):447–478, 2011. ']
1368	387	[6]	Specifically, the top d eigenvectors of the normalized Laplacian matrix are used as the feature vectors. // DeepWalk ==[13]==: DeepWalk learns latent features for vertices by modeling random walks as sentences of a natural language, and then re-formulating language modeling as its learning objective. // Since DeepWalk is only applicable to unweighted edges, all weights are set to 1 as inputs to DeepWalk.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[13]	['[13]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
1369	387	[1]	Since DeepWalk is only applicable to unweighted edges, all weights are set to 1 as inputs to DeepWalk. // LINE ==[17]==: LINE is proposed to embed large-scale information networks, and is applicable to directed, undirected, weighted and unweighted networks. // The proposed EOE is applicable to large-scale network as well since it can be solved in polynomial time.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[17]	['[17]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1370	387	[70]	Link Prediction. // The link prediction problem ==[12]== refers to inferring new interactions between network vertices by measuring the similarity between them. //  We deploy two scenarios of link prediction for evaluation of the latent features, which are future link prediction and missing link prediction.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[12]	['[12]  D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007. ']
1371	387	[4, 3, 355, 5]	The proposed EOE model is related to general graph embedding or network embedding methods to learn latent representations for graph or network vertices. // A couple of graph or network embedding methods have been proposed previously ==[15, 21, 8, 3]==, but they are originally designed for dimension reduction of existing features. // Specifically, their objectives are to learn low-dimensional latent representations of existing features so that learning complexity brought by feature dimension would be significantly reduced.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[15, 21, 8, 3]	['[15]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ', '[21]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. science, 290(5500):2319–2323, 2000. ', '[8]  T. F. Cox and M. A. Cox. Multidimensional scaling. CRC press, 2000. ', '[3]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ']
1372	387	[154]	In our scenarios, there exist no features of network vertices but the network edge information. // Another graph embedding method called graph factorization ==[1]== learns latent features by utilizing network edges. // It presents graphs as matrices where matrix elements correspond to edges between vertices, and then learns latent features by matrix factorization.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[1]	['[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd international conference on World Wide Web, pages 37–48. International World Wide Web Conferences Steering Committee, 2013. ']
1373	387	[6]	The latter regulation is important because it would preserve the information that certain vertices are not likely to interact, which is a part of network structure information as well. // A recent network embedding model called DeepWalk ==[13]== embeds local link information obtained from random walks. // It is motivated by the connection between degree distribution of networks and word frequency distribution of natural languages.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[13]	['[13]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710. ACM, 2014. ']
1374	387	[1]	Moreover, DeepWalk can only handle unweighted networks, while the proposed EOE model is applicable to both weighted and unweighted networks. // The state-of-the-art related model is LINE ==[17]== for largescale information network embedding. // LINE preserves both interaction information and non-interaction information, which is similar to the proposed EOE.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[17]	['[17]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1375	387	[542]	The gradient ∂L(U) ∂ua i is not presented before, but can be easily obtained by removing polynomials including M from the gradient ∂L(U,M,V ) ∂ua i. // For the learning rate, we employ the backtracking line search ==[2]== to learn an appropriate one for each of the iteration. // The condition to determine whether all minimizations converge is that the relative loss between current iteration and last iteration is smaller than a considerably small value, such as 0.001.	hoe	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[2]	['[2]  L. Armijo. Minimization of functions having lipschitz continuous first partial derivatives. Pacific Journal of mathematics, 16(1):1–3, 1966. ']
1376	387	[1]	Embedding Visualization. // Visualization of embeddings on a two dimensional space is an important application of network embedding ==[17]==. //  If the learned embeddings preserve the network structure well, visualization provides an easy way to generate the layout of a network.	b	Embedding of Embedding (EOE) : Joint Embedding for Coupled Heterogeneous Networks	[17]	['[17]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pages 1067–1077. International World Wide Web Conferences Steering Committee, 2015. ']
1377	8	[139, 543]	To ensure satisfactory performance, this densely-labeled subgraph should be sufficiently large so that dependency relationships among network structure and node labels can be accurately learned. // However, acquiring labels of inter-connected nodes in a network can be very expensive and time-consuming ==[2, 7]==. // Therefore, very often the provided network can be only sparsely labeled, with a very small portion of labeled nodes that may not comprise a connected subgraph.	h-	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[2, 7]	['[2]  B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. In Proceedings of the 2nd SNA-KDD Workshop, pages 1–19, 2008. 1571 ', '[7]  L. K. McDowell and D. W. Aha. Labels or attributes? rethinking the neighbors for collective classification in sparsely-labeled networks. In Proceedings of CIKM, pages 847–852, 2013. ']
1378	8	[544, 543]	To tackle the label sparsity, two branches of research studies have been proposed in recent literature. // The first is semi-supervised collective classification ==[6, 7]==, which leverages unlabeled nodes to enhance classification performance. // These methods typically first use a bootstrap step to predict labels for all unlabeled nodes, through a classifier trained on labeled nodes using only content features.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[6, 7]	['[6]  L. K. Mcdowell and D. W. Aha. Semi-supervised collective classification via hybrid label regularization. In Proceedings of ICML, pages 975–982, 2012. ', '[7]  L. K. McDowell and D. W. Aha. Labels or attributes? rethinking the neighbors for collective classification in sparsely-labeled networks. In Proceedings of CIKM, pages 847–852, 2013. ']
1379	8	[142, 545, 546]	After that, predicted labels, together with known labels, are used to construct relational features and perform collective inference. // The second line of research focuses on increasing network connectivity between unlabeled and labeled nodes ==[3, 5, 14]==. // This is achieved by either adding extra edges based on node attribute similarity, or constructing a latent graph from different information sources.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[3, 5, 14]	['[3]  B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Falloutsos. Using ghost edges for classification in sparsely labeled networks. In Proceedings of SIGKDD, pages 256–264, 2008. ', '[5]  S. A. Macskassy. Improving learning in networked data by combining explicit and mined links. In Proceedings of AAAI, pages 590–595, 2007. ', '[14]  X. Shi, Y. Li, and P. Yu. Collective prediction with latent graphs. In Proceedings of CIKM, pages 1127–1136, 2011. ']
1380	8	[544, 543]	Our main idea of tackling node label sparsity is twofold: (1) leverage random walks to find path dependencies between unlabeled nodes and sparsely labeled nodes in the network; and (2) learn a latent network representation that fully embeds nodes’ content information and augmented network structure to minimize classification loss. // Intuitively, the proposed DMF is motivated by observations that, for sparsely labeled networks, an unlabeled node in the network may not have labeled neighbors or have very low structure similarity to labeled nodes, so existing semi-supervised ==[6, 7]== and connectivity-based methods [3, 5, 14] would not have satisfactory performance. // Alternatively, we can use network walks to examine unlabeled nodes’ correlations to labeled nodes.	h-	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[6, 7]	['[6]  L. K. Mcdowell and D. W. Aha. Semi-supervised collective classification via hybrid label regularization. In Proceedings of ICML, pages 975–982, 2012. ', '[7]  L. K. McDowell and D. W. Aha. Labels or attributes? rethinking the neighbors for collective classification in sparsely-labeled networks. In Proceedings of CIKM, pages 847–852, 2013. ']
1381	8	[142, 545, 546]	Our main idea of tackling node label sparsity is twofold: (1) leverage random walks to find path dependencies between unlabeled nodes and sparsely labeled nodes in the network; and (2) learn a latent network representation that fully embeds nodes’ content information and augmented network structure to minimize classification loss. // Intuitively, the proposed DMF is motivated by observations that, for sparsely labeled networks, an unlabeled node in the network may not have labeled neighbors or have very low structure similarity to labeled nodes, so existing semi-supervised [6, 7] and connectivity-based methods ==[3, 5, 14]== would not have satisfactory performance. // Alternatively, we can use network walks to examine unlabeled nodes’ correlations to labeled nodes.	h-	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[3, 5, 14]	['[3]  B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Falloutsos. Using ghost edges for classification in sparsely labeled networks. In Proceedings of SIGKDD, pages 256–264, 2008. ', '[5]  S. A. Macskassy. Improving learning in networked data by combining explicit and mined links. In Proceedings of AAAI, pages 590–595, 2007. ', '[14]  X. Shi, Y. Li, and P. Yu. Collective prediction with latent graphs. In Proceedings of CIKM, pages 1127–1136, 2011. ']
1382	8	[7]	Because a given network has an exponential number of walk combinations, this essentially increases the relationship density for sparsely labeled networks. // In order to utilize network walks to guide node classification with minimum loss, we leverage an existing TextAssociated DeepWalk (TADW) approach ==[18]==, which embeds nodes’ linkage information together with their content features into a low-dimensional space. // To enable supervised information to be propagated from labeled nodes to unlabeled nodes, we formulate a new matrix factorization objective function that integrates network representation learning with an empirical loss minimization of a linear classifier on labeled nodes.	ho	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[18]	['[18]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang. Network representation learning with rich text information. In Proceedings of IJCAI, pages 2111–2117, 2015.']
1383	8	[547, 68]	Collective classification. // For networked data classification, collective classification (CC) has received considerable attention in recent years ==[8, 13]==. // Its key idea is to construct a new set of features that summarize label dependencies to improve classification accuracy.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[8, 13]	['[8]  L. K. McDowell, K. M. Gupta, and D. W. Aha. Cautious collective classification. Journal of Machine Learning Research, 10:2777–2836, 2009. ', '[13]  P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008. ']
1384	8	[544]	The variants of semi-supervised algorithms mainly differ in how they train a best classifier from all available features. // A recent study by Mcdowell and Aha ==[6]== provided a comprehensive comparison of different semi-supervised CC algorithms and found that learning a hybrid classifier from content features and relational features can best boost the performance of semi-supervised CC algorithms. // In a follow-up work, McDowell and Aha [7] pointed out that training discriminative classifiers additionally with neighbors’ attributes can yield further accuracy gains for semi-supervised collective classification.	h+	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[6]	['[6]  L. K. Mcdowell and D. W. Aha. Semi-supervised collective classification via hybrid label regularization. In Proceedings of ICML, pages 975–982, 2012. ']
1385	8	[543]	A recent study by Mcdowell and Aha [6] provided a comprehensive comparison of different semi-supervised CC algorithms and found that learning a hybrid classifier from content features and relational features can best boost the performance of semi-supervised CC algorithms. // In a follow-up work, McDowell and Aha ==[7]== pointed out that training discriminative classifiers additionally with neighbors’ attributes can yield further accuracy gains for semi-supervised collective classification. // Another line of research has focused more on increasing network connectivity between unlabeled nodes and labeled nodes [3, 5, 14].	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[7]	['[7]  L. K. McDowell and D. W. Aha. Labels or attributes? rethinking the neighbors for collective classification in sparsely-labeled networks. In Proceedings of CIKM, pages 847–852, 2013. ']
1386	8	[142, 545, 546]	In a follow-up work, McDowell and Aha [7] pointed out that training discriminative classifiers additionally with neighbors’ attributes can yield further accuracy gains for semi-supervised collective classification. // Another line of research has focused more on increasing network connectivity between unlabeled nodes and labeled nodes ==[3, 5, 14]==. // Macskassy [5] enriched the linkage of the original network by adding extra links based on node attribute similarity, and a weighted-vote relational neighbor with relational labeling (wwRN+RL) classifier was used to classify unlabeled nodes.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[3, 5, 14]	['[3]  B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Falloutsos. Using ghost edges for classification in sparsely labeled networks. In Proceedings of SIGKDD, pages 256–264, 2008. ', '[5]  S. A. Macskassy. Improving learning in networked data by combining explicit and mined links. In Proceedings of AAAI, pages 590–595, 2007. ', '[14]  X. Shi, Y. Li, and P. Yu. Collective prediction with latent graphs. In Proceedings of CIKM, pages 1127–1136, 2011. ']
1387	8	[545]	Another line of research has focused more on increasing network connectivity between unlabeled nodes and labeled nodes [3, 5, 14]. // Macskassy ==[5]== enriched the linkage of the original network by adding extra links based on node attribute similarity, and a weighted-vote relational neighbor with relational labeling (wwRN+RL) classifier was used to classify unlabeled nodes. // Gallagher et al. [3] proposed to add “ghost edges” created via random walk with restart, to a network, which enables information propagation from labeled nodes to unlabeled ones.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[5]	['[5]  S. A. Macskassy. Improving learning in networked data by combining explicit and mined links. In Proceedings of AAAI, pages 590–595, 2007. ']
1388	8	[142]	Macskassy [5] enriched the linkage of the original network by adding extra links based on node attribute similarity, and a weighted-vote relational neighbor with relational labeling (wwRN+RL) classifier was used to classify unlabeled nodes. // Gallagher et al. ==[3]== proposed to add “ghost edges” created via random walk with restart, to a network, which enables information propagation from labeled nodes to unlabeled ones. // In [14], a Latent Network Propagation model (LNP) was proposed that constructs a latent graph from different information sources and uses label propagation to predict labels of unlabeled nodes.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[3]	['[3]  B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Falloutsos. Using ghost edges for classification in sparsely labeled networks. In Proceedings of SIGKDD, pages 256–264, 2008. ']
1389	8	[546]	Gallagher et al. [3] proposed to add “ghost edges” created via random walk with restart, to a network, which enables information propagation from labeled nodes to unlabeled ones. // In ==[14]==, a Latent Network Propagation model (LNP) was proposed that constructs a latent graph from different information sources and uses label propagation to predict labels of unlabeled nodes. // However, most of these methods consider either local dependencies between network structure and labels, or correlations between nodes’ content features and labels, but ignore the important role of their interplay for jointly classifying labels of related nodes.	h-	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[14]	['[14]  X. Shi, Y. Li, and P. Yu. Collective prediction with latent graphs. In Proceedings of CIKM, pages 1127–1136, 2011. ']
1390	8	[256]	Our work is also related to network representation learning that learns a latent representation of network nodes such that nodes close to each other in the original network space are also similar to each other in the learned representation space. // Chen et al. ==[1]== firstly proposed a method for embeddingvertices on a directed graph into a vector space by considering graph link structure. // The proposed method preserves the locality property of vertices on a directed graph in the embedded space, by using random walks.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[1]	['[1]  M. Chen, Q. Yang, and X. Tang. Directed graph embedding. In Proceedings of IJCAI, pages 2707–2712, 2007. ']
1391	8	[33]	The proposed method preserves the locality property of vertices on a directed graph in the embedded space, by using random walks. // Tang and Liu ==[16]== proposed to extract latent social dimensions based on network connectivity, with each dimension representing a likely affiliation among social actors. // Perozzi et al. [12] proposed DeepWalk that learns latent representations of network nodes, which encode structural regularities in a continuous, low-dimensional vector space immediately available for statistical machine learning algorithms.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[16]	['[16]  L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of SIGKDD, pages 817–826, 2009. ']
1392	8	[6]	Tang and Liu [16] proposed to extract latent social dimensions based on network connectivity, with each dimension representing a likely affiliation among social actors. // Perozzi et al. ==[12]== proposed DeepWalk that learns latent representations of network nodes, which encode structural regularities in a continuous, low-dimensional vector space immediately available for statistical machine learning algorithms. //  DeepWalk uses random walks to expand the neighborhood of a node, thus providing a more global view of the network.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[12]	['[12]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deep walk: Online learning of social representations. In Proceedings of SIGKDD, pages 701–710, 2014. ']
1393	8	[1]	It was showed to outperform other latent representation learning methods, especially when labeled nodes are scarce. // Despite of its empirical effectiveness, DeepWalk does not provide a clear interpretation about what network properties are preserved, as pointed out in ==[15]==. // Instead, Tang et al. [15] proposed a new algorithm called LINE that learns node representations by explicitly modeling two local pairwise node proximities, i.e., the first-order proximity between pairs of nodes directly linked to each other, and the second-order proximity between pairs of nodes sharing common neighbors.	ho	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[15]	['[15]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale information network embedding. In Proceedings of WWW, pages 1067–1077, 2015. ']
1394	8	[1]	Despite of its empirical effectiveness, DeepWalk does not provide a clear interpretation about what network properties are preserved, as pointed out in [15]. // Instead, Tang et al. ==[15]== proposed a new algorithm called LINE that learns node representations by explicitly modeling two local pairwise node proximities, i.e., the first-order proximity between pairs of nodes directly linked to each other, and the second-order proximity between pairs of nodes sharing common neighbors. // All of these methods have solely considered the link structure for network representation learning, but ignored the interplay between nodes’ content features and structural features.	h-	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[15]	['[15]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: Large-scale information network embedding. In Proceedings of WWW, pages 1067–1077, 2015. ']
1395	8	[7]	All of these methods have solely considered the link structure for network representation learning, but ignored the interplay between nodes’ content features and structural features. // The most recent work related to ours is text-associated DeepWalk (TADW) ==[18]==, which incorporates text features of nodes into network representation learning. // In [18], DeepWalk was proved to be equivalent to matrix factorization. Based on this view, text features of nodes are naturally incorporated into the matrix factorization framework. By capturing interactions between text features and network structures, TADW can obtain better network representations for network node classification.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[18]	['[18]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang. Network representation learning with rich text information. In Proceedings of IJCAI, pages 2111–2117, 2015.']
1396	8	[7]	The most recent work related to ours is text-associated DeepWalk (TADW) [18], which incorporates text features of nodes into network representation learning. // In ==[18]==, DeepWalk was proved to be equivalent to matrix factorization. Based on this view, text features of nodes are naturally incorporated into the matrix factorization framework. // By capturing interactions between text features and network structures, TADW can obtain better network representations for network node classification.	h+	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[18]	['[18]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang. Network representation learning with rich text information. In Proceedings of IJCAI, pages 2111–2117, 2015.']
1397	8	[6]	Text-Associated DeepWalk. // Motivated by DeepWalk ==[12]==, Text-Associated DeepWalk (TADW) [18] incorporates node text features into network representation learning. // DeepWalk originally generalizes the Skip-Gram model [9] from word representation learning to node representation learning. In DeepWalk, nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[12]	['[12]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deep walk: Online learning of social representations. In Proceedings of SIGKDD, pages 701–710, 2014. ']
1398	8	[7]	Text-Associated DeepWalk. // Motivated by DeepWalk [12], Text-Associated DeepWalk (TADW) ==[18]== incorporates node text features into network representation learning. // DeepWalk originally generalizes the Skip-Gram model [9] from word representation learning to node representation learning. In DeepWalk, nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[18]	['[18]  C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y. Chang. Network representation learning with rich text information. In Proceedings of IJCAI, pages 2111–2117, 2015.']
1399	8	[36]	Motivated by DeepWalk [12], Text-Associated DeepWalk (TADW) [18] incorporates node text features into network representation learning. // DeepWalk originally generalizes the Skip-Gram model ==[9]== from word representation learning to node representation learning. In DeepWalk, nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes. //  In DeepWalk, nodes in networks are taken as words in natural language and a set of short random walk sequences on the given network are generated to capture context for nodes.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[9]	['[9]  T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119, 2013. ']
1400	8	[127]	We compare the proposed DMF algorithm with the following baseline methods. // ICA: The ICA algorithm ==[11]== is one of the most popularly used baselines for collective classification. //  We train a logistic regression classifier on labeled nodes using their node content features and relational features calculated as the number of positive neighbors and the number of negative neighbors.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[11]	['[11]  J. Neville and D. Jensen. Iterative classification in relational data. In Proceedings of AAAI-2000 Workshop on Learning Statistical Models from Relational Data, pages 13–20, 2000. ']
1401	8	[546]	A collective inference is then performed with the learned classifier to predict labels of unlabeled nodes. // LNP: LNP ==[14]== is a recently proposed algorithm for performing collective classification in sparsely labeled networks. // It builds a latent graph by linearly combining five graphs: kstep graph, label similarity graph, attribute similarity graph, prediction confidence graph, and structure similarity graph.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[14]	['[14]  X. Shi, Y. Li, and P. Yu. Collective prediction with latent graphs. In Proceedings of CIKM, pages 1127–1136, 2011. ']
1402	8	[543]	After the latent graph is constructed, label propagation is performed to classify unlabeled nodes. // Semi-supervised RCI (Semi-RCI): Semi-RCI ==[7]== is the state-of-the-art semi-supervised collective classification algorithm that exploits node content features together with neighbor features and neighbor labels to improve the learning performance of the ICA algorithm. // Semi-RCI first bootstraps the labels of unlabeled nodes via the classifier trained on labeled nodes using node features and neighbor features.	b	Collective Classification via Discriminative Matrix Factorization on Sparsely Labeled Networks	[7]	['[7]  L. K. McDowell and D. W. Aha. Labels or attributes? rethinking the neighbors for collective classification in sparsely-labeled networks. In Proceedings of CIKM, pages 847–852, 2013. ']
1403	110	[26, 6, 1]	However, for very large graphs, common information retrieval and mining tasks such as link prediction, node classification, clustering, and recommendation are time-consuming. // This motivated a lot of interest ==[6, 22, 31]== in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors. // A good embedding preserves the proximity (i.e., similarity) between vertices in the original graph.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[6, 22, 31]	['[6]  S. Cao, W. Lu, and Q. Xu. Grarep: Learning graph representations with global structural information. In CIKM, pages 891–900. ACM, 2015. ', '[22]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ', '[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1404	110	[548]	Search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces. // Heterogeneous information networks (HINs), such as DBLP ==[15]==, YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types. // These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11, 14, 17, 20].	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[15]	['[15]  M. Ley. The dblp computer science bibliography: Evolution, research issues, perspectives. In SPIRE, pages 1–10. Springer, 2002. ']
1405	110	[549]	Search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces. // Heterogeneous information networks (HINs), such as DBLP [15], YAGO ==[26]==, DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types. // These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11, 14, 17, 20].	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[26]	['[26]  F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In WWW, pages 697–706, 2007. ']
1406	110	[550]	Search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces. // Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia ==[2]== and Freebase [4], are networks with nodes and edges that may belong to multiple types. // These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11, 14, 17, 20].	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[2]	['[2]  S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. Dbpedia: A nucleus for a web of open data. In The Semantic Web, pages 722–735. Springer, 2007. ']
1407	110	[400]	Search and analysis can then be applied on the embedding with the help of efficient algorithms and indexing approaches for vector spaces. // Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase ==[4]==, are networks with nodes and edges that may belong to multiple types. // These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11, 14, 17, 20].	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[4]	['[4]  K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, pages 1247–1250, 2008. ']
1408	110	[551, 552, 553, 554]	Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types. // These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge ==[11, 14, 17, 20]==. //  Figure 1 illustrates a HIN, which describes the relationships between objects (graph vertices) of different types (e.g., author, paper, venue and topic).	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[11, 14, 17, 20]	['[11]  N. Jayaram, A. Khan, C. Li, X. Yan, and R. Elmasri. Querying knowledge graphs by example entity tuples. TKDE, 27(10):2797–2811, 2015. ', '[14]  N. Lao and W. W. Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53–67, 2010. ', '[17]  C. Meng, R. Cheng, S. Maniu, P. Senellart, and W. Zhang. Discovering meta-paths in large heterogeneous information networks. In WWW, pages 754–764. ACM, 2015. ', '[20]  D. Mottin, M. Lissandrini, Y. Velegrakis, and T. Palpanas. Exemplar queries: Give me an example of what you need. PVLDB, 7(5):365–376, 2014. ']
1409	110	[224]	For example, in the HIN in Figure 1, author a1 is close to both a2 and v1, but these relationships have different semantics. a2 is a co-author of a1, while v1 is a venue where a1 has a paper published. // Meta path ==[29]== is a recently proposed proximity model in HINs. // A meta path is a sequence of object types with edge types in between modeling a particular relationship.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1410	110	[224]	Based on meta paths, several proximity measures have been proposed. // For example, PathCount ==[29]== counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarXiv:1701.05291v1 [cs.AI] 19 Jan 2017 mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1411	110	[552]	Based on meta paths, several proximity measures have been proposed. // For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) ==[14]== measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarXiv:1701.05291v1 [cs.AI] 19 Jan 2017 mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[14]	['[14]  N. Lao and W. W. Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53–67, 2010. ']
1412	110	[224]	For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarmance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search ==[29]==, link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30]. // Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1413	110	[555, 556, 522]	For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarmance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction ==[27, 28, 35]==, recommendation [34], classification [12, 13] and clustering [30]. // Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[27, 28, 35]	['[27]  Y. Sun, R. Barber, M. Gupta, C. C. Aggarwal, and J. Han. Co-author relationship prediction in heterogeneous bibliographic networks. In ASONAM, pages 121–128. IEEE, 2011. ', '[28]  Y. Sun, J. Han, C. C. Aggarwal, and N. V. Chawla. When will it happen?: relationship prediction in heterogeneous information networks. In WSDM, pages 663–672. ACM, 2012. ', '[35]  J. Zhang, P. S. Yu, and Z.-H. Zhou. Meta-path based multi-network collective link prediction. In SIGKDD, pages 1286–1295. ACM, 2014.']
1414	110	[151]	For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarmance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation ==[34]==, classification [12, 13] and clustering [30]. // Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[34]	['[34]  X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In WSDM, pages 283–292. ACM, 2014. ']
1415	110	[557, 558]	For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarmance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification ==[12, 13]== and clustering [30]. // Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[12, 13]	['[12]  M. Ji, J. Han, and M. Danilevsky. Ranking-based classification of heterogeneous information networks. In SIGKDD, pages 1298–1306. ACM, 2011. ', '[13]  X. Kong, P. S. Yu, Y. Ding, and D. J. Wild. Meta path-based collective classification in heterogeneous information networks. In CIKM, pages 1567–1571. ACM, 2012. ']
1416	110	[225]	For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance. // These measures have been shown to have better perforarmance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering ==[30]==. // Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.	h+	Heterogeneous Information Network Embedding for Meta Path based Proximity	[30]	['[30]  Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu. Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks. TKDD, 7(3):11, 2013. ']
1417	110	[109, 41]	These measures have been shown to have better perforarXiv:1701.05291v1 [cs.AI] 19 Jan 2017 mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30]. // Although there are a few works on embedding HINs ==[7, 21]==, none of them is designed for meta path based proximity in general HINs. //  To fill this gap, in this paper, we propose HINE, which learns a transformation of the objects (i.e., vertices) in a HIN to a low-dimensional space, such that the meta path based proximities between objects are preserved.	h-	Heterogeneous Information Network Embedding for Meta Path based Proximity	[7, 21]	['[7]  S. Chang, W. Han, J. Tang, G.-J. Qi, C. C. Aggarwal, and T. S. Huang. Heterogeneous network embedding via deep architectures. In SIGKDD, pages 119–128. ACM, 2015. ', '[21]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang. Tri-party deep network representation. In IJCAI, pages 1895–1901, 2016. ']
1418	110	[224]	More specifically, we define an appropriate objective function that preserves the relative proximity based rankings the vertices in the original and the embedded space. // As shown in ==[29]==, meta paths with too large lengths are not very informative; therefore, we only consider meta paths up to a given length threshold l. // We also investigate the use of negative sampling [19] in order to accelerate the optimization process.	ho	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1419	110	[36]	As shown in [29], meta paths with too large lengths are not very informative; therefore, we only consider meta paths up to a given length threshold l. // We also investigate the use of negative sampling ==[19]== in order to accelerate the optimization process. // We conduct extensive experiments on four real HIN datasets to compare our proposed HINE method with state-of-the-art network embedding methods (i.e., LINE [31] and DeepWalk [22]), which do not consider meta path based proximity. Our experimental results show that our HINE method with PCRW as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used.	ho	Heterogeneous Information Network Embedding for Meta Path based Proximity	[19]	['[19]  T. Mikolov and J. Dean. Distributed representations of words and phrases and their compositionality. pages 3111–3119, 2013. ']
1420	110	[1]	We also investigate the use of negative sampling [19] in order to accelerate the optimization process. // We conduct extensive experiments on four real HIN datasets to compare our proposed HINE method with state-of-the-art network embedding methods (i.e., LINE ==[31]== and DeepWalk [22]), which do not consider meta path based proximity. // Our experimental results show that our HINE method with PCRW as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used.	h-	Heterogeneous Information Network Embedding for Meta Path based Proximity	[31]	['[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1421	110	[6]	We also investigate the use of negative sampling [19] in order to accelerate the optimization process. // We conduct extensive experiments on four real HIN datasets to compare our proposed HINE method with state-of-the-art network embedding methods (i.e., LINE [31] and DeepWalk ==[22]==), which do not consider meta path based proximity. // Our experimental results show that our HINE method with PCRW as the meta path based proximity measure outperforms all alternative approaches in most of the qualitative measures used.	h-	Heterogeneous Information Network Embedding for Meta Path based Proximity	[22]	['[22]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1422	110	[557]	Lately, there has been an increasing interest in both academia and industry in the effective search and analysis of information from HINs. // The problem of classifying objects in a HIN by authority propagation is studied in ==[12]==. // Follow-up work [13] investigates a collective classification problem in HINs using meta path based dependencies.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[12]	['[12]  M. Ji, J. Han, and M. Danilevsky. Ranking-based classification of heterogeneous information networks. In SIGKDD, pages 1298–1306. ACM, 2011. ']
1423	110	[558]	The problem of classifying objects in a HIN by authority propagation is studied in [12]. // Follow-up work ==[13]== investigates a collective classification problem in HINs using meta path based dependencies. // PathSelClus [30] is a link based clustering algorithm for HINs, in which a user can specify her clustering preference by providing some examples as seeds.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[13]	['[13]  X. Kong, P. S. Yu, Y. Ding, and D. J. Wild. Meta path-based collective classification in heterogeneous information networks. In CIKM, pages 1567–1571. ACM, 2012. ']
1424	110	[225]	Follow-up work [13] investigates a collective classification problem in HINs using meta path based dependencies. // PathSelClus ==[30]== is a link based clustering algorithm for HINs, in which a user can specify her clustering preference by providing some examples as seeds. // The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.g., in recommender systems).	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[30]	['[30]  Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu. Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks. TKDD, 7(3):11, 2013. ']
1425	110	[555, 556, 522]	PathSelClus [30] is a link based clustering algorithm for HINs, in which a user can specify her clustering preference by providing some examples as seeds. // The problem of link prediction on HINs has been extensively studied ==[27, 28, 35]==, due to its important applications (e.g., in recommender systems). // A related problem is entity recommendation in HINs [34], which takes advantage of the different types of relationships in HINs to provide better recommendations.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[27, 28, 35]	['[27]  Y. Sun, R. Barber, M. Gupta, C. C. Aggarwal, and J. Han. Co-author relationship prediction in heterogeneous bibliographic networks. In ASONAM, pages 121–128. IEEE, 2011. ', '[28]  Y. Sun, J. Han, C. C. Aggarwal, and N. V. Chawla. When will it happen?: relationship prediction in heterogeneous information networks. In WSDM, pages 663–672. ACM, 2012. ', '[35]  J. Zhang, P. S. Yu, and Z.-H. Zhou. Meta-path based multi-network collective link prediction. In SIGKDD, pages 1286–1295. ACM, 2014.']
1426	110	[151]	The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.g., in recommender systems). // A related problem is entity recommendation in HINs ==[34]==, which takes advantage of the different types of relationships in HINs to provide better recommendations. //  Meta Path and Proximity Measures.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[34]	['[34]  X. Yu, X. Ren, Y. Sun, Q. Gu, B. Sturt, U. Khandelwal, B. Norick, and J. Han. Personalized entity recommendation: A heterogeneous information network approach. In WSDM, pages 283–292. ACM, 2014. ']
1427	110	[224]	Meta Path and Proximity Measures. // Meta path ==[29]== is a general model for the proximity between objects in a HIN. // Several measures have been proposed for the proximity between objects w.r.t. a given meta path P.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1428	110	[224]	Several measures have been proposed for the proximity between objects w.r.t. a given meta path P. // PathCount measures the number of meta path instances connecting the two objects, and PathSim is a normalized version of it ==[29]==. // Path constrained random walk (PCRW) was firstly proposed [14] for the task of relationship retrieval over bibliographic networks.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1429	110	[552]	PathCount measures the number of meta path instances connecting the two objects, and PathSim is a normalized version of it [29]. // Path constrained random walk (PCRW) was firstly proposed ==[14]== for the task of relationship retrieval over bibliographic networks. // Later, [17] proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on PCRW.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[14]	['[14]  N. Lao and W. W. Cohen. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53–67, 2010. ']
1430	110	[553]	Path constrained random walk (PCRW) was firstly proposed [14] for the task of relationship retrieval over bibliographic networks. // Later, ==[17]== proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on PCRW. // Finally, HeteSim [25] is recently proposed as an extension of meta path based SimRank.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[17]	['[17]  C. Meng, R. Cheng, S. Maniu, P. Senellart, and W. Zhang. Discovering meta-paths in large heterogeneous information networks. In WWW, pages 754–764. ACM, 2015. ']
1431	110	[559]	Later, [17] proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on PCRW.  // Finally, HeteSim ==[25]== is recently proposed as an extension of meta path based SimRank. // In this paper, we focus on the two most popular proximity measures, i.e., PathCount and PCRW.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[25]	['[25]  C. Shi, X. Kong, Y. Huang, S. Y. Philip, and B. Wu. Hetesim: A general framework for relevance measure in heterogeneous networks. TKDE, 26(10):2479–2492, 2014. ']
1432	110	[5, 355, 4, 3]	Network Embedding. // Traditional dimensionality reduction techniques ==[3,8,24,32]== typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph. // Graph factorization [1] finds a low-dimensional representation of a graph through matrix factorization, after representing the graph as an adjacency matrix.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[3, 8, 24, 32]	['[3]  M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585–591, 2001. ', '[8]  T. F. Cox and M. A. Cox. Multidimensional scaling. CRC press, 2000. ', '[24]  S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, 2000. ', '[32]  J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000. ']
1433	110	[154]	Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph. // Graph factorization ==[1]== finds a low-dimensional representation of a graph through matrix factorization, after representing the graph as an adjacency matrix. // However, since these general techniques are not designed for networks, they do not necessarily preserve the global network structure, as pointed out in [31].	h-	Heterogeneous Information Network Embedding for Meta Path based Proximity	[1]	['[1]  A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWW, pages 37–48. ACM, 2013. ']
1434	110	[1]	Graph factorization [1] finds a low-dimensional representation of a graph through matrix factorization, after representing the graph as an adjacency matrix. // However, since these general techniques are not designed for networks, they do not necessarily preserve the global network structure, as pointed out in ==[31]==. // Recently, DeepWalk [22] is proposed as a method for learning the latent representations of the nodes of a social network, from truncated random walks in the network.	ho	Heterogeneous Information Network Embedding for Meta Path based Proximity	[31]	['[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1435	110	[6]	However, since these general techniques are not designed for networks, they do not necessarily preserve the global network structure, as pointed out in [31]. // Recently, DeepWalk ==[22]== is proposed as a method for learning the latent representations of the nodes of a social network, from truncated random walks in the network. // DeepWalk combines random walk proximity with the SkipGram model [18], a language model that maximizes the co-occurrence probability among the words that appear within a window in a sentence.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[22]	['[22]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1436	110	[35]	Recently, DeepWalk [22] is proposed as a method for learning the latent representations of the nodes of a social network, from truncated random walks in the network. // DeepWalk combines random walk proximity with the SkipGram model ==[18]==, a language model that maximizes the co-occurrence probability among the words that appear within a window in a sentence. //  However, DeepWalk has certain weaknesses when applied to our problem settings.  First, the random walk proximity it adopts does not consider the heterogeneity of a HIN.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[18]	['[18]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
1437	110	[1]	In this paper, we use PCRW, which extends random walk based proximity to be applied for HINs. // Second, as pointed out in ==[31]==, DeepWalk can only preserve secondorder proximity, leading to poor performance in some tasks, such as link recover and classification, which require first-order proximity to be well-preserved. // LINE [31] is a recently proposed embedding approach for largescale networks.	ho	Heterogeneous Information Network Embedding for Meta Path based Proximity	[31]	['[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1438	110	[1]	Second, as pointed out in [31], DeepWalk can only preserve secondorder proximity, leading to poor performance in some tasks, such as link recover and classification, which require first-order proximity to be well-preserved. // LINE ==[31]== is a recently proposed embedding approach for largescale networks. // Although it uses an explicit objective function to preserve the network structure, its performance suffers from the way it learns the vector representations.	h-	Heterogeneous Information Network Embedding for Meta Path based Proximity	[31]	['[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1439	110	[19]	GraRep [6] further extends DeepWalk to utilize high-order proximities. GraRep does not scale well in large networks due to the expensive computation of the power of a matrix and the involvement of SVD in the learning process. // SDNE ==[33]== is a semi-supervised deep model that captures the non-linear structural information over the network. // The source code of SDNE is not available, so this approach cannot be reproduced and compared to ours.	h-	Heterogeneous Information Network Embedding for Meta Path based Proximity	[33]	['[33]  D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In SIGKDD, pages 1225–1234. ACM, 2016. ']
1440	110	[81]	The source code of SDNE is not available, so this approach cannot be reproduced and compared to ours. // Similarly, ==[5]== embeds entities in knowledge bases using an innovative neural network architecture and TriDNR [21] extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information. //  Our current work focuses on meta path based proximities, so it does not consider any other information in the HIN besides the network structure and the types of nodes and edges.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[5]	['[5]  A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases. In AAAI, 2011. ']
1441	110	[41]	The source code of SDNE is not available, so this approach cannot be reproduced and compared to ours. // Similarly, [5] embeds entities in knowledge bases using an innovative neural network architecture and TriDNR ==[21]== extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information. //  Our current work focuses on meta path based proximities, so it does not consider any other information in the HIN besides the network structure and the types of nodes and edges.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[21]	['[21]  S. Pan, J. Wu, X. Zhu, C. Zhang, and Y. Wang. Tri-party deep network representation. In IJCAI, pages 1895–1901, 2016. ']
1442	110	[224]	Generally speaking, the number of possible meta paths grows exponentially with their length and can be infinite for certain HIN schema (in this case, the computation of s(oi, oj ) is infeasible). // As pointed out in ==[29]==, shorter meta paths are more informative than longer ones, because longer meta paths link more remote objects (which are less related semantically). // Therefore, we use a truncated estimation of proximity, which only considers meta paths up to a length threshold l.	ho	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1443	110	[6]	Therefore, we use a truncated estimation of proximity, which only considers meta paths up to a length threshold l. // This is also consistent with previous works on network embedding, which aim at reserving loworder proximity (e.g., ==[22]== reserves only second-order proximity, and [31] first-order and second-order proximities). // Therefore, we define as the truncated proximity between two objects oi and oj .	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[22]	['[22]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1444	110	[1]	Therefore, we use a truncated estimation of proximity, which only considers meta paths up to a length threshold l. // This is also consistent with previous works on network embedding, which aim at reserving loworder proximity (e.g., [22] reserves only second-order proximity, and ==[31]== first-order and second-order proximities). // Therefore, we define as the truncated proximity between two objects oi and oj .	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[31]	['[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1445	110	[6]	We compare the following network embedding approaches. // DeepWalk ==[22]== is a recently proposed social network embedding method (see Section 2.3 for details). //  In our experiment settings, we ignore the heterogeneity and directly feed the HINs for embedding.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[22]	['[22]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In SIGKDD, pages 701–710. ACM, 2014. ']
1446	110	[1]	In our experiment settings, we ignore the heterogeneity and directly feed the HINs for embedding. // LINE ==[31]== is a method that preserves first-order and secondorder proximities between vertices (see Section 2.3 for details). //  For each object, it computes two vectors; one for the first-order and one for the second-order proximities separately and then concatenates them.	b	Heterogeneous Information Network Embedding for Meta Path based Proximity	[31]	['[31]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In WWW, pages 1067–1077. ACM, 2015. ']
1447	110	[400]	GAME. // We extracted from Freebase ==[4]== a HIN, which is related to video games. // The HIN consists of 4,095 games (G), 1,578 publishers (P), 2,043 developers (D) and 197 designers (S).	hoe	Heterogeneous Information Network Embedding for Meta Path based Proximity	[4]	['[4]  K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, pages 1247–1250, 2008. ']
1448	110	[224]	DBLP. The schema of DBLP network is shown in Figure 2(a). // We use a subset of DBLP, i.e., DBLP-4-Area taken of ==[29]==, which contains 5,237 papers (P), 5,915 authors (A), 18 venues (V), 4,479 topics (T). // The authors are from 4 areas: database, data mining, machine learning and information retrieval.	ho	Heterogeneous Information Network Embedding for Meta Path based Proximity	[29]	['[29]  Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB, 4(11):992–1003, 2011. ']
1449	371	[560]	Can cascades be predicted? // While many believe that cascades are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted through a mixture of signals ==[10]==. // Indeed, cascades of microblogs/Tweets [42, 38, 43, 20, 11, 17], photos [10], videos [2] and academic papers [31] are proved to be predictable to some extent.	b	DeepCas: an End-to-end Predictor of Information Cascades	[10]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ']
1450	371	[561, 562, 563, 564, 565, 566]	While many believe that cascades are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted through a mixture of signals [10]. // Indeed, cascades of microblogs/Tweets ==[42, 38, 43, 20, 11, 17]==, photos [10], videos [2] and academic papers [31] are proved to be predictable to some extent. // In most of these studies, cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features [38, 10, 11, 20].	b	DeepCas: an End-to-end Predictor of Information Cascades	[42, 38, 43, 20, 11, 17]	['[42]  L. Yu, P. Cui, F. Wang, C. Song, and S. Yang. From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics. In Proc. of ICDM, 2015. ', '[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[43]  Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec. Seismic: A self-exciting point process model for predicting tweet popularity. In Proc. of SIGKDD, 2015.', '[20]  M. Jenders, G. Kasneci, and F. Naumann. Analyzing and predicting viral tweets. In Proc. of WWW, 2013. ', '[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ', '[17]  A. Guille and H. Hacid. A predictive model for the temporal dynamics of information diffusion in online social networks. In Proc. of WWW, 2012. ']
1451	371	[560]	While many believe that cascades are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted through a mixture of signals [10]. // Indeed, cascades of microblogs/Tweets [42, 38, 43, 20, 11, 17], photos ==[10]==, videos [2] and academic papers [31] are proved to be predictable to some extent. // In most of these studies, cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features [38, 10, 11, 20].	b	DeepCas: an End-to-end Predictor of Information Cascades	[10]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ']
1452	371	[567]	While many believe that cascades are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted through a mixture of signals [10]. // Indeed, cascades of microblogs/Tweets [42, 38, 43, 20, 11, 17], photos [10], videos ==[2]== and academic papers [31] are proved to be predictable to some extent. // In most of these studies, cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features [38, 10, 11, 20].	b	DeepCas: an End-to-end Predictor of Information Cascades	[2]	['[2]  C. Bauckhage, F. Hadiji, and K. Kersting. How viral are viral videos. In Proc. of ICWSM, 2015. ']
1453	371	[568]	While many believe that cascades are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted through a mixture of signals [10]. // Indeed, cascades of microblogs/Tweets [42, 38, 43, 20, 11, 17], photos [10], videos [2] and academic papers ==[31]== are proved to be predictable to some extent. // In most of these studies, cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features [38, 10, 11, 20].	b	DeepCas: an End-to-end Predictor of Information Cascades	[31]	['[31]  H.-W. Shen, D. Wang, C. Song, and A.-L. Barabási. Modeling and predicting popularity dynamics via reinforced poisson processes. arXiv preprint arXiv:1401.0778, 2014. ']
1454	371	[562, 560, 565, 564]	Indeed, cascades of microblogs/Tweets [42, 38, 43, 20, 11, 17], photos [10], videos [2] and academic papers [31] are proved to be predictable to some extent. // In most of these studies, cascade prediction is cast as classification or regression problems and be solved with machine learning techniques that incorporate many features ==[38, 10, 11, 20]==. // On one hand, many of these features are specific to the particular platform or the particular type of information being diffused.	b	DeepCas: an End-to-end Predictor of Information Cascades	[38, 10, 11, 20]	['[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ', '[20]  M. Jenders, G. Kasneci, and F. Naumann. Analyzing and predicting viral tweets. In Proc. of WWW, 2013. ']
1455	371	[560]	On one hand, many of these features are specific to the particular platform or the particular type of information being diffused. // For example, whether a photo was posted with a caption is shown to be predictive of how widely it spread on Facebook ==[10]==; specific wording on Tweets is shown to help them gain more retweets [33]. // These features are indicative but cannot be generalized to other platforms or to other types of cascades.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[10]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ']
1456	371	[569]	On one hand, many of these features are specific to the particular platform or the particular type of information being diffused. // For example, whether a photo was posted with a caption is shown to be predictive of how widely it spread on Facebook [10]; specific wording on Tweets is shown to help them gain more retweets ==[33]==. // These features are indicative but cannot be generalized to other platforms or to other types of cascades.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[33]	['[33]  C. Tan, L. Lee, and B. Pang. The effect of wording on message propagation: Topic-and author-controlled natural experiments on twitter. arXiv preprint arXiv:1405.1438, 2014. ']
1457	371	[560, 561, 562]	These features are indicative but cannot be generalized to other platforms or to other types of cascades. // On the other hand, a common set of features, those extracted from the network structure of the cascade, are reported to be predictive by multiple studies ==[10, 42, 38]==. // Many of these features are carefully designed based on the prior knowledge from network theory and empirical analyses, such as centrality of nodes, community structures, tie strength, and structural holes.	b	DeepCas: an End-to-end Predictor of Information Cascades	[10, 42, 38]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[42]  L. Yu, P. Cui, F. Wang, C. Song, and S. Yang. From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics. In Proc. of ICDM, 2015. ', '[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ']
1458	371	[560]	There are also ad hoc features that appear very predictive, but their success is intriguing and sometimes magical. // For example, Cheng et al. ==[10]== found that one of the most indicative feature to the growth of a cascade is whether any of the first a few reshares are not directly connected to the root of the diffusion. // We consider this as a major deficiency of these machine learning approaches: their performance heavily depends on the feature representations, yet there is no common principle of how to design and measure the features.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[10]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ']
1459	371	[570]	RELATED WORK. // In a networked environment, people tend to be influenced by their neighbors’ behavior and decisions ==[13]==. // Opinions, product advertisements, or political propaganda could spread over the network through a chain reaction of such influence, a process known as the information cascade [37, 5, 1].	ho	DeepCas: an End-to-end Predictor of Information Cascades	[13]	['[13]  D. Easley and J. Kleinberg. Networks, crowds, and markets. Cambridge University Press, 2010. ']
1460	371	[571, 572, 573]	In a networked environment, people tend to be influenced by their neighbors’ behavior and decisions [13]. // Opinions, product advertisements, or political propaganda could spread over the network through a chain reaction of such influence, a process known as the information cascade ==[37, 5, 1]==. // We present the first deep learning method to predict the future size of information cascades	b	DeepCas: an End-to-end Predictor of Information Cascades	[37, 5, 1]	['[37]  I. Welch. Sequential sales, learning, and cascades. The Journal of finance, 1992. ', '[5]  S. Bikhchandani, D. Hirshleifer, and I. Welch. A theory of fads, fashion, custom, and cultural change as informational cascades. Journal of political Economy, 1992. ', '[1]  A. V. Banerjee. A simple model of herd behavior. The Quarterly Journal of Economics, 1992. ']
1461	371	[561, 562, 564, 565, 566, 563]	Cascade Prediction. // Cascades of particular types of information are empirically proved to be predictable to some extent, including Tweets/microblogs ==[42, 38, 20, 11, 17, 43]==, photos [10], videos [2] and academic papers [31]. // One treats cascade prediction as a classification problem [38, 20, 10, 11], which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold).	b	DeepCas: an End-to-end Predictor of Information Cascades	[42, 38, 20, 11, 17, 43]	['[42]  L. Yu, P. Cui, F. Wang, C. Song, and S. Yang. From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics. In Proc. of ICDM, 2015. ', '[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[20]  M. Jenders, G. Kasneci, and F. Naumann. Analyzing and predicting viral tweets. In Proc. of WWW, 2013. ', '[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ', '[17]  A. Guille and H. Hacid. A predictive model for the temporal dynamics of information diffusion in online social networks. In Proc. of WWW, 2012. ', '[43]  Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec. Seismic: A self-exciting point process model for predicting tweet popularity. In Proc. of SIGKDD, 2015.']
1462	371	[560]	Cascade Prediction. // Cascades of particular types of information are empirically proved to be predictable to some extent, including Tweets/microblogs [42, 38, 20, 11, 17, 43], photos ==[10]==, videos [2] and academic papers [31]. // One treats cascade prediction as a classification problem [38, 20, 10, 11], which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold).	b	DeepCas: an End-to-end Predictor of Information Cascades	[10]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ']
1463	371	[567]	Cascade Prediction. // Cascades of particular types of information are empirically proved to be predictable to some extent, including Tweets/microblogs [42, 38, 20, 11, 17, 43], photos [10], videos ==[2]== and academic papers [31]. // One treats cascade prediction as a classification problem [38, 20, 10, 11], which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold).	b	DeepCas: an End-to-end Predictor of Information Cascades	[2]	['[2]  C. Bauckhage, F. Hadiji, and K. Kersting. How viral are viral videos. In Proc. of ICWSM, 2015. ']
1464	371	[568]	Cascade Prediction. // Cascades of particular types of information are empirically proved to be predictable to some extent, including Tweets/microblogs [42, 38, 20, 11, 17, 43], photos [10], videos [2] and academic papers ==[31]==. // One treats cascade prediction as a classification problem [38, 20, 10, 11], which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold).	b	DeepCas: an End-to-end Predictor of Information Cascades	[31]	['[31]  H.-W. Shen, D. Wang, C. Song, and A.-L. Barabási. Modeling and predicting popularity dynamics via reinforced poisson processes. arXiv preprint arXiv:1401.0778, 2014. ']
1465	371	[562, 564, 560, 565]	Cascades of particular types of information are empirically proved to be predictable to some extent, including Tweets/microblogs [42, 38, 20, 11, 17, 43], photos [10], videos [2] and academic papers [31]. // One treats cascade prediction as a classification problem ==[38, 20, 10, 11]==, which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold). // The other formulates cascade prediction as a regression problem, which predicts the numerical properties (e.g., size) of a cascade in the future [38, 35].	b	DeepCas: an End-to-end Predictor of Information Cascades	[38, 20, 10, 11]	['[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[20]  M. Jenders, G. Kasneci, and F. Naumann. Analyzing and predicting viral tweets. In Proc. of WWW, 2013. ', '[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ']
1466	371	[562, 300]	One treats cascade prediction as a classification problem [38, 20, 10, 11], which predicts whether or not a piece of information will become popular and wide-spread (above a certain threshold). // The other formulates cascade prediction as a regression problem, which predicts the numerical properties (e.g., size) of a cascade in the future ==[38, 35]==. // This line of work can be further categorized by whether it outputs the final size of a cascade [43] or the size as a function of time (i.e., the growth of the cascade) [42].	b	DeepCas: an End-to-end Predictor of Information Cascades	[38, 35]	['[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[35]  O. Tsur and A. Rappoport. What’s in a hashtag?: content based prediction of the spread of ideas in microblogging communities. In Proc. of WSDM, 2012. ']
1467	371	[563]	The other formulates cascade prediction as a regression problem, which predicts the numerical properties (e.g., size) of a cascade in the future [38, 35]. // This line of work can be further categorized by whether it outputs the final size of a cascade ==[43]== or the size as a function of time (i.e., the growth of the cascade) [42]. // Either way, most of the methods identified temporal properties, topological structure of the cascade at the early stage, root and early adopters of the information, and the content being spread as the most predictive factors.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[43]	['[43]  Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec. Seismic: A self-exciting point process model for predicting tweet popularity. In Proc. of SIGKDD, 2015.']
1468	371	[561]	The other formulates cascade prediction as a regression problem, which predicts the numerical properties (e.g., size) of a cascade in the future [38, 35]. // This line of work can be further categorized by whether it outputs the final size of a cascade [43] or the size as a function of time (i.e., the growth of the cascade) ==[42]==. // Either way, most of the methods identified temporal properties, topological structure of the cascade at the early stage, root and early adopters of the information, and the content being spread as the most predictive factors.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[42]	['[42]  L. Yu, P. Cui, F. Wang, C. Song, and S. Yang. From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics. In Proc. of ICDM, 2015. ']
1469	371	[574, 575]	These factors are utilized for cascade prediction in two fashions. // The first mainly designs generative models of the cascade process based on temporal or structural features, which can be as simple as certain macroscopic distributions (e.g., of cascade size over time) ==[23, 3]==, or stochastic processes that explain the microscopic actions of passing along the information [42]. // These generative models make various strong assumptions and oversimplify the reality.	b	DeepCas: an End-to-end Predictor of Information Cascades	[23, 3]	['[23]  K. Lerman and R. Ghosh. Information contagion: An empirical study of the spread of news on digg and twitter social networks. ICWSM, 10:90–97, 2010. ', '[3]  C. Bauckhage, K. Kersting, and F. Hadiji. Mathematical models of fads explain the temporal dynamics of internet memes. In Proc. of ICWSM, 2013. ']
1470	371	[561]	These factors are utilized for cascade prediction in two fashions. // The first mainly designs generative models of the cascade process based on temporal or structural features, which can be as simple as certain macroscopic distributions (e.g., of cascade size over time) [23, 3], or stochastic processes that explain the microscopic actions of passing along the information ==[42]==. // These generative models make various strong assumptions and oversimplify the reality.	b	DeepCas: an End-to-end Predictor of Information Cascades	[42]	['[42]  L. Yu, P. Cui, F. Wang, C. Song, and S. Yang. From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics. In Proc. of ICDM, 2015. ']
1471	371	[562, 560, 564, 565]	As a result, they generally underperform in real prediction tasks. // Alternatively, these factors may be represented through handcrafted features, which are extracted from the data, combined, and weighted by discriminative machine learning algorithms to perform the classification or the regression tasks ==[38, 10, 20, 11]==. // Most work in this fashion uses standard supervised learning models (e.g. logistic regression, SVM, or random forests), the performance of which heavily rely on the quality of the features.	b	DeepCas: an End-to-end Predictor of Information Cascades	[38, 10, 20, 11]	['[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[20]  M. Jenders, G. Kasneci, and F. Naumann. Analyzing and predicting viral tweets. In Proc. of WWW, 2013. ', '[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ']
1472	371	[560, 562, 562]	Some of the most predictive features are tied to particular platforms or particular cascades and are hard to be generalized, such as the ones mentioned in the Section 1. // Some features are closely related to the structural properties of the social network, such as degree==[10, 38]==, density[10, 17], and community structures [38]. // These features could generalize over domains and platforms, but many may still involve arbitrary and hard decisions in computation, such as what to choose from hundreds of community detection algorithms available [14] and how to detect structural holes [41].	h-	DeepCas: an End-to-end Predictor of Information Cascades	[10, 38]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ']
1473	371	[560, 566]	Some of the most predictive features are tied to particular platforms or particular cascades and are hard to be generalized, such as the ones mentioned in the Section 1. // Some features are closely related to the structural properties of the social network, such as degree[10, 38], density==[10, 17]==, and community structures [38]. // These features could generalize over domains and platforms, but many may still involve arbitrary and hard decisions in computation, such as what to choose from hundreds of community detection algorithms available [14] and how to detect structural holes [41].	h-	DeepCas: an End-to-end Predictor of Information Cascades	[10, 17]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[17]  A. Guille and H. Hacid. A predictive model for the temporal dynamics of information diffusion in online social networks. In Proc. of WWW, 2012. ']
1474	371	[562, 562]	Some of the most predictive features are tied to particular platforms or particular cascades and are hard to be generalized, such as the ones mentioned in the Section 1. // Some features are closely related to the structural properties of the social network, such as degree[10, 38], density[10, 17], and community structures ==[38]==. // These features could generalize over domains and platforms, but many may still involve arbitrary and hard decisions in computation, such as what to choose from hundreds of community detection algorithms available [14] and how to detect structural holes [41].	h-	DeepCas: an End-to-end Predictor of Information Cascades	[38]	['[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ', '[38]  L. Weng, F. Menczer, and Y.-Y. Ahn. Predicting successful memes using network and community structure. arXiv preprint arXiv:1403.6199, 2014. ']
1475	371	[71]	Some features are closely related to the structural properties of the social network, such as degree[10, 38], density[10, 17], and community structures [38]. // These features could generalize over domains and platforms, but many may still involve arbitrary and hard decisions in computation, such as what to choose from hundreds of community detection algorithms available ==[14]== and how to detect structural holes [41]. //  Besides, there are also heuristic features that perform very well in particular scenarios but it is hard to explain why they are designed as is.	b	DeepCas: an End-to-end Predictor of Information Cascades	[14]	['[14]  S. Fortunato. Community detection in graphs. Physics reports, 2010. ']
1476	371	[576]	Some features are closely related to the structural properties of the social network, such as degree[10, 38], density[10, 17], and community structures [38]. // These features could generalize over domains and platforms, but many may still involve arbitrary and hard decisions in computation, such as what to choose from hundreds of community detection algorithms available [14] and how to detect structural holes ==[41]==. //  Besides, there are also heuristic features that perform very well in particular scenarios but it is hard to explain why they are designed as is.	b	DeepCas: an End-to-end Predictor of Information Cascades	[41]	['[41]  Y. Yang, J. Tang, C. W.-k. Leung, Y. Sun, Q. Chen, J. Li, and Q. Yang. Rain: Social role-aware information diffusion. In Proc. of AAAI, 2015. ']
1477	371	[6, 1, 37]	Our work is also related to the literature of representation learning for graphs. Networks are traditionally represented as affiliation matrices or discrete sets of nodes and edges. // Modern representation learning methods attempt to represent nodes as high-dimensional vectors in a continuous space (a.k.a., node embeddings) so that nodes with similar embedding vectors share similar structural properties (e.g., ==[29, 34, 16]==). // Rather than learning the representation of each node, recent work also attempts to learn the representation of subgraph structures [28, 26, 39, 40].	b	DeepCas: an End-to-end Predictor of Information Cascades	[29, 34, 16]	['[29]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proc. of SIGKDD, 2014. ', '[34]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proc. of WWW, 2015. ', '[16]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proc. of SIGKDD, 2016. ']
1478	371	[381, 219, 446, 577]	Modern representation learning methods attempt to represent nodes as high-dimensional vectors in a continuous space (a.k.a., node embeddings) so that nodes with similar embedding vectors share similar structural properties (e.g., [29, 34, 16]). // Rather than learning the representation of each node, recent work also attempts to learn the representation of subgraph structures ==[28, 26, 39, 40]==. // Much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text [4] and image [21].	b	DeepCas: an End-to-end Predictor of Information Cascades	[28, 26, 39, 40]	['[28]  M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. 2016. ', '[26]  A. Narayanan, M. Chandramohan, L. Chen, Y. Liu, and S. Saminathan. subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs. arXiv preprint arXiv:1606.08928, 2016. ', '[39]  P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proc. of SIGKDD, 2015. ', '[40]  P. Yanardag and S. Vishwanathan. A structural smoothing framework for robust graph comparison. In Proc. of NIPS, 2015. ']
1479	371	[147]	Rather than learning the representation of each node, recent work also attempts to learn the representation of subgraph structures [28, 26, 39, 40]. // Much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text ==[4]== and image [21]. // For example, DeepWalk [29] makes an analogy between the nodes in networks and the words in natural language and uses fixed-length random walk paths to stimulate the “context” of a node so that node representations can be learned using the same method of learning word representations [25].	b	DeepCas: an End-to-end Predictor of Information Cascades	[4]	['[4]  Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. JMLR, 2003. ']
1480	371	[149]	Rather than learning the representation of each node, recent work also attempts to learn the representation of subgraph structures [28, 26, 39, 40]. // Much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text [4] and image ==[21]==. // For example, DeepWalk [29] makes an analogy between the nodes in networks and the words in natural language and uses fixed-length random walk paths to stimulate the “context” of a node so that node representations can be learned using the same method of learning word representations [25].	b	DeepCas: an End-to-end Predictor of Information Cascades	[21]	['[21]  A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Proc. of NIPS, 2012. ']
1481	371	[6]	Much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text [4] and image [21]. // For example, DeepWalk ==[29]== makes an analogy between the nodes in networks and the words in natural language and uses fixed-length random walk paths to stimulate the “context” of a node so that node representations can be learned using the same method of learning word representations [25]. // The representation of a graph can then be calculated by averaging the embeddings of all nodes.	b	DeepCas: an End-to-end Predictor of Information Cascades	[29]	['[29]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proc. of SIGKDD, 2014. ']
1482	371	[35]	Much of this work is inspired by the huge success of representation learning and deep learning applied to various domains such as text [4] and image [21]. // For example, DeepWalk [29] makes an analogy between the nodes in networks and the words in natural language and uses fixed-length random walk paths to stimulate the “context” of a node so that node representations can be learned using the same method of learning word representations ==[25]==. // The representation of a graph can then be calculated by averaging the embeddings of all nodes.	b	DeepCas: an End-to-end Predictor of Information Cascades	[25]	['[25]  T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. ']
1483	371	[578, 579, 220]	The representation of a graph can thenbe calculated by averaging the embeddings of all nodes. // Another line of related work comes from the domain of graph kernels, which computes pairwise similarities between graphs ==[7, 15, 32]==. // For example, the Weisfeiler-Lehman subtree kernel (WL) [32] computes the graph similarity based on the sub-trees in each graph.	b	DeepCas: an End-to-end Predictor of Information Cascades	[7, 15, 32]	['[7]  K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In Proc. of ICDM, 2005. ', '[15]  T. Gärtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efficient alternatives. In Learning Theory and Kernel Machines. 2003. ', '[32]  N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 2011. ']
1484	371	[220]	Another line of related work comes from the domain of graph kernels, which computes pairwise similarities between graphs [7, 15, 32]. // For example, the Weisfeiler-Lehman subtree kernel (WL) ==[32]== computes the graph similarity based on the sub-trees in each graph. //  Some studies have applied deep learning techniques to improve graph kernels [39, 27].	b	DeepCas: an End-to-end Predictor of Information Cascades	[32]	['[32]  N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 2011. ']
1485	371	[560, 580, 565, 581]	Features. // We include all structural features that could be generalized across data sets from recent studies of cascade prediction ==[10, 18, 11, 36]==. //  These features include: Centrality and Density. Degree of nodes in the cascade graph g and the global network G, average and 90th percentile of the local and global degrees of nodes in g, number of leaf nodes in g, edge density of g, and the number of nodes and edges in the frontier graph of the cascade, which is composed of nodes that are not in g but are neighbors of nodes in g.	ho	DeepCas: an End-to-end Predictor of Information Cascades	[10, 18, 11, 36]	['[10]  J. Cheng, L. Adamic, P. A. Dow, J. M. Kleinberg, and J. Leskovec. Can cascades be predicted? In Proc. of WWW, 2014. ', '[18]  R. Guo, E. Shaabani, A. Bhatnagar, and P. Shakarian. Toward order-of-magnitude cascade prediction. In Proc. of ASONAM, 2015. ', '[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ', '[36]  J. Ugander, L. Backstrom, C. Marlow, and J. Kleinberg. Structural diversity in social contagion. PNAS, 2012. ']
1486	371	[565]	We feed the feature vectors to MLP (denoted as Features-deep). // OSLOR selects important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors ==[11]==. // Node2vec [16] is selected as a representative of node embedding methods.	b	DeepCas: an End-to-end Predictor of Information Cascades	[11]	['[11]  P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. Cascading outbreak prediction in networks: a data-driven approach. In Proc. of SIGKDD, 2013. ']
1487	371	[37]	OSLOR selects important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors [11]. // Node2vec ==[16]== is selected as a representative of node embedding methods. // Node2vec is a generalization of DeepWalk [29], which is reported to be outperforming alternative methods such as DeepWalk and LINE [34].	h+	DeepCas: an End-to-end Predictor of Information Cascades	[16]	['[16]  A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proc. of SIGKDD, 2016. ']
1488	371	[6]	Node2vec [16] is selected as a representative of node embedding methods. // Node2vec is a generalization of DeepWalk ==[29]==, which is reported to be outperforming alternative methods such as DeepWalk and LINE [34]. //  We generate walks from two sources: (1) the set of cascade graphs {g} (2) the global network G.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[29]	['[29]  B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In Proc. of SIGKDD, 2014. ']
1489	371	[1]	Node2vec [16] is selected as a representative of node embedding methods. // Node2vec is a generalization of DeepWalk [29], which is reported to be outperforming alternative methods such as DeepWalk and LINE ==[34]==. //  We generate walks from two sources: (1) the set of cascade graphs {g} (2) the global network G.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[34]	['[34]  J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network embedding. In Proc. of WWW, 2015. ']
1490	371	[582]	The average of embeddings of all nodes in a cascade graph is fed through MLP to make the prediction. // Embedded-IC ==[8]== represents nodes by two types of embeddings: as a sender or as a receiver. // For prediction, the original paper used Monte-Carlo simulations to estimate infections probabilities of each individual user.	b	DeepCas: an End-to-end Predictor of Information Cascades	[8]	['[8]  S. Bourigault, S. Lamprier, and P. Gallinari. Representation learning for information diffusion through social networks: an embedded cascade model. In Proc. of WSDM, 2016. ']
1491	371	[381]	We therefore report the performance of the latter. // PSCN applies convolutional neural networks (CNN) to locally connected regions from graphs ==[28]==. // We apply PSCN to both the diffusion graphs and the frontier graphs. The last hidden layer of the cascade graph and that of the frontier graph are concatenated to make the final prediction.	b	DeepCas: an End-to-end Predictor of Information Cascades	[28]	['[28]  M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. 2016. ']
1492	371	[381]	Graph kernels. // There are a set of state-of-the-art graph kernels ==[28]==: the shortest-path kernel (SP) [7], the random walk kernel (RW) [15], and the Weisfeiler-Lehman subtree kernel (WL) [32]. // The RW kernel and the SP kernel are too computationally inefficient, which did not complete after 10 days for a single data set in our experiment.	b	DeepCas: an End-to-end Predictor of Information Cascades	[28]	['[28]  M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. 2016. ']
1493	371	[578]	Graph kernels. // There are a set of state-of-the-art graph kernels [28]: the shortest-path kernel (SP) ==[7]==, the random walk kernel (RW) [15], and the Weisfeiler-Lehman subtree kernel (WL) [32]. // The RW kernel and the SP kernel are too computationally inefficient, which did not complete after 10 days for a single data set in our experiment.	b	DeepCas: an End-to-end Predictor of Information Cascades	[7]	['[7]  K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In Proc. of ICDM, 2005. ']
1494	371	[579]	Graph kernels. // There are a set of state-of-the-art graph kernels [28]: the shortest-path kernel (SP) [7], the random walk kernel (RW) ==[15]==, and the Weisfeiler-Lehman subtree kernel (WL) [32]. // The RW kernel and the SP kernel are too computationally inefficient, which did not complete after 10 days for a single data set in our experiment.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[15]	['[15]  T. Gärtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efficient alternatives. In Learning Theory and Kernel Machines. 2003. ']
1495	371	[220]	Graph kernels. // There are a set of state-of-the-art graph kernels [28]: the shortest-path kernel (SP) [7], the random walk kernel (RW) [15], and the Weisfeiler-Lehman subtree kernel (WL) ==[32]==. // The RW kernel and the SP kernel are too computationally inefficient, which did not complete after 10 days for a single data set in our experiment.	h-	DeepCas: an End-to-end Predictor of Information Cascades	[32]	['[32]  N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 2011. ']
1496	371	[381, 577]	The RW kernel and the SP kernel are too computationally inefficient, which did not complete after 10 days for a single data set in our experiment. // We therefore exclude them from the comparison, a decision also made by in ==[28, 40]==. // For the WL kernel, we experiment with two settings: WL-degree, where node degree is used as the node attribute to build subgraphs for each cascade and frontier graph; WL-id, where node id is used as the attribute. The second setting is to test whether node identity information could be incorporated into graph kernel methods.	ho	DeepCas: an End-to-end Predictor of Information Cascades	[28, 40]	['[28]  M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. 2016. ', '[40] P. Yanardag and S. Vishwanathan. A structural smoothing framework for robust graph comparison. In Proc. of NIPS, 2015.']
1497	371	[583]	These features characterize either global or local network properties, and are listed in Figure 4. // In each subfigure, we layout the cascade graphs as data points in the test set to a 2-D space by feeding their vector representations output by the last hidden layer of DeepCas to t-SNE ==[6]==, a commonly used visualization algorithm. // Cascade graphs with similar vector representations are placed closely.	ho	DeepCas: an End-to-end Predictor of Information Cascades	[6]	['[6] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. JSTAT, 2008.']
1498	680	[774]	which are estimated by maximum likelihood estimation. // Perplexity (PPL) ==[13]==, an information theoretic metric that measures the quality of a probabilistic model, is a way to evaluate LMs. // Lower PPL indicates a better model.	b	A Survey on Neural Network Language Models	[13]	['[13] [Jelinek et al., 1977] F. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. Perplexity—a measure of the difficulty of speech recognition tasks. JASA, 62(S1):S63–S63, December 1977.']
1499	680	[147]	Therefore, NNs are expected to be applied to LMs, even other NLP tasks, to cover the discreteness, combination, and sparsity of natural language. // The first FFNN Language Model (FFNNLM) presented by ==[6]== fights the curse of dimensionality by learning a distributed representation for words, which represents a word as a low dimensional vector, called embedding. // FFNNLM performs better than n-gram LM.	b	A Survey on Neural Network Language Models	[6]	['[6] [Bengio et al., 2003] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. JMLR, 3:1137–1155, 2003.']
1500	680	[390]	FFNNLM performs better than n-gram LM. // Then, RNN Language Model (RNNLM) ==[18]== also was proposed. // Since then, the NNLM has gradually become the mainstream LM and has rapidly developed.	b	A Survey on Neural Network Language Models	[18]	['[18] [Mikolov et al., 2010] T. Mikolov, M. Karafiat, L. Burget, ´J. Cernocky, and S. Khudanpur. Recurrent neural network ´ based language model. In INTERSPEECH, pages 1045–1048, 2010.']
1501	680	[722]	Since then, the NNLM has gradually become the mainstream LM and has rapidly developed. // Long Short-term Memory RNN Language Model (LSTM-RNNLM) ==[31]== was proposed for the difficulty of learning long-term dependence. // Various improvements were proposed for reducing the cost of training and evaluation and PPL such wt−n+1 wt−2 wt−1 … H U W tanh softmax P(wt|context) C(wt−n+1) C(wt−2) C(wt−1).	b	A Survey on Neural Network Language Models	[31]	['[31] [Sundermeyer et al., 2012] M. Sundermeyer, R. Schluter, ¨ and H. Ney. LSTM neural networks for language modeling. In INTERSPEECH, pages 194–197, 2012.']
1502	680	[598]	Finally, conclusions are given, and new research directions of NNLMs are discussed. // Classic Neural Network Language Models, FFNN Language Models ==[37]== tried to introduce NNs into LMs. // Although their model performs better than the baseline ngram LM, their model with poor generalization ability cannot capture context-dependent features due to no hidden layer.	b	A Survey on Neural Network Language Models	[37]	['[37] [Xu and Rudnicky, 2000] W. Xu and A. Rudnicky. Can artificial neural networks learn language models? In ICSLP. INTERSPEECH, pages 202–205, 2000.']
1503	680	[147]	Inspired by the ngram LMs, FFNNLMs consider the previous words as the context for predicting the next word. // ==[6]== proposed the architecture of the original FFNNLM, as shown in Figure 1. // This FFNNLM can be expressed as: y = b + W x + U tanh(d + Hx), (4) w(t) s(t) y(t) s(t-1) x(t) U V.	b	A Survey on Neural Network Language Models	[6]	['[6] [Bengio et al., 2003] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. JMLR, 3:1137–1155, 2003.']
1504	680	[390, 653, 596]	Bengio et al., proposed the architecture of the original FFNNLM, as shown in Figure 1. // This FFNNLM can be expressed as: y = b + W x + U tanh(d + Hx), (4) w(t) s(t) y(t) s(t-1) x(t) U V delayed Figure 2: The RNNLM proposed by ==[18, 19, 20]==. // The RNN has an internal state that changes with the input on each time step, taking into account all previous contexts.	b	A Survey on Neural Network Language Models	[18, 19, 20]	['[18] [Mikolov et al., 2010] T. Mikolov, M. Karafiat, L. Burget, ´J. Cernocky, and S. Khudanpur. Recurrent neural network ´ based language model. In INTERSPEECH, pages 1045–1048, 2010.', '[19] [Mikolov et al., 2011a] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur. Extensions of recurrent ´ neural network language model. In Proc. of IEEE ICASSP,pages 5528–5531, 2011.', '[20] [Mikolov et al., 2011b] T. Mikolov, S. Kombrink, A. Deoras, and L. Burget. RNNLM - Recurrent Neural Network Language Modeling Toolkit. In IEEE ASRU, page 4, 2011.']
1505	680	[36]	The word representation is a by-product of LMs, which is used to improve other NLP tasks. // Based on FFNNLM, two word representation models, CBOW and Skip-gram , were proposed by ==[22]==. // FFNNLM overcomes the curse of dimensions by converting words into low-dimensional vectors.	h+	A Survey on Neural Network Language Models	[22]	['[22] [Mikolov et al., 2013] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. NIPS, 2013.']
1506	680	[147]	Moreover, fully connected NN needs to learn many trainable parameters, even though these parameters are less than n-gram LM, which still is expensive and inefficient. // RNN Language Models ==[6]== proposed the idea of using RNN for LMs. // They claimed that introducing more structure and parameter sharing into NNs could capture longer contextual information.	b	A Survey on Neural Network Language Models	[6]	['[6] [Bengio et al., 2003] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. JMLR, 3:1137–1155, 2003.']
1507	680	[390, 653]	They claimed that introducing more structure and parameter sharing into NNs could capture longer contextual information. // The first RNNLM was proposed by ==[18, 19]==. // As shown in Figure 2, at time step t, the RNNLM can be described as: xt = [w T t ; s T t−1 ] T, st = f(Uxt + b), yt = g(V st + d), where U, W, V are weight matrixes; b, d are the biases of the state layer and the output layer respectively. 	b	A Survey on Neural Network Language Models	[18, 19]	['[18] [Mikolov et al., 2010] T. Mikolov, M. Karafiat, L. Burget, ´J. Cernocky, and S. Khudanpur. Recurrent neural network ´ based language model. In INTERSPEECH, pages 1045–1048, 2010.', '[19] [Mikolov et al., 2011a] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur. Extensions of recurrent ´ neural network language model. In Proc. of IEEE ICASSP,pages 5528–5531, 2011.']
1508	680	[390, 653]	The first RNNLM was proposed by [18, 19]. // As shown in Figure 2, at time step t, the RNNLM can be described as: xt = [w T t ; s T t−1 ] T, st = f(Uxt + b), yt = g(V st + d), (5) where U, W, V are weight matrixes; b, d are the biases of the state layer and the output layer respectively; in ==[18, 19]==, f is the sigmoid function, and g is the Softmax function. // RNNLMs could be trained by the backpropagation through time (BPTT) or the truncated BPTT algorithm.	b	A Survey on Neural Network Language Models	[18, 19]	['[18] [Mikolov et al., 2010] T. Mikolov, M. Karafiat, L. Burget, ´J. Cernocky, and S. Khudanpur. Recurrent neural network ´ based language model. In INTERSPEECH, pages 1045–1048, 2010.', '[19] [Mikolov et al., 2011a] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur. Extensions of recurrent ´ neural network language model. In Proc. of IEEE ICASSP,pages 5528–5531, 2011.']
1509	680	[722]	LSTM-RNN Language Models Long Short-Term Memory (LSTM) RNN solved this problem. // ==[31]== introduced LSTM into LM and proposed LSTM-RNNLM.  // Except for the memory unit and the part of NN, the architecture of LSTM-RNNLM is almost the same as RNNLM.	b	A Survey on Neural Network Language Models	[31]	['[31] [Sundermeyer et al., 2012] M. Sundermeyer, R. Schluter, ¨ and H. Ney. LSTM neural networks for language modeling. In INTERSPEECH, pages 194–197, 2012.']
1510	680	[664]	For example, man in superman has the same meaning as the one in policeman. // ==[21]== explored RNNLM and FFNNLM at the character level. // Character-level NNLM can be used for solving out-of-vocabulary (OOV) word problem, improving the modeling of uncommon and unknown words because character features reveal structural similarities between words.	b	A Survey on Neural Network Language Models	[21]	['[21] [Mikolov et al., 2012] T. Mikolov, I. Sutskever, A. Deoras, H. Le, and S. Kombrink. Subword Language Modeling With Neural Networks. 2012.']
1511	680	[684]	One approach is to organize character-level features word by word and then use them for word-level LMs. // ==[14]== proposed Convolutional Neural Network (CNN) for extracting character-level feature and LSTM for receiving these character-level features of the word in a time step. // Hwang and Sung solved the problem of character-level NNLMs using a hierarchical RNN architecture consisting of multiple modules with different time scales.	b	A Survey on Neural Network Language Models	[14]	['[14] [Kim et al., 2015] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush. Character-aware neural language models. CoRR, abs/1508.06615, 2015.']
1512	680	[747]	Kim et al., proposed Convolutional Neural Network (CNN) for extracting character-level feature and LSTM for receiving these character-level features of the word in a time step. // ==[12]== solved the problem of character-level NNLMs using a hierarchical RNN architecture consisting of multiple modules with different time scales. // Another solution is to input character- and word-level features into NNLM simultaneously.	b	A Survey on Neural Network Language Models	[12]	['[12] [Hwang and Sung, 2016] K. Hwang and W. Sung. Character-level language modeling with hierarchical recurrent neural networks. CoRR, abs/1609.03777, 2016.']
1513	680	[733]	Miyamoto and Cho suggested interpolating word feature vectors with character feature vectors extracted from words by BiLSTM and inputting interpolation vectors into LSTM. // ==[34]== proposed a character-word LSTM-RNNLM that directly concatenated character- and word-level feature vectors and input concatenations into the network. // Character-aware LM directly uses character-level LM as character feature extractor for word-level LM.	b	A Survey on Neural Network Language Models	[34]	['[34] [Verwimp et al., 2017] L. Verwimp, J. Pelemans, H. Van hamme, and P. Wambacq. Character-word LSTM language models. In Proc. of EACL Volume 1: Long Papers, pages 417–427, 2017.']
1514	680	[765]	However, similarity can also be derived from word shape features (affixes, uppercase, hyphens, etc.) or other annotations (such as POS). // Inspired by factored LMs, ==[3]== proposed a factored NNLMs, a novel neural probabilistic LM that can learn the mapping from words and specific features of the words to continuous spaces. // Many studies have explored the selection of factors.	b	A Survey on Neural Network Language Models	[3]	['[3] [Alexandrescu and Kirchhoff, 2006] A. Alexandrescu and K. Kirchhoff. Factored neural language models. In HLTNAACL, 2006. ']
1515	680	[736]	Different linguistic features are considered first. // ==[36]== introduced morphological, grammatical, and semantic features to extend RNNLMs. // Adel et al., also ex- plored the effects of other factors, such as part-of-speech tags, Brown word clusters, open class words, and clusters of open class word embeddings.	b	A Survey on Neural Network Language Models	[36]	['[36] [Wu et al., 2012] Y. Wu, X. Lu, H. Yamamoto, S. Matsuda, C. Hori, and H. Kashioka. Factored language model based on recurrent neural network. In COLING, Proc. of the Conference: Technical Papers, pages 2835–2850, 2012.']
1516	680	[638]	Wu et al., introduced morphological, grammatical, and semantic features to extend RNNLMs. // ==[1]== also explored the effects of other factors, such as part-of-speech tags, Brown word clusters, open class words, and clusters of open class word embeddings. // Experimental results showed that Brown word clusters, part-of-speech tags, and open words are most effective for a Mandarin-English Code-Switching task.	b	A Survey on Neural Network Language Models	[1]	['[1] [Adel et al., 2015] H. Adel, N. T. Vu, K. Kirchhoff, D. Telaar, and T. Schultz. Syntactic and semantic features for code-switching factored language models. IEEE/ACM Trans. Audio, Speech & Language Processing, 23(3):431– 440, 2015.']
1517	680	[617]	Contextual information also was explored. // For example, ==[17]== used the distribution of topics calculated from fixed-length blocks of previous words. // Wang and Cho proposed a new approach to incorporating corpus bag-of-words (BoW) context into language modeling.	b	A Survey on Neural Network Language Models	[17]	['[17] [Mikolov and Zweig, 2012] T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. In IEEE SLT, pages 234–239, 2012.']
1518	680	[715]	For example, Mikolov and Zweig used the distribution of topics calculated from fixed-length blocks of previous words. //==[35]== proposed a new approach to incorporating corpus bag-of-words (BoW) context into language modeling. // Besides, some methods based on text-independent factors were proposed.	b	A Survey on Neural Network Language Models	[35]	['[35] [Wang and Cho, 2015] T. Wang and K. Cho. Larger-context language modelling. CoRR, abs/1511.03729, 2015.']
1519	680	[666]	Besides, some methods based on text-independent factors were proposed. // ==[2]== proposed a Neural Knowledge Language Model that applied the notation knowledge provided by knowledge graph for RNNLMs. // The factored model allows the model to summarize word classes with same characteristics.	b	A Survey on Neural Network Language Models	[2]	['[2] [Ahn et al., 2016] S. Ahn, H. Choi, T. Parnamaa, and Y. Ben- ¨ gio. A neural knowledge language model. CoRR, abs/1608.00318, 2016.']
1520	680	[696, 734]	A bidirectional NN can be established, which is conditional on future data. // ==[10, 4]== introduced bidirectional RNN and LSTM neural networks (BiRNN and BiLSTM) into speech recognition or other NLP tasks. // The BiRNNs utilize past and future contexts by processing the input data in both directions.	b	A Survey on Neural Network Language Models	[10, 4]	['[10] [Graves et al., 2013] A. Graves, N. Jaitly, and A. Mohamed. Hybrid speech recognition with deep bidirectional LSTM. In IEEE Workshop on ASRU, pages 273–278, 2013.', '[4][Bahdanau et al., 2014] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.']
1521	680	[599]	The BiRNNs utilize past and future contexts by processing the input data in both directions. // One of the most popular works of the bidirectional model is the ELMo model ==[26]==, a new deep contextualized word representation based on BiLSTM-RNNLMs. // The vectors of the embedding layer of a pre-trained ELMo model is the learned representation vector of words in the vocabulary.	b	A Survey on Neural Network Language Models	[26]	['[26] [Peters et al., 2018] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In Proc. of NAACLHLT Volume 1, pages 2227–2237, 2018.']
1522	680	[661]	Cache mechanism was originally proposed for reducing PPL of NNLMs. // ==[30]== attempted to combine FFNNLM with the cache mechanism and proposed a structure of the cache-based NNLMs, which leads to discrete probability change. // To solve this problem, et al., proposed a continuous cache model, where the change depends on the inner product of the hidden representations.	b	A Survey on Neural Network Language Models	[30]	['[30] [Soutner et al., 2012] D. Soutner, Z. Loose, L. Muller, and ¨ A. Prazak. Neural network language model with cache. In ´ TSD, pages 528–534. 2012.']
1523	680	[616]	Soutner et al., attempted to combine FFNNLM with the cache mechanism and proposed a structure of the cache-based NNLMs, which leads to discrete probability change. // To solve this problem, ==[9]== proposed a continuous cache model, where the change depends on the inner product of the hidden representations. // Another type of cache mechanism is that cache is used as a speed-up technique for NNLMs.	b	A Survey on Neural Network Language Models	[9]	['[9] [Grave et al., 2016] E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. CoRR, abs/1612.04426, 2016.']
1524	680	[771]	The main idea of this method is to store the outputs and states of LMs in a hash table for future prediction given the same contextual history. // For example, ==[11]== proposed the use of four caches to accelerate model reasoning. // The caches are respectively Query to Language Model Probability Cache, History to Hidden State Vector Cache, History to Class Normalization Factor Cache, and History and Class Id to Sub-vocabulary Normalization Factor Cache.	b	A Survey on Neural Network Language Models	[11]	['[11] [Huang et al., 2014] Z. Huang, G. Zweig, and B. Dumoulin. Cache based recurrent neural network language model inference for first pass speech recognition. In IEEE ICASSP, pages 6354–6358, 2014.']
1525	680	[734]	Similar to human beings, LM with the attention mechanism uses the long history efficiently by selecting useful word representations from them. // ==[4]== first proposed the application of the attention mechanism to NLP tasks (machine translation in this paper). // Tran et al., 2016 and Mei et al., proved that the attention mechanism could improve the performance of RNNLMs.	b	A Survey on Neural Network Language Models	[4]	['[4][Bahdanau et al., 2014] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.']
1526	680	[642, 597]	Bahdanau et al., first proposed the application of the attention mechanism to NLP tasks (machine translation in this paper). // ==[32, 16]== proved that the attention mechanism could improve the performance of RNNLMs. // The attention mechanism obtains the target areas that need to be focused on by a set of attention coefficients for each input.	b	A Survey on Neural Network Language Models	[32, 16]	['[32] [Tran et al., 2016] K. M. Tran, A. Bisazza, and C. Monz. Recurrent memory networks for language modeling. In NAACL HLT, pages 321–331, 2016.', '[16] [Mei et al., 2016] H. Mei, M. Bansal, and M. R. Walter. Coherent dialogue with attention-based language models. CoRR, abs/1611.06997, 2016.']
1527	680	[707]	On the basis of the above improved attention mechanisms, some com[petitive LMs or methods of word representation are proposed. // Transformer proposed by ==[33]== is the basis for the development of the subsequent models. // Transformer is a novel structure based entirely on the attention mechanism, which consists of an encoder and a decoder.	b	A Survey on Neural Network Language Models	[33]	['[33] Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In NIPS, pages 6000– 6010, 2017.']
1528	680	[606]	Transformer is a novel structure based entirely on the attention mechanism, which consists of an encoder and a decoder. // Since then, GPT and BERT ==[7]== have been proposed. // The main difference is that GPT uses Transformer’s decoder, and BERT uses Transformer’s encoder.	b	A Survey on Neural Network Language Models	[7]	['[7] [Devlin et al., 2018] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.']
1529	680	[670]	Transformer is a novel structure based entirely on the attention mechanism, which consists of an encoder and a decoder. // Since then, GPT ==[27]== and BERT have been proposed. // The main difference is that GPT uses Transformer’s decoder, and BERT uses Transformer’s encoder.	b	A Survey on Neural Network Language Models	[27]	['[27] [Radford et al., 2018] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving Language Understanding by Generative Pre-Training. 2018.']
1530	680	[90]	Hierarchical Softmax Some methods based on hierarchical Softmax that decompose target conditional probability into multiples of some conditional probabilities are proposed. // ==[25]== used a hierarchical binary tree (by the similarity from Wordnet) of an output layer, in which the V words in vocabulary are regarded as its leaves. // This technique allows exponentially fast calculations of word probabilities and their gradients.	b	A Survey on Neural Network Language Models	[25]	['[25] [Morin and Bengio, 2005] F. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In Proc. of AISTATS, 2005.']
1531	680	[89]	However, it performs much worse than non-hierarchical one despite using expert knowledge. // ==[24]== improved it by a simple feature-based algorithm for automatically building word trees from data. // The performance of the above two models is mostly dependent on the tree, which is usually heuristically constructed.	b	A Survey on Neural Network Language Models	[24]	['[24] [Mnih and Hinton, 2008] A. Mnih and G. E. Hinton. A scalable hierarchical distributed language model. In Proc. Of NIPS, pages 1081–1088, 2008.']
1532	680	[653]	Since then, many scholars have improved this model. // Hierarchical models based on word frequency classification ==[19]== and Brown clustering were proposed. // It was proved that the model with Brown clustering performed better.	b	A Survey on Neural Network Language Models	[19]	['[19] [Mikolov et al., 2011a] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur. Extensions of recurrent ´ neural network language model. In Proc. of IEEE ICASSP,pages 5528–5531, 2011.']
1533	680	[766]	It was proved that the model with Brown clustering performed better. // ==[38]== proposed a speed optimal classification, i.e., a dynamic programming algorithm that determines the classes by minimizing the running time of the model. // Hierarchical Softmax significantly reduces model parameters without increasing PPL.	b	A Survey on Neural Network Language Models	[38]	['[38] [Zweig and Makarychev, 2013] G. Zweig and K. Makarychev. Speed regularization and optimality in word classing. In IEEE ICASSP, pages 8237–8241, 2013.']
1534	680	[738]	Experimental results showed that adopting importance sampling leads to ten times faster the training of NNLMs without significantly increasing PPL. // ==[5]== proposed an adaptive importance sampling method using an adaptive n-gram model instead of the simple unigram model. // Other improvements have been proposed, such as parallel training of small models to estimate loss for importance sampling, multiple importance sampling and likelihood weighting scheme, two-stage sampling, and so on.	b	A Survey on Neural Network Language Models	[5]	['[5] [Bengio and Senecal, 2008] Y. Bengio and J. Senecal. Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Trans. Neural Networks, 19(4):713–722, 2008.']
1535	746	[636, 721, 599, 662, 620]	// Introduction Unsupervised representation learning has been highly successful in the domain of natural language processing ==[7, 19, 24, 25, 10]==. // Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[7, 19, 24, 25, 10]	['[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neuralinformation processing systems, pages 3079–3087, 2015.', '[19] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:Contextualized word vectors. In Advances in Neural Information Processing Systems, pages6294–6305, 2017.', '[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprintarXiv:1802.05365, 2018.', '[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training. 2018.', '[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.']
1536	746	[636, 599, 662]	Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. // AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model ==[7, 24, 25]==. // Specifically, given a text sequence x = (x1, · · · , xT ), AR language modeling factorizes the likelihood into a forward product p(x) = QT t=1 p(xt | x<t) or a backward one p(x) = Q1 t=T p(xt | x>t).	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[7, 24, 25]	['[7] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neuralinformation processing systems, pages 3079–3087, 2015.', '[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprintarXiv:1802.05365, 2018.', '[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training.  2018.']
1537	746	[620]	In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. // A notable example is BERT ==[10]==, which has been the state-of-the-art pretraining approach. // Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[10]	['[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.']
1538	746	[615]	Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. // In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language ==[9]==. // Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations.	h-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[9]	['[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-lengthcontext. arXiv preprint arXiv:1901.02860, 2019.']
1539	746	[615]	In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining. // Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL ==[9]== into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. // Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[9]	['[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-lengthcontext. arXiv preprint arXiv:1901.02860, 2019.']
1540	746	[620]	Empirically, XLNet achieves state-of-the-art results on 18 tasks, i.e., 7 GLUE language understanding tasks, 3 reading comprehension tasks including SQuAD and RACE, 7 text classification tasks including Yelp and IMDB, and the ClueWeb09-B document ranking task. // Under a set of fair comparison experiments, XLNet consistently outperforms BERT ==[10]== on multiple benchmarks. // Related Work The idea of permutation-based AR modeling has been explored in [32, 11], but there are several key differences.	h-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[10]	['[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.']
1541	746	[602, 621]	Under a set of fair comparison experiments, XLNet consistently outperforms BERT [10] on multiple benchmarks. // Related Work The idea of permutation-based AR modeling has been explored in ==[32, 11]==, but there are several key differences. // Previous models are orderless, while XLNet is essentially order-aware with positional encodings.	r-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[32, 11]	['[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neuralautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–7220, 2016.', '[11] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881–889, 2015.']
1542	746	[620]	Input noise: The input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pretrain-finetune discrepancy. // Replacing [MASK] with original tokens as in ==[10]== does not solve the problem because original tokens can be only used with a small probability — otherwise Eq. (2) will be trivial to optimize. //	h-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[10]	['[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.']
1543	746	[602]	A natural question to ask is whether there exists a pretraining objective that brings the advantages of both while avoiding their weaknesses. // Borrowing ideas from orderless NADE ==[32]==, we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. // Specifically, for a sequence x of length T, there are T! different orders to perform a valid autoregressive factorization.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[32]	['[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neuralautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–7220, 2016.']
1544	746	[615]	For unselected tokens, their query representations need not be computed, which saves speed and memory. // Incorporating Ideas from Transformer-XL Since our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL ==[9]==, into our pretraining framework, and name our method after it. // To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL).	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[9]	['[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-lengthcontext. arXiv preprint arXiv:1901.02860, 2019.']
1545	746	[620]	Specifically, the input to our model is similar to BERT: [A, SEP, B, SEP, CLS], where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. // Although we follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction ==[10]== as it does not show consistent improvement in our ablation study (see Section 3.7). // Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments.	r-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[10]	['[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.']
1546	746	[615]	There are two benefits of using relative segment encodings. // First, the inductive bias of relative encodings improves generalization ==[9]==. // Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings.	h+	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[9]	['[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-lengthcontext. arXiv preprint arXiv:1901.02860, 2019.']
1547	746	[640]	To prove a general point beyond one example, we now turn to more formal expressions. // Inspired by previous work ==[38]==, given a sequence x = [x1, · · · , xT ], we define a set of target-context pairs of interest, I = {(x, U)}, where U is a set of tokens in x that form a context of x. // Intuitively, we want the model to learn the dependency of x on U through a pretraining loss term log p(x | U).	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[38]	['[38] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmaxbottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.']
1548	746	[662]	In other words, the XLNet objective contains more effective training signals, which empirically leads to better performance in Section 3. // Comparison with Language Modeling Borrowing examples and notations from Section 2.6.1, a standard AR language model like GPT ==[25]== is only able to cover the dependency (x = York, U = {New}) but not (x = New, U = {York}). // XLNet, on the other hand, is able to cover both in expectation over all factorization orders.	r-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[25]	['[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving languageunderstanding by generative pre-training.  2018.']
1549	746	[599]	In comparison, XLNet is able to cover all dependencies in expectation. // Approaches like ELMo ==[24]== concatenate forward and backward language models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions. // Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task.	r-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[24]	['[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprintarXiv:1802.05365, 2018.']
1550	746	[691, 602, 710]	Our single model outperforms the best ensemble by 7.6 points in accuracy. // Bridging the Gap Between Language Modeling and Pretraining With a deep root in density estimation ==[4, 32, 21]==, language modeling has been a rapidly-developing research area [9, 1, 3]. // However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[4, 32, 21]	['[4] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layerneural networks. In Advances in Neural Information Processing Systems, pages 400–406, 2000.', '[32] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neuralautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–7220, 2016.', '[21] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neuralnetworks. arXiv preprint arXiv:1601.06759, 2016.']
1551	746	[615, 659, 756]	Our single model outperforms the best ensemble by 7.6 points in accuracy. // 2.6.3 Bridging the Gap Between Language Modeling and Pretraining With a deep root in density estimation3 [4, 32, 21], language modeling has been a rapidly-developing research area ==[9, 1, 3]==. // However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section 2.6.2.	r-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[9, 1, 3]	['[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le,and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-lengthcontext. arXiv preprint arXiv:1901.02860, 2019.', '[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-levellanguage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.', '[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018.']
1552	746	[629]	As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress. // Experiments 3.1 Pretraining and Implementation Following BERT [10], we use the BooksCorpus ==[41]== and English Wikipedia as part of our pretraining data, which have 13GB plain text combined. // In addition, we include Giga5 (16GB text) [23], ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[41]	['[41] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations bywatching movies and reading books. In Proceedings of the IEEE international conference oncomputer vision, pages 19–27, 2015.']
1553	746	[700]	Experiments 3.1 Pretraining and Implementation Following BERT [10], we use the BooksCorpus [41] and English Wikipedia as part of our pretraining data, which have 13GB plain text combined. // In addition, we include Giga5 (16GB text) ==[23]==, ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. // We use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 78GB text respectively.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[23]	['[23] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigawordfifth edition, linguistic data consortium. Technical report, Technical Report. Linguistic DataConsortium, Philadelphia, Tech. Rep., 2011.']
1554	746	[612]	We use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 78GB text respectively. // After tokenization with SentencePiece ==[16]==, we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. // Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[16]	['[16] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.']
1555	746	[769]	All BERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large). // The RACE dataset ==[17]== contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. // This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[17]	['[17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scalereading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.']
1556	746	[683]	This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. // Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD ==[26]==. // As a result, this dataset serves as a challenging benchmark for long text understanding.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[26]	['[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerablequestions for squad. arXiv preprint arXiv:1806.03822, 2018.']
1557	746	[770]	SQuAD is a large-scale reading comprehension dataset with two tasks. // SQuAD 1.1 ==[27]== contains questions that always have a corresponding answer in the given passages, while SQuAD 2.0 [26] introduces unanswerable questions. // To finetune an XLNet on SQuAD 2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering [10].	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[27]	['[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questionsfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.']
1558	746	[683]	SQuAD is a large-scale reading comprehension dataset with two tasks. // SQuAD 1.1 [27] contains questions that always have a corresponding answer in the given passages, while SQuAD 2.0 ==[26]== introduces unanswerable questions. // To finetune an XLNet on SQuAD 2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering [10].	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[26]	['[26] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerablequestions for squad. arXiv preprint arXiv:1806.03822, 2018.']
1559	746	[620]	SQuAD 1.1 [27] contains questions that always have a corresponding answer in the given passages, while SQuAD 2.0 [26] introduces unanswerable questions. // To finetune an XLNet on SQuAD 2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering ==[10]==. // Since v1.1 and v2.0 share the same answerable questions in the training set, we simply remove the answerability prediction part from the model finetuned on v2.0 for evaluation on v1.1.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[10]	['[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.']
1560	746	[694]	Since v1.1 and v2.0 share the same answerable questions in the training set, we simply remove the answerability prediction part from the model finetuned on v2.0 for evaluation on v1.1. // As the top leaderboard entries all employ some form of data augmentation, we jointly train an XLNet on SQuAD 2.0 and NewsQA ==[31]== for our leaderboard submission. // As shown in Table 2, XLNet obtains the state-of-the-art single model results on the leaderboard, outperforming a series of BERT-based methods.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[31]	['[31] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprintarXiv:1611.09830, 2016.']
1561	746	[724]	According to Table 3, XLNet achieves new state-of-the-art results on all the considered datasets, reducing the error rate by 16%, 18%, 5%, 9% and 5% on IMDB, Yelp-2, Yelp-5, Amazon-2, and Amazon-5 respectively compared to BERT. // The GLUE dataset ==[34]== is a collection of 9 natural language understanding tasks. // The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results.	b	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[34]	['[34] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019.In the Proceedings of ICLR.']
1562	746	[702]	Only single-task training is employed for the four large datasets. // For QNLI, we employed a pairwise relevance ranking scheme as in ==[18]== for our test set submission. // However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[18]	['[18] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504, 2019.']
1563	746	[688]	However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. // For WNLI, we use the loss described in ==[15]==. // A multi-task ensemble XLNet achieves the state-of-the-art results on 7 out of 9 tasks on the public leaderboard.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[15]	['[15] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and ThomasLukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprintarXiv:1905.06290, 2019.']
1564	746	[639]	All models are pretrained on the same data. // ClueWeb09-B Dataset Following the setting in previous work ==[8]==, we use the ClueWeb09-B dataset to evaluate the performance on document ranking. // The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[8]	['[8] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networksfor soft-matching n-grams in ad-hoc search. In Proceedings of the eleventh ACM internationalconference on web search and data mining, pages 126–134. ACM, 2018.']
1565	746	[686]	Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. // We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network ==[37]== to rank the documents. // According to Table 5, XLNet substantially outperforms the other methods, including a BERT model that uses the same training procedure as ours.	ho	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[37]	['[37] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neuralad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIRconference on research and development in information retrieval, pages 55–64. ACM, 2017.']
1566	746	[650]	This illustrates that XLNet learns better low-level word embeddings than BERT. // Note that for fair comparison we exclude the results in ==[36]== as it uses additional entity-related data. // 3.7 Ablation Study We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics.	r-	XLNet- Generalized Autoregressive Pretraining for Language Understanding	[36]	['[36] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for documentranking. In Proceedings of the 40th International ACM SIGIR conference on research anddevelopment in information retrieval, pages 763–772. ACM, 2017.']
1567	606	[599, 618]	We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. // Unlike recent language representation models ==[32, 34]==, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. // As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.	r-	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[32, 34]	['[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.', '[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI. ']
1568	606	[636, 599, 618, 732]	It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% , MultiNLI accuracy to 86.7% , SQuAD v1.1 question answering Test F1 to 93.2 and SQuAD v2.0 Test F1 to 83.1 . // Language model pre-training has been shown to be effective for improving many natural language processing tasks ==[14, 32, 34, 18]==. // These include sentence-level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[14, 32, 34, 18]	['[14] Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.', '[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.', '[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI. ', '[18] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI.']
1569	606	[751, 742]	Language model pre-training has been shown to be effective for improving many natural language processing tasks. // These include sentence-level tasks such as natural language inference ==[6, 46]== and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level. // There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[6, 46]	['[6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP. Association for Computational Linguistics.', '[46] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471. Adina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.']
1570	606	[593, 591]	Language model pre-training has been shown to be effective for improving many natural language processing tasks. // These include sentence-level tasks such as natural language inference and paraphrasing, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level ==[40, 35]==. // There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[40, 35]	['[40] Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.', '[35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.']
1571	606	[599]	There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. // The feature-based approach, such as ELMo ==[32]==, uses task-specific architectures that include the pre-trained representations as additional features. // The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) , introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[32]	['[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.']
1572	606	[618]	The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. // The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) ==[34]==, introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. // The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[34]	['[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI. ']
1573	606	[707]	The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. // For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer ==[42]==. // Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[42]	['[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010. ']
1574	606	[729, 706, 697]	There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches. // Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural ==[7, 3, 5]== and neural methods. // Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch .	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[7, 3, 5]	['[7] Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.', '[3] Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853.', '[5] John Blitzer, Ryan McDonald, and Fernando Pereira.2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120–128. Association for Computational Linguistics.']
1575	606	[717, 36, 167]	There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches. // Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural ==[9, 27, 30]== methods. // Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch .	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[9, 27, 30]	['[9] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.', '[27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.', '[30] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543.']
1576	606	[759]	Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural and neural methods. // Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch ==[41]==. // To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[41]	['[41] Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394.']
1577	606	[717, 36]	Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. // To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context ==[9, 27]==. // These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings .	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[9, 27]	['[9] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.', '[27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.']
1578	606	[735, 595, 663]	To pretrain word embedding vectors, left-to-right language modeling objectives have been used, as well as objectives to discriminate correct from incorrect words in left and right context. // These approaches have been generalized to coarser granularities, such as sentence embeddings ==[21, 51, 24]== or paragraph embeddings. // To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[21, 51, 24]	['[21] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.', '[51] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.', '[24] Lajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. In International Conference on Learning Representations.']
1579	606	[735, 595]	These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. // To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence ==[21, 51]==, or denoising autoencoder derived objectives. // ELMo and its predecessor generalize traditional word embedding research along a different dimension.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[21, 51]	['[21] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.', '[51] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.']
1580	606	[692]	These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. // To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives ==[17]==. // ELMo and its predecessor generalize traditional word embedding research along a different dimension.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[17]	['[17] Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. ']
1581	606	[599]	To train sentence representations, prior work has used objectives to rank candidate next sentences, left-to-right generation of next sentence words given a representation of the previous sentence, or denoising autoencoder derived objectives. // ELMo and its predecessor ==[32]== generalize traditional word embedding research along a different dimension. // They extract context-sensitive features from a left-to-right and a right-to-left language model.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[32]	['[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.']
1582	606	[636, 732, 618]	Unsupervised Fine-tuning Approaches As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text. // More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task ==[14, 18, 34]==. // The advantage of these approaches is that few parameters need to be learned from scratch.	h+	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[14, 18, 34]	['[14] Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.', '[18] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI.', '[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI. ']
1583	606	[618]	The advantage of these approaches is that few parameters need to be learned from scratch. // At least partly due to this advantage, OpenAI GPT ==[34]== achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark. //	h+	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[34]	['[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI. ']
1584	606	[636, 656]	Transfer Learning from Supervised Data There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference and machine translation. // Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet ==[14, 48]==. // We introduce BERT and its detailed implementation in this section.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[14, 48]	['[14] Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.', '[48] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328. ']
1585	606	[658]	A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. // We use WordPiece embeddings ==[47]== with a 30,000 token vocabulary. // The first token of every sequence is always a special classification token ([CLS]).	ho	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[47]	['[47] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.']
1586	606	[337]	In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. // In contrast to denoising auto-encoders ==[43]==, we only predict the masked words rather than reconstructing the entire input. // Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning.	r-	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[43]	['[43] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.']
1587	606	[735, 595]	Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. // For the pre-training corpus we use the BooksCorpus (800M words) ==[21, 51]== and English Wikipedia (2,500M words). // For Wikipedia we extract only the text passages and ignore lists, tables, and headers.	ho	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[21, 51]	['[21] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302.', '[51] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.']
1588	606	[701]	Experiments In this section, we present BERT fine-tuning results on 11 NLP tasks. // GLUE The General Language Understanding Evaluation (GLUE) benchmark ==[44]== is a collection of diverse natural language understanding tasks. // Detailed descriptions of GLUE datasets are included in Appendix B.1.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[44]	['[44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355.']
1589	606	[591]	The effect of model size is explored more thoroughly in Section 5.2. // 4.2 SQuAD v1.1 The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs ==[35]==. // Given a question and a passage from 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[35]	['[35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.']
1590	606	[719, 605, 627, 599, 599, 732]	We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. // Table 2 shows top leaderboard entries as well as results from top published systems ==[36, 10, 11, 32, 32, 18]==. // The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[36, 10, 11, 32, 32, 18]	['[36] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR. ', '[10] Quora question pairs. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In ACL.', '[11] Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914– 1925.', '[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.', '[32] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL.', '[18] Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI.']
1591	606	[705]	The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. // We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA ==[20]== befor fine-tuning on SQuAD. // Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system.	ho	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[20]	['[20] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.']
1592	606	[749, 755]	We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. // The results compared to prior leaderboard entries and top published work ==[38, 45]== are shown in Table 3, excluding systems that use BERT as one of their components. // We observe a +5.1 F1 improvement over the previous best system.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[38, 45]	['[38] Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehensionwith unanswerable questions. arXiv preprint arXiv:1810.06638.', '[45] Wei Wang, Ming Yan, and Chen Wu. 2018b. Multigranularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. ']
1593	606	[608]	// For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters ==[2]==. // By contrast, BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters.	b	BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding	[2]	['[2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.']
1594	684	[147, 617]	The count-based models are simple to train, but probabilities of rare n-grams can be poorly estimated due to data sparsity (despite smoothing techniques). // Neural Language Models (NLM) address the n-gram data sparsity issue through parameterization of words as vectors (word embeddings) and using them as inputs to a neural network ==[3, 25]==. // The parameters are learned as part of the training process.	b	Character-Aware Neural Language Models	[3, 25]	['[3] Bengio, Y.; Ducharme, R.; and Vincent, P. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research 3:1137–1155.', '[25] Mikolov, T., and Zweig, G. 2012. Context Dependent Recurrent Neural Network Language Model. In Proceedings of SLT. Mikolov, T.; Karafiat, M.; Burget, L.; Cernocky, J.; and Khudanpur,S. 2010. Recurrent Neural Network Based Language Model. In Proceedings of INTERSPEECH.']
1595	684	[643]	Character-level Convolutional Neural Network In our model, the input at time t is an output from a character-level convolutional neural network (CharCNN), which we describe in this section. // CNNs ==[20]== have achieved state-of-the-art results on computer vision and have also been shown to be effective for various NLP tasks. // 	h+	Character-Aware Neural Language Models	[20]	['[20] LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. D. 1989. Handwritten Digit Recognition with a Backpropagation Network. In Proceedings of NIPS. ']
1596	684	[306]	// CNNs have achieved state-of-the-art results on computer vision and have also been shown to be effective for various NLP tasks ==[10]==. // Architectures employed for NLP applications differ in that they typically involve temporal rather than spatial convolutions.	h+	Character-Aware Neural Language Models	[10]	['[10] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research 12:2493–2537.']
1597	684	[306, 614, 687, 307, 713, 772, 641]	Suffixes likewise refer to character n-grams which end with the end-of-word character. // ==[10, 32, 17, 18, 39, 40, 21]==, and we posit that further gains could be achieved by employing highway layers on top of existing CNN architectures. // We also anecdotally note that (1) having one to two highway layers was important, but more highway layers generally resulted in similar performance (though this may depend on the size of the datasets), (2) having more convolutional layers before max-pooling did not help, and (3) highway layers did not improve models that only used word embeddings as inputs.	b	Character-Aware Neural Language Models	[10, 32, 17, 18, 39, 40, 21]	['[10] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; Kavukcuoglu, K.; and Kuksa, P. 2011. Natural Language Processing (almost) from Scratch. Journal of Machine Learning Research 12:2493–2537.', '[32] Shen, Y.; He, X.; Gao, J.; Deng, L.; and Mesnil, G. 2014. A Latent Semantic Model with Convolutional-pooling Structure for Information Retrieval. In Proceedings of CIKM.', '[17] Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A Convolutional Neural Network for Modelling Sentences. In Proceedings of ACL. ', '[18] Kim, Y. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of EMNLP.', '[39] Zhang, S.; Jiang, H.; Xu, M.; Hou, J.; and Dai, L. 2015. The FixedSize Ordinally-Forgetting Encoding Method for Neural Network Language Models. In Proceedings of ACL.', '[40] Zhang, X.; Zhao, J.; and LeCun, Y. 2015. Character-level Convolutional Networks for Text Classification. In Proceedings of NIPS. 2749', '[21] Lei, T.; Barzilay, R.; and Jaakola, T. 2015. Molding CNNs for Text: Non-linear, Non-consecutive Convolutions. In Proceedings of EMNLP.']
1598	684	[617]	Related Work Neural Language Models (NLM) encompass a rich family of neural network architectures for language modeling. // Some example architectures include feed-forward, recurrent ==[25]==, sum-product, log-bilinear, and convolutional networks. //  In order to address the rare word problem, Alexandrescu and Kirchhoff (2006)—building on analogous work on count-based n-gram language models by Bilmes and Kirchhoff (2003)—represent a word as a set of shared factor embeddings.	b	Character-Aware Neural Language Models	[25]	['[25] Mikolov, T., and Zweig, G. 2012. Context Dependent Recurrent Neural Network Language Model. In Proceedings of SLT. Mikolov, T.; Karafiat, M.; Burget, L.; Cernocky, J.; and Khudanpur,S. 2010. Recurrent Neural Network Based Language Model. In Proceedings of INTERSPEECH.']
1599	684	[646]	Related Work Neural Language Models (NLM) encompass a rich family of neural network architectures for language modeling. // Some example architectures include feed-forward, recurrent, sum-product ==[8]==, log-bilinear, and convolutional networks. //  In order to address the rare word problem, Alexandrescu and Kirchhoff (2006)—building on analogous work on count-based n-gram language models by Bilmes and Kirchhoff (2003)—represent a word as a set of shared factor embeddings.	b	Character-Aware Neural Language Models	[8]	['[8] Cheng, W. C.; Kok, S.; Pham, H. V.; Chieu, H. L.; and Chai, K. M. 2014. Language Modeling with Sum-Product Networks. In Proceedings of INTERSPEECH.']
1600	684	[610]	Related Work Neural Language Models (NLM) encompass a rich family of neural network architectures for language modeling. // Some example architectures include feed-forward, recurrent, sum-product [8], log-bilinear, and convolutional networks ==[37]==. //  In order to address the rare word problem, Alexandrescu and Kirchhoff (2006)—building on analogous work on count-based n-gram language models by Bilmes and Kirchhoff (2003)—represent a word as a set of shared factor embeddings.	b	Character-Aware Neural Language Models	[37]	['[37] Wang, M.; Lu, Z.; Li, H.; Jiang, W.; and Liu, Q. 2015. genCNN: A Convolutional Architecture for Word Sequence Prediction. In Proceedings of ACL. Werbos, P. 1990. Back-propagation Through Time: what it does and how to do it. In Proceedings of IEEE. ']
1601	684	[617, 664]	Character-level models obviate the need for morphological tagging or manual feature engineering, and have the attractive property of being able to generate novel words. // However they are generally outperformed by word-level models ==[25, 27]==. // Outside of language modeling, improvements have been reported on part-of-speech tagging and named entity recognition by representing a word as a concatenation of its word embedding and an output from a characterlevel CNN, and using the combined representation as features in a Conditional Random Field (CRF).	h+	Character-Aware Neural Language Models	[25, 27]	['[25] Mikolov, T., and Zweig, G. 2012. Context Dependent Recurrent Neural Network Language Model. In Proceedings of SLT. Mikolov, T.; Karafiat, M.; Burget, L.; Cernocky, J.; and Khudanpur,S. 2010. Recurrent Neural Network Based Language Model. In Proceedings of INTERSPEECH.', '[27] Mikolov, T.; Sutskever, I.; Deoras, A.; Le, H.-S.; Kombrink, S.; and Cernocky, J. 2012. Subword Language Modeling with Neural Networks. ']
1602	670	[623]	The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised learning in natural language processing (NLP). // Most deep learning methods require substantial amounts of manually labeled data, which restricts their applicability in many domains that suffer from a dearth of annotated resources ==[61]==. // In these situations, models that can leverage linguistic information from unlabeled data provide a valuable alternative to gathering more annotation, which can be time-consuming and expensive.	b	Improving Language Understanding by Generative Pre-Training	[61]	['[61] Y. Tsvetkov. Opportunities and challenges in working with low-resource languages. CMU, 2017.']
1603	670	[125, 36, 167]	Further, even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost. // The most compelling evidence for this so far has been the extensive use of pretrained word embeddings ==[10, 39, 42]== to improve performance on a range of NLP tasks [8, 11, 26, 45]. // Leveraging more than word-level information from unlabeled text, however, is challenging for two main reasons.	b	Improving Language Understanding by Generative Pre-Training	[10, 39, 42]	['[10] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2008.', '[39] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.', '[42] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.']
1604	670	[679, 306, 307, 727]	Further, even in cases where considerable supervision is available, learning good representations in an unsupervised fashion can provide a significant performance boost. // The most compelling evidence for this so far has been the extensive use of pretrained word embeddings [10, 39, 42] to improve performance on a range of NLP tasks ==[8, 11, 26, 45]==. // Leveraging more than word-level information from unlabeled text, however, is challenging for two main reasons.	b	Improving Language Understanding by Generative Pre-Training	[8, 11, 26, 45]	['[8] D. Chen and C. Manning. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740–750, 2014.', '[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537, 2011.', '[26] Y. Kim. Convolutional neural networks for sentence classification. EMNLP, 2014.', '[45] Y. Qi, D. S. Sachan, M. Felix, S. J. Padmanabhan, and G. Neubig. When and why are pre-trained word embeddings useful for neural machine translation? NAACL, 2018.']
1605	670	[599]	First, it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. // Recent research has looked at various objectives such as language modeling ==[44]==, machine translation [38], and discourse coherence [22], with each method outperforming the others on different tasks. // Second, there is no consensus on the most effective way to transfer these learned representations to the target task. Existing techniques involve a combination of making task-specific changes to the model architecture [43, 44], using intricate learning schemes [21] and adding auxiliary learning objectives [50].	b	Improving Language Understanding by Generative Pre-Training	[44]	['[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. NAACL, 2018.']
1606	670	[721]	First, it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. // Recent research has looked at various objectives such as language modeling [44], machine translation ==[38]==, and discourse coherence [22], with each method outperforming the others on different tasks. // Second, there is no consensus on the most effective way to transfer these learned representations to the target task. Existing techniques involve a combination of making task-specific changes to the model architecture [43, 44], using intricate learning schemes [21] and adding auxiliary learning objectives [50].	b	Improving Language Understanding by Generative Pre-Training	[38]	['[38] B. McCann, J. Bradbury, C. Xiong, and R. Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pages 6297–6308, 2017.']
1607	670	[728]	First, it is unclear what type of optimization objectives are most effective at learning text representations that are useful for transfer. // Recent research has looked at various objectives such as language modeling [44], machine translation [38], and discourse coherence ==[22]==, with each method outperforming the others on different tasks. // Second, there is no consensus on the most effective way to transfer these learned representations to the target task. Existing techniques involve a combination of making task-specific changes to the model architecture [43, 44], using intricate learning schemes [21] and adding auxiliary learning objectives [50].	b	Improving Language Understanding by Generative Pre-Training	[22]	['[22] Y. Jernite, S. R. Bowman, and D. Sontag. Discourse-based objectives for fast unsupervised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017.']
1608	670	[741, 599]	Second, there is no consensus on the most effective way to transfer these learned representations to the target task. // Existing techniques involve a combination of making task-specific changes to the model architecture ==[43, 44]==, using intricate learning schemes [21] and adding auxiliary learning objectives [50]. // These uncertainties have made it difficult to develop effective semi-supervised learning approaches for language processing.	h-	Improving Language Understanding by Generative Pre-Training	[43, 44]	['[43] M. E. Peters, W. Ammar, C. Bhagavatula, and R. Power. Semi-supervised sequence tagging with bidirectional language models. ACL, 2017.', '[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. NAACL, 2018.']
1609	670	[732]	Second, there is no consensus on the most effective way to transfer these learned representations to the target task. // Existing techniques involve a combination of making task-specific changes to the model architecture [43, 44], using intricate learning schemes ==[21]== and adding auxiliary learning objectives [50]. // These uncertainties have made it difficult to develop effective semi-supervised learning approaches for language processing.	h-	Improving Language Understanding by Generative Pre-Training	[21]	['[21] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. Association for Computational Linguistics (ACL), 2018.']
1610	670	[625]	Second, there is no consensus on the most effective way to transfer these learned representations to the target task. // Existing techniques involve a combination of making task-specific changes to the model architecture [43, 44], using intricate learning schemes [21] and adding auxiliary learning objectives ==[50]==. // These uncertainties have made it difficult to develop effective semi-supervised learning approaches for language processing.	h-	Improving Language Understanding by Generative Pre-Training	[50]	['[50] M. Rei. Semi-supervised multitask learning for sequence labeling. ACL, 2017.']
1611	670	[707]	Subsequently, we adapt these parameters to a target task using the corresponding supervised objective. // For our model architecture, we use the Transformer ==[62]==, which has been shown to perform strongly on various tasks such as machine translation ==[62]==, document generation [34], and syntactic parsing [29]. // This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.	h+	Improving Language Understanding by Generative Pre-Training	[62]	['[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010, 2017.']
1612	670	[707]	Subsequently, we adapt these parameters to a target task using the corresponding supervised objective. // For our model architecture, we use the Transformer ==[62]==, which has been shown to perform strongly on various tasks such as machine translation ==[62]==, document generation [34], and syntactic parsing [29]. // This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.	h+	Improving Language Understanding by Generative Pre-Training	[62]	['[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010, 2017.']
1613	670	[773]	Subsequently, we adapt these parameters to a target task using the corresponding supervised objective. // For our model architecture, we use the Transformer [62], which has been shown to perform strongly on various tasks such as machine translation [62], document generation ==[34]==, and syntactic parsing [29]. // This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.	h+	Improving Language Understanding by Generative Pre-Training	[34]	['[34] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. ICLR, 2018.']
1614	670	[654]	Subsequently, we adapt these parameters to a target task using the corresponding supervised objective. // For our model architecture, we use the Transformer [62], which has been shown to perform strongly on various tasks such as machine translation [62], document generation [34], and syntactic parsing ==[29]==. // This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks.	h+	Improving Language Understanding by Generative Pre-Training	[29]	['[29] N. Kitaev and D. Klein. Constituency parsing with a self-attentive encoder. ACL, 2018.']
1615	670	[611]	This model choice provides us with a more structured memory for handling long-term dependencies in text, compared to alternatives like recurrent networks, resulting in robust transfer performance across diverse tasks. // During transfer, we utilize task-specific input adaptations derived from traversal-style approaches ==[52]==, which process structured text input as a single contiguous sequence of tokens. // As we demonstrate in our experiments, these adaptations enable us to fine-tune effectively with minimal changes to the architecture of the pre-trained model.	ho	Improving Language Understanding by Generative Pre-Training	[52]	['[52] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Kocisk ˇ y, and P. Blunsom. Reasoning about entailment ` with neural attention. arXiv preprint arXiv:1509.06664, 2015.']
1616	670	[730]	Our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. // For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) ==[40]==, 5.7% on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on the recently introduced GLUE multi-task benchmark [64]. // We also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks.	b	Improving Language Understanding by Generative Pre-Training	[40]	['[40] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46–51, 2017.']
1617	670	[739]	Our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. // For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question answering (RACE) ==[30]==, 1.5% on textual entailment (MultiNLI) [66] and 5.5% on the recently introduced GLUE multi-task benchmark [64]. // We also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks.	b	Improving Language Understanding by Generative Pre-Training	[30]	['[30] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. Race: Large-scale reading comprehension dataset from examinations. EMNLP, 2017.']
1618	670	[601]	Our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. // For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) ==[66]== and 5.5% on the recently introduced GLUE multi-task benchmark [64]. // We also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks.	b	Improving Language Understanding by Generative Pre-Training	[66]	['[66] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. NAACL, 2018.']
1619	670	[701]	Our general task-agnostic model outperforms discriminatively trained models that employ architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. // For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test) [40], 5.7% on question answering (RACE) [30], 1.5% on textual entailment (MultiNLI) [66] and 5.5% on the recently introduced GLUE multi-task benchmark ==[64]==. // We also analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks.	b	Improving Language Understanding by Generative Pre-Training	[64]	['[64] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.']
1620	670	[634, 767, 780]	Semi-supervised learning for NLP Our work broadly falls under the category of semi-supervised learning for natural language. // This paradigm has attracted significant interest, with applications to tasks like sequence labeling ==[24, 33, 57]== or text classification [41, 70]. // The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model [33].	b	Improving Language Understanding by Generative Pre-Training	[24, 33, 57]	['[24] F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuurmans. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 209–216. Association for Computational Linguistics, 2006.', '[33] P. Liang. Semi-supervised learning for natural language. PhD thesis, Massachusetts Institute of Technology, 2005.', '[57] J. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. Proceedings of ACL-08: HLT, pages 665–673, 2008.']
1621	670	[589, 632]	Semi-supervised learning for NLP Our work broadly falls under the category of semi-supervised learning for natural language. // This paradigm has attracted significant interest, with applications to tasks like sequence labeling [24, 33, 57] or text classification ==[41, 70]==. // The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model [33].	b	Improving Language Understanding by Generative Pre-Training	[41, 70]	['[41] K. Nigam, A. McCallum, and T. Mitchell. Semi-supervised text classification using em. Semi-Supervised Learning, pages 33–56, 2006.', '[70] X. Zhu. Semi-supervised learning literature survey. 2005.']
1622	670	[767]	This paradigm has attracted significant interest, with applications to tasks like sequence labeling [24, 33, 57] or text classification [41, 70]. // The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model ==[33]==. // Over the last few years, researchers have demonstrated the benefits of using word embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve performance on a variety of tasks [8, 11, 26, 45].	b	Improving Language Understanding by Generative Pre-Training	[33]	['[33] P. Liang. Semi-supervised learning for natural language. PhD thesis, Massachusetts Institute of Technology, 2005.']
1623	670	[306, 36, 167]	The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model [33]. // Over the last few years, researchers have demonstrated the benefits of using word embeddings ==[11, 39, 42]==, which are trained on unlabeled corpora, to improve performance on a variety of tasks [8, 11, 26, 45]. // These approaches, however, mainly transfer word-level information, whereas we aim to capture higher-level semantics.	b	Improving Language Understanding by Generative Pre-Training	[11, 39, 42]	['[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537, 2011.', '[39] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.', '[42] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.']
1624	670	[679, 306, 307, 727]	The earliest approaches used unlabeled data to compute word-level or phrase-level statistics, which were then used as features in a supervised model [33]. // Over the last few years, researchers have demonstrated the benefits of using word embeddings [11, 39, 42], which are trained on unlabeled corpora, to improve performance on a variety of tasks ==[8, 11, 26, 45]==. // These approaches, however, mainly transfer word-level information, whereas we aim to capture higher-level semantics.	b	Improving Language Understanding by Generative Pre-Training	[8, 11, 26, 45]	['[8] D. Chen and C. Manning. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 740–750, 2014.', '[11] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537, 2011.', '[26] Y. Kim. Convolutional neural networks for sentence classification. EMNLP, 2014.', '[45] Y. Qi, D. S. Sachan, M. Felix, S. J. Padmanabhan, and G. Neubig. When and why are pre-trained word embeddings useful for neural machine translation? NAACL, 2018.']
1625	670	[779, 64, 668, 663, 728, 675, 744, 709]	Recent approaches have investigated learning and utilizing more than word-level semantics from unlabeled data. // Phrase-level or sentence-level embeddings, which can be trained using an unlabeled corpus, have been used to encode text into suitable vector representations for various target tasks ==[28, 32, 1, 36, 22, 12, 56, 31]==. // Unsupervised pre-training Unsupervised pre-training is a special case of semi-supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective.	b	Improving Language Understanding by Generative Pre-Training	[28, 32, 1, 36, 22, 12, 56, 31]	['[28] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. Skip-thought vectors. In Advances in neural information processing systems, pages 3294–3302, 2015.', '[32] Q. Le and T. Mikolov. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188–1196, 2014.', '[1] S. Arora, Y. Liang, and T. Ma. A simple but tough-to-beat baseline for sentence embeddings. 2016.', '[36] L. Logeswaran and H. Lee. An efficient framework for learning sentence representations. ICLR, 2018.', '[22] Y. Jernite, S. R. Bowman, and D. Sontag. Discourse-based objectives for fast unsupervised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017.', '[12] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes. Supervised learning of universal sentence representations from natural language inference data. EMNLP, 2017.', '[56] S. Subramanian, A. Trischler, Y. Bengio, and C. J. Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.', '[31] G. Lample, L. Denoyer, and M. Ranzato. Unsupervised machine translation using monolingual corpora only. ICLR, 2018.']
1626	670	[478, 689, 337]	Unsupervised pre-training Unsupervised pre-training is a special case of semi-supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective. // Early works explored the use of the technique in image classification ==[20, 49, 63]== and regression tasks [3]. // Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks.	b	Improving Language Understanding by Generative Pre-Training	[20, 49, 63]	['[20] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006.', '[49] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efficient learning of sparse representations with an energy-based model. In Advances in neural information processing systems, pages 1137–1144, 2007.', '[63] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM, 2008.']
1627	670	[347]	Unsupervised pre-training Unsupervised pre-training is a special case of semi-supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective. // Early works explored the use of the technique in image classification [20, 49, 63] and regression tasks ==[3]==. // Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks.	b	Improving Language Understanding by Generative Pre-Training	[3]	['[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pages 153–160, 2007.']
1628	670	[714]	Early works explored the use of the technique in image classification [20, 49, 63] and regression tasks [3]. // Subsequent research ==[15]== demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. // In recent work, the method has been used to help train deep neural networks on various tasks like image classification [69], speech recognition [68], entity disambiguation [17] and machine translation [48].	h+	Improving Language Understanding by Generative Pre-Training	[15]	['[15] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625–660, 2010.']
1629	670	[667]	Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. // In recent work, the method has been used to help train deep neural networks on various tasks like image classification ==[69]==, speech recognition [68], entity disambiguation [17] and machine translation [48]. // The closest line of work to ours involves pre-training a neural network using a language modeling objective and then fine-tuning it on a target task with supervision.	h+	Improving Language Understanding by Generative Pre-Training	[69]	['[69] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In CVPR, volume 1, page 6, 2017.']
1630	670	[752]	Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. // In recent work, the method has been used to help train deep neural networks on various tasks like image classification [69], speech recognition ==[68]==, entity disambiguation [17] and machine translation [48]. // The closest line of work to ours involves pre-training a neural network using a language modeling objective and then fine-tuning it on a target task with supervision.	h+	Improving Language Understanding by Generative Pre-Training	[68]	['[68] D. Yu, L. Deng, and G. Dahl. Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.']
1631	670	[607]	Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. // In recent work, the method has been used to help train deep neural networks on various tasks like image classification [69], speech recognition [68], entity disambiguation ==[17]== and machine translation [48]. // The closest line of work to ours involves pre-training a neural network using a language modeling objective and then fine-tuning it on a target task with supervision.	h+	Improving Language Understanding by Generative Pre-Training	[17]	['[17] Z. He, S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang. Learning entity representation for entity disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 30–34, 2013.']
1632	670	[708]	Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. // In recent work, the method has been used to help train deep neural networks on various tasks like image classification [69], speech recognition [68], entity disambiguation [17] and machine translation ==[48]==. // The closest line of work to ours involves pre-training a neural network using a language modeling objective and then fine-tuning it on a target task with supervision.	h+	Improving Language Understanding by Generative Pre-Training	[48]	['[48] P. Ramachandran, P. J. Liu, and Q. V. Le. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016.']
1633	670	[636]	// Dai et al. ==[13]== and Howard and Ruder [21] follow this method to improve text classification. // However, although the pre-training phase helps capture some linguistic information, their usage of LSTM models restricts their prediction ability to a short range.	h-	Improving Language Understanding by Generative Pre-Training	[13]	['[13] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3079–3087, 2015.']
1634	670	[732]	 // Dai et al. [13] and Howard and Ruder ==[21]== follow this method to improve text classification. // However, although the pre-training phase helps capture some linguistic information, their usage of LSTM models restricts their prediction ability to a short range.	h-	Improving Language Understanding by Generative Pre-Training	[21]	['[21] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. Association for Computational Linguistics (ACL), 2018.']
1635	670	[625]	Early work by Collobert and Weston [10] used a wide variety of auxiliary NLP tasks such as POS tagging, chunking, named entity recognition, and language modeling to improve semantic role labeling. // More recently, Rei ==[50]== added an auxiliary language modeling objective to their target task objective and demonstrated performance gains on sequence labeling tasks. // Our experiments also use an auxiliary objective, but as we show, unsupervised pre-training already learns several linguistic aspects relevant to target tasks.	ho	Improving Language Understanding by Generative Pre-Training	[50]	['[50] M. Rei. Semi-supervised multitask learning for sequence labeling. ACL, 2017.']
1636	670	[773]	These parameters are trained using stochastic gradient descent [51]. // In our experiments, we use a multi-layer Transformer decoder ==[34]== for the language model, which is a variant of the transformer [62]. // This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens: h0 = UWe + Wp hl = transformer_block(hl−1)∀i ∈ [1, n] P(u) = softmax(hnWT e ) (2) where U = (u−k, .	ho	Improving Language Understanding by Generative Pre-Training	[34]	['[34] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and N. Shazeer. Generating wikipedia by summarizing long sequences. ICLR, 2018.']
1637	670	[707]	These parameters are trained using stochastic gradient descent [51]. // In our experiments, we use a multi-layer Transformer decoder [34] for the language model, which is a variant of the transformer ==[62]==. // This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens: h0 = UWe + Wp hl = transformer_block(hl−1)∀i ∈ [1, n] P(u) = softmax(hnWT e ) (2) where U = (u−k, .	b	Improving Language Understanding by Generative Pre-Training	[62]	['[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010, 2017.']
1638	670	[599]	Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks. // Previous work proposed learning task specific architectures on top of transferred representations ==[44]==. // Such an approach re-introduces a significant amount of task-specific customization and does not use transfer learning for these additional architectural components.	r-	Improving Language Understanding by Generative Pre-Training	[44]	['[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. NAACL, 2018.']
1639	670	[611]	Such an approach re-introduces a significant amount of task-specific customization and does not use transfer learning for these additional architectural components. // Instead, we use a traversal-style approach ==[52]==, where we convert structured inputs into an ordered sequence that our pre-trained model can process. // These input transformations allow us to avoid making extensive changes to the architecture across tasks.	ho	Improving Language Understanding by Generative Pre-Training	[52]	['[52] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Kocisk ˇ y, and P. Blunsom. Reasoning about entailment ` with neural attention. arXiv preprint arXiv:1509.06664, 2015.']
1640	670	[595]	Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers. // Unsupervised pre-training We use the BooksCorpus dataset ==[71]== for training the language model. // It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance.	ho	Improving Language Understanding by Generative Pre-Training	[71]	['[71] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27, 2015.']
1641	670	[599]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo ==[44]==, is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[44]	['[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. NAACL, 2018.']
1642	670	[630]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI ==[5]==, MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[5]	['[5] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural language inference. EMNLP, 2015.']
1643	670	[601]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI ==[66]==, Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[66]	['[66] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. NAACL, 2018.']
1644	670	[701]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI ==[64]==, RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[64]	['[64] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.']
1645	670	[644]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE ==[4]==, SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[4]	['[4] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo. The fifth pascal recognizing textual entailment challenge. In TAC, 2009.']
1646	670	[753]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail ==[25]== Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[25]	['[25] T. Khot, A. Sabharwal, and P. Clark. Scitail: A textual entailment dataset from science question answering. In Proceedings of AAAI, 2018.']
1647	670	[739]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE ==[30]==, Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[30]	['[30] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. Race: Large-scale reading comprehension dataset from examinations. EMNLP, 2017.']
1648	670	[730]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze ==[40]== Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[40]	['[40] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46–51, 2017.']
1649	670	[731]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus ==[14]==, Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[14]	['[14] W. B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.']
1650	670	[588]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs ==[9]==, STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[9]	['[9] Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. https://data.quora.com/First-QuoraDataset-Release-Question-Pairs, 2018.']
1651	670	[693]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark ==[6]== Classification Stanford Sentiment Treebank-2 [54], CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[6]	['[6] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017.']
1652	670	[471]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 ==[54]==, CoLA [65] but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[54]	['[54] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.']
1653	670	[603]	Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. // An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size with Natural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25] Question Answering RACE [30], Story Cloze [40] Sentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6] Classification Stanford Sentiment Treebank-2 [54], CoLA ==[65]== but is shuffled at a sentence level - destroying long-range structure. // Our language model achieves a very low token level perplexity of 18.4 on this corpus.	b	Improving Language Understanding by Generative Pre-Training	[65]	['[65] A. Warstadt, A. Singh, and S. R. Bowman. Corpus of linguistic acceptability. http://nyu-mll.github.io/cola, 2018.']
1654	670	[707]	Our language model achieves a very low token level perplexity of 18.4 on this corpus. // Our model largely follows the original transformer work ==[62]==. // We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads).	ho	Improving Language Understanding by Generative Pre-Training	[62]	['[62] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010, 2017.']
1655	670	[413]	For the position-wise feed-forward networks, we used 3072 dimensional inner states. // We used the Adam optimization scheme ==[27]== with a max learning rate of 2.5e-4. // The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule.	ho	Improving Language Understanding by Generative Pre-Training	[27]	['[27] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.']
1656	670	[604]	Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient. // We used a bytepair encoding (BPE) vocabulary with 40,000 merges ==[53]== and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. // We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights.	ho	Improving Language Understanding by Generative Pre-Training	[53]	['[53] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.']
1657	670	[619]	We also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights. // For the activation function, we used the Gaussian Error Linear Unit (GELU) ==[18]==. // We used learned position embeddings instead of the sinusoidal version proposed in the original work.	ho	Improving Language Understanding by Generative Pre-Training	[18]	['[18] D. Hendrycks and K. Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. arXiv preprint arXiv:1606.08415, 2016.']
1658	670	[701]	4.2 Supervised fine-tuning We perform experiments on a variety of supervised tasks including natural language inference, question answering, semantic similarity, and text classification. // Some of these tasks are available as part of the recently released GLUE multi-task benchmark ==[64]==, which we make use of. // Figure 1 provides an overview of all the tasks and datasets.	ho	Improving Language Understanding by Generative Pre-Training	[64]	['[64] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.']
1659	670	[609]	We use the recently released RACE dataset [30], consisting of English passages with associated questions from middle and high school exams. // This corpus has been shown to contain more reasoning type questions that other datasets like CNN ==[19]== or SQuaD [47], providing the perfect evaluation for our model which is trained to handle long-range contexts. // In addition, we evaluate on the Story Cloze Test [40], which involves selecting the correct ending to multi-sentence stories from two options.	b	Improving Language Understanding by Generative Pre-Training	[19]	['[19] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1693–1701, 2015.']
1660	670	[591]	We use the recently released RACE dataset [30], consisting of English passages with associated questions from middle and high school exams. // This corpus has been shown to contain more reasoning type questions that other datasets like CNN [19] or SQuaD ==[47]==, providing the perfect evaluation for our model which is trained to handle long-range contexts. // In addition, we evaluate on the Story Cloze Test [40], which involves selecting the correct ending to multi-sentence stories from two options.	b	Improving Language Understanding by Generative Pre-Training	[47]	['[47] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. EMNLP, 2016.']
1661	670	[603]	Classification Finally, we also evaluate on two different text classification tasks. // The Corpus of Linguistic Acceptability (CoLA) ==[65]== contains expert judgements on whether a sentence is grammatical or not, and tests the innate linguistic bias of trained models. // The Stanford Sentiment Treebank (SST-2) [54], on the other hand, is a standard binary classification task.	b	Improving Language Understanding by Generative Pre-Training	[65]	['[65] A. Warstadt, A. Singh, and S. R. Bowman. Corpus of linguistic acceptability. http://nyu-mll.github.io/cola, 2018.']
1662	670	[471]	The Corpus of Linguistic Acceptability (CoLA) [65] contains expert judgements on whether a sentence is grammatical or not, and tests the innate linguistic bias of trained models. // The Stanford Sentiment Treebank (SST-2) ==[54]==, on the other hand, is a standard binary classification task. // Our model obtains an score of 45.4 on CoLA, which is an especially big jump over the previous best result of 35.0, showcasing the innate linguistic bias learned by our model.	b	Improving Language Understanding by Generative Pre-Training	[54]	['[54] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.']
1663	599	[36, 167]	We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. // Pre-trained word representations ==[33, 38]== are a key component in many neural language understanding models. // However, learning high quality representations can be challenging.	b	Deep contextualized word representations	[33, 38]	['[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. ', '[38] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.']
1664	599	[763, 721]	For this reason, we call them ELMo (Embeddings from Language Models) representations. // Unlike previous approaches for learning contextualized word vectors ==[39, 29]==, ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. // More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.	r-	Deep contextualized word representations	[39, 29]	['[39] Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.', '[29] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS 2017.']
1665	599	[721]	The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. // For tasks where direct comparisons are possible, ELMo outperforms CoVe ==[29]==, which computes contextualized representations using a neural machine translation encoder. // Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM.	r-	Deep contextualized word representations	[29]	['[29] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS 2017.']
1666	599	[600, 36, 167]	Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. // Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors ==[52, 33, 38]== are a standard component of most state-ofthe-art NLP architectures, including for question answering, textual entailment and semantic role labeling. // However, these approaches for learning word vectors only allow a single contextindependent representation for each word.	r-	Deep contextualized word representations	[52, 33, 38]	['[52] Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL.', '[33] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS. ', '[38] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.']
1667	599	[649, 626]	Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. // Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors are a standard component of most state-ofthe-art NLP architectures, including for question answering ==[26, 45]==, textual entailment and semantic role labeling. // However, these approaches for learning word vectors only allow a single contextindependent representation for each word.	r-	Deep contextualized word representations	[26, 45]	['[26] Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2017. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556 .', '[45] Prajit Ramachandran, Peter Liu, and Quoc Le. 2017. Improving sequence to sequence learning with unlabeled data. In EMNLP.']
1668	599	[647]	Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. // Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors are a standard component of most state-ofthe-art NLP architectures, including for question answering, textual entailment ==[5]== and semantic role labeling. // However, these approaches for learning word vectors only allow a single contextindependent representation for each word.	r-	Deep contextualized word representations	[5]	['[5] Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In ACL.']
1669	599	[631, 633]	Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM. Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems. // Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors are a standard component of most state-ofthe-art NLP architectures, including for question answering, textual entailment and semantic role labeling ==[16, 24]==. // However, these approaches for learning word vectors only allow a single contextindependent representation for each word.	r-	Deep contextualized word representations	[16, 24]	['[16] Luheng He, Kenton Lee, Mike Lewis, and Luke S. Zettlemoyer. 2017. Deep semantic role labeling: What works and what’s next. In ACL.', '[24] Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In EMNLP.']
1670	599	[651]	However, these approaches for learning word vectors only allow a single contextindependent representation for each word. // Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ==[2]== or learning separate vectors for each word sense. // Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes.	h+	Deep contextualized word representations	[2]	['[2] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL 5:135–146.']
1671	599	[678]	Other recent work has also focused on learning context-dependent representations. // context2vec ==[30]== uses a bidirectional Long Short Term Memory (LSTM) to encode the context around a pivot word. // Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system or an unsupervised language model (Peters et al., 2017).	b	Deep contextualized word representations	[30]	['[30] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional lstm. In CoNLL.']
1672	599	[721]	context2vec uses a bidirectional Long Short Term Memory (LSTM) to encode the context around a pivot word. // Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system ==[29]== or an unsupervised language model. // Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora.	b	Deep contextualized word representations	[29]	['[29] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS 2017.']
1673	599	[763]	context2vec uses a bidirectional Long Short Term Memory (LSTM) to encode the context around a pivot word. // Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system or an unsupervised language model ==[39]==. // Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora.	b	Deep contextualized word representations	[39]	['[39] Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.']
1674	599	[677]	Previous work has also shown that different layers of deep biRNNs encode different types of information. // For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing ==[15]== or CCG super tagging. // In an RNN-based encoder-decoder machine translation system, Belinkov et al.	b	Deep contextualized word representations	[15]	['[15] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In EMNLP 2017.']
1675	599	[678]	It showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer. // Finally, the top layer of an LSTM for encoding word context ==[30]== has been shown to learn representations of word sense. // We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision.	b	Deep contextualized word representations	[30]	['[30] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional lstm. In CoNLL.']
1676	599	[167]	In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model. // ELMo Embeddings from Language Models Unlike most widely used word embeddings ==[38]==, ELMo word representations are functions of the entire input sentence, as described in this section. // They are computed on top of two-layer biLMs with character convolutions (Sec.	r-	Deep contextualized word representations	[38]	['[38] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.']
1677	599	[745, 699, 748]	// Recent state-of-the-art neural language models ==[19, 31, 32]== compute a context-independent token representation via token embeddings or a CNN over characters then pass it through L layers of forward LSTMs. // At each position k, each LSTM layer outputs a context-dependent representation.	b	Deep contextualized word representations	[19, 31, 32]	['[19] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam ´ Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. CoRR abs/1602.02410. Rafal Jozefowicz, Wojciech Zaremba, and Ilya ´ Sutskever. 2015. An empirical exploration of recurrent network architectures. In ICML.', '[31] Gabor Melis, Chris Dyer, and Phil Blunsom. 2017. On the state of the art of evaluation in neural language models. CoRR abs/1707.05589. ', '[32] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. CoRR abs/1708.02182.']
1678	599	[763]	For inclusion in a downstream model, ELMo collapses all layers in R into a single vector, ELMo. // In the simplest case, ELMo just selects the top layer as in TagLM ==[39]== and CoVe. // More generally, we compute a task specific weighting of all biLM layers.	ho	Deep contextualized word representations	[39]	['[39] Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.']
1679	599	[768, 413, 723, 704, 594]	γ is of practical importance to aid the optimization process (see supplemental material for details). // Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization ==[1, 21, 23, 54, 58]== to each biLM layer before weighting. // Using biLM for supervised NLP tasks Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model.	b	Deep contextualized word representations	[1, 21, 23, 54, 58]	['[1] Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR abs/1607.06450. Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James R. Glass. 2017. What do neural machine translation models learn about morphology? In ACL.', '[21] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, Ishaan Gulrajani James Bradbury, Victor Zhong, Romain Paulus, and Richard Socher. 2016.', '[23] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL-HLT.', '[54] John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. In EMNLP.', '[58] Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text classification improved by integrating bidirectional lstm with twodimensional max pooling. In COLING.']
1680	599	[681]	Where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs. // Finally, we found it beneficial to add a moderate amount of dropout to ELMo ==[50]== and in some cases to regularize the ELMo weights by adding λkwk 2 2 to the loss. // This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.	b	Deep contextualized word representations	[50]	['[50] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15:1929–1958.']
1681	599	[717]	In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary. // After training for 10 epochs on the 1B Word Benchmark ==[4]==, the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM. // Generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower.	b	Deep contextualized word representations	[4]	['[4] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH.']
1682	599	[591]	In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details. // The Stanford Question Answering Dataset (SQuAD) ==[44]== contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph. // Our baseline model is an improved version of the Bidirectional Attention Flow model in Seo et al.	b	Deep contextualized word representations	[44]	['[44] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP.']
1683	599	[775]	Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. // A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% ==[14]==. // Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering “Who did what to whom”.	r-	Deep contextualized word representations	[14]	['[14] Yichen Gong, Heng Luo, and Jian Zhang. 2018. Natural language inference over interaction space. In ICLR.']
1684	599	[718]	Named entity extraction The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). // Following recent state-of-the-art systems, the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss ==[22]==. // 	b	Deep contextualized word representations	[22]	['[22] Ask me anything: Dynamic memory networks for natural language processing. In ICML.John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.']
1685	599	[635]	// To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus ==[34]==, and then take the average representation for each sense. // At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training.	ho	Deep contextualized word representations	[34]	['[34] George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In HLT.']
1686	167	[669]	Semantic vector space models of language represent each word with a real-valued vector. // These vectors can be used as features in a variety of applications, such as information retrieval, document classification, question answering ==[23]==, named entity recognition, and parsing. // Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations.	b	GloVe- Global Vectors for Word Representation	[23]	['[23] Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the SIGIR Conference on Research and Development in Informaion Retrieval.']
1687	167	[759]	Semantic vector space models of language represent each word with a real-valued vector. // These vectors can be used as features in a variety of applications, such as information retrieval, document classification, question answering, named entity recognition ==[25]==, and parsing. // Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations.	b	GloVe- Global Vectors for Word Representation	[25]	['[25] Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.']
1688	167	[725, 712]	Semantic vector space models of language represent each word with a real-valued vector. // These vectors can be used as features in a variety of applications, such as information retrieval, document classification, question answering, named entity recognition, and parsing ==[13, 22]==. // Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations.	b	GloVe- Global Vectors for Word Representation	[13, 22]	['[13] Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. CoNLL-2013.', '[22] Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. In ACL. 1542']
1689	167	[36]	This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations. // The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) and 2) local context window methods, such as the skip-gram model of ==[15]==. // 	b	GloVe- Global Vectors for Word Representation	[15]	['[15] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119. ']
1690	167	[711]	A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. // A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method ==[19]==, in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. // An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval.	h+	GloVe- Global Vectors for Word Representation	[19]	['[19] Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut. 2006. An improvedmodel of semantic similarity based on lexical co-occurence. Communications of the ACM,8:627–633. ']
1691	167	[35]	We found that α = 3/4 gives a modest improvement over a linear version with α = 1. // Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in ==[14]==. // Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models.	h+	GloVe- Global Vectors for Word Representation	[14]	['[14] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In ICLR Workshop Papers.']
1692	167	[754]	This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words relationship to one another. // For all our experiments, we set xmax = 100, α = 3/4, and train the model using AdaGrad ==[8]==, stochastically sampling nonzero elements from X, with initial learning rate of 0.05. // We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more details about the convergence rate).	ho	GloVe- Global Vectors for Word Representation	[8]	['[8] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12. ']
1693	167	[695]	When X is symmetric, W are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently. // On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results ==[4]==. // To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.	b	GloVe- Global Vectors for Word Representation	[4]	['[4] Dan C. Ciresan, Alessandro Giusti, Luca M. Gambardella, and Jurgen Schmidhuber. 2012. Deep ¨ neural networks segment neuronal membranes in electron microscopy images. In NIPS, pages 2852–2860.']
1694	36	[778]	Distributed representations of words in a vector space help learning algorithms to achieve better performance in natural language processing tasks by grouping similar words. // One of the earliest use of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams ==[13]==. // This idea has since been applied to statistical language modeling with considerable success [1].	h+	Distributed Representations of Words and Phrases and their Compositionality	[13]	['[13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by backpropagating errors. Nature, 323(6088):533–536, 1986.']
1695	36	[147]	One of the earliest use of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. // This idea has since been applied to statistical language modeling with considerable success ==[1]==. // The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].	h+	Distributed Representations of Words and Phrases and their Compositionality	[1]	['[1] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155, 2003.']
1696	36	[657, 665]	This idea has since been applied to statistical language modeling with considerable success [1]. // The follow up work includes applications to automatic speech recognition and machine translation ==[14, 7]==, and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9]. // Recently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data.	b	Distributed Representations of Words and Phrases and their Compositionality	[14, 7]	['[14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.', '[7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno University of Technology, 2012.']
1697	36	[125, 781, 622, 676, 628, 776, 126]	This idea has since been applied to statistical language modeling with considerable success [1]. // The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks ==[2, 20, 15, 3, 18, 19, 9]==. // Recently, Mikolov et al. [8] introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data. 	b	Distributed Representations of Words and Phrases and their Compositionality	[2, 20, 15, 3, 18, 19, 9]	['[2] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2008.', '[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annotation. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Three, pages 2764–2770. AAAI Press, 2011.', '[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, 2011.', '[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML, 513–520, 2011.', '[18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In Journal of Artificial Intelligence Research, 37:141-188, 2010.', '[19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. In Transactions of the Association for Computational Linguistics (TACL), 353–366, 2013.', '[9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT, 2013.']
1698	36	[35]	The follow up work includes applications to automatic speech recognition and machine translation [14, 7], and a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9]. // Recently, Mikolov et al. ==[8]== introduced the Skip-gram model, an efficient method for learning highquality vector representations of words from large amounts of unstructured text data. // Unlike most of the previously used neural network architectures for learning word vectors, training of the Skipgram model (see Figure 1) does not involve dense matrix multiplications.	b	Distributed Representations of Words and Phrases and their Compositionality	[8]	['[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop, 2013.']
1699	36	[91]	We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words. // In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) ==[4]== for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work [8]. // Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words.	ho	Distributed Representations of Words and Phrases and their Compositionality	[4]	['[4] Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361, 2012.']
1700	36	[35]	We show that subsampling of frequent words during training results in a significant speedup (around 2x - 10x), and improves accuracy of the representations of less frequent words. // In addition, we present a simplified variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results in faster training and better vector representations for frequent words, compared to more complex hierarchical softmax that was used in the prior work ==[8]==. // Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words.	ho	Distributed Representations of Words and Phrases and their Compositionality	[8]	['[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop, 2013.']
1701	36	[622]	Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive. // Other techniques that aim to represent meaning of sentences by composing the word vectors, such as the recursive autoencoders ==[15]==, would also benefit from using phrase vectors instead of the word vectors. // The extension from word based to phrase based models is relatively simple.	b	Distributed Representations of Words and Phrases and their Compositionality	[15]	['[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, 2011.']
1702	36	[90]	Hierarchical Softmax A computationally efficient approximation of the full softmax is the hierarchical softmax. // In the context of neural network language models, it was first introduced by Morin and Bengio ==[12]==. // The main advantage is that instead of evaluating W output nodes in the neural network to obtain the probability distribution, it is needed to evaluate only about log2 (W) nodes.	h+	Distributed Representations of Words and Phrases and their Compositionality	[12]	['[12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252, 2005.']
1703	36	[89]	The structure of the tree used by the hierarchical softmax has a considerable effect on the performance. // Mnih and Hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy ==[10]==. // In our work we use a binary Huffman tree, as it assigns short codes to the frequent words which results in fast training.	h+	Distributed Representations of Words and Phrases and their Compositionality	[10]	['[10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081–1088, 2009.']
1704	36	[91]	It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models [5, 8]. // Negative Sampling An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was introduced by Gutmann and Hyvarinen ==[4]== and applied to language modeling by Mnih and Teh [11]. // NCE posits that a good model should be able to differentiate data from noise by means of logistic regression.	h+	Distributed Representations of Words and Phrases and their Compositionality	[4]	['[4] Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361, 2012.']
1705	36	[716]	It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models [5, 8]. // Negative Sampling An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was introduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh ==[11]==. // NCE posits that a good model should be able to differentiate data from noise by means of logistic regression.	h+	Distributed Representations of Words and Phrases and their Compositionality	[11]	['[11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.']
1706	36	[125]	NCE posits that a good model should be able to differentiate data from noise by means of logistic regression. // This is similar to hinge loss used by Collobert and Weston ==[2]== who trained the models by ranking the data above noise. // While NCE can be shown to approximately maximize the log probability of the softmax, the Skipgram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality.	h+	Distributed Representations of Words and Phrases and their Compositionality	[2]	['[2] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2008.']
1707	36	[35]	// It can be argued that the linearity of the skip-gram model makes its vectors more suitable for such linear analogical reasoning, but the results of Mikolov et al. ==[8]== also show that the vectors learned by the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this task significantly as the amount of the training data increases, suggesting that non-linear models also have a preference for a linear structure of the word representations. // Learning Phrases As discussed earlier, many phrases have a meaning that is not a simple composition of the meanings of its individual words.	ho	Distributed Representations of Words and Phrases and their Compositionality	[8]	['[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop, 2013.']
1708	36	[35]	We show how to train distributed representations of words and phrases with the Skip-gram model and demonstrate that these representations exhibit linear structure that makes precise analogical reasoning possible. // The techniques introduced in this paper can be used also for training the continuous bag-of-words model introduced in ==[8]==. // We successfully trained models on several orders of magnitude more data than the previously published models, thanks to the computationally efficient model architecture.	ho	Distributed Representations of Words and Phrases and their Compositionality	[8]	['[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop, 2013.']
1709	36	[690]	Combination of these two approaches gives a powerful yet simple way how to represent longer pieces of text, while having minimal computational complexity. // Our work can thus be seen as complementary to the existing approach that attempts to represent phrases using recursive matrix-vector operations ==[16]==. // We made the code for training the word and phrase vectors based on the techniques described in this paper available as an open-source project4 .	ho	Distributed Representations of Words and Phrases and their Compositionality	[16]	['[16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality Through Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2012.']
1710	733	[722]	Language models (LMs) play a crucial role in many speech and language processing tasks, among others speech recognition, machine translation and optical character recognition. // The current state of the art are recurrent neural network (RNN) based LMs, and more specifically long short-term memory models (LSTM) LMs ==[24]== and their variants (e.g. gated recurrent units (GRU). // LSTMs and GRUs are usually very similar in performance, with GRU models often even outperforming LSTM models despite the fact that they have less parameters to train. 	b	Character-Word LSTM Language Models	[24]	['[24] Martin Sundermeyer, Ralf Schluter, and Hermann Ney. ¨ 2012. LSTM Neural Networks for Language Modeling. Proceedings Interspeech, pages 1724–1734.']
1711	733	[624]	 // Kim et al. achieve state-of-the-art results in language modeling for several languages by combining a character-level CNN with highway ==[23]== and LSTM layers. // However, the major improvement is achieved by adding the highway layers: for a small model size, the purely character-level model without highway layers does not perform better than the word-level model, even though the character model has two hidden layers of 300 LSTM units each and is compared to a word model of two hidden layers of only 200 units (in order to keep the number of parameters similar).	r-	Character-Word LSTM Language Models	[23]	['[23] Rupesh K. Srivastava, Klaus Greff, and Jurgen Schmid- ¨ huber. 2015. Training Very Deep Networks. Neural Information Processing Systems Conference (NIPS), pages 2377–2385.']
1712	733	[684]	We can conclude that in various NLP tasks, characters have recently been introduced in several different manners. // However, the models investigated in related work are either not tested on a competitive baseline or do not perform better than our models ==[14]==. // In this paper, we introduce a new and straightforward manner to incorporate characters in a LM that (as far as we know) has not been investigated before.	r-	Character-Word LSTM Language Models	[14]	['[14] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. 2016. Character-Aware Neural Language Models. Proc. Conference on Artificial Intelligence (AAAI), pages 2741–2749. ']
1713	733	[703]	The total number of epochs is fixed to 13/39. // During training, 25% of the neurons are dropped ==[22]== for the small model and 50% for the large model. // The weights are randomly initialized to small values (between -0.1 and 0.1 for the small model and between -0.05 and 0.05 for the large model) based on a uniform distribution.	b	Character-Word LSTM Language Models	[22]	['[22] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15:1929–1958. ']
1714	733	[390]	Enabling the comparison between different models. // We adopt the same pre-processing as used by previous work ==[18]== to facilitate comparison, which implies that the dataset contains only lowercase characters (the size of the character vocabulary is 48). // Unknown words are mapped to hunki, but since we do not have the original text, we cannot use the characters of the unknown words for PTB.	ho	Character-Word LSTM Language Models	[18]	['[18] Toma´s Mikolov, Martin Karafi ˘ at, Luk ´ a´s Burget, Jan ˘ Cernock ˘ y, and Sanjeev Khudanpur. 2010. Recurrent ´ neural network based language model. ProceedingsInterspeech, pages 1045–1048.']
1715	777	[590, 720]	Although the traditional approach for language modeling is good at capturing statistical co-occurrences of entities when they are observed frequently in a corpus (e.g., words like verbs, pronouns, and prepositions), it is in general limited in its ability to encode or decode knowledge, which are often represented by named entities such as person names, place names, years, etc. // In fact, it has been shown that the traditional approaches trained with a very large corpus have some ability to encode/decode knowledge to some extent ==[40, 34]==. // However, we claim that simply feeding a larger corpus into a bigger model hardly results in a good knowledge language model.	r-	A Neural Knowledge Language Model	[40, 34]	['[40] Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint arXiv:1506.05869,2015.', '[34] Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau.Building end-to-end dialogue systems using generative hierarchical neural networks. 30th AAAIConference on Artificial Intelligence, 2015.']
1716	777	[652]	However, we claim that simply feeding a larger corpus into a bigger model hardly results in a good knowledge language model. // The primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge-related words ==[16]==. // In particular, for applications such as question answering [21, 42, 8] and dialogue modeling [40, 34, 41], these words are of our main interest.	ho	A Neural Knowledge Language Model	[16]	['[16] Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing the unknown words. ACL 2016, 2016.']
1717	777	[740, 762, 655]	The primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge-related words [16]. // In particular, for applications such as question answering ==[21, 42, 8]== and dialogue modeling [40, 34, 41], these words are of our main interest. // Specifically, in the recurrent neural network language model (RNNLM) [26] the computational complexity is linearly dependent on the number of vocabulary words.	b	A Neural Knowledge Language Model	[21, 42, 8]	['[21] Mohit Iyyer, Jordan L Boyd-Graber, Leonardo Max Batista Claudino, Richard Socher, and Hal Daumé III. A neural network for factoid question answering over paragraphs. In EMNLP 2014, pages 633–644, 2014.', '[42] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-completequestion answering: A set of prerequisite toy tasks. ICLR 2016, 2016.', '[8] Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.']
1718	777	[590, 720, 592]	The primary reason for this is the difficulty in learning good representations for rare or unknown words because these are a majority of the knowledge-related words [16]. // In particular, for applications such as question answering [21, 42, 8] and dialogue modeling ==[40, 34, 41]==, these words are of our main interest. // Specifically, in the recurrent neural network language model (RNNLM) [26] the computational complexity is linearly dependent on the number of vocabulary words.	b	A Neural Knowledge Language Model	[40, 34, 41]	['[40] Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint arXiv:1506.05869,2015.', '[34] Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau.Building end-to-end dialogue systems using generative hierarchical neural networks. 30th AAAIConference on Artificial Intelligence, 2015.', '[41] Jason Weston. Dialog-based language learning. arXiv preprint arXiv:1604.06045, 2016.']
1719	777	[674]	In particular, for applications such as question answering [21, 42, 8] and dialogue modeling [40, 34, 41], these words are of our main interest. // Specifically, in the recurrent neural network language model (RNNLM) ==[26]== the computational complexity is linearly dependent on the number of vocabulary words. // Thus, including all words of a language is computationally prohibitive.	b	A Neural Knowledge Language Model	[26]	['[26] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Re- `current neural network based language model. In INTERSPEECH 2010, volume 2, page 3,2010.']
1720	777	[750]	In this paper, we propose a Neural Knowledge Language Model (NKLM) as a step towards addressing the limitations of traditional language modeling when it comes to exploiting factual knowledge. // In particular, we incorporate symbolic knowledge information from knowledge graphs ==[32]== into the RNNLM. // A knowledge graph (KG) is a collection of facts which have a form of (subject, relationship, object).	ho	A Neural Knowledge Language Model	[32]	['[32] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review ofrelational machine learning for knowledge graphs: From multi-relational link prediction toautomated knowledge graph construction. arXiv preprint arXiv:1503.00759, 2015.']
1721	777	[760, 613]	KGs are managed and updated in a similar way that Wikipedia pages are managed to date. // The KG embedding methods ==[10, 9]==, which is an extension of word embedding techniques [4] from neural language models, provide distributed representations for the entities in the KG. // In addition, the graph can be traversed for reasoning [15].	b	A Neural Knowledge Language Model	[10, 9]	['[10] Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structuredembeddings of knowledge bases. In AAAI 2011, 2011.', '[9] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, pages 2787–2795, 2013.']
1722	777	[671]	KGs are managed and updated in a similar way that Wikipedia pages are managed to date. // The KG embedding methods [10, 9], which is an extension of word embedding techniques ==[4]== from neural language models, provide distributed representations for the entities in the KG. // In addition, the graph can be traversed for reasoning [15].	b	A Neural Knowledge Language Model	[4]	['[4] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilisticlanguage model. In Journal of Machine Learning Research, 2003.']
1723	777	[761, 685]	One option is to generate a “vocabulary word” from the vocabulary softmax as is in the RNNLM. // The other option is to generate a “knowledge word” by copying a word contained in the description of the predicted fact ==[16, 14]==. // Considering that the fact description is short and often consists of out-of-vocabulary words, we predict the position of the chosen word within the fact description.	b	A Neural Knowledge Language Model	[16, 14]	['[16] Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointingthe unknown words. ACL 2016, 2016.', '[14] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. Incorporating copying mechanism insequence-to-sequence learning. CoRR, abs/1603.06393, 2016.']
1724	777	[660]	To this end, we introduce a new dataset, called WikiFacts. // For each topic in the dataset, a set of facts from the Freebase KG ==[7]== and a Wikipedia description of the same topic is provided along with the alignment information. // This alignment is done automatically by performing string matching between the fact description and the Wikipedia description.	ho	A Neural Knowledge Language Model	[7]	['[7] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: acollaboratively created graph database for structuring human knowledge. In Proceedings ofthe 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250.ACM, 2008.']
1725	777	[674, 648]	Related Work Language modeling is among the most important challenges in natural language processing and understanding. // Beyond its usage as a standalone application, it has been an indispensable component in many language/speech tasks such as speech recognition ==[26, 1]==, machine translation [17], and dialogue systems [40, 34]. // Incorporating a language model into such downstream tasks leads to remarkable performance improvements, e.g., by filtering out many grammatically correct but statistically unlikely outcomes.	b	A Neural Knowledge Language Model	[26, 1]	['[26] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Re- `current neural network based language model. In INTERSPEECH 2010, volume 2, page 3,2010.', ' [1] Ebru Arisoy, Tara N Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. Deep neuralnetwork language models. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20–28.Association for Computational Linguistics, 2012.']
1726	777	[637]	Related Work Language modeling is among the most important challenges in natural language processing and understanding. // Beyond its usage as a standalone application, it has been an indispensable component in many language/speech tasks such as speech recognition [26, 1], machine translation ==[17]==, and dialogue systems [40, 34]. // Incorporating a language model into such downstream tasks leads to remarkable performance improvements, e.g., by filtering out many grammatically correct but statistically unlikely outcomes.	b	A Neural Knowledge Language Model	[17]	['[17] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, FethiBougares, Holger Schwenk, and Yoshua Bengio. On using monolingual corpora in neuralmachine translation. arXiv preprint arXiv:1503.03535, 2015.']
1727	777	[590, 720]	Related Work Language modeling is among the most important challenges in natural language processing and understanding. // Beyond its usage as a standalone application, it has been an indispensable component in many language/speech tasks such as speech recognition [26, 1], machine translation [17], and dialogue systems ==[40, 34]==. // Incorporating a language model into such downstream tasks leads to remarkable performance improvements, e.g., by filtering out many grammatically correct but statistically unlikely outcomes.	b	A Neural Knowledge Language Model	[40, 34]	['[40] Oriol Vinyals and Quoc Le. A neural conversational model. arXiv preprint arXiv:1506.05869,2015.', '[34] Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau.Building end-to-end dialogue systems using generative hierarchical neural networks. 30th AAAIConference on Artificial Intelligence, 2015.']
1728	777	[671, 674]	Incorporating a language model into such downstream tasks leads to remarkable performance improvements, e.g., by filtering out many grammatically correct but statistically unlikely outcomes. // There have been remarkable advances in language modeling research based on neural networks ==[4, 26]==. // In particular, the RNNLMs [26] are interesting for their ability to take advantage of longer term temporal dependencies without a strong conditional independence assumption.	b	A Neural Knowledge Language Model	[4, 26]	['[4] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilisticlanguage model. In Journal of Machine Learning Research, 2003.', '[26] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Re- `current neural network based language model. In INTERSPEECH 2010, volume 2, page 3,2010.']
1729	777	[390]	There have been remarkable advances in language modeling research based on neural networks [4, 26]. // In particular, the RNNLMs ==[26]== are interesting for their ability to take advantage of longer term temporal dependencies without a strong conditional independence assumption. // We do not investigate the reasoning ability in this paper but highlight this example because explicit representation of facts would help handling such examples.	h+	A Neural Knowledge Language Model	[26]	['[26] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, volume 2, page 3,2010.']
1730	777	[389]	It is especially 1We do not investigate the reasoning ability in this paper but highlight this example because explicit representation of facts would help handling such examples. // note worthy that the RNNLM using the Long Short-Term Memory (LSTM) ==[20]== has recently advanced to the level of outperforming carefully-tuned large-scale traditional n-gram based language models [23]. // There have been many efforts to speedup the language models so that it can cover a larger vocabulary.	b	A Neural Knowledge Language Model	[20]	['[20] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,9(8):1735–1780, 1997.']
1731	777	[90, 89]	There have been many efforts to speedup the language models so that it can cover a larger vocabulary. // These methods approximate the softmax output using hierarchical softmax ==[31, 29]==, importance sampling [5, 22], noise contrastive estimation [30], etc. // However, although helpful to mitigate the computational problem, this approaches still suffer from the statistical problem due to rare or unknown words.	r-	A Neural Knowledge Language Model	[31, 29]	['[31] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. AISTATS 2005, page 246, 2005.', '[29] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. InAdvances in neural information processing systems, pages 1081–1088, 2009.']
1732	777	[764, 673]	There have been many efforts to speedup the language models so that it can cover a larger vocabulary. // These methods approximate the softmax output using hierarchical softmax [31, 29], importance sampling ==[5, 22]==, noise contrastive estimation [30], etc. // However, although helpful to mitigate the computational problem, this approaches still suffer from the statistical problem due to rare or unknown words.	r-	A Neural Knowledge Language Model	[5, 22]	['[5] Yoshua Bengio and Jean-Sebastien Senecal. Quick training of probabilistic neural nets byimportance sampling. In AISTATS 2003, 2003.', '[22] Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. ACL 2015, 2015.']
1733	777	[716]	There have been many efforts to speedup the language models so that it can cover a larger vocabulary. // These methods approximate the softmax output using hierarchical softmax [31, 29], importance sampling [5, 22], noise contrastive estimation ==[30]==, etc. // However, although helpful to mitigate the computational problem, this approaches still suffer from the statistical problem due to rare or unknown words.	r-	A Neural Knowledge Language Model	[30]	['[30] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. ICML 2012, 2012.']
1734	777	[761, 682]	Having the UNK word as the output of a generative language model is also inconvenient when the generated sentence is part of a user interface (e.g, dialogue system). // To help deal with the rare/unknown word problem, the pointer networks [39] have been adopted to implement the copy-mechanism ==[16, 14]== and applied to machine translation and text summarization. // With this approach, the (unknown) word to copy from the context is inferred from other (known) context words.	h+	A Neural Knowledge Language Model	[16, 14]	['[16] Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointingthe unknown words. ACL 2016, 2016.', '[14] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. Incorporating copying mechanism in sequence-to-sequence learning. CoRR, abs/1603.06393, 2016.']
1735	777	[672]	Context-dependent (or topic-based) language models have been studied to better capture long-term dependencies, by learning some context representation from the history. // ==[12]== modeled the topic as a latent variable and proposed an EM-based approach. // In [27], the topic features are learned by latent Dirichlet allocation (LDA) [6].	b	A Neural Knowledge Language Model	[12]	['[12] Daniel Gildea and Thomas Hofmann. Topic-based language models using em. EuroSpeech1999, 1999.']
1736	777	[698]	[12] modeled the topic as a latent variable and proposed an EM-based approach. // In ==[27]==, the topic features are learned by latent Dirichlet allocation (LDA) [6]. // Our knowledge memory is also related to the recent literature on neural networks with external memory [2, 43, 36, 13] which is applied to question answering [43, 8, 44] and reading comprehension [18, 19].	b	A Neural Knowledge Language Model	[27]	['[27] Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network languagemodel. In Spoken Language Technology Workshop (SLT), 2012 IEEE, pages 234–239. IEEE,2012.']
1737	777	[305]	[12] modeled the topic as a latent variable and proposed an EM-based approach. // In [27], the topic features are learned by latent Dirichlet allocation (LDA) ==[6]==. // Our knowledge memory is also related to the recent literature on neural networks with external memory [2, 43, 36, 13] which is applied to question answering [43, 8, 44] and reading comprehension [18, 19].	b	A Neural Knowledge Language Model	[6]	['[6] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal ofmachine Learning research, 3:993–1022, 2003.']
1738	777	[726, 737]	In [27], the topic features are learned by latent Dirichlet allocation (LDA) [6]. // Our knowledge memory is also related to the recent literature on neural networks with external memory ==[2, 43, 36, 13]== which is applied to question answering [43, 8, 44] and reading comprehension [18, 19]. // In [43], given simple sentences as facts which are stored in the external memory, the question answering task is studied.	b	A Neural Knowledge Language Model	[2, 43, 36, 13]	['[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.', '[43] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. ICLR 2015, 2015.', '[36] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memorynetworks. NIPS 2015, 2015.', '[13] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprintarXiv:1410.5401, 2014.']
1739	777	[655, 757]	In [27], the topic features are learned by latent Dirichlet allocation (LDA) [6]. // Our knowledge memory is also related to the recent literature on neural networks with external memory [2, 43, 36, 13] which is applied to question answering ==[43, 8, 44]== and reading comprehension [18, 19]. // In [43], given simple sentences as facts which are stored in the external memory, the question answering task is studied.	b	A Neural Knowledge Language Model	[43, 8, 44]	['[43] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. ICLR 2015, 2015.', '[8] Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075, 2015.', '[44] Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, and Xiaoming Li. Neural generativequestion answering. arXiv preprint arXiv:1512.01337, 2015.']
1740	777	[609, 758]	In [27], the topic features are learned by latent Dirichlet allocation (LDA) [6]. // Our knowledge memory is also related to the recent literature on neural networks with external memory [2, 43, 36, 13] which is applied to question answering [43, 8, 44] and reading comprehension ==[18, 19]==. // In [43], given simple sentences as facts which are stored in the external memory, the question answering task is studied.	b	A Neural Knowledge Language Model	[18, 19]	['[18] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1684–1692, 2015.', '[19] Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks principle: Reading children’s books with explicit memory representations. arXiv preprint arXiv:1511.02301, 2015.']
